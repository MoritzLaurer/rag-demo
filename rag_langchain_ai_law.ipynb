{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MoritzLaurer/rag-demo/blob/master/rag_langchain_ai_law.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating a RAG pipeline with LangChain and Hugging Face Endpoints or OpenAI"
      ],
      "metadata": {
        "id": "53l_qbyq4LPe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook provides a quick demo for creating and evaluating a Retrieval Augmented Generation (RAG) pipeline with LangChain and Hugging Face Endpoints or OpenAI.\n",
        "\n",
        "The demo has the following main steps:\n",
        "1. Create an example vector database: The demo downloads 440 position paper PDFs which stakeholders had submitted to the EU public consultation on the EU White Paper on AI in 2020. These PDFs are processed and ingested in a vector database.\n",
        "2. We then automatically generate test questions about a sample of the texts with an LLM\n",
        "3. Then we create a RAG pipeline and feed the generated test questions into the RAG pipeline as user queries\n",
        "4. RAG evaluation:\n",
        "  - Retriever quality: If we ask a generated question to the RAG pipeline, does the pipeline's retriever retrieve the same original text which was used to generated the question? This provides an indication of retriever (and reranker) quality. Note that this indicator is imperfect, as the retriever could also retrieve other texts that help the RAG pipeline generate good answers beyond only the original text used for generating the test question.\n",
        "  - Answer quality: We also use an LLM to evaluate answer quality more broadly. This is particularly important for RAG systems, as RAG outputs are unstructured text and these are hard to evaluate with standard metrics like ROUGE, BERTScore etc. Standard metrics require a reference \"gold\" answer, which is expensive to create at scale.\n"
      ],
      "metadata": {
        "id": "PsYcqBav4VfY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tocCqFNnQqgo"
      },
      "source": [
        "## Install packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "myUZd_CK4zeL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff0a9398-6844-4100-a43e-dda63d2377cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain~=0.0.352 in /usr/local/lib/python3.10/dist-packages (0.0.354)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain~=0.0.352) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain~=0.0.352) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain~=0.0.352) (3.9.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain~=0.0.352) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain~=0.0.352) (0.6.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain~=0.0.352) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.8 in /usr/local/lib/python3.10/dist-packages (from langchain~=0.0.352) (0.0.10)\n",
            "Requirement already satisfied: langchain-core<0.2,>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from langchain~=0.0.352) (0.1.8)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.77 in /usr/local/lib/python3.10/dist-packages (from langchain~=0.0.352) (0.0.77)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain~=0.0.352) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain~=0.0.352) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain~=0.0.352) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain~=0.0.352) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain~=0.0.352) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain~=0.0.352) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain~=0.0.352) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain~=0.0.352) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain~=0.0.352) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain~=0.0.352) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain~=0.0.352) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain~=0.0.352) (2.4)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.5->langchain~=0.0.352) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.5->langchain~=0.0.352) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain~=0.0.352) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain~=0.0.352) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain~=0.0.352) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain~=0.0.352) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain~=0.0.352) (2023.11.17)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain~=0.0.352) (3.0.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.5->langchain~=0.0.352) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.5->langchain~=0.0.352) (1.2.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain~=0.0.352) (1.0.0)\n",
            "Requirement already satisfied: langchainhub~=0.1.14 in /usr/local/lib/python3.10/dist-packages (0.1.14)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchainhub~=0.1.14) (2.31.0)\n",
            "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in /usr/local/lib/python3.10/dist-packages (from langchainhub~=0.1.14) (2.31.0.20240106)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub~=0.1.14) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub~=0.1.14) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub~=0.1.14) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub~=0.1.14) (2023.11.17)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2->langchainhub~=0.1.14)\n",
            "  Downloading urllib3-2.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
            "Downloading urllib3-2.1.0-py3-none-any.whl (104 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 104.6/104.6 kB 2.2 MB/s eta 0:00:00\n",
            "Installing collected packages: urllib3\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.26.18\n",
            "    Uninstalling urllib3-1.26.18:\n",
            "      Successfully uninstalled urllib3-1.26.18\n",
            "Successfully installed urllib3-2.1.0\n",
            "Requirement already satisfied: openai~=1.6.0 in /usr/local/lib/python3.10/dist-packages (1.6.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai~=1.6.0) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai~=1.6.0) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai~=1.6.0) (0.26.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai~=1.6.0) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai~=1.6.0) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai~=1.6.0) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai~=1.6.0) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai~=1.6.0) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai~=1.6.0) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai~=1.6.0) (2023.11.17)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai~=1.6.0) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai~=1.6.0) (0.14.0)\n",
            "Requirement already satisfied: tiktoken~=0.5.2 in /usr/local/lib/python3.10/dist-packages (0.5.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken~=0.5.2) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken~=0.5.2) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken~=0.5.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken~=0.5.2) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken~=0.5.2) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken~=0.5.2) (2023.11.17)\n",
            "Requirement already satisfied: huggingface_hub~=0.20.1 in /usr/local/lib/python3.10/dist-packages (0.20.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub~=0.20.1) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub~=0.20.1) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub~=0.20.1) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub~=0.20.1) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub~=0.20.1) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub~=0.20.1) (4.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub~=0.20.1) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub~=0.20.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub~=0.20.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub~=0.20.1) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub~=0.20.1) (2023.11.17)\n",
            "Requirement already satisfied: sentence_transformers~=2.2.2 in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers~=2.2.2) (4.35.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers~=2.2.2) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers~=2.2.2) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers~=2.2.2) (0.16.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers~=2.2.2) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers~=2.2.2) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers~=2.2.2) (1.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers~=2.2.2) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence_transformers~=2.2.2) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers~=2.2.2) (0.20.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers~=2.2.2) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers~=2.2.2) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers~=2.2.2) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers~=2.2.2) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers~=2.2.2) (4.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers~=2.2.2) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers~=2.2.2) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers~=2.2.2) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers~=2.2.2) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers~=2.2.2) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers~=2.2.2) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers~=2.2.2) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers~=2.2.2) (0.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers~=2.2.2) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers~=2.2.2) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers~=2.2.2) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence_transformers~=2.2.2) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence_transformers~=2.2.2) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers~=2.2.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers~=2.2.2) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers~=2.2.2) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers~=2.2.2) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence_transformers~=2.2.2) (1.3.0)\n",
            "Requirement already satisfied: qdrant-client~=1.7.0 in /usr/local/lib/python3.10/dist-packages (1.7.0)\n",
            "Requirement already satisfied: grpcio>=1.41.0 in /usr/local/lib/python3.10/dist-packages (from qdrant-client~=1.7.0) (1.60.0)\n",
            "Requirement already satisfied: grpcio-tools>=1.41.0 in /usr/local/lib/python3.10/dist-packages (from qdrant-client~=1.7.0) (1.60.0)\n",
            "Requirement already satisfied: httpx>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from httpx[http2]>=0.14.0->qdrant-client~=1.7.0) (0.26.0)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.10/dist-packages (from qdrant-client~=1.7.0) (1.23.5)\n",
            "Requirement already satisfied: portalocker<3.0.0,>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from qdrant-client~=1.7.0) (2.8.2)\n",
            "Requirement already satisfied: pydantic>=1.10.8 in /usr/local/lib/python3.10/dist-packages (from qdrant-client~=1.7.0) (1.10.13)\n",
            "Collecting urllib3<2.0.0,>=1.26.14 (from qdrant-client~=1.7.0)\n",
            "  Using cached urllib3-1.26.18-py2.py3-none-any.whl.metadata (48 kB)\n",
            "Requirement already satisfied: protobuf<5.0dev,>=4.21.6 in /usr/local/lib/python3.10/dist-packages (from grpcio-tools>=1.41.0->qdrant-client~=1.7.0) (4.25.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from grpcio-tools>=1.41.0->qdrant-client~=1.7.0) (67.7.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.14.0->httpx[http2]>=0.14.0->qdrant-client~=1.7.0) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.14.0->httpx[http2]>=0.14.0->qdrant-client~=1.7.0) (2023.11.17)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.14.0->httpx[http2]>=0.14.0->qdrant-client~=1.7.0) (1.0.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.14.0->httpx[http2]>=0.14.0->qdrant-client~=1.7.0) (3.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.14.0->httpx[http2]>=0.14.0->qdrant-client~=1.7.0) (1.3.0)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.14.0->httpx[http2]>=0.14.0->qdrant-client~=1.7.0) (0.14.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.10/dist-packages (from httpx[http2]>=0.14.0->qdrant-client~=1.7.0) (4.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10.8->qdrant-client~=1.7.0) (4.9.0)\n",
            "Requirement already satisfied: hyperframe<7,>=6.0 in /usr/local/lib/python3.10/dist-packages (from h2<5,>=3->httpx[http2]>=0.14.0->qdrant-client~=1.7.0) (6.0.1)\n",
            "Requirement already satisfied: hpack<5,>=4.0 in /usr/local/lib/python3.10/dist-packages (from h2<5,>=3->httpx[http2]>=0.14.0->qdrant-client~=1.7.0) (4.0.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.14.0->httpx[http2]>=0.14.0->qdrant-client~=1.7.0) (1.2.0)\n",
            "Using cached urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
            "Installing collected packages: urllib3\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.1.0\n",
            "    Uninstalling urllib3-2.1.0:\n",
            "      Successfully uninstalled urllib3-2.1.0\n",
            "Successfully installed urllib3-1.26.18\n",
            "Requirement already satisfied: PyMuPDF~=1.23.7 in /usr/local/lib/python3.10/dist-packages (1.23.8)\n",
            "Requirement already satisfied: PyMuPDFb==1.23.7 in /usr/local/lib/python3.10/dist-packages (from PyMuPDF~=1.23.7) (1.23.7)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
            "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "qdrant-client 1.7.0 requires urllib3<2.0.0,>=1.26.14, but you have urllib3 2.1.0 which is incompatible.\n",
            "tensorboard 2.15.1 requires protobuf<4.24,>=3.19.6, but you have protobuf 4.25.1 which is incompatible.\n",
            "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
            "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
            "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
            "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
            "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
            "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorboard 2.15.1 requires protobuf<4.24,>=3.19.6, but you have protobuf 4.25.1 which is incompatible.\n",
            "types-requests 2.31.0.20240106 requires urllib3>=2, but you have urllib3 1.26.18 which is incompatible.\n",
            "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
            "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "pip install --upgrade pip -q\n",
        "pip install langchain~=0.0.352\n",
        "pip install langchainhub~=0.1.14\n",
        "pip install openai~=1.6.0\n",
        "pip install tiktoken~=0.5.2\n",
        "pip install transformers>=4.35.2\n",
        "pip install huggingface_hub~=0.20.1\n",
        "pip install sentence_transformers~=2.2.2\n",
        "pip install qdrant-client~=1.7.0\n",
        "pip install PyMuPDF~=1.23.7\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "\n",
        "# for using hugging face models\n",
        "login(token=userdata.get('HF_TOKEN'))\n",
        "\n",
        "# for using OAI models\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_KEY')"
      ],
      "metadata": {
        "id": "z-DBbhcmqdal",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da6859be-15ba-4397-8f91-80dabe0f979e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FE9Aj1bcQnMN"
      },
      "source": [
        "## Prepare example data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Download PDF data"
      ],
      "metadata": {
        "id": "GKSJzyg9tinB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## download PDF data\n",
        "import os\n",
        "import zipfile\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "# URL of the zip file in your GitHub repo (make sure it's the raw file URL)\n",
        "zip_url = 'https://github.com/MoritzLaurer/rag-demo/blob/master/data/position-papers-pdfs.zip?raw=true'\n",
        "\n",
        "# Download the zip file\n",
        "print(\"Downloading zip file...\")\n",
        "response = requests.get(zip_url)\n",
        "zip_content = BytesIO(response.content)\n",
        "\n",
        "# Define the extraction path\n",
        "extract_path = '/content/data'\n",
        "\n",
        "# Create directory if it doesn't exist\n",
        "if not os.path.exists(extract_path):\n",
        "    os.makedirs(extract_path)\n",
        "\n",
        "# Extract the zip file\n",
        "print(\"Extracting zip file...\")\n",
        "with zipfile.ZipFile(zip_content, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"Extraction completed.\")\n",
        "\n",
        "file_paths = [f for f in os.listdir(extract_path) if os.path.isfile(os.path.join(extract_path, f))]\n",
        "print(f\"{len(file_paths)} PDF files downloaded.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oa-86eodrEez",
        "outputId": "e2333113-52c6-4027-8f23-01adab614daf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading zip file...\n",
            "Extracting zip file...\n",
            "Extraction completed.\n",
            "440 PDF files downloaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-mL7xkUf3F6"
      },
      "source": [
        "### Process data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# parse the raw PDFs into machine-readable docs\n",
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "directory = \"./data\"\n",
        "\n",
        "docs = []\n",
        "for pdf_path in tqdm(os.listdir(directory)):\n",
        "  try:\n",
        "    docs.append(PyMuPDFLoader(os.path.join(directory, pdf_path)).load())\n",
        "  except Exception as e:\n",
        "    print(\"Exception: \", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "0dc2f589ddc04d0e8a1cc5790ab7850f",
            "3000184fccbb483eaa5d5dbc5927b8c6",
            "d0e842db28af4e18ba7ceae09339ad45",
            "e323edea49e44fcc901f6251fd5e9068",
            "414437d86bb54a89947e6a05d74138a2",
            "401c430e78ee4bdda4039165ca0c6006",
            "a44b494649c9495fb9ec16d0bb14c067",
            "346c3323948d454e8054ebe948fc4603",
            "c372309272214fd59a9476e2685c17bb",
            "3dc78c4f999745b08ab6a88763317c2f",
            "9fa11cfe31af4d1c897b3493e4d275cd"
          ]
        },
        "id": "_bjaESlGec45",
        "outputId": "0f92ea0f-8a2d-455e-8575-37e0be05189b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/440 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0dc2f589ddc04d0e8a1cc5790ab7850f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exception:  cannot open broken document\n",
            "Exception:  cannot open broken document\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split the docs into shorter chunks that fit into LLM context windows\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, SentenceTransformersTokenTextSplitter\n",
        "\n",
        "# text splitter based on the tokenizer of a model of your choosing\n",
        "#to make texts fit exactly a transformer's context window size\n",
        "text_splitter = SentenceTransformersTokenTextSplitter(\n",
        "    chunk_overlap=48, tokens_per_chunk=256, model_name='sentence-transformers/all-mpnet-base-v2'\n",
        ")\n",
        "# alternative faster text splitter\n",
        "#text_splitter = RecursiveCharacterTextSplitter(\n",
        "#    chunk_size=1000, chunk_overlap=100, add_start_index=True, separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
        "#)\n",
        "\n",
        "docs_processed = [text_splitter.split_documents(doc) for doc in docs]\n",
        "docs_processed = [item for sublist in docs_processed for item in sublist]\n",
        "\n",
        "print(len(docs_processed))\n",
        "\n",
        "docs_processed[:1]"
      ],
      "metadata": {
        "id": "T1wNg46flg68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e253cd4d-68e9-402a-d885-6f4bd4743c1c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14726\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='francisco javier diez dept. artificial intelligence computer science school - uned juan del rosal, 16. 28040 madrid. spain office phone : + 34 - 913. 987. 161 mobile phone : + 34 - 646. 794. 342 email : fjdiez @ dia. uned. es www. ia. uned. es / ~ fjdiez madrid, june 8th 2020 additional comments about the white paper on artificial intelligence - a european approach ai should not be restricted to data - driven applications. human expertise has played a significant role in ai since its inception, and will continue do so in the future. there - fore, the definition at the top of page 2 should say : “ simply put, ai is a collection of technologies that combine knowledge, human expertise, data, algorithms and com - puting power. ” the document does not mention how ai will impact the doctor - patient relation, for better or worse. there is a risk of dehumanizing medicine, but also the potential of liberating health staff from tedious tasks and reducing human errors, and to assist people with disabilities and elderly people living alone — see the paper “ how ai is humanizing health care ” by mit technology review insights. the document', metadata={'source': './data/F530473-comments-about-White-Paper-AI-Europe.pdf', 'file_path': './data/F530473-comments-about-White-Paper-AI-Europe.pdf', 'page': 0, 'total_pages': 1, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': ''})]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Sample data to reduce embedding and generation costs"
      ],
      "metadata": {
        "id": "XBcPIqYjMWu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.seed(42)\n",
        "\n",
        "# sample corpus for embedding\n",
        "n_sample_texts = 100\n",
        "index_random = random.sample(range(len(docs_processed)), 100)\n",
        "docs_samp = [docs_processed[i] for i in index_random]\n",
        "\n",
        "# sample a smaller set of texts to generate questions from\n",
        "n_questions = 5\n",
        "docs_for_q_generation = docs_samp[:n_questions]\n",
        "docs_for_q_generation = [doc.page_content for doc in docs_for_q_generation]\n"
      ],
      "metadata": {
        "id": "hUIhFQoc6GtE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Automatic question generation for evaluation\n",
        "\n",
        "This section generates questions which users could ask about a specific text in the database. This allows us to assess:  \n",
        "- If we ask a generated question to the RAG pipeline, does the pipeline's retriever retrieve the same text which was used to generated the question? This provides an indication of retriever (and reranker) quality.\n",
        "- Beyond the original text used for generating the question, the retriever might retrieve other texts that are also help the RAG pipeline generate good answers. We therefore also use an LLM to evaluate answer quality more broadly."
      ],
      "metadata": {
        "id": "tKOmYhRe7vxm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create an huggingface inference endpoint to run any LLM\n",
        "# intro: https://www.philschmid.de/inference-endpoints-iac\n",
        "# docs: https://huggingface.co/docs/huggingface_hub/v0.20.1/en/package_reference/hf_api#huggingface_hub.HfApi.create_inference_endpoint\n",
        "from huggingface_hub import create_inference_endpoint\n",
        "from huggingface_hub import HfApi\n",
        "api = HfApi()\n",
        "\n",
        "create_new_endpoint = False\n",
        "model_for_endpoint = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"  #\"mistralai/Mixtral-8x7B-Instruct-v0.1\",  #\"HuggingFaceH4/zephyr-7b-beta\",\n",
        "endpoint_name = \"mixtral-8x7b-instruct-v0-1-test\"\n",
        "\n",
        "if create_new_endpoint:\n",
        "  # define TGI as custom image\n",
        "  custom_image = {\n",
        "      \"health_route\": \"/health\",  # Health route for TGI\n",
        "      \"env\": {\n",
        "          \"MAX_BATCH_PREFILL_TOKENS\": \"2048\", # can be adjusted to your needs\n",
        "          \"MAX_INPUT_LENGTH\": \"1024\", # can be adjusted to your needs\n",
        "          \"MAX_TOTAL_TOKENS\": \"1512\", # can be adjusted to your needs\n",
        "          \"MODEL_ID\": \"/repository\",  # IE will save the model in /repository\n",
        "      },\n",
        "      \"url\": \"ghcr.io/huggingface/text-generation-inference:1.3.3\",\n",
        "  }\n",
        "\n",
        "  # Create Inference Endpoint to run Zephyr 7B\n",
        "  print(\"Creating Inference Endpoint\")\n",
        "  hf_endpoint = create_inference_endpoint(\n",
        "      name=endpoint_name,\n",
        "      repository=model_for_endpoint,\n",
        "      framework=\"pytorch\",\n",
        "      task=\"text-generation\",\n",
        "      vendor=\"aws\",\n",
        "      region=\"us-east-1\",\n",
        "      type=\"protected\",\n",
        "      instance_size=\"2xlarge\",  #\"medium\",\n",
        "      instance_type=\"p4de\",  #\"g5.2xlarge\",  # A10G GPU. Pricing: https://huggingface.co/pricing#endpoints\n",
        "      accelerator=\"gpu\",\n",
        "      namespace=\"HF-test-lab\",  # your user or organisation name on the HF hub\n",
        "      custom_image=custom_image,\n",
        "  )\n",
        "  #curl https://api.endpoints.huggingface.cloud/v2/endpoint/MoritzLaurer \\ -X POST \\ -d '{\"compute\":{\"accelerator\":\"gpu\",\"instanceSize\":\"2xlarge\",\"instanceType\":\"p4de\",\"scaling\":{\"maxReplica\":1,\"minReplica\":0}},\"model\":{\"framework\":\"pytorch\",\"image\":{\"custom\":{\"health_route\":\"/health\",\"env\":{\"MAX_BATCH_PREFILL_TOKENS\":\"2048\",\"MAX_INPUT_LENGTH\":\"1024\",\"MAX_TOTAL_TOKENS\":\"1512\",\"QUANTIZE\":\"bitsandbytes\",\"MODEL_ID\":\"/repository\"},\"url\":\"ghcr.io/huggingface/text-generation-inference:1.3.4\"}},\"repository\":\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\"task\":\"text-generation\"},\"name\":\"aws-mixtral-8x7b-instruct-v0-1\",\"provider\":{\"region\":\"us-east-1\",\"vendor\":\"aws\"},\"type\":\"protected\"}' \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer XXXXX\"\n",
        "  print(\"Waiting for endpoint to be deployed\")\n",
        "  hf_endpoint.wait()\n",
        "\n",
        "  print(\"Endpoint ready\")\n",
        "\n",
        "\n",
        "else:\n",
        "  print(\"Waiting for endpoint to be resumed\")\n",
        "  hf_endpoint = api.get_inference_endpoint(name=endpoint_name, namespace=\"HF-test-lab\")\n",
        "  hf_endpoint.resume()  # resume only works if endpoint was explicitly paused. If endpoint scaled to 0, need to send a request to wake it up\n",
        "  hf_endpoint.wait()\n",
        "  print(\"Endpoint ready\")\n",
        "\n",
        "  # to manage an existing endpoint, use:\n",
        "  #hf_endpoint.resume()\n",
        "  #hf_endpoint.pause()\n",
        "  #hf_endpoint.delete()\n",
        "  # Endpoints should automatically scale to 0 after 15 minutes to avoid unnecessary costs\n",
        "  # But you can delete it manually just to be save"
      ],
      "metadata": {
        "id": "5Ciu9hbwyFNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.llms import HuggingFaceEndpoint\n",
        "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
        "\n",
        "provider_for_question_generation = \"HF\"\n",
        "\n",
        "\n",
        "if provider_for_question_generation == \"HF\":\n",
        "  chat_model = HuggingFaceEndpoint(\n",
        "    endpoint_url=hf_endpoint.url,  #\"https://ytjpei7t003tedav.us-east-1.aws.endpoints.huggingface.cloud\",\n",
        "    task=\"text-generation\",\n",
        "    huggingfacehub_api_token=userdata.get('HF_TOKEN'),\n",
        "    model_kwargs={}\n",
        "  )\n",
        "\n",
        "elif provider_for_question_generation == \"OAI\":\n",
        "  # https://platform.openai.com/docs/api-reference/chat\n",
        "  chat_model = ChatOpenAI(\n",
        "      model=\"gpt-3.5-turbo-1106\",  #\"gpt-3.5-turbo-1106\",  # \"gpt-4-1106-preview\"\n",
        "      temperature=0.2, max_tokens=1024,\n",
        "      n=1, top_p=0.95,\n",
        "      frequency_penalty=0.0,  # Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.\n",
        "      presence_penalty=0.0,  # Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.\n",
        "      #response_format={ \"type\": \"json_object\" },\n",
        "      seed=42,\n",
        "  )\n",
        "\n"
      ],
      "metadata": {
        "id": "mO2OTlAX7xzQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "import numpy as np\n",
        "\n",
        "# we generate both a question and answer\n",
        "# having an answer which according to the LLM follows from the question makes it easier to judge the quality of the question\n",
        "instruction_qa_gen = \"\"\"\\\n",
        "Your task is to write a factoid question and and answer given a context.\n",
        "\n",
        "Your factoid question should be answerable with a specific, concise piece of factual information from the context.\n",
        "Your factoid question should be formulated in the same style as questions users could ask in a search engine. \\\n",
        "This means that your factoid question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
        "\n",
        "After writing the factoid question, also write the corresponding answer that is clearly grounded in the context.\n",
        "\n",
        "Always answer in this JSON response format: {{\"question\": \"...\", \"answer\": \"...\"}}\n",
        "\n",
        "context: {context}\\n\n",
        "JSON response: \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "prompt_question_gen = ChatPromptTemplate.from_template(instruction_qa_gen)\n",
        "\n",
        "chain_question_gen = prompt_question_gen | chat_model\n",
        "\n",
        "question_answer_dic_lst = []\n",
        "for context in docs_for_q_generation:\n",
        "  print(\"Context:\\n\", context)\n",
        "\n",
        "  output_question_dic = chain_question_gen.invoke({\"context\": context})\n",
        "\n",
        "  if provider_for_question_generation == \"OAI\":\n",
        "    output_question_dic = output_question_dic.content\n",
        "\n",
        "  try:\n",
        "    output_question_judge_dic = ast.literal_eval(output_question_dic)\n",
        "  except:\n",
        "    output_question_judge_dic = {\"question\": np.nan, \"answer\": np.nan}\n",
        "\n",
        "  question_answer_dic_lst.append(output_question_judge_dic)\n",
        "  print(\"\\nGenerated question with answer:\\n\", output_question_judge_dic, \"\\n\")\n",
        "\n",
        "question_lst = [dic[\"question\"] for dic in question_answer_dic_lst]\n",
        "answer_lst = [dic[\"answer\"] for dic in question_answer_dic_lst]\n"
      ],
      "metadata": {
        "id": "Fsg4SXAT7xzR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5d7b9bd-3ba1-462f-ee58-6e178e919745"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context:\n",
            " regulatory framework for artificial intelligence in the european union non - paper of the czech republic 26 november 2019 the czech republic recognizes the importance of technologies commonly known as artificial intelligence and their increasing impact on our everyday lives. the european commission, with the support of the european parliament, has expressed its intention to introduce a new ai regulatory framework. so far, the eu has identified several issues related to ai, namely questions of safety and liability, privacy protection, data, copyright ( iprs ), consumer protection as well as protection of fundamental human rights. it puts a strong emphasis on setting up an ethical background for the use of ai. this approach is referred to as human - centric ai and should be based on implementing european values into the research and development of ai systems from the very beginning to maintain a high level of protection of human rights and democracy. thirty years after the fall of the iron curtain, europe should be in the frontline of preventing a new one based on the misuse of ai. the czech republic believes that the important opportunities for ai deployment based on european values are a focus on ai for social good and benefit to society and ai for security and safety in society. while the former deals with the use of ai for environmental applications, enabling social entrepreneurship, job creation or innovative ways\n",
            "\n",
            "Generated question with answer:\n",
            " {'question': \"What is the Czech Republic's view on the focus of AI deployment?\", 'answer': 'The Czech Republic believes that the important opportunities for AI deployment should focus on AI for social good and benefit to society, and AI for security and safety in society.'} \n",
            "\n",
            "Context:\n",
            " 38 classifying an ai ’ s application context 3. 2. 2 dependence on the decision ( y - axis ) the y - axis shows the dependence of the potentially affected parties on the algorithmic decision, thus addressing the options to avoid the potential harm indicated on the x - axis. the better the chances are of avoiding exposure to the potential negative consequences of a decision or the damage caused by it, the further left on the y - axis the adm system lands. the three main factors that play a role in assessing dependence on the decision are control, switchability and redress. • decisions and actions of an ai system additionally filtered through meaningful human interaction ( e. g. the purchase of recommended items in an online shop ) imply a lower demand for regulation than machines acting without human intermediaries ( e. g. the emergency shutdown of a nuclear power station ). this is expressed as control. • the ability to change the ai system for another ( e. g. by switching the operator ) or avoid being exposed to an algorithmic decision altogether is called switchability. a one - sided relationship of dependence between producers or operators and users and monopolistic ( including governmental ) structures lead to dependence on one or a few systems. in the worst case, the user\n",
            "\n",
            "Generated question with answer:\n",
            " {'question': \"How is control over an AI system's decisions and actions assessed in the context?\", 'answer': \"Control is assessed by considering the level of human interaction in the AI system's decisions and actions. More human interaction implies lower demand for regulation and thus, more control.\"} \n",
            "\n",
            "Context:\n",
            " ( in terms of research, startups, infrastructures ) and a credible opportunity to become a global leader. relevant and focused investments are necessary, as well as building on the excellence and best practices already in place ( and under development ), and developing tools and solutions able to maximise ai applicability while being fully compliant with regulatory, policy, and ethical frameworks. in addition, it is necessary to create an eu - wide innovation environment, avoiding replications and fragmentation between member states and related communities. the business, economic and societal context to which ai is applied needs to be considered as decisions are not made in a vacuum but within the socio - economic context of a society of humans which, in the european case, requires the ai application to be trustworthy. an interdisciplinary approach has to be followed to cover all possible domains benefiting from the use of ai. the ai whitepaper could pay more attention to human behaviour in relation to ai. for instance, the recent debate on the deployment of corona tracking apps has shown again how important it is to jointly assess the technological and societal aspects of data - driven innovations. therefore, the scope should be enlarged accordingly. 1 com ( 2020 ) 65 final “ on artificial intelligence - a european approach to excellence and trust ” 2 zill\n",
            "\n",
            "Generated question with answer:\n",
            " {'question': 'What approach should be followed in the application of AI according to the EU whitepaper?', 'answer': 'The EU whitepaper suggests following an interdisciplinary approach in the application of AI, covering all possible domains and considering the business, economic, and societal context. It also emphasizes the importance of jointly assessing the technological and societal aspects of data-driven innovations.'} \n",
            "\n",
            "Context:\n",
            " other databases have been made public so that the academic community could catch up with industry research. the figure below shows the evolution of the datasets used in facial recognition, with the effect of increasing scale and a generalization of images showing faces with less and less supervision and constraint : ages, poses and expressions vary, then external elements obscure certain parts of the faces, either by cropping them or putting makeup on them. evolution of facial recognition databases since 199456 56 this diagram is taken from mei wang and weihong deng ( 2018 ), op. cit., p. 13.\n",
            "\n",
            "Generated question with answer:\n",
            " {'question': 'When did the evolution of facial recognition databases start?', 'answer': 'The evolution of facial recognition databases started in 1994.'} \n",
            "\n",
            "Context:\n",
            " is mainly confronted with people of white skin color during training, it may not recognize people of darker skin color, or recognizes them less often, as people. an empirical study with precisely these results shows that this concern is not unfounded. 159 150 see also, on bias potentially introduced by reliance on postal codes, kroll et al. ‘ accountable algorithms ’, 165 u. pa. l. rev. ( 2016 ), 633 ( 681, 685 ) ; kamarinou, millard and singh, ‘ machine learning with personal data ’, queen mary school of law legal studies research paper 247 / 2016, 16. 151 see thusing, in : munchener kommentar, agg, 8th edition 2018, § 3 para. 15. 152 see, for a discussion, ellis and watson, op. cit. supra note 87, 171 - 174, 381 et seqq. 153 european commission, op. cit. supra note 1, p. 19. 154 see the references supra note 28 and the cases supra note 24. 155 see, e. g., zemel et al., ‘ learning fair representations ’, proceedings of the 30th international conference on machine learning ( 2013 )\n",
            "\n",
            "Generated question with answer:\n",
            " {'question': 'What study shows that AI may not recognize people of darker skin color as often as people with white skin color?', 'answer': 'An empirical study, as mentioned in the context, shows that this concern is not unfounded.'} \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# good alternativ critique prompts: https://github.com/A-Roucher/RAG_cookbook/blob/master/retrieval_augmented_generation.ipynb\n",
        "\n",
        "instruction_question_judge = \"\"\"\\\n",
        "Your task is to score the quality of a question that has been written based on a specific context.\n",
        "\n",
        "Your scoring criteria for assessing the question are:\n",
        "- ambiguity: Can the question be clearly, unambiguously answered with the given context?\n",
        "- form and verbosity: Is the question formulated like a question that a user could ask to a search engine? The question should not be accompanied by an answer or other text that users would not ask in a search query\n",
        "\n",
        "Your quality score should be in the range of 0 to 100.\\\n",
        "100 means a very good question, 0 means a very bad question, 50 means a mediocre question.\n",
        "\n",
        "First briefly reason step-by-step to assess the extent to which the question fulfills these criteria. Your reasoning should be short.\n",
        "Then return the quality score.\n",
        "\n",
        "Always answer in this JSON evaluation format: {{\"reason\": \"...\", \"score\": \"...\"}}\n",
        "\n",
        "context: \"{context}\"\\n\n",
        "question: \"{question}\"\\n\n",
        "JSON evaluation: \"\"\"\n",
        "\n",
        "\n",
        "prompt_question_judge = ChatPromptTemplate.from_template(instruction_question_judge)\n",
        "\n",
        "chain = prompt_question_judge | chat_model\n",
        "\n",
        "question_judgement_lst = []\n",
        "for qa_dic, context in zip(question_answer_dic_lst, docs_for_q_generation):\n",
        "  print(\"Question:\", qa_dic[\"question\"])\n",
        "  print(\"Context:\", context)\n",
        "\n",
        "  output_question_judgement = chain.invoke({\"question\": qa_dic[\"question\"].strip().replace(\"\\n\", \" \"), \"context\": context.strip()})\n",
        "\n",
        "  if provider_for_question_generation == \"OAI\":\n",
        "    output_question_judgement = output_question_judgement.content\n",
        "\n",
        "  question_judgement_lst.append(output_question_judgement)\n",
        "  print(\"\\nJudgement:\\n\", output_question_judgement, \"\\n\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WpAUC2A8fSv6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be142941-fea5-46db-baea-d2d61853bb05"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is the Czech Republic's view on the focus of AI deployment?\n",
            "Context: regulatory framework for artificial intelligence in the european union non - paper of the czech republic 26 november 2019 the czech republic recognizes the importance of technologies commonly known as artificial intelligence and their increasing impact on our everyday lives. the european commission, with the support of the european parliament, has expressed its intention to introduce a new ai regulatory framework. so far, the eu has identified several issues related to ai, namely questions of safety and liability, privacy protection, data, copyright ( iprs ), consumer protection as well as protection of fundamental human rights. it puts a strong emphasis on setting up an ethical background for the use of ai. this approach is referred to as human - centric ai and should be based on implementing european values into the research and development of ai systems from the very beginning to maintain a high level of protection of human rights and democracy. thirty years after the fall of the iron curtain, europe should be in the frontline of preventing a new one based on the misuse of ai. the czech republic believes that the important opportunities for ai deployment based on european values are a focus on ai for social good and benefit to society and ai for security and safety in society. while the former deals with the use of ai for environmental applications, enabling social entrepreneurship, job creation or innovative ways\n",
            "\n",
            "Judgement:\n",
            " \n",
            "{\"reason\": \"The question is unambiguous and clearly refers to the view of the Czech Republic on the focus of AI deployment. The form of the question is also correct and concise, similar to a search engine query. The question does not contain any answer or additional text that a user would not include in a search query.\", \"score\": \"95\"} \n",
            "\n",
            "Question: How is control over an AI system's decisions and actions assessed in the context?\n",
            "Context: 38 classifying an ai ’ s application context 3. 2. 2 dependence on the decision ( y - axis ) the y - axis shows the dependence of the potentially affected parties on the algorithmic decision, thus addressing the options to avoid the potential harm indicated on the x - axis. the better the chances are of avoiding exposure to the potential negative consequences of a decision or the damage caused by it, the further left on the y - axis the adm system lands. the three main factors that play a role in assessing dependence on the decision are control, switchability and redress. • decisions and actions of an ai system additionally filtered through meaningful human interaction ( e. g. the purchase of recommended items in an online shop ) imply a lower demand for regulation than machines acting without human intermediaries ( e. g. the emergency shutdown of a nuclear power station ). this is expressed as control. • the ability to change the ai system for another ( e. g. by switching the operator ) or avoid being exposed to an algorithmic decision altogether is called switchability. a one - sided relationship of dependence between producers or operators and users and monopolistic ( including governmental ) structures lead to dependence on one or a few systems. in the worst case, the user\n",
            "\n",
            "Judgement:\n",
            " \n",
            "{\"reason\": \"The question is unambiguous and directly relates to the context. It is formulated as a question that a user could ask to a search engine. The question does not contain any answer or other text that users would not ask in a search query.\", \"score\": \"95\"} \n",
            "\n",
            "Question: What approach should be followed in the application of AI according to the EU whitepaper?\n",
            "Context: ( in terms of research, startups, infrastructures ) and a credible opportunity to become a global leader. relevant and focused investments are necessary, as well as building on the excellence and best practices already in place ( and under development ), and developing tools and solutions able to maximise ai applicability while being fully compliant with regulatory, policy, and ethical frameworks. in addition, it is necessary to create an eu - wide innovation environment, avoiding replications and fragmentation between member states and related communities. the business, economic and societal context to which ai is applied needs to be considered as decisions are not made in a vacuum but within the socio - economic context of a society of humans which, in the european case, requires the ai application to be trustworthy. an interdisciplinary approach has to be followed to cover all possible domains benefiting from the use of ai. the ai whitepaper could pay more attention to human behaviour in relation to ai. for instance, the recent debate on the deployment of corona tracking apps has shown again how important it is to jointly assess the technological and societal aspects of data - driven innovations. therefore, the scope should be enlarged accordingly. 1 com ( 2020 ) 65 final “ on artificial intelligence - a european approach to excellence and trust ” 2 zill\n",
            "\n",
            "Judgement:\n",
            " \n",
            "{\"reason\": \"The question is unambiguous and clearly relates to the context provided. It is formulated as a question that a user could ask to a search engine. There is no accompanying answer or other text that users would not ask in a search query. The question is relevant to the context and asks for the approach to AI according to the EU whitepaper.\", \"score\": \"95\"} \n",
            "\n",
            "Question: When did the evolution of facial recognition databases start?\n",
            "Context: other databases have been made public so that the academic community could catch up with industry research. the figure below shows the evolution of the datasets used in facial recognition, with the effect of increasing scale and a generalization of images showing faces with less and less supervision and constraint : ages, poses and expressions vary, then external elements obscure certain parts of the faces, either by cropping them or putting makeup on them. evolution of facial recognition databases since 199456 56 this diagram is taken from mei wang and weihong deng ( 2018 ), op. cit., p. 13.\n",
            "\n",
            "Judgement:\n",
            " \n",
            "{\"reason\": \"The question is unambiguous and can be clearly answered with the given context. The question is formulated like a question that a user could ask to a search engine. There is no accompanying answer or other text that users would not ask in a search query.\", \"score\": \"100\"} \n",
            "\n",
            "Question: What study shows that AI may not recognize people of darker skin color as often as people with white skin color?\n",
            "Context: is mainly confronted with people of white skin color during training, it may not recognize people of darker skin color, or recognizes them less often, as people. an empirical study with precisely these results shows that this concern is not unfounded. 159 150 see also, on bias potentially introduced by reliance on postal codes, kroll et al. ‘ accountable algorithms ’, 165 u. pa. l. rev. ( 2016 ), 633 ( 681, 685 ) ; kamarinou, millard and singh, ‘ machine learning with personal data ’, queen mary school of law legal studies research paper 247 / 2016, 16. 151 see thusing, in : munchener kommentar, agg, 8th edition 2018, § 3 para. 15. 152 see, for a discussion, ellis and watson, op. cit. supra note 87, 171 - 174, 381 et seqq. 153 european commission, op. cit. supra note 1, p. 19. 154 see the references supra note 28 and the cases supra note 24. 155 see, e. g., zemel et al., ‘ learning fair representations ’, proceedings of the 30th international conference on machine learning ( 2013 )\n",
            "\n",
            "Judgement:\n",
            " \n",
            "{\"reason\": \"The question is unambiguous and formulated like a search query. It asks for a specific study that shows a certain result, which is a clear and concise request for information.\", \"score\": \"95\"} \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# parsing the JSON output can lead to errors\n",
        "# with open-source models, which don't enforce JSON as well as OAI\n",
        "import ast\n",
        "import numpy as np\n",
        "\n",
        "#output_question_judge_dic = []\n",
        "output_question_score = []\n",
        "output_question_reason = []\n",
        "for output in question_judgement_lst:\n",
        "  try:\n",
        "    output_question_judge_dic = ast.literal_eval(output)\n",
        "    output_question_score.append(int(output_question_judge_dic[\"score\"]))\n",
        "    output_question_reason.append(output_question_judge_dic[\"reason\"])\n",
        "\n",
        "  except:\n",
        "    print(\"This JSON output could not be parsed: \", output)\n",
        "    #output_question_judge_dic.append(np.nan)\n",
        "    output_question_score.append(np.nan)\n",
        "    output_question_reason.append(np.nan)\n",
        "\n"
      ],
      "metadata": {
        "id": "iOg2Q4lQLIcB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_questions = pd.DataFrame({\n",
        "  \"question\": question_lst,\n",
        "  \"answer\": answer_lst,\n",
        "  \"score_question\": output_question_score,\n",
        "  \"score_reason\": output_question_reason,\n",
        "  \"context\": docs_for_q_generation,\n",
        "})\n",
        "\n",
        "df_questions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "ZymdLlpaPSYy",
        "outputId": "56a194ce-ce65-4e6d-f134-77db1bd79ec4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            question  \\\n",
              "0  What is the Czech Republic's view on the focus...   \n",
              "1  How is control over an AI system's decisions a...   \n",
              "2  What approach should be followed in the applic...   \n",
              "3  When did the evolution of facial recognition d...   \n",
              "4  What study shows that AI may not recognize peo...   \n",
              "\n",
              "                                              answer  score_question  \\\n",
              "0  The Czech Republic believes that the important...              95   \n",
              "1  Control is assessed by considering the level o...              95   \n",
              "2  The EU whitepaper suggests following an interd...              95   \n",
              "3  The evolution of facial recognition databases ...             100   \n",
              "4  An empirical study, as mentioned in the contex...              95   \n",
              "\n",
              "                                        score_reason  \\\n",
              "0  The question is unambiguous and clearly refers...   \n",
              "1  The question is unambiguous and directly relat...   \n",
              "2  The question is unambiguous and clearly relate...   \n",
              "3  The question is unambiguous and can be clearly...   \n",
              "4  The question is unambiguous and formulated lik...   \n",
              "\n",
              "                                             context  \n",
              "0  regulatory framework for artificial intelligen...  \n",
              "1  38 classifying an ai ’ s application context 3...  \n",
              "2  ( in terms of research, startups, infrastructu...  \n",
              "3  other databases have been made public so that ...  \n",
              "4  is mainly confronted with people of white skin...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e0494614-76c7-4154-aefe-26d771532489\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>score_question</th>\n",
              "      <th>score_reason</th>\n",
              "      <th>context</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is the Czech Republic's view on the focus...</td>\n",
              "      <td>The Czech Republic believes that the important...</td>\n",
              "      <td>95</td>\n",
              "      <td>The question is unambiguous and clearly refers...</td>\n",
              "      <td>regulatory framework for artificial intelligen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How is control over an AI system's decisions a...</td>\n",
              "      <td>Control is assessed by considering the level o...</td>\n",
              "      <td>95</td>\n",
              "      <td>The question is unambiguous and directly relat...</td>\n",
              "      <td>38 classifying an ai ’ s application context 3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What approach should be followed in the applic...</td>\n",
              "      <td>The EU whitepaper suggests following an interd...</td>\n",
              "      <td>95</td>\n",
              "      <td>The question is unambiguous and clearly relate...</td>\n",
              "      <td>( in terms of research, startups, infrastructu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>When did the evolution of facial recognition d...</td>\n",
              "      <td>The evolution of facial recognition databases ...</td>\n",
              "      <td>100</td>\n",
              "      <td>The question is unambiguous and can be clearly...</td>\n",
              "      <td>other databases have been made public so that ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What study shows that AI may not recognize peo...</td>\n",
              "      <td>An empirical study, as mentioned in the contex...</td>\n",
              "      <td>95</td>\n",
              "      <td>The question is unambiguous and formulated lik...</td>\n",
              "      <td>is mainly confronted with people of white skin...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e0494614-76c7-4154-aefe-26d771532489')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e0494614-76c7-4154-aefe-26d771532489 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e0494614-76c7-4154-aefe-26d771532489');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b4e65552-c931-45e5-8e3c-51bd98e75c87\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b4e65552-c931-45e5-8e3c-51bd98e75c87')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b4e65552-c931-45e5-8e3c-51bd98e75c87 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# run critique prompt and save question with context etc.\n",
        "# to csv file that can be loaded downstream"
      ],
      "metadata": {
        "id": "qEJHJASFhh-S"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG pipeline"
      ],
      "metadata": {
        "id": "EY0NxZbt5uMf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retrival\n",
        "\n",
        "Optimization potential: different retrievers, different rerankers, multi-retrievers"
      ],
      "metadata": {
        "id": "a5bjM8nLnyUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# detailed RAG docs: https://python.langchain.com/docs/use_cases/question_answering/\n",
        "# FAISS cookbook: https://python.langchain.com/docs/expression_language/cookbook/retrieval\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.embeddings import OpenAIEmbeddings, HuggingFaceEmbeddings, HuggingFaceInferenceAPIEmbeddings\n",
        "from langchain.schema import StrOutputParser\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma, Qdrant\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import Distance, VectorParams\n"
      ],
      "metadata": {
        "id": "yQucSUUF5seT"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ! issue: langchain vector store wrappers don't seem to allow adjustment to dimensions, only accept OAI default 1.5k\n",
        "# using qdrant directly instead of langchain wrapper\n",
        "\n",
        "provider_retrieval_model = \"HF\"\n",
        "\n",
        "client_path = f\"./vectorstore\"\n",
        "collection_name = f\"collection\"\n",
        "\n",
        "if provider_retrieval_model == \"HF\":\n",
        "  qdrantClient = QdrantClient(path=client_path, prefer_grpc=True)\n",
        "\n",
        "  embeddings = HuggingFaceInferenceAPIEmbeddings(\n",
        "      api_key=userdata.get('HF_TOKEN'), model_name=\"sentence-transformers/all-MiniLM-l6-v2\"\n",
        "  )\n",
        "\n",
        "  dim = 384\n",
        "\n",
        "elif provider_retrieval_model == \"OAI\":\n",
        "\n",
        "  qdrantClient = QdrantClient(path=client_path, prefer_grpc=True)\n",
        "\n",
        "  embeddings = OpenAIEmbeddings(\n",
        "          model=\"text-embedding-ada-002\",\n",
        "          openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
        "  )\n",
        "\n",
        "  dim = 1536\n",
        "\n",
        "\n",
        "qdrantClient.create_collection(\n",
        "    collection_name=collection_name,\n",
        "    vectors_config=VectorParams(size=dim, distance=Distance.COSINE),\n",
        ")\n",
        "\n",
        "vectorstore = Qdrant(\n",
        "    client=qdrantClient,\n",
        "    collection_name=collection_name,\n",
        "    embeddings=embeddings,\n",
        ")\n",
        "\n",
        "vectorstore.add_documents(docs_samp)"
      ],
      "metadata": {
        "id": "iEJozhRA-4mQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab4da5a8-de26-4384-955f-bbe520fdc8e5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['657e22287c7043e8b5a72335ac98d8ec',\n",
              " '249b39850b6a48bc85bd2b0bf16c978f',\n",
              " '205f6124599b4b07871a7a8dad95af03',\n",
              " '9cf4e924a18f4c569643d973fca8b8f9',\n",
              " 'a030a62e2fa7407da855f8f3f3eb56f2',\n",
              " '3376375246c243b9a5860d97f6aa1563',\n",
              " '45fb7ea8f2934f739bd2d9eb12d56c80',\n",
              " 'ed2da93c017942f19ee5d909f6ddd215',\n",
              " 'e91e3ad2c10b4a0689c8713f34bc84b4',\n",
              " '5c5a3c541e7945b98f2930428148601a',\n",
              " '6b7cd0c2fe0f485b8310cc5266e2283d',\n",
              " '399468f318254a6a98f7095fc2f99a08',\n",
              " '928971f4247b418e9bc7d247191dbb81',\n",
              " '6f396122933e4caa89234d94e908a07a',\n",
              " 'daa022fc82dc4da68e5b73db871bf068',\n",
              " '0f4fdfa1a98b4ebdbb1ee939b226635c',\n",
              " '362980c3c58d4aa3bdbd6351a2eeb169',\n",
              " 'cc7c1e9756294caab35dc40555fb7864',\n",
              " '466ef8c24a3049a8a684d63a57d4c134',\n",
              " '06615430f5ae46838776983a30b89c22',\n",
              " '93bfbd39abd04a3b9a3a2174bba076ea',\n",
              " '31a24aea41e24878a41d40faee7f3df5',\n",
              " 'cc2b030a4a384e2b9de99632b66a8f8c',\n",
              " 'bbde5fa814bc44f59b3bad1145ec843e',\n",
              " 'c22519e6bf4046a99fa2a7be059e23a2',\n",
              " 'b991a549f1a1427d95a478bc6212dece',\n",
              " '4783d4c99cdc45c7bb5df9f18df08902',\n",
              " 'dc910c6f2dc243e1af913c90c6dc53e6',\n",
              " '717a27384d7e49d5a7d35fdf2008e101',\n",
              " '510953a8165848638483a2ae6e583d6e',\n",
              " 'f1be0b1aab2f407c9d8298e43844a65d',\n",
              " '220e99cb8cfd470abb4965af6b712f15',\n",
              " '0ebfc53ab68442f98a056ca57eb14138',\n",
              " 'ab551296a38646bc9a5379a0fe728210',\n",
              " '0b5c2ea56a004afd8c3fc095c0f897e6',\n",
              " '7087ae4438a24813b141e4b92fbd2b6b',\n",
              " '819d205a1fe9403cb22fea9f78ad502f',\n",
              " 'c2418d20279349e19c499e405b0aaaab',\n",
              " '7dad4e89874540299db6a77b0ec60d69',\n",
              " '8e2b4dc12cb84972bc637b66e8df16f1',\n",
              " '74a317ca89f841deaddd74787b6b9d5a',\n",
              " 'cb5a7b7b1b344c3e8408a531dd59b51f',\n",
              " 'f7b2924b45844cd29ae373c693926faa',\n",
              " 'c3b338dc44c94d65bf38747106b51045',\n",
              " '38c5c619862d4fcfb36715bd8e15413e',\n",
              " 'd9607e38a4b94337b28d2f97b98650e7',\n",
              " '0ee62297aaa64bf2a44b8b8006321889',\n",
              " '61fb42485e1d4217a8821c29395a96b1',\n",
              " 'e7071e129fd644769ab5ff5e4f985b2a',\n",
              " '42544a3cad3945d9b440109ad405811e',\n",
              " '63aa5f5a2b7645db91c9e508886eca8d',\n",
              " 'af7da21ad33a4416972189f15002749b',\n",
              " 'e34fcfa5355449859df8db6d31faefd2',\n",
              " '90c0d8cf8d054b0a93073bbc4f776081',\n",
              " 'f99d95c062764a6a8de7213dd44cae4c',\n",
              " 'da95922a4945493a800c79098929cb3e',\n",
              " '3e9275685b324d0d97fdbba0e9551a65',\n",
              " '689fe5b34c724a079ff37ba6173a084f',\n",
              " '9ea52d7181174df488a1340a123c745e',\n",
              " '64cb59745da34f83ae92557b18a546c8',\n",
              " 'db247093c45347a5819c4c0873d492b4',\n",
              " '3fd256a12eb24f1c8b0169eafe9126b2',\n",
              " 'cb21bae2a97c46cd896eb992b4a08779',\n",
              " '879e8b6904c446a4b00c9254031a3a71',\n",
              " '6ed792392f32460cb22ed3e29500571b',\n",
              " '9fab8814db3e411ca12a7d804dcebfab',\n",
              " 'b9f33ac62546499eb1dfbb07d5ffa368',\n",
              " '864ed3aba4f746f68f417a473f118053',\n",
              " '5ce0611b333e47cb9cf868c2be77e9b7',\n",
              " '45d18b88c91a4a2b9f9ac2c3af44c436',\n",
              " 'cec4517df6144464aadc130ae0a438fe',\n",
              " '82e411a546314a36b28a4b80361bc1ac',\n",
              " '7f8d65c671b042028ab8a3badb378157',\n",
              " '4a98e744116940b28a91574fbe86210d',\n",
              " '0aed3d1851d74564be8d0b5a19fe0715',\n",
              " '59f9cca5cd0d411fa9e20aae36059510',\n",
              " '56c11ce5dd894a93b35d346f00225345',\n",
              " 'd0de0fd69efa418f87159764cd88c02b',\n",
              " '31620c82fd2a48e188632f297c42ee10',\n",
              " 'b2e84101582c477ca22e19911b0c6a62',\n",
              " 'ba34f069250c4f92aecaa47936c38e9f',\n",
              " '3fc362f5e4334a71ae360201b6afcb2c',\n",
              " 'c5fbcd00d4a94de7b448eab5ede3736e',\n",
              " '130819d7f2594c7bbb83b7ae774b0bff',\n",
              " 'c3afd960d95443249049136021824a1c',\n",
              " '8db21d13ac37471bb491a61c65f79c3b',\n",
              " '4aae9b22e0044dd8b26b4e2413c65f2a',\n",
              " 'b08004abe82643d2a02e02efc643f1bd',\n",
              " '433d644177bd48e1b587e4ec648d0823',\n",
              " 'fc6313734622400796ae65c6f85f95e8',\n",
              " '19a61fb6bfea48dd9508e3dfcb44bce3',\n",
              " '40b100a0784b44d69fdf19a31f907357',\n",
              " '5c6185f489964fdf9a3e3d7e751d2484',\n",
              " 'b1dd310f2358404294c76db3ef6d9a12',\n",
              " 'bafb2eb599ec400b899562bbfc525e7f',\n",
              " '7edf585d6fbc4184ad550b74301692c9',\n",
              " 'f03d02fe15914dc797c1e0976bba9ba3',\n",
              " '40b6556d9295436fa495f8534f0145bd',\n",
              " '7ad19e8b32c0442781ac01cd1a3ea567',\n",
              " '8d599b6959434c69a7c20046c80930e7']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "context_retrieved_lst = []\n",
        "for question in question_lst:\n",
        "  retriever = vectorstore.as_retriever(\n",
        "      search_type=\"similarity\",\n",
        "      search_kwargs={\"k\": 1}\n",
        "  )\n",
        "\n",
        "  context_retrieved = retriever.get_relevant_documents(\n",
        "      question\n",
        "  )\n",
        "\n",
        "  def format_docs(docs):\n",
        "      return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "  context_retrieved = format_docs(context_retrieved)\n",
        "\n",
        "  context_retrieved_lst.append(context_retrieved)\n",
        "  #print(context_retrieved)\n"
      ],
      "metadata": {
        "id": "1qGMOP0gQ8iB"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check if retrieved context for question is same as context used for generating the question\n",
        "# note that this is an imperfect measure, because the retriever might\n",
        "# retrieve other texts that are equally relevant as the text used for generating the question\n",
        "context_for_q_generation = [doc for doc in docs_for_q_generation]\n",
        "correct_context_retrieved = [a == b for a, b in zip(context_for_q_generation, context_retrieved_lst)]\n",
        "\n",
        "retrieval_accuracy = sum(correct_context_retrieved) / len(correct_context_retrieved)\n",
        "print(retrieval_accuracy)\n"
      ],
      "metadata": {
        "id": "Y3n3hP_3G2Pk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74bf566f-7edf-47b4-a109-600d1ecf8ed9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# add reranking step\n",
        "# challenge: reranking with HF models not implemented in langchain\n",
        "# only cohere reranker seems implemented: https://python.langchain.com/docs/integrations/retrievers/cohere-reranker\n",
        "\n",
        "\"\"\"import torch\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-base')\n",
        "model = AutoModelForSequenceClassification.from_pretrained('BAAI/bge-reranker-base')\n",
        "model.eval()\n",
        "\n",
        "context_question_pairs_lst = []\n",
        "for question in question_lst:\n",
        "  context_question_pairs_lst.append([[question, context] for context in context_retrieved_lst])\n",
        "\n",
        "#pairs = [['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']]\n",
        "\n",
        "for context_question_pair in context_question_pairs_lst:\n",
        "  with torch.no_grad():\n",
        "      inputs = tokenizer(context_question_pair, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
        "      scores = model(**inputs, return_dict=True).logits.view(-1, ).float()\n",
        "      print(scores)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "afrC3UwIXsSZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "outputId": "ed245e2a-a47c-4c1b-9a3c-3e1f16c95852"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"import torch\\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\\n\\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-base')\\nmodel = AutoModelForSequenceClassification.from_pretrained('BAAI/bge-reranker-base')\\nmodel.eval()\\n\\ncontext_question_pairs_lst = []\\nfor question in question_lst:\\n  context_question_pairs_lst.append([[question, context] for context in context_retrieved_lst])\\n\\n#pairs = [['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']]\\n\\nfor context_question_pair in context_question_pairs_lst:\\n  with torch.no_grad():\\n      inputs = tokenizer(context_question_pair, padding=True, truncation=True, return_tensors='pt', max_length=512)\\n      scores = model(**inputs, return_dict=True).logits.view(-1, ).float()\\n      print(scores)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer generation\n",
        "\n",
        "Optimization potential: different LLMs, different prompt templates"
      ],
      "metadata": {
        "id": "IrE01KH_n4LO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "prompt_qa_template = \"\"\"\\\n",
        "Your task is to answer a question based on a context.\n",
        "Your answer should be concise and you should only return your answer.\n",
        "\n",
        "context: {context}\n",
        "question: {question}\n",
        "answer: \"\"\"\n",
        "\n",
        "prompt_qa_template = PromptTemplate.from_template(prompt_qa_template)\n"
      ],
      "metadata": {
        "id": "RnBR_tyHRD6B"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFaceEndpoint\n",
        "\n",
        "provider_answer_model = \"HF\"\n",
        "\n",
        "if provider_answer_model == \"HF\":\n",
        "  llm_qa = HuggingFaceEndpoint(\n",
        "    endpoint_url=hf_endpoint.url,  #\"https://nqoa2is3qe7y82ww.us-east-1.aws.endpoints.huggingface.cloud\",\n",
        "    task=\"text-generation\",\n",
        "    huggingfacehub_api_token=userdata.get('HF_TOKEN'),\n",
        "    model_kwargs={}\n",
        "  )\n",
        "elif provider_answer_model == \"OAI\":\n",
        "  llm_qa = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "chain = prompt_qa_template | llm_qa | StrOutputParser()\n",
        "\n",
        "answer_lst = []\n",
        "for question, context in zip(question_lst , context_retrieved_lst):\n",
        "  answer = chain.invoke({\"context\": context, \"question\": question})\n",
        "  answer_lst.append(answer)\n"
      ],
      "metadata": {
        "id": "vpARrdu-lP2S"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Automatic LLM evaluation of generated answer"
      ],
      "metadata": {
        "id": "6bpcF8ZuohC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this scoring prompt can be freely adapted to evaluation criteria\n",
        "# of different use-cases\n",
        "\n",
        "instruction_judge_answer = \"\"\"\\\n",
        "Your task is to score the quality of an answer to a question in a given context.\n",
        "\n",
        "Your scoring criteria for assessing the answer are:\n",
        "- pertinence: Does the answer directly answer the question?\n",
        "- context grounding: Is the answer clearly grounded in the context? To be well grounded, the answer does not need to explicitly reference the context.\n",
        "- conciseness: Is the answer concise without unnecessary verbosity?\n",
        "\n",
        "Your quality score should be in the range of 0 to 100.\\\n",
        "100 means a very good answer, 0 means a very bad answer, 50 means a mediocre answer.\n",
        "\n",
        "First briefly reason step-by-step to assess the extent to which the answer fulfills these criteria. Your reasoning should be short.\n",
        "Then return the quality score.\n",
        "\n",
        "Always answer in this JSON evaluation format: {{\"reason\": \"...\", \"score\": \"...\"}}\n",
        "\n",
        "context: {context}\\n\n",
        "question: \"{question}\"\\n\n",
        "answer: \"{answer}\"\\n\n",
        "JSON evaluation: \"\"\"\n",
        "\n",
        "instruction_judge_answer = ChatPromptTemplate.from_template(instruction_judge_answer)\n",
        "\n",
        "# currently need to use OAI here, because it enforces JSON very well\n",
        "llm_evaluation = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "chain = instruction_judge_answer | llm_evaluation\n",
        "\n",
        "\n",
        "output_quality_lst = []\n",
        "for answer, question, context_retrieved in zip(answer_lst, question_lst, context_retrieved_lst):\n",
        "\n",
        "  output_quality = chain.invoke({\n",
        "      \"context\": context_retrieved.strip(),\n",
        "      \"question\": question.strip().replace(\"\\n\", \" \"),\n",
        "      \"answer\": answer.strip().replace(\"\\n\", \" \")\n",
        "  })\n",
        "\n",
        "  output_quality_lst.append(output_quality.content)\n",
        "\n"
      ],
      "metadata": {
        "id": "NEDmoocl7xzS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50abb2dd-6b43-467b-c838-5e7df55648bf"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:115: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use langchain_openai.ChatOpenAI instead.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# parsing the JSON output can lead to errors\n",
        "# with open-source models, which don't enforce JSON as well as OAI\n",
        "import ast\n",
        "\n",
        "output_quality_dic = [ast.literal_eval(output) for output in output_quality_lst]\n",
        "output_quality_score = [int(dic[\"score\"]) for dic in output_quality_dic]\n",
        "output_quality_reason = [dic[\"reason\"] for dic in output_quality_dic]\n"
      ],
      "metadata": {
        "id": "eIAgcti6JfAv"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results"
      ],
      "metadata": {
        "id": "OcAmsd5YVqq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_results = pd.DataFrame({\n",
        "    \"question\": question_lst,\n",
        "    \"answer\": answer_lst,\n",
        "    \"answer_quality_score\": output_quality_score,\n",
        "    \"answer_quality_reason\": output_quality_reason,\n",
        "    \"correct_context\": [a == b for a, b in zip(context_for_q_generation, context_retrieved_lst)],\n",
        "    \"context_retrieved\": context_retrieved_lst,\n",
        "    \"context_for_q_generation\": context_for_q_generation\n",
        "})\n",
        "\n",
        "mean_answer_score = df_results[\"answer_quality_score\"].mean()\n",
        "retrieval_accuracy = sum(df_results[\"correct_context\"]) / len(df_results[\"correct_context\"])\n",
        "\n",
        "print(f\"Retrieval accuracy: {retrieval_accuracy}\")\n",
        "print(f\"Mean answer socre: {mean_answer_score}\")\n",
        "print(\"\\n\")\n",
        "\n",
        "df_results\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 732
        },
        "id": "Glmejky1I2Uv",
        "outputId": "6893d11c-d444-4560-91fd-7ae7730cffb1"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieval accuracy: 1.0\n",
            "Mean answer socre: 89.0\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            question  \\\n",
              "0  What is the Czech Republic's view on the focus...   \n",
              "1  How is control over an AI system's decisions a...   \n",
              "2  What approach should be followed in the applic...   \n",
              "3  When did the evolution of facial recognition d...   \n",
              "4  What study shows that AI may not recognize peo...   \n",
              "\n",
              "                                              answer  answer_quality_score  \\\n",
              "0  \\nThe Czech Republic believes that the importa...                   100   \n",
              "1  \\nControl over an AI system's decisions and ac...                    90   \n",
              "2  \\nThe EU whitepaper suggests following an inte...                   100   \n",
              "3                                               1994                    75   \n",
              "4  \\tAn empirical study shows that AI may not rec...                    80   \n",
              "\n",
              "                               answer_quality_reason  correct_context  \\\n",
              "0  The answer directly answers the question by st...             True   \n",
              "1  The answer directly answers the question by ex...             True   \n",
              "2  The answer directly answers the question by su...             True   \n",
              "3  The answer directly answers the question and i...             True   \n",
              "4  The answer directly answers the question and i...             True   \n",
              "\n",
              "                                   context_retrieved  \\\n",
              "0  regulatory framework for artificial intelligen...   \n",
              "1  38 classifying an ai ’ s application context 3...   \n",
              "2  ( in terms of research, startups, infrastructu...   \n",
              "3  other databases have been made public so that ...   \n",
              "4  is mainly confronted with people of white skin...   \n",
              "\n",
              "                            context_for_q_generation  \n",
              "0  regulatory framework for artificial intelligen...  \n",
              "1  38 classifying an ai ’ s application context 3...  \n",
              "2  ( in terms of research, startups, infrastructu...  \n",
              "3  other databases have been made public so that ...  \n",
              "4  is mainly confronted with people of white skin...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f06eef9c-cd09-46ac-a9ef-77d72badfef6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>answer_quality_score</th>\n",
              "      <th>answer_quality_reason</th>\n",
              "      <th>correct_context</th>\n",
              "      <th>context_retrieved</th>\n",
              "      <th>context_for_q_generation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is the Czech Republic's view on the focus...</td>\n",
              "      <td>\\nThe Czech Republic believes that the importa...</td>\n",
              "      <td>100</td>\n",
              "      <td>The answer directly answers the question by st...</td>\n",
              "      <td>True</td>\n",
              "      <td>regulatory framework for artificial intelligen...</td>\n",
              "      <td>regulatory framework for artificial intelligen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How is control over an AI system's decisions a...</td>\n",
              "      <td>\\nControl over an AI system's decisions and ac...</td>\n",
              "      <td>90</td>\n",
              "      <td>The answer directly answers the question by ex...</td>\n",
              "      <td>True</td>\n",
              "      <td>38 classifying an ai ’ s application context 3...</td>\n",
              "      <td>38 classifying an ai ’ s application context 3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What approach should be followed in the applic...</td>\n",
              "      <td>\\nThe EU whitepaper suggests following an inte...</td>\n",
              "      <td>100</td>\n",
              "      <td>The answer directly answers the question by su...</td>\n",
              "      <td>True</td>\n",
              "      <td>( in terms of research, startups, infrastructu...</td>\n",
              "      <td>( in terms of research, startups, infrastructu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>When did the evolution of facial recognition d...</td>\n",
              "      <td>1994</td>\n",
              "      <td>75</td>\n",
              "      <td>The answer directly answers the question and i...</td>\n",
              "      <td>True</td>\n",
              "      <td>other databases have been made public so that ...</td>\n",
              "      <td>other databases have been made public so that ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What study shows that AI may not recognize peo...</td>\n",
              "      <td>\\tAn empirical study shows that AI may not rec...</td>\n",
              "      <td>80</td>\n",
              "      <td>The answer directly answers the question and i...</td>\n",
              "      <td>True</td>\n",
              "      <td>is mainly confronted with people of white skin...</td>\n",
              "      <td>is mainly confronted with people of white skin...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f06eef9c-cd09-46ac-a9ef-77d72badfef6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f06eef9c-cd09-46ac-a9ef-77d72badfef6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f06eef9c-cd09-46ac-a9ef-77d72badfef6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6098b18c-12af-4e71-9143-dd46cbf32381\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6098b18c-12af-4e71-9143-dd46cbf32381')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6098b18c-12af-4e71-9143-dd46cbf32381 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0dc2f589ddc04d0e8a1cc5790ab7850f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3000184fccbb483eaa5d5dbc5927b8c6",
              "IPY_MODEL_d0e842db28af4e18ba7ceae09339ad45",
              "IPY_MODEL_e323edea49e44fcc901f6251fd5e9068"
            ],
            "layout": "IPY_MODEL_414437d86bb54a89947e6a05d74138a2"
          }
        },
        "3000184fccbb483eaa5d5dbc5927b8c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_401c430e78ee4bdda4039165ca0c6006",
            "placeholder": "​",
            "style": "IPY_MODEL_a44b494649c9495fb9ec16d0bb14c067",
            "value": "100%"
          }
        },
        "d0e842db28af4e18ba7ceae09339ad45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_346c3323948d454e8054ebe948fc4603",
            "max": 440,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c372309272214fd59a9476e2685c17bb",
            "value": 440
          }
        },
        "e323edea49e44fcc901f6251fd5e9068": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3dc78c4f999745b08ab6a88763317c2f",
            "placeholder": "​",
            "style": "IPY_MODEL_9fa11cfe31af4d1c897b3493e4d275cd",
            "value": " 440/440 [00:32&lt;00:00, 23.26it/s]"
          }
        },
        "414437d86bb54a89947e6a05d74138a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "401c430e78ee4bdda4039165ca0c6006": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a44b494649c9495fb9ec16d0bb14c067": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "346c3323948d454e8054ebe948fc4603": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c372309272214fd59a9476e2685c17bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3dc78c4f999745b08ab6a88763317c2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9fa11cfe31af4d1c897b3493e4d275cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}