{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MoritzLaurer/rag-demo/blob/master/rag_langchain_ai_law.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating a RAG pipeline with LangChain and Hugging Face Endpoints or OpenAI"
      ],
      "metadata": {
        "id": "53l_qbyq4LPe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook provides a quick demo for creating and evaluating a Retrieval Augmented Generation (RAG) pipeline with LangChain and Hugging Face Endpoints or OpenAI.\n",
        "\n",
        "The demo has the following main steps:\n",
        "1. Create an example vector database: The demo downloads 440 position paper PDFs which stakeholders had submitted to the EU public consultation on the EU White Paper on AI in 2020. These PDFs are processed and ingested in a vector database.\n",
        "2. We then automatically generate questions about a sample of the texts with an LLM\n",
        "3. Then we create a RAG pipeline and feed the generated questions into the RAG pipeline as user queries\n",
        "4. RAG evaluation:\n",
        "  - Retriever quality: If we ask a generated question to the RAG pipeline, does the pipeline's retriever retrieve the same original text which was used to generated the question? This provides an indication of retriever (and reranker) quality. This indicator is, however, imperfect, as the retriever could also retrieve other texts that help the RAG pipeline generate good answers beyond only the original text used for generating the question.\n",
        "  - Answer quality: We also use an LLM to evaluate answer quality more broadly.\n"
      ],
      "metadata": {
        "id": "PsYcqBav4VfY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tocCqFNnQqgo"
      },
      "source": [
        "## Install packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "myUZd_CK4zeL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2961a3e4-d0db-4b9c-aa41-97cf5bad8ba5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 11.1 MB/s eta 0:00:00\n",
            "Collecting langchain~=0.0.352\n",
            "  Downloading langchain-0.0.354-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain~=0.0.352) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain~=0.0.352) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain~=0.0.352) (3.9.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain~=0.0.352) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain~=0.0.352)\n",
            "  Downloading dataclasses_json-0.6.3-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain~=0.0.352)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.8 (from langchain~=0.0.352)\n",
            "  Downloading langchain_community-0.0.8-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting langchain-core<0.2,>=0.1.5 (from langchain~=0.0.352)\n",
            "  Downloading langchain_core-0.1.5-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.77 (from langchain~=0.0.352)\n",
            "  Downloading langsmith-0.0.77-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain~=0.0.352) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain~=0.0.352) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain~=0.0.352) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain~=0.0.352) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain~=0.0.352) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain~=0.0.352) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain~=0.0.352) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain~=0.0.352) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain~=0.0.352) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain~=0.0.352)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain~=0.0.352)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain~=0.0.352)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.5->langchain~=0.0.352) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.5->langchain~=0.0.352) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain~=0.0.352) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain~=0.0.352) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain~=0.0.352) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain~=0.0.352) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain~=0.0.352) (2023.11.17)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain~=0.0.352) (3.0.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.5->langchain~=0.0.352) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.5->langchain~=0.0.352) (1.2.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain~=0.0.352)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading langchain-0.0.354-py3-none-any.whl (803 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 803.3/803.3 kB 17.0 MB/s eta 0:00:00\n",
            "Downloading dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n",
            "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading langchain_community-0.0.8-py3-none-any.whl (1.5 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 56.4 MB/s eta 0:00:00\n",
            "Downloading langchain_core-0.1.5-py3-none-any.whl (205 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 205.3/205.3 kB 15.9 MB/s eta 0:00:00\n",
            "Downloading langsmith-0.0.77-py3-none-any.whl (48 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 48.2/48.2 kB 3.5 MB/s eta 0:00:00\n",
            "Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 49.4/49.4 kB 3.4 MB/s eta 0:00:00\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, langsmith, jsonpatch, langchain-core, dataclasses-json, langchain-community, langchain\n",
            "Successfully installed dataclasses-json-0.6.3 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.354 langchain-community-0.0.8 langchain-core-0.1.5 langsmith-0.0.77 marshmallow-3.20.1 mypy-extensions-1.0.0 typing-inspect-0.9.0\n",
            "Collecting langchainhub~=0.1.14\n",
            "  Downloading langchainhub-0.1.14-py3-none-any.whl.metadata (478 bytes)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchainhub~=0.1.14) (2.31.0)\n",
            "Collecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub~=0.1.14)\n",
            "  Downloading types_requests-2.31.0.20231231-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub~=0.1.14) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub~=0.1.14) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub~=0.1.14) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub~=0.1.14) (2023.11.17)\n",
            "Downloading langchainhub-0.1.14-py3-none-any.whl (3.4 kB)\n",
            "Downloading types_requests-2.31.0.20231231-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: types-requests, langchainhub\n",
            "Successfully installed langchainhub-0.1.14 types-requests-2.31.0.20231231\n",
            "Collecting openai~=1.6.0\n",
            "  Downloading openai-1.6.1-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai~=1.6.0) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai~=1.6.0) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai~=1.6.0)\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai~=1.6.0) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai~=1.6.0) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai~=1.6.0) (4.66.1)\n",
            "Collecting typing-extensions<5,>=4.7 (from openai~=1.6.0)\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai~=1.6.0) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai~=1.6.0) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai~=1.6.0) (2023.11.17)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai~=1.6.0)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai~=1.6.0)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 2.8 MB/s eta 0:00:00\n",
            "Downloading openai-1.6.1-py3-none-any.whl (225 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 225.4/225.4 kB 11.9 MB/s eta 0:00:00\n",
            "Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 75.9/75.9 kB 5.2 MB/s eta 0:00:00\n",
            "Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.9/76.9 kB 6.3 MB/s eta 0:00:00\n",
            "Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Installing collected packages: typing-extensions, h11, httpcore, httpx, openai\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 openai-1.6.1 typing-extensions-4.9.0\n",
            "Collecting tiktoken~=0.5.2\n",
            "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken~=0.5.2) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken~=0.5.2) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken~=0.5.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken~=0.5.2) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken~=0.5.2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken~=0.5.2) (2023.11.17)\n",
            "Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 7.1 MB/s eta 0:00:00\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.5.2\n",
            "Requirement already satisfied: huggingface_hub~=0.20.1 in /usr/local/lib/python3.10/dist-packages (0.20.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub~=0.20.1) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub~=0.20.1) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub~=0.20.1) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub~=0.20.1) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub~=0.20.1) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub~=0.20.1) (4.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub~=0.20.1) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub~=0.20.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub~=0.20.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub~=0.20.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub~=0.20.1) (2023.11.17)\n",
            "Collecting sentence_transformers~=2.2.2\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86.0/86.0 kB 1.4 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers~=2.2.2) (4.35.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers~=2.2.2) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers~=2.2.2) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers~=2.2.2) (0.16.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers~=2.2.2) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers~=2.2.2) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers~=2.2.2) (1.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers~=2.2.2) (3.8.1)\n",
            "Collecting sentencepiece (from sentence_transformers~=2.2.2)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 8.8 MB/s eta 0:00:00\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers~=2.2.2) (0.20.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers~=2.2.2) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers~=2.2.2) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers~=2.2.2) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers~=2.2.2) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers~=2.2.2) (4.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers~=2.2.2) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers~=2.2.2) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers~=2.2.2) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers~=2.2.2) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers~=2.2.2) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers~=2.2.2) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers~=2.2.2) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers~=2.2.2) (0.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers~=2.2.2) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers~=2.2.2) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers~=2.2.2) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence_transformers~=2.2.2) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence_transformers~=2.2.2) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers~=2.2.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers~=2.2.2) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers~=2.2.2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers~=2.2.2) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence_transformers~=2.2.2) (1.3.0)\n",
            "Building wheels for collected packages: sentence_transformers\n",
            "  Building wheel for sentence_transformers (setup.py): started\n",
            "  Building wheel for sentence_transformers (setup.py): finished with status 'done'\n",
            "  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=457d53ef067d018806dc78498f0c72eda8dd75ed297138cf5e36b8303cc8c34a\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence_transformers\n",
            "Installing collected packages: sentencepiece, sentence_transformers\n",
            "Successfully installed sentence_transformers-2.2.2 sentencepiece-0.1.99\n",
            "Collecting qdrant-client~=1.7.0\n",
            "  Downloading qdrant_client-1.7.0-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: grpcio>=1.41.0 in /usr/local/lib/python3.10/dist-packages (from qdrant-client~=1.7.0) (1.60.0)\n",
            "Collecting grpcio-tools>=1.41.0 (from qdrant-client~=1.7.0)\n",
            "  Downloading grpcio_tools-1.60.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: httpx>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from httpx[http2]>=0.14.0->qdrant-client~=1.7.0) (0.26.0)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.10/dist-packages (from qdrant-client~=1.7.0) (1.23.5)\n",
            "Collecting portalocker<3.0.0,>=2.7.0 (from qdrant-client~=1.7.0)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: pydantic>=1.10.8 in /usr/local/lib/python3.10/dist-packages (from qdrant-client~=1.7.0) (1.10.13)\n",
            "Collecting urllib3<2.0.0,>=1.26.14 (from qdrant-client~=1.7.0)\n",
            "  Downloading urllib3-1.26.18-py2.py3-none-any.whl.metadata (48 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 48.9/48.9 kB 2.0 MB/s eta 0:00:00\n",
            "Collecting protobuf<5.0dev,>=4.21.6 (from grpcio-tools>=1.41.0->qdrant-client~=1.7.0)\n",
            "  Downloading protobuf-4.25.1-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from grpcio-tools>=1.41.0->qdrant-client~=1.7.0) (67.7.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.14.0->httpx[http2]>=0.14.0->qdrant-client~=1.7.0) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.14.0->httpx[http2]>=0.14.0->qdrant-client~=1.7.0) (2023.11.17)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.14.0->httpx[http2]>=0.14.0->qdrant-client~=1.7.0) (1.0.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.14.0->httpx[http2]>=0.14.0->qdrant-client~=1.7.0) (3.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.14.0->httpx[http2]>=0.14.0->qdrant-client~=1.7.0) (1.3.0)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.14.0->httpx[http2]>=0.14.0->qdrant-client~=1.7.0) (0.14.0)\n",
            "Collecting h2<5,>=3 (from httpx[http2]>=0.14.0->qdrant-client~=1.7.0)\n",
            "  Downloading h2-4.1.0-py3-none-any.whl (57 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.5/57.5 kB 3.6 MB/s eta 0:00:00\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10.8->qdrant-client~=1.7.0) (4.9.0)\n",
            "Collecting hyperframe<7,>=6.0 (from h2<5,>=3->httpx[http2]>=0.14.0->qdrant-client~=1.7.0)\n",
            "  Downloading hyperframe-6.0.1-py3-none-any.whl (12 kB)\n",
            "Collecting hpack<5,>=4.0 (from h2<5,>=3->httpx[http2]>=0.14.0->qdrant-client~=1.7.0)\n",
            "  Downloading hpack-4.0.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.14.0->httpx[http2]>=0.14.0->qdrant-client~=1.7.0) (1.2.0)\n",
            "Downloading qdrant_client-1.7.0-py3-none-any.whl (203 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 203.7/203.7 kB 6.6 MB/s eta 0:00:00\n",
            "Downloading grpcio_tools-1.60.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.8/2.8 MB 14.9 MB/s eta 0:00:00\n",
            "Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 143.8/143.8 kB 9.8 MB/s eta 0:00:00\n",
            "Downloading protobuf-4.25.1-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 294.6/294.6 kB 12.2 MB/s eta 0:00:00\n",
            "Installing collected packages: urllib3, protobuf, portalocker, hyperframe, hpack, h2, grpcio-tools, qdrant-client\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.7\n",
            "    Uninstalling urllib3-2.0.7:\n",
            "      Successfully uninstalled urllib3-2.0.7\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "Successfully installed grpcio-tools-1.60.0 h2-4.1.0 hpack-4.0.0 hyperframe-6.0.1 portalocker-2.8.2 protobuf-4.25.1 qdrant-client-1.7.0 urllib3-1.26.18\n",
            "Collecting PyMuPDF~=1.23.7\n",
            "  Downloading PyMuPDF-1.23.8-cp310-none-manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting PyMuPDFb==1.23.7 (from PyMuPDF~=1.23.7)\n",
            "  Downloading PyMuPDFb-1.23.7-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Downloading PyMuPDF-1.23.8-cp310-none-manylinux2014_x86_64.whl (4.4 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.4/4.4 MB 15.5 MB/s eta 0:00:00\n",
            "Downloading PyMuPDFb-1.23.7-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (30.6 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 30.6/30.6 MB 57.4 MB/s eta 0:00:00\n",
            "Installing collected packages: PyMuPDFb, PyMuPDF\n",
            "Successfully installed PyMuPDF-1.23.8 PyMuPDFb-1.23.7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
            "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\n",
            "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
            "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
            "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
            "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorboard 2.15.1 requires protobuf<4.24,>=3.19.6, but you have protobuf 4.25.1 which is incompatible.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 4.25.1 which is incompatible.\n",
            "types-requests 2.31.0.20231231 requires urllib3>=2, but you have urllib3 1.26.18 which is incompatible.\n",
            "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
            "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "pip install --upgrade pip -q\n",
        "pip install langchain~=0.0.352\n",
        "pip install langchainhub~=0.1.14\n",
        "pip install openai~=1.6.0\n",
        "pip install tiktoken~=0.5.2\n",
        "pip install transformers>=4.35.2\n",
        "pip install huggingface_hub~=0.20.1\n",
        "pip install sentence_transformers~=2.2.2\n",
        "pip install qdrant-client~=1.7.0\n",
        "pip install PyMuPDF~=1.23.7\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "\n",
        "# for using hugging face models\n",
        "login(token=userdata.get('HF_TOKEN'))\n",
        "\n",
        "# for using OAI models\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_KEY')"
      ],
      "metadata": {
        "id": "z-DBbhcmqdal",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "331db798-5059-4cf6-8f85-84c5631aad92"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FE9Aj1bcQnMN"
      },
      "source": [
        "## Prepare example data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Download PDF data"
      ],
      "metadata": {
        "id": "GKSJzyg9tinB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## download PDF data\n",
        "import os\n",
        "import zipfile\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "# URL of the zip file in your GitHub repo (make sure it's the raw file URL)\n",
        "zip_url = 'https://github.com/MoritzLaurer/rag-demo/blob/master/data/position-papers-pdfs.zip?raw=true'\n",
        "\n",
        "# Download the zip file\n",
        "print(\"Downloading zip file...\")\n",
        "response = requests.get(zip_url)\n",
        "zip_content = BytesIO(response.content)\n",
        "\n",
        "# Define the extraction path\n",
        "extract_path = '/content/data'\n",
        "\n",
        "# Create directory if it doesn't exist\n",
        "if not os.path.exists(extract_path):\n",
        "    os.makedirs(extract_path)\n",
        "\n",
        "# Extract the zip file\n",
        "print(\"Extracting zip file...\")\n",
        "with zipfile.ZipFile(zip_content, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"Extraction completed.\")\n",
        "\n",
        "file_paths = [f for f in os.listdir(extract_path) if os.path.isfile(os.path.join(extract_path, f))]\n",
        "print(f\"{len(file_paths)} PDF files downloaded.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oa-86eodrEez",
        "outputId": "c6900297-28ca-45cf-bfe1-181bf4ebb5bd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading zip file...\n",
            "Extracting zip file...\n",
            "Extraction completed.\n",
            "440 PDF files downloaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-mL7xkUf3F6"
      },
      "source": [
        "### Process data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PDFMinerLoader\n",
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "directory = \"./data\"\n",
        "\n",
        "docs = []\n",
        "for pdf_path in tqdm(os.listdir(directory)):\n",
        "  try:\n",
        "    docs.append(PyMuPDFLoader(os.path.join(directory, pdf_path)).load())\n",
        "  except Exception as e:\n",
        "    print(\"Exception: \", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "ce3a2b0a48e3489a8057eef410d90250",
            "63d4e7a3504c43909e64706c8f241253",
            "5105b22d98ba4be79a1278a44b797bb7",
            "c055cc2e695046bba2403dfe05eca6d2",
            "47da0a0159de4794997b7f494bf37607",
            "5fc3b0837ce847f1b686db5532fd0761",
            "48632e2349a34761b598315bab5e069f",
            "432ab0fbec124bdfb995c9b230f5f4b1",
            "b20e369efab04023933e18e56eea9cec",
            "cb3d5cd96d324dc1a5321c0ec179754a",
            "dcf0878e0482457ebd897c0fb60f5089"
          ]
        },
        "id": "_bjaESlGec45",
        "outputId": "3484094e-5eeb-4657-f971-613bdb214c97"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/440 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ce3a2b0a48e3489a8057eef410d90250"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exception:  cannot open broken document\n",
            "Exception:  cannot open broken document\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
        "\n",
        "\"\"\"text_splitter = CharacterTextSplitter(\n",
        "    separator = \" \",\n",
        "    chunk_size = 1000,\n",
        "    chunk_overlap  = 30,\n",
        "    length_function = len,\n",
        "    is_separator_regex = False,\n",
        ")\"\"\"\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
        ")\n",
        "\n",
        "docs_processed = [text_splitter.split_documents(doc) for doc in docs]\n",
        "\n",
        "docs_processed = [item for sublist in docs_processed for item in sublist]\n",
        "print(len(docs_processed))\n",
        "\n",
        "docs_processed[:1]"
      ],
      "metadata": {
        "id": "T1wNg46flg68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1333fce-ae89-4f98-fdec-c6ed932a3153"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16932\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='EC White Paper: Consultation Response \\nJune 2020 \\n© 2020, Loughborough University, UKRI Project REF: ES/S010416/1  \\n \\n1 of 26 \\nResponse \\nto \\nthe \\nEuropean \\nCommission’s \\nConsultation on Artificial Intelligence: A European \\napproach to excellence and trust \\nThis document is a response to the European Commission’s Consultation on Artificial Intelligence, from \\nLoughborough University systems engineering researchers Dr Melanie King and Paul Timms, written as part \\nof the TECHNGI Academic Research Project (UKRI Project Ref: ES/S010416/1).  TECHNGI (Technology \\nDriven Next Generation Insurance) is a cross-disciplinary research project investigating the opportunities and \\nchallenges for the UK insurance industry arising from the application of new AI technologies, including machine \\nlearning, distributed ledger, automated processing, and the explosion of available dataa. \\nWe provide both general comments on the white paper [1], and address more specific responses to the three', metadata={'source': './data/F530382-SUBMITTED_Formal_response_to_the_European_Commission_V5.pdf', 'file_path': './data/F530382-SUBMITTED_Formal_response_to_the_European_Commission_V5.pdf', 'page': 0, 'total_pages': 26, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': '', 'start_index': 2})]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Sample data to reduce embedding and generation costs"
      ],
      "metadata": {
        "id": "XBcPIqYjMWu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.seed(42)\n",
        "\n",
        "# sample corpus for embedding\n",
        "index_random = random.sample(range(len(docs_processed)), 100)\n",
        "docs_processed_samp = [docs_processed[i] for i in index_random]\n",
        "\n",
        "# sample some contexts generate questions from\n",
        "docs_processed_for_q_generation = docs_processed_samp[:5]\n"
      ],
      "metadata": {
        "id": "hUIhFQoc6GtE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Automatic question generation for evaluation\n",
        "\n",
        "This section generates questions which users could ask about a specific text in the database. This allows us to assess:  \n",
        "- If we ask a generated question to the RAG pipeline, does the pipeline's retriever retrieve the same text which was used to generated the question? This provides an indication of retriever (and reranker) quality.\n",
        "- Beyond the original text used for generating the question, the retriever might retrieve other texts that are also help the RAG pipeline generate good answers. We therefore also use an LLM to evaluate answer quality more broadly."
      ],
      "metadata": {
        "id": "tKOmYhRe7vxm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create an huggingface inference endpoint to run any LLM\n",
        "# intro: https://www.philschmid.de/inference-endpoints-iac\n",
        "# docs: https://huggingface.co/docs/huggingface_hub/v0.20.1/en/package_reference/hf_api#huggingface_hub.HfApi.create_inference_endpoint\n",
        "from huggingface_hub import create_inference_endpoint\n",
        "from huggingface_hub import HfApi\n",
        "api = HfApi()\n",
        "\n",
        "\n",
        "# define TGI as custom image\n",
        "custom_image = {\n",
        "    \"health_route\": \"/health\",  # Health route for TGI\n",
        "    \"env\": {\n",
        "        \"MAX_BATCH_PREFILL_TOKENS\": \"2048\", # can be adjusted to your needs\n",
        "        \"MAX_INPUT_LENGTH\": \"1024\", # can be adjusted to your needs\n",
        "        \"MAX_TOTAL_TOKENS\": \"1512\", # can be adjusted to your needs\n",
        "        \"MODEL_ID\": \"/repository\",  # IE will save the model in /repository\n",
        "    },\n",
        "    \"url\": \"ghcr.io/huggingface/text-generation-inference:1.3.3\",\n",
        "}\n",
        "\n",
        "# Create Inference Endpoint to run Zephyr 7B\n",
        "print(\"Creating Inference Endpoint for Zephyr 7B\")\n",
        "hf_endpoint = create_inference_endpoint(\n",
        "    name=\"zehpyr-ie-test\",\n",
        "    repository=\"HuggingFaceH4/zephyr-7b-beta\",\n",
        "    framework=\"pytorch\",\n",
        "    task=\"text-generation\",\n",
        "    vendor=\"aws\",\n",
        "    region=\"us-east-1\",\n",
        "    type=\"protected\",\n",
        "    instance_size=\"medium\",\n",
        "    instance_type=\"g5.2xlarge\",  # A10G GPU. Pricing: https://aws.amazon.com/sagemaker/pricing/?p=pm&c=sm&z=4\n",
        "    accelerator=\"gpu\",\n",
        "    namespace=\"MoritzLaurer\",  # your user or organisation name on the HF hub\n",
        "    custom_image=custom_image,\n",
        ")\n",
        "\n",
        "print(\"Waiting for endpoint to be deployed\")\n",
        "hf_endpoint.wait()\n",
        "\n",
        "print(\"Endpoint ready\")\n",
        "\n",
        "# to manage an existing endpoint, use:\n",
        "#hf_endpoint = api.get_inference_endpoint(\"zehpyr-ie-test\")\n",
        "#hf_endpoint.resume()\n",
        "#hf_endpoint.pause()\n",
        "#hf_endpoint.delete()"
      ],
      "metadata": {
        "id": "5Ciu9hbwyFNQ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.llms import HuggingFaceEndpoint\n",
        "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
        "\n",
        "provider_for_question_generation = \"HF\"\n",
        "\n",
        "\n",
        "if provider_for_question_generation == \"HF\":\n",
        "  chat_model = HuggingFaceEndpoint(\n",
        "    endpoint_url=hf_endpoint.url,  #\"https://ytjpei7t003tedav.us-east-1.aws.endpoints.huggingface.cloud\",\n",
        "    task=\"text-generation\",\n",
        "    huggingfacehub_api_token=userdata.get('HF_TOKEN'),\n",
        "    model_kwargs={}\n",
        "  )\n",
        "\n",
        "elif provider_for_question_generation == \"OAI\":\n",
        "  # https://platform.openai.com/docs/api-reference/chat\n",
        "  chat_model = ChatOpenAI(\n",
        "      model=\"gpt-3.5-turbo-1106\",  #\"gpt-3.5-turbo-1106\",  # \"gpt-4-1106-preview\"\n",
        "      temperature=0.2, max_tokens=1024,\n",
        "      n=1, top_p=0.95,\n",
        "      frequency_penalty=0.0,  # Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.\n",
        "      presence_penalty=0.0,  # Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.\n",
        "      #response_format={ \"type\": \"json_object\" },\n",
        "      seed=42,\n",
        "  )\n",
        "\n"
      ],
      "metadata": {
        "id": "mO2OTlAX7xzQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "instruction_question_gen = \"\"\"\\\n",
        "Your task is to write a factoid question given a context.\n",
        "Your factoid question should be answerable with a specific, concise piece of factual information from the context.\n",
        "Your factoid question should be formulated in the same style as questions users could ask in a search engine. \\\n",
        "This means that your factoid question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
        "\n",
        "context: {context}\\n\n",
        "factoid question: \"\"\"\n",
        "\n",
        "\n",
        "prompt_question_gen = ChatPromptTemplate.from_template(instruction_question_gen)\n",
        "\n",
        "chain = prompt_question_gen | chat_model\n",
        "\n",
        "questions_lst = []\n",
        "for context in docs_processed_for_q_generation:\n",
        "  print(\"Context:\\n\", context.page_content)\n",
        "  output_question = chain.invoke({\"context\": context.page_content})\n",
        "  if provider_for_question_generation == \"OAI\":\n",
        "    output_question = output_question.content\n",
        "  print(\"\\nGenerated question:\\n\", output_question, \"\\n\")\n",
        "  questions_lst.append(output_question)\n"
      ],
      "metadata": {
        "id": "Fsg4SXAT7xzR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e979c022-a7b3-4950-87ce-4a43e8314238"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context:\n",
            " access to school-made teaching material covering the achieved or \n",
            "partially achieved curriculum. Therefore teachers need telecom \n",
            "support and home-working status as well as authorship \n",
            "recognition. \n",
            "\"A School\", as we put it, \"is a seamless process for transferring \n",
            "knowledge and experimenting newly acquired knowledge.\" \n",
            "From the user point of view, this is a crucial issue. For Education \n",
            "Bodies, it is a critical mission. For multimedia technologists, it is a \n",
            "real challenge. \n",
            "As Professor Ruberti used to say: \"Information Technology and \n",
            "Telecommunications are the only ways today to help teachers \n",
            "upgrading the Education System's performances.\" \n",
            "Are all the players in this area convinced that they have to \n",
            "proceed this way ? \n",
            "What do think of it our newest regulatory bodies? \n",
            "Are they ready to enforce the introduction of Isdn as a bottom \n",
            "line for the Universal Service definition ? \n",
            "And what about our governments ? \n",
            "Would it be possible to ask them yearly accounts of their\n",
            "\n",
            "Generated question:\n",
            " \n",
            "What is the role of Information Technology and Telecommunications in upgrading the Education System's performances, as stated by Professor Ruberti? \n",
            "\n",
            "Context:\n",
            " got materials engineering that’s coming in. And all of that \n",
            "needs to be combined rapidly in the development process.” \n",
            "The need to understand how multiple engineering disciplines \n",
            "work with each other leads to an educational opportunity. \n",
            "Forty-eight percent of engineers say they would most benefit \n",
            "from training outside of the core areas in which they are \n",
            "experienced.\n",
            "48%\n",
            "say they would most benefit from \n",
            "training outside of the core areas \n",
            "in which they are experienced\n",
            "of engineers\n",
            "Competencies: Bridging Educational and Skills Gaps\n",
            "\n",
            "Generated question:\n",
            " \n",
            "\"What percentage of engineers say they would benefit from training outside of their core areas, according to the context provided?\" \n",
            "\n",
            "Context:\n",
            " wymogów należy ograniczyć do zastosowań SI wysokiego ryzyka (takich, w których ewentualne \n",
            "szkody wyrządzone przez systemy SI są szczególnie wysokie)? \n",
            "Tak \n",
            "Nie \n",
            "Inne \n",
            "Nie mam zdania \n",
            "Jeżeli wybrali Państwo odpowiedź „Inne”, proszę podać szczegóły: \n",
            "Zadanie ochrony praw jednostek przez zapewnienie etycznego działania systemów SI nie powinno \n",
            "być ograniczane do zastosowań SI wysokiego ryzyka, ani delegowane na interesariuszy. Unijne ramy \n",
            "prawne powinny określić przejrzyste warunki rozwoju systemów SI i przeciwdziałać naruszeniom \n",
            "praw jednostek. Jednocześnie ramy te nie powinny blokować rozwoju zastosowań SI tam gdzie nie \n",
            "jest to niezbędne, tak aby zachować konkurencyjność europejskiej gospodarce cyfrowej.  Mając te \n",
            "względy na uwadze, za celowe można uznać:  \n",
            "- wprowadzenie obowiązku przeprowadzania badania wpływu systemów SI na prawa człowieka przed \n",
            "wdrożeniu systemu SI (nieograniczone jedynie do przewidywanych konsekwencji przetwarzania\n",
            "\n",
            "Generated question:\n",
            " \n",
            "What is the recommended approach for ensuring the protection of individual rights in the development of SI systems, as outlined in the European Union's legal framework? \n",
            "\n",
            "alternative factoid question: \n",
            "What should be required before deploying SI systems, according to the European Union's legal framework for protecting individual rights? \n",
            "\n",
            "alternative factoid question: \n",
            "What type of analysis should be conducted before implementing SI systems, as recommended by the European Union's legal framework for \n",
            "\n",
            "Context:\n",
            " deployer can often absolve themselves for any damage caused by occurring faults. The \n",
            "vendor or deployer will in these cases claim to have been uninvolved in the creation \n",
            "process and hence will not assume responsibility for software faults, instead referring to\n",
            "\n",
            "Generated question:\n",
            " \n",
            "\n",
            "what is the legal stance of deployers regarding software faults in the given context? \n",
            "\n",
            "Context:\n",
            " should allow the benefits of AI to be realized before introducing regulation that could potentially limit the \n",
            "growth of the industry. While allowing for flexibility and innovation, the EU approach to AI should be as \n",
            "harmonized as possible and developed with the participation of industry and relevant stakeholders. This \n",
            "would allow for AI applications to be rolled out EU-wide reaping the benefits of scale that the Single \n",
            "Market should offer. The EU rules should also allow for a competitive EU environment that would allow \n",
            "start-ups and new payment providers using and developing AI to scale up. It is also important to promote \n",
            "private sector engagement and public/private partnerships to create a framework that could lead to ethical \n",
            "AI applications that contribute to Europe’s digital development. \n",
            " \n",
            "Given that technology is global, EPIF also welcome the EU’s intention to continue promoting cooperation \n",
            "on AI on the international stage.\n",
            "\n",
            "Generated question:\n",
            " \n",
            "\n",
            "What is the EU's stance on regulating AI, and how does it aim to balance flexibility, innovation, and harmonization in its approach? \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG pipeline"
      ],
      "metadata": {
        "id": "EY0NxZbt5uMf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retrival\n",
        "\n",
        "Optimization potential: different retrievers, different rerankers, multi-retrievers"
      ],
      "metadata": {
        "id": "a5bjM8nLnyUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# detailed RAG docs: https://python.langchain.com/docs/use_cases/question_answering/\n",
        "# FAISS cookbook: https://python.langchain.com/docs/expression_language/cookbook/retrieval\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.embeddings import OpenAIEmbeddings, HuggingFaceEmbeddings, HuggingFaceInferenceAPIEmbeddings\n",
        "from langchain.schema import StrOutputParser\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma, Qdrant\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import Distance, VectorParams\n"
      ],
      "metadata": {
        "id": "yQucSUUF5seT"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ! issue: langchain vector store wrappers don't seem to allow adjustment to dimensions, only accept OAI default 1.5k\n",
        "# using qdrant directly instead of langchain wrapper\n",
        "\n",
        "provider_retrieval_model = \"HF\"\n",
        "\n",
        "client_path = f\"./vectorstore\"\n",
        "collection_name = f\"collection\"\n",
        "\n",
        "if provider_retrieval_model == \"HF\":\n",
        "  qdrantClient = QdrantClient(path=client_path, prefer_grpc=True)\n",
        "\n",
        "  embeddings = HuggingFaceInferenceAPIEmbeddings(\n",
        "      api_key=userdata.get('HF_TOKEN'), model_name=\"sentence-transformers/all-MiniLM-l6-v2\"\n",
        "  )\n",
        "\n",
        "  dim = 384\n",
        "\n",
        "elif provider_retrieval_model == \"OAI\":\n",
        "\n",
        "  qdrantClient = QdrantClient(path=client_path, prefer_grpc=True)\n",
        "\n",
        "  embeddings = OpenAIEmbeddings(\n",
        "          model=\"text-embedding-ada-002\",\n",
        "          openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
        "  )\n",
        "\n",
        "  dim = 1536\n",
        "\n",
        "\n",
        "qdrantClient.create_collection(\n",
        "    collection_name=collection_name,\n",
        "    vectors_config=VectorParams(size=dim, distance=Distance.COSINE),\n",
        ")\n",
        "\n",
        "vectorstore = Qdrant(\n",
        "    client=qdrantClient,\n",
        "    collection_name=collection_name,\n",
        "    embeddings=embeddings,\n",
        ")\n",
        "\n",
        "vectorstore.add_documents(docs_processed_samp)"
      ],
      "metadata": {
        "id": "iEJozhRA-4mQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1f6518a-f361-45be-fef6-e0b599c2f9f5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['28c3c99dfb1949a4a86d9833ce55fc5e',\n",
              " '69ec7cd463b947c7970f27d071b70bdc',\n",
              " '3d4653e0b63c4a498b4a54fa43a3d6c2',\n",
              " '2b31dd2ee13b48a1aac6518dadd67314',\n",
              " '4d2cbb5f53194e729e8b9497ca9dc3f6',\n",
              " '99ae290808e44bb09bd008c2e649898c',\n",
              " '7629e15f8d8745a6ab89349386457186',\n",
              " '62574f2f9e4449179b2715c69510e2cd',\n",
              " '99d38b296a0541d0acb6e330fcd58e48',\n",
              " '181e9dd9378848528c66a162138ad7df',\n",
              " '34a3e74b37c7448d8cf00fcab3fa6ba9',\n",
              " '68f8cb9563f842acb69549ef148e607f',\n",
              " '5ce8ac570ee546e6ae5b279fbb7c31dc',\n",
              " '08d75ae58032475789f67a8cee497340',\n",
              " '046cda3cd10e4f3fa6c2c72107ae595e',\n",
              " '4e9e717e653d4530b62755e8097c4df6',\n",
              " '617806d2d540448983428a329dade2d5',\n",
              " '757fc890e6c54e00833be22a9c2b2453',\n",
              " '7af6c7d36fc246bfa221c3f8be9088bf',\n",
              " '27d5a0c3879844689da7d0fcd1744ee7',\n",
              " '80d26b5ac016439c8c9ca52271d8436e',\n",
              " '42a1683ebc114b3dbb3bb2528038fa58',\n",
              " '56970c1f3c824acba4d60a118416c580',\n",
              " '6904e3761d8145b7ad53bc667145552d',\n",
              " '8ab4c34263454c0bb63107067cabd45d',\n",
              " 'bf5a13231d3640a28339da855901aa56',\n",
              " '26ba9b4b01324365afa9e7048766ac16',\n",
              " '37f0223cc95f47a1bade78902bec4047',\n",
              " 'caff5d788d2d497dadab3d4c96fb1f97',\n",
              " '674d359954114c378a2f3f4a5eabb6a6',\n",
              " '59498e0be9d049fda7b61c7086a9d239',\n",
              " '5cc4bc0268b643a9b4aea1882a02e786',\n",
              " 'f6920a1ad7674ed8a5d1b2661a282808',\n",
              " 'd4d03474edc14233abd92f757ee62a34',\n",
              " '0ea41b20e58c4f98b4191b8927e679e0',\n",
              " 'e5eecb14124e405eb2b8c3fb6f385f35',\n",
              " '3e89a74dadf0450daf5aa2e2ab5be895',\n",
              " '68387036e7174c909c59caa027f49a51',\n",
              " 'a774f8a9000e40a1a2dc06d0b1dfc9d7',\n",
              " '96f305fb206449efac088c8dfa2d3cb9',\n",
              " 'b7cc054c757646a2893c4710e1d4abe4',\n",
              " 'e3ad95788cd14c15be8e4d1b6c6247f7',\n",
              " '2b1e3e87cbe947d6949058609ecc7a02',\n",
              " 'f88f8722c4c0456db7a67e1aa5a3f5ee',\n",
              " '16c52951a8be494dabc451fb4e0bb7bc',\n",
              " 'd552f164a8e9491bbcb2f1913dcd9d84',\n",
              " '664f67ef32dc43cd9a1a33dfa1944262',\n",
              " 'cb7e9729f22749faaff0e0526d3b3901',\n",
              " '9b7ca7e9a4d44a03af99833a1f55dd55',\n",
              " '322ecbd4b9854586b2c83436ea8895e3',\n",
              " '13ca13ff29124a699e33e47b14f3f85d',\n",
              " '94f9e45961d44a2e812a1ce0f335e758',\n",
              " 'a8f98b9090204062a633a8218e981d1a',\n",
              " '75f1b1fa08854855a82cd28326e52237',\n",
              " 'de9971a1b1d04d7ab575ea662568f701',\n",
              " 'b3424f45420b4d8580c28429390de0c0',\n",
              " 'cc4a8856bbaa4f1aafc435a33d991ce2',\n",
              " '0625e26b2f324e719a89c00ff4b7eb9c',\n",
              " 'a07fb17179aa4201ab8081d4f15a1a75',\n",
              " 'cb67c368494e48088c35156b78cee986',\n",
              " '1d842406ff394bd688fdf524b09aef36',\n",
              " 'c2ae37e0b41c4ca4aa342c6522206c50',\n",
              " '8d8b89e024194263a6b8d210e3923ea4',\n",
              " 'bce6377a44e4446d9e5e61e11289b78c',\n",
              " 'dc7bc0815a6041fe8285eb8d0f1e8ec0',\n",
              " '9f44a542dd5047b089b81298f3b80f28',\n",
              " 'f16a3ce18c484873b00d9a47d2b55e1e',\n",
              " 'a516e79b3e874bf1ba2aa79e5796a8ea',\n",
              " 'b38498acf7f8444a9784956881b6df19',\n",
              " '4947c9d6f8614aaf862f9ee7ee05c812',\n",
              " '6ca30f735d6f40a288fb1d6c75f265e6',\n",
              " '34ceb3278ed04e77b8cd5ba2f9e1db64',\n",
              " 'cac1c9df98414659bd044980e663b01c',\n",
              " 'ec01ff7a95a94976af6a373381b45bfd',\n",
              " '93c5b35937dd4a8087b17e225ff7ebff',\n",
              " '4686ab18fd0644779d8c767c3a57d0fa',\n",
              " '521b9ff9701e42d4991d4717773c39fb',\n",
              " '48426fc5380c449f8f384abd30cd51a5',\n",
              " 'f96ae5278ce34c109a9a3e806b9b6841',\n",
              " '190be1086a92470aba5bf2fa0ae8931b',\n",
              " 'f8e0c2ce6aa54080b76aacd132e9c737',\n",
              " '8f4e9203ee83492d96f0dc5b83426b1d',\n",
              " 'b0ee725613cf46a7b62cfb465350b6f8',\n",
              " '649e53c093fb4e028d739e9d641325ac',\n",
              " '20f574de227a49049371e420cad0f01b',\n",
              " '93cec3632328457d97ed64ebbdfd89dd',\n",
              " '613e0629a88c48dfa24117e98a38eff7',\n",
              " '523750924c7e46c99f9939e5f474c761',\n",
              " 'c43faa1357a6422e9db4fd341fb71557',\n",
              " '84932737d87f4eb1971de7b4eea740b5',\n",
              " '67e425b30dad4d7e9b48f4af2b64fb23',\n",
              " '285cdee69a8f4183acd53cf51e7600f3',\n",
              " '04e8e376ddf24dd88cfaa8b442bd269c',\n",
              " '548d76a80ecb4a97a7bfb93fafe13f68',\n",
              " '0b71dc52ec2b400ea67204b68a1208af',\n",
              " 'e1ada13877ac40a9856419d091765d57',\n",
              " '58b50ec729b14d36b5439f50f19b5f47',\n",
              " 'fa91bcfa9de94222b287a47dded751f4',\n",
              " 'e31e16ef4e9444a9985b109bd349cb9e',\n",
              " '280e90be58ba44889de2ca9453e2575e']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "context_retrieved_lst = []\n",
        "for question in questions_lst:\n",
        "  retriever = vectorstore.as_retriever(\n",
        "      search_type=\"similarity\",\n",
        "      search_kwargs={\"k\": 1}\n",
        "  )\n",
        "\n",
        "  context_retrieved = retriever.get_relevant_documents(\n",
        "      question\n",
        "  )\n",
        "\n",
        "  def format_docs(docs):\n",
        "      return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "  context_retrieved = format_docs(context_retrieved)\n",
        "\n",
        "  context_retrieved_lst.append(context_retrieved)\n",
        "  #print(context_retrieved)\n"
      ],
      "metadata": {
        "id": "1qGMOP0gQ8iB"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check if retrieved context for question is same as context used for generating the question\n",
        "# note that this is an imperfect measure, because the retriever might\n",
        "# retrieve other texts that are equally relevant as the text used for generating the question\n",
        "context_for_q_generation = [doc.page_content for doc in docs_processed_for_q_generation]\n",
        "correct_context_retrieved = [a == b for a, b in zip(context_for_q_generation, context_retrieved_lst)]\n",
        "\n",
        "retrieval_accuracy = sum(correct_context_retrieved) / len(correct_context_retrieved)\n",
        "print(retrieval_accuracy)\n"
      ],
      "metadata": {
        "id": "Y3n3hP_3G2Pk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70c9517c-5493-4c39-ef37-6f6a6497d38a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# add reranking step\n",
        "# challenge: reranking with HF models not implemented in langchain\n",
        "# only cohere reranker seems implemented: https://python.langchain.com/docs/integrations/retrievers/cohere-reranker\n",
        "\n",
        "\"\"\"import torch\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-base')\n",
        "model = AutoModelForSequenceClassification.from_pretrained('BAAI/bge-reranker-base')\n",
        "model.eval()\n",
        "\n",
        "context_question_pairs_lst = []\n",
        "for question in questions_lst:\n",
        "  context_question_pairs_lst.append([[question, context] for context in context_retrieved_lst])\n",
        "\n",
        "#pairs = [['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']]\n",
        "\n",
        "for context_question_pair in context_question_pairs_lst:\n",
        "  with torch.no_grad():\n",
        "      inputs = tokenizer(context_question_pair, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
        "      scores = model(**inputs, return_dict=True).logits.view(-1, ).float()\n",
        "      print(scores)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "afrC3UwIXsSZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "c962b682-7ad6-473e-ce93-510488b0b35c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"import torch\\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\\n\\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-base')\\nmodel = AutoModelForSequenceClassification.from_pretrained('BAAI/bge-reranker-base')\\nmodel.eval()\\n\\ncontext_question_pairs_lst = []\\nfor question in questions_lst:\\n  context_question_pairs_lst.append([[question, context] for context in context_retrieved_lst])\\n\\n#pairs = [['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']]\\n\\nfor context_question_pair in context_question_pairs_lst:\\n  with torch.no_grad():\\n      inputs = tokenizer(context_question_pair, padding=True, truncation=True, return_tensors='pt', max_length=512)\\n      scores = model(**inputs, return_dict=True).logits.view(-1, ).float()\\n      print(scores)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer generation\n",
        "\n",
        "Optimization potential: different LLMs, different prompt templates"
      ],
      "metadata": {
        "id": "IrE01KH_n4LO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "prompt_qa_template = \"\"\"\\\n",
        "Your task is to answer a question based on a context.\n",
        "Your answer should be concise and you should only return your answer.\n",
        "\n",
        "context: {context}\n",
        "question: {question}\n",
        "answer: \"\"\"\n",
        "\n",
        "prompt_qa_template = PromptTemplate.from_template(prompt_qa_template)\n"
      ],
      "metadata": {
        "id": "RnBR_tyHRD6B"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFaceEndpoint\n",
        "\n",
        "provider_answer_model = \"HF\"\n",
        "\n",
        "if provider_answer_model == \"HF\":\n",
        "  llm_qa = HuggingFaceEndpoint(\n",
        "    endpoint_url=hf_endpoint.url,  #\"https://nqoa2is3qe7y82ww.us-east-1.aws.endpoints.huggingface.cloud\",\n",
        "    task=\"text-generation\",\n",
        "    huggingfacehub_api_token=userdata.get('HF_TOKEN'),\n",
        "    model_kwargs={}\n",
        "  )\n",
        "elif provider_answer_model == \"OAI\":\n",
        "  llm_qa = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "chain = prompt_qa_template | llm_qa | StrOutputParser()\n",
        "\n",
        "answer_lst = []\n",
        "for question, context in zip(questions_lst , context_retrieved_lst):\n",
        "  answer = chain.invoke({\"context\": context, \"question\": question})\n",
        "  answer_lst.append(answer)\n"
      ],
      "metadata": {
        "id": "vpARrdu-lP2S"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Automatic LLM evaluation of generated answer"
      ],
      "metadata": {
        "id": "6bpcF8ZuohC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this scoring prompt can be freely adapted to evaluation criteria\n",
        "# of different use-cases\n",
        "\n",
        "instruction_judge_answer = \"\"\"\\\n",
        "Your task is to score the quality of an answer to a question in a given context.\n",
        "\n",
        "Your scoring criteria for assessing the answer are:\n",
        "- pertinence: Does the answer directly answer the question?\n",
        "- context grounding: Is the answer clearly grounded in the context? To be well grounded, the answer does not need to explicitly reference the context.\n",
        "- conciseness: Is the answer concise without unnecessary verbosity?\n",
        "\n",
        "Your quality score should be in the range of 0 to 100.\\\n",
        "100 means a very good answer, 0 means a very bad answer, 50 means a mediocre answer.\n",
        "\n",
        "First briefly reason step-by-step to assess the extent to which the answer fulfills these criteria. Your reasoning should be short.\n",
        "Then return the quality score.\n",
        "\n",
        "Always answer in this JSON evaluation format: {{\"reason\": \"...\", \"score\": \"...\"}}\n",
        "\n",
        "context: {context}\\n\n",
        "question: {question}\\n\n",
        "answer: {answer}\\n\n",
        "JSON evaluation: \"\"\"\n",
        "\n",
        "instruction_judge_answer = ChatPromptTemplate.from_template(instruction_judge_answer)\n",
        "\n",
        "# currently need to use OAI here, because it enforces JSON very well\n",
        "llm_evaluation = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "chain = instruction_judge_answer | llm_evaluation\n",
        "\n",
        "\n",
        "output_quality_lst = []\n",
        "for answer, question, context_retrieved in zip(answer_lst, questions_lst, context_retrieved_lst):\n",
        "\n",
        "  output_quality = chain.invoke({\n",
        "      \"context\": context_retrieved,\n",
        "      \"question\": question,\n",
        "      \"answer\": answer\n",
        "  })\n",
        "\n",
        "  output_quality_lst.append(output_quality.content)\n",
        "\n"
      ],
      "metadata": {
        "id": "NEDmoocl7xzS"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parsing the JSON output can lead to errors\n",
        "# with open-source models, which don't enforce JSON as well as OAI\n",
        "import ast\n",
        "\n",
        "output_quality_dic = [ast.literal_eval(output) for output in output_quality_lst]\n",
        "output_quality_score = [int(dic[\"score\"]) for dic in output_quality_dic]\n",
        "output_quality_reason = [dic[\"reason\"] for dic in output_quality_dic]\n"
      ],
      "metadata": {
        "id": "eIAgcti6JfAv"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results"
      ],
      "metadata": {
        "id": "OcAmsd5YVqq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_results = pd.DataFrame({\n",
        "    \"question\": questions_lst,\n",
        "    \"answer\": answer_lst,\n",
        "    \"answer_quality_score\": output_quality_score,\n",
        "    \"answer_quality_reason\": output_quality_reason,\n",
        "    \"correct_context\": [a == b for a, b in zip(context_for_q_generation, context_retrieved_lst)],\n",
        "    \"context_retrieved\": context_retrieved_lst,\n",
        "    \"context_for_q_generation\": context_for_q_generation\n",
        "})\n",
        "\n",
        "mean_answer_score = df_results[\"answer_quality_score\"].mean()\n",
        "retrieval_accuracy = sum(df_results[\"correct_context\"]) / len(df_results[\"correct_context\"])\n",
        "\n",
        "print(f\"Retrieval accuracy: {retrieval_accuracy}\")\n",
        "print(f\"Mean answer socre: {mean_answer_score}\")\n",
        "print(\"\\n\")\n",
        "\n",
        "df_results\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1819
        },
        "id": "Glmejky1I2Uv",
        "outputId": "c708728d-8cfb-4249-e913-f49ee20d30aa"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieval accuracy: 0.8\n",
            "Mean answer socre: 90.0\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            question  \\\n",
              "0  \\nWhat is the role of Information Technology a...   \n",
              "1  \\n\"What percentage of engineers say they would...   \n",
              "2  \\nWhat is the recommended approach for ensurin...   \n",
              "3  \\n\\nwhat is the legal stance of deployers rega...   \n",
              "4  \\n\\nWhat is the EU's stance on regulating AI, ...   \n",
              "\n",
              "                                              answer  answer_quality_score  \\\n",
              "0  \\nAccording to Professor Ruberti, the roles of...                    70   \n",
              "1  48% of engineers say they would benefit from t...                    80   \n",
              "2  \\nThe recommended approach for ensuring the pr...                   100   \n",
              "3  \\n\\nDeployers in the given context can often a...                   100   \n",
              "4  \\n\\nThe EU's stance on regulating AI is to all...                   100   \n",
              "\n",
              "                               answer_quality_reason  correct_context  \\\n",
              "0  The answer directly answers the question by st...             True   \n",
              "1  The answer directly answers the question and i...             True   \n",
              "2  The answer directly answers the question by st...            False   \n",
              "3  The answer directly answers the question by st...             True   \n",
              "4  The answer directly answers the question by ex...             True   \n",
              "\n",
              "                                   context_retrieved  \\\n",
              "0  access to school-made teaching material coveri...   \n",
              "1  got materials engineering that’s coming in. An...   \n",
              "2  including rules on the burden of proof, should...   \n",
              "3  deployer can often absolve themselves for any ...   \n",
              "4  should allow the benefits of AI to be realized...   \n",
              "\n",
              "                            context_for_q_generation  \n",
              "0  access to school-made teaching material coveri...  \n",
              "1  got materials engineering that’s coming in. An...  \n",
              "2  wymogów należy ograniczyć do zastosowań SI wys...  \n",
              "3  deployer can often absolve themselves for any ...  \n",
              "4  should allow the benefits of AI to be realized...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a26045fa-0028-4aab-9c62-8e0faa1dce8d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>answer_quality_score</th>\n",
              "      <th>answer_quality_reason</th>\n",
              "      <th>correct_context</th>\n",
              "      <th>context_retrieved</th>\n",
              "      <th>context_for_q_generation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\\nWhat is the role of Information Technology a...</td>\n",
              "      <td>\\nAccording to Professor Ruberti, the roles of...</td>\n",
              "      <td>70</td>\n",
              "      <td>The answer directly answers the question by st...</td>\n",
              "      <td>True</td>\n",
              "      <td>access to school-made teaching material coveri...</td>\n",
              "      <td>access to school-made teaching material coveri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\\n\"What percentage of engineers say they would...</td>\n",
              "      <td>48% of engineers say they would benefit from t...</td>\n",
              "      <td>80</td>\n",
              "      <td>The answer directly answers the question and i...</td>\n",
              "      <td>True</td>\n",
              "      <td>got materials engineering that’s coming in. An...</td>\n",
              "      <td>got materials engineering that’s coming in. An...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\\nWhat is the recommended approach for ensurin...</td>\n",
              "      <td>\\nThe recommended approach for ensuring the pr...</td>\n",
              "      <td>100</td>\n",
              "      <td>The answer directly answers the question by st...</td>\n",
              "      <td>False</td>\n",
              "      <td>including rules on the burden of proof, should...</td>\n",
              "      <td>wymogów należy ograniczyć do zastosowań SI wys...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\\n\\nwhat is the legal stance of deployers rega...</td>\n",
              "      <td>\\n\\nDeployers in the given context can often a...</td>\n",
              "      <td>100</td>\n",
              "      <td>The answer directly answers the question by st...</td>\n",
              "      <td>True</td>\n",
              "      <td>deployer can often absolve themselves for any ...</td>\n",
              "      <td>deployer can often absolve themselves for any ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\\n\\nWhat is the EU's stance on regulating AI, ...</td>\n",
              "      <td>\\n\\nThe EU's stance on regulating AI is to all...</td>\n",
              "      <td>100</td>\n",
              "      <td>The answer directly answers the question by ex...</td>\n",
              "      <td>True</td>\n",
              "      <td>should allow the benefits of AI to be realized...</td>\n",
              "      <td>should allow the benefits of AI to be realized...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a26045fa-0028-4aab-9c62-8e0faa1dce8d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a26045fa-0028-4aab-9c62-8e0faa1dce8d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a26045fa-0028-4aab-9c62-8e0faa1dce8d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-138f7f6c-ce74-402a-8048-cfccee042953\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-138f7f6c-ce74-402a-8048-cfccee042953')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-138f7f6c-ce74-402a-8048-cfccee042953 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ce3a2b0a48e3489a8057eef410d90250": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_63d4e7a3504c43909e64706c8f241253",
              "IPY_MODEL_5105b22d98ba4be79a1278a44b797bb7",
              "IPY_MODEL_c055cc2e695046bba2403dfe05eca6d2"
            ],
            "layout": "IPY_MODEL_47da0a0159de4794997b7f494bf37607"
          }
        },
        "63d4e7a3504c43909e64706c8f241253": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5fc3b0837ce847f1b686db5532fd0761",
            "placeholder": "​",
            "style": "IPY_MODEL_48632e2349a34761b598315bab5e069f",
            "value": "100%"
          }
        },
        "5105b22d98ba4be79a1278a44b797bb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_432ab0fbec124bdfb995c9b230f5f4b1",
            "max": 440,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b20e369efab04023933e18e56eea9cec",
            "value": 440
          }
        },
        "c055cc2e695046bba2403dfe05eca6d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb3d5cd96d324dc1a5321c0ec179754a",
            "placeholder": "​",
            "style": "IPY_MODEL_dcf0878e0482457ebd897c0fb60f5089",
            "value": " 440/440 [00:30&lt;00:00, 28.90it/s]"
          }
        },
        "47da0a0159de4794997b7f494bf37607": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5fc3b0837ce847f1b686db5532fd0761": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48632e2349a34761b598315bab5e069f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "432ab0fbec124bdfb995c9b230f5f4b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b20e369efab04023933e18e56eea9cec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cb3d5cd96d324dc1a5321c0ec179754a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dcf0878e0482457ebd897c0fb60f5089": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}