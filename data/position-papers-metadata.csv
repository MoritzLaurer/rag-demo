Reference;Feedback date;Language;User type;First name;Surname;Scope;Organisation name;Transparency register number;Organisation size;Country;Publication privacy settings;"In your opinion, how important are the six actions proposed in section 4 of the White Paper on AI (1-5: 1 is not important at all, 5 is very important)?
: Working with Member states";"In your opinion, how important are the six actions proposed in section 4 of the White Paper on AI (1-5: 1 is not important at all, 5 is very important)?
: Focussing the efforts of the research and innovation community";"In your opinion, how important are the six actions proposed in section 4 of the White Paper on AI (1-5: 1 is not important at all, 5 is very important)?
: Skills";"In your opinion, how important are the six actions proposed in section 4 of the White Paper on AI (1-5: 1 is not important at all, 5 is very important)?
: Focus on SMEs";"In your opinion, how important are the six actions proposed in section 4 of the White Paper on AI (1-5: 1 is not important at all, 5 is very important)?
: Partnership with the private sector";"In your opinion, how important are the six actions proposed in section 4 of the White Paper on AI (1-5: 1 is not important at all, 5 is very important)?
: Promoting the adoption of AI by the public sector";"Are there other actions that should be considered?
";"In your opinion, how important is it in each of these areas to align policies and strengthen coordination as described in section 4.A of the White Paper (1-5: 1 is not important at all, 5 is very important)?
: Strengthen excellence in research";"In your opinion, how important is it in each of these areas to align policies and strengthen coordination as described in section 4.A of the White Paper (1-5: 1 is not important at all, 5 is very important)?
: Establish world-reference testing facilities for AI";"In your opinion, how important is it in each of these areas to align policies and strengthen coordination as described in section 4.A of the White Paper (1-5: 1 is not important at all, 5 is very important)?
: Promote the uptake of AI by business and the public sector";"In your opinion, how important is it in each of these areas to align policies and strengthen coordination as described in section 4.A of the White Paper (1-5: 1 is not important at all, 5 is very important)?
: Increase the financing for start-ups innovating in AI";"In your opinion, how important is it in each of these areas to align policies and strengthen coordination as described in section 4.A of the White Paper (1-5: 1 is not important at all, 5 is very important)?
: Develop skills for AI and adapt existing training programmes";"In your opinion, how important is it in each of these areas to align policies and strengthen coordination as described in section 4.A of the White Paper (1-5: 1 is not important at all, 5 is very important)?
: Build up the European data space";"Are there other areas that that should be considered?
";"In your opinion how important are the three actions proposed in sections 4.B, 4.C and 4.E of the White Paper on AI (1-5: 1 is not important at all, 5 is very important)?
: Support the establishment of a lighthouse research centre that is world class and able to attract the best minds";"In your opinion how important are the three actions proposed in sections 4.B, 4.C and 4.E of the White Paper on AI (1-5: 1 is not important at all, 5 is very important)?
: Network of existing AI research excellence centres";"In your opinion how important are the three actions proposed in sections 4.B, 4.C and 4.E of the White Paper on AI (1-5: 1 is not important at all, 5 is very important)?
: Set up a public-private partnership for industrial research";"Are there any other actions to strengthen the research and innovation community that should be given a priority?
";"In your opinion, how important are each of these tasks of the specialised Digital Innovation Hubs mentioned in section 4.D of the White Paper in relation to SMEs (1-5: 1 is not important at all, 5 is very important)?
: Help to raise SME’s awareness about potential benefits of AI";"In your opinion, how important are each of these tasks of the specialised Digital Innovation Hubs mentioned in section 4.D of the White Paper in relation to SMEs (1-5: 1 is not important at all, 5 is very important)?
: Provide access to testing and reference facilities";"In your opinion, how important are each of these tasks of the specialised Digital Innovation Hubs mentioned in section 4.D of the White Paper in relation to SMEs (1-5: 1 is not important at all, 5 is very important)?
: Promote knowledge transfer and support the development of AI expertise for SMEs";"In your opinion, how important are each of these tasks of the specialised Digital Innovation Hubs mentioned in section 4.D of the White Paper in relation to SMEs (1-5: 1 is not important at all, 5 is very important)?
: Support partnerships between SMEs, larger enterprises and academia around AI projects";"In your opinion, how important are each of these tasks of the specialised Digital Innovation Hubs mentioned in section 4.D of the White Paper in relation to SMEs (1-5: 1 is not important at all, 5 is very important)?
: Provide information about equity financing for AI startups";"Are there any other tasks that you consider important for specialised Digital Innovations Hubs?
";"In your opinion, how important are the following concerns about AI (1-5: 1 is not important at all, 5 is very important)?
: AI may endanger safety";"In your opinion, how important are the following concerns about AI (1-5: 1 is not important at all, 5 is very important)?
: AI may breach fundamental rights (such as human dignity, privacy, data protection, freedom of expression, workers' rights etc.)";"In your opinion, how important are the following concerns about AI (1-5: 1 is not important at all, 5 is very important)?
: The use of AI may lead to discriminatory outcomes
";"In your opinion, how important are the following concerns about AI (1-5: 1 is not important at all, 5 is very important)?
: AI may take actions for which the rationale cannot be explained";"In your opinion, how important are the following concerns about AI (1-5: 1 is not important at all, 5 is very important)?
: AI may make it more difficult for persons having suffered harm to obtain compensation";"In your opinion, how important are the following concerns about AI (1-5: 1 is not important at all, 5 is very important)?
: AI is not always accurate";"Do you have any other concerns about AI that are not mentioned above? Please specify:
";Do you think that the concerns expressed above can be addressed by applicable EU legislation? If not, do you think that there should be specific new rules for AI systems?;Other, please specify;If you think that new rules are necessary for AI system, do you agree that the introduction of new compulsory requirements should be limited to high-risk applications (where the possible harm caused by the AI system is particularly high)?;"Other, please specify:
";Do you agree with the approach to determine “high-risk” AI applications proposed in Section 5.B of the White Paper?;"Other, please specify:
";"If you wish, please indicate the AI application or use that is most concerning (“high-risk”) from your perspective:
";In your opinion, how important are the following mandatory requirements of a possible future regulatory framework for AI (as section 5.D of the White Paper) (1-5: 1 is not important at all, 5 is very important)?: The quality of training data sets;In your opinion, how important are the following mandatory requirements of a possible future regulatory framework for AI (as section 5.D of the White Paper) (1-5: 1 is not important at all, 5 is very important)?: The keeping of records and data;In your opinion, how important are the following mandatory requirements of a possible future regulatory framework for AI (as section 5.D of the White Paper) (1-5: 1 is not important at all, 5 is very important)?: Information on the purpose and the nature of AI systems;In your opinion, how important are the following mandatory requirements of a possible future regulatory framework for AI (as section 5.D of the White Paper) (1-5: 1 is not important at all, 5 is very important)?: Robustness and accuracy of AI systems;In your opinion, how important are the following mandatory requirements of a possible future regulatory framework for AI (as section 5.D of the White Paper) (1-5: 1 is not important at all, 5 is very important)?: Human oversight;In your opinion, how important are the following mandatory requirements of a possible future regulatory framework for AI (as section 5.D of the White Paper) (1-5: 1 is not important at all, 5 is very important)?: Clear liability and safety rules;In addition to the existing EU legislation, in particular the data protection framework, including the General Data Protection Regulation and the Law Enforcement Directive, or, where relevant, the new possibly mandatory requirements foreseen above (see question above), do you think that the use of remote biometric identification systems (e.g. face recognition) and other technologies which may be used in public spaces need to be subject to further EU-level guidelines or regulation:;Please specify your answer:;Do you believe that a voluntary labelling system (Section 5.G of the White Paper) would be useful for AI systems that are not considered high-risk in addition to existing legislation?;"Do you have any further suggestion on a voluntary labelling system?
";What is the best way to ensure that AI is trustworthy, secure and in respect of European values and rules?;"Please specify any other enforcement system:
";"Do you have any further suggestion on the assessment of compliance?
";"The current product safety legislation already supports an extended concept of safety protecting against all kind of risks arising from the product according to its use. However, which particular risks stemming from the use of artificial intelligence do you think should be further spelled out to provide more legal certainty?

";"In your opinion, are there any further risks to be expanded on to provide more legal certainty?
";Do you think that the safety legislative framework should consider new risk assessment procedures for products subject to important changes during their lifetime?;"Do you have any further considerations regarding risk assessment procedures?
";Do you think that the current EU legislative framework for liability (Product Liability Directive) should be amended to better cover the risks engendered by certain AI applications?;"Do you have any further considerations regarding the question above?
";"Do you think that the current national liability rules should be adapted for the operation of AI to better ensure proper compensation for damage and a fair allocation of liability? 
";Please specify the AI applications:;Do you have any further considerations regarding the question above?;"You can upload a document here:

"
F529892;19-06-2020 23:58;English;Academic/Research Institution;Adrien;ABECASSIS;;Governance of AI Research Group;;Micro (< 10 employees);United States;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;See written response;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;See written response;5 - Very important;5 - Very important;5 - Very important;See written response;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;See written response;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;See written response;There is a need for a new legislation;;Other;See written response;;;See written response;3 - Neutral;3 - Neutral;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;;Collective risks ans externalities - See written response;Yes;See written response;Yes;;Yes, for all AI applications;;See written response;Governance_of_AI_Research_Group_EU_Commission_AI_White_Paper_Consultation.pdf
F529891;19-06-2020 22:38;English;Academic/Research Institution;Alejandro;Saucedo;;European Technology Policy Committee (EUTPC) of the Association for Computing Machinery;133002517679-87;Large (250 or more);United States;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important; Yes. Working with general public is quite key. Also collaboration and partnership with organisations that represent professionals and other type of individuals would be quite key. These could encompass organisations like the ACM, but also standard bodies like the ISO, and open source foundations like the Linux Foundation and the NumFocus (which are the governing body of several key open source AI toolsets).;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important; It will be quite important to build the financial infrastructure to be able to back high-risk ventures given both the current economic climate together with the high risk nature of advanced research-based and highly innovative companies. If the ecosystem is not supportive towards SMEs and startups taking big leaps with high reward high risk opportunities the opportunity for innovations to come from Europe will be reduced.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Cybersecurity concerns will be quite critical as new emerging technologies are introduced in production systems, new loopholes may also be introduced. The skills and best practices required to mitigate these risks will be critical to ensure high profile and high negative impact incidents don’t take place due to security flaws of machine learning systems being exploited. ;No opinion;;Other;The compulsory requirements, and more importantly the level of scrutiny required should certainly be proportionate to the risk involved as suggested, however there should still be a compulsory step / process that must ensure this is evaluated upfront, and reasonable explanation can be provided (retrospectively) in the case it’s decided the more involved “compulsory processes” are not required.;;;;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Other special requirements in addition to those mentioned in the question above should be imposed (please specify);" Facial Recognition is an area of AI technology where ADM (automated decision making) takes place about humans. It is therefore essential to understand the basic nature of ADM.
- ADM is never neutral.
- The creators of ADM processes are responsible for their results. ADM processes are not only created by their creators.
ADM processes must be traceable so that they can be subjected to democratic control.
- Democratic societies have a duty to create this accountability through a combination of technology, regulation and appropriate oversight institutions.
- We must decide how much of our freedom we want to transfer to ADM, and these discussions should be carried out through forums that provide the public, including minorities, a voice.

The ACM Europe TPC are currently working on a position statement on Facial Recognition systems which we would be interested to share in more detail with the European Commission.";No opinion;Data labelling for AI data is often a task that may require specialised expert domain knowledge, or at the minimum structure control and processes (depending on what the exploratory or predictive analysis task is). Therefore a voluntary labelling system may not uphold the required standards required to ensure quality data, especially in use cases where data labelling would require a much larger upfront investment. Instead, having other ways where datasets could be scrutinised directly.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Although the answer provided is “a combination of ex-ante compliance and ex-post enforcement mechanisms”, this is something that would require industry- and even use-case-specific context requirements. In this situation industry standards, and consequently standardisation bodies, would play a key role, and should be engaged accordingly to ensure relevant and reasonable assessments processes and surveillance processes are established both internally and by external organisations.;Mental health risks;The risks mentioned above are certainly critical, primarily due to the yet un-certain risks that AI poses in society and the individual. Particularly the mental health risks of the impact of these technologies is not yet fully understood, and this has led to loopholes in old regulatory frameworks to be exploited for corporate or personal benefit. It is critical to ensure mental health risks are one critical risk factor to be addressed and taken into consideration or production AI systems.;No opinion;;No opinion;;No opinion;;;
F529890;19-06-2020 21:54;English;Other;Jan-Philipp;BECK;;EIT Health e.V.;632741433403-20;Medium (< 250 employees);Germany;The feedback can be published with your personal information;5 - Very important;4 - Important;5 - Very important;3 - Neutral;4 - Important;5 - Very important;"1. AI benchmarking;
2. Developing sectoral guidance adapted to existing legal and risk management frameworks, i.c. for health & healthy ageing.
See also the attached paper of the EIT Health Consultative Group.";4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;;"1. Making use of European innovation programs (i.c. EIT Health) to test and validate implementation
guidance for existing regulation (such as Medical Device Regulation)
2. Making use of European innovation programs (i.c. EIT Health) to test and validate codes of conduct and other forms of self-regulation or anticipatory regulation
3. developing guidance and best practices for collaboration in the AI ecosystems, such as between small and large companies and between AI innovators and AI users.";4 - Important;5 - Very important;3 - Neutral;"1. Support exchange of experiences and development of guidance for responsible and fast innovation in
'innovation-regulatory sandboxes'.
2. Tune ethics guidance for R&I to sectoral realities, notably for health, addressing the balance between
individual health and public health interests
3. Develop GDPR guidance and promote harmonised GDPR choices (including national implementation)
with a view to enabling R&I.
See also the attached paper of the EIT Health Consultative Group.";5 - Very important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;Provide assistance and financing for (pre-)certification of AI-based solutions, for exampke for compliance with the Medical Devices Regulation. See the attached paper of the EIT Health Consultative Group with concrete suggestions.;4 - Important;4 - Important;4 - Important;5 - Very important;3 - Neutral;5 - Very important;"1. In particular in health human oversight remains necessary, for accuracy, to protect autonomy of patients and of health professionals, and to preempt manipulation.
2. Nevertheless, AI brings critical, life-saving benefits without full transparency at the level of the algorithms.
Therefore, promote realistic expectations about AI transparency, and rather emphasise explainability. Provide guidance and where necessary legal interpretation to allow for responsible explainability.";Other;Current legislation such as Medical Device Regulation needs clarification, interpretation, and ultimately possibly adaptation. New approaches such as preCert (FDA/USA) should be tried out. See the attached paper of the EIT Health Consultative Group with concrete suggestions.;Yes;;Yes;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;No opinion;;Much;Inspiration can be obtained from the German Datenethik Kommission. Labelling should in first instance at least remain lightweight, notably for SMEs. Labelling should be critically monitored for truthfulness and relevance.;Other enforcement system;In a number of cases compliance is a legal requirement of existing law and will involve competent authorities and certification bodies (i.e. self-assessment is not sufficient, e.g for class IIa/IIb/III medical devices).;See the attached paper of the EIT Health Consultative Group with suggestions for assessment.;Mental health risks;Product safety is not (at European level) about services such as health services of which beyond telehealth and mHealth there will be ever more. This needs to be urgently clarified. See also the attached paper of the EIT Health Consultative Group.;Yes;"However, notably in the field of health it would be a mistake to ignore the existing and extensive ex-ante and post-market risk assessment procedures and well-developed ways of monitoring, reporting, and accountability. So first start adapting those risk management practices (several also implement regulatory requirements) before coming up with new ones.
See also the attached paper of the EIT Health Consultative Group which discusses this in extenso.";Yes;"1. Address services and software.
2. Ensure harmonisation across the EU.";No opinion;;Certainly to promote harmonisation.;EIT_Health_Consultative_Group_on_EC_Data_Strategy_and_AI_White_Paper_-_31_May_2020_FINAL.pdf
F529889;19-06-2020 17:58;Polish;Public authority;Malgorzata;Pek;National;on behalf of: Chairman of the National Broadcasting Council of Poland;;Medium (< 250 employees);Poland;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;feedback_Consultation_on_the_White_Paper_on_Artificial_Intelligence.docx
F529888;19-06-2020 17:17;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;Please refer to our position paper in the attachment, which includes additional suggestions for actions.;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;Please refer to our position paper in the attachment, which includes additional suggestions for actions and additional areas to be considered.;4 - Important;5 - Very important;5 - Very important;Please refer to our position paper in the attachment, which includes additional suggestions for actions and additional areas to be considered.;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;Please refer to our position paper in the attachment, which includes additional suggestions for actions and additional areas to be considered.;4 - Important;4 - Important;3 - Neutral;4 - Important;4 - Important;5 - Very important;Please refer to our position paper in the attachment, which includes additional suggestions for actions and additional areas to be considered.;Other;The majority of DIGITAL SME’s members thought that there may be some gaps in current legislation to address the concerns listed under the point above or that there may be a need for new legislation. At the same time, they expressed weariness about overregulation, and many identified this as an important risk. ;No opinion;;;;"•	Autonomous vehicles and AI software applications in medicine and healthcare, if not directly supervised by humans.	
•	Legal AI and Health AI can have the most short-term-identifiable high-risk impact.	
•	Health insurance.
•	At the same time, while those areas are high risk, there can also be a high reward: For instance, autonomous vehicles can help reduce CO2 emissions.
•	Need to consider also high-risk in terms of competition and dominance.
";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Other special requirements in addition to those mentioned in the question above should be imposed (please specify);Answers from DIGITAL SME members were contradictory in this matter. Some thought that no further guidelines or regulations are needed and some thought that use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place. Others stated that special requirements in addition to those mentioned in the question above should be imposed but did not specify in detail.;No opinion;Our membership had divergent opinions. While it can be useful, there may be bureaucratic hurdles and such a system could be useless if only voluntary. This requires further discussion on EU level involving different stakeholder groups, in particular SMEs. ;A combination of ex-ante compliance and ex-post enforcement mechanisms;;The combined answers of DIGITAL SME’s members led to the conclusion that a combination of ex-ante compliance and ex-post enforcement mechanisms is the best way to ensure that AI is trustworthy, secure and in respect of European values and rules.;Risks related to the loss of connectivity;The majority of DIGITAL SME’s members thought that cyber risks and personal security risks should be further spelled out. Risks related to the loss of connectivity were also mentioned, while mental health risks were not mentioned.;Yes;The majority of DIGITAL SME’s members agree that safety legislative framework should consider new risk assessment procedures for products subject to important changes during their lifetime.;No opinion;DIGITAL SME’s had diverging views on whether the current EU legislative framework for liability (Product Liability Directive) should be amended to better cover the risks engendered by certain AI applications. ;Yes, for specific AI applications;DIGITAL SME members agreed that the current national liability rules should be adapted for the operation of AI to better ensure proper compensation for damage and a fair allocation of liability, either for all AI applications or for specific AI applications (no agreement on the focus).; Please refer to our position paper in the attachment, which includes additional suggestions for actions and additional areas to be considered.;DIGITAL_SME_Position_Paper_AI_White_Paper_FINAL_DRAFT.pdf
F529887;19-06-2020 17:06;English;Public authority;MARIA;ALVAREZ;National;National Commission on Markets and Competition (CNMC) and Catalan Competition Authority (ACCO);;Large (250 or more);Spain;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;CNMC_S_AND_ACCO_S_JOINT_CONTRIBUTION__TO_THE_PUBLIC_CONSULTATION_ON_AI.pdf
F529886;19-06-2020 17:00;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;No opinion;4 - Important;No opinion;No opinion;No opinion;;4 - Important;4 - Important;3 - Neutral;3 - Neutral;3 - Neutral;4 - Important;;4 - Important;4 - Important;3 - Neutral;Social impact research. How is the deployment of AI shaping individual citizens and our societies? Are we okay with that?;No opinion;No opinion;No opinion;No opinion;No opinion;;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;"AI will no doubt influence the way our society works and further develops. That's why we are calling for a comprehensive impact assessment for all AI but also for an ongoing dialogue with society. Are we on the right path and where is this leading us?
How to ensure that we leave no one behind?";There is a need for a new legislation;;No;;;;biometric mass surveillance;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Not at all;Rather than a voluntary system, we call for a comprehensive impact assessment for all AI deployment.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;As certain AI applications evolve over time, continuous monitoring and regular evaluation is important. ;Yes;;Yes, for all AI applications;;Clear guidance and regulation is good for both developers and users.;
F529885;19-06-2020 16:49;English;Business Association;Martin;Uhnak;;Slovak Alliance for Innovation Economy;;Micro (< 10 employees);Slovakia;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;3 - Neutral;;5 - Very important;4 - Important;4 - Important;;5 - Very important;4 - Important;5 - Very important;5 - Very important;4 - Important;;4 - Important;4 - Important;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;;Current legislation may have some gaps;;Other;"See attached joint position of 15 organizations and associations from 10
CEE countries.";;;;3 - Neutral;3 - Neutral;5 - Very important;3 - Neutral;5 - Very important;3 - Neutral;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);;Rather not;self regulation;Other enforcement system;"Ex ante conformity assessment requirements as recommended by the White Paper do
not strike the right balance. A combination of ex-ante risk self-assessment and
ex-post enforcement for high risk AI applications would likely achieve similar
results within much faster timeframes and without risking unduly stopping
innovation and creating unnecessary burdens. See attached joint position of 15
organizations and associations from 10 CEE countries.";;Mental health risks;;Yes;"The current liability framework remains fit for purpose, being both effective
and technology neutral, so sweeping changes are not needed. The long-standing
provisions of European liability law have worked well, and there has been no
showing of problems sufficient to warrant changing them and introducing the risk
of unintended consequences. See attached joint position of 15 organizations and
associations from 10 CEE countries.";No;"The current liability framework remains fit for purpose, being both effective
and technology neutral, so sweeping changes are not needed. The long-standing
provisions of European liability law have worked well, and there has been no
showing of problems sufficient to warrant changing them and introducing the risk
of unintended consequences. See attached joint position of 15 organizations and
associations from 10 CEE countries.";No;;;Joint_Position_on_AI_White_Paper_CEEcountries.pdf
F529884;19-06-2020 16:15;English;Academic/Research Institution;Mateja;Durovic;;King's College London TELOS Research Centre (Centre for Technology, Ethics, Law and Society);No;Medium (< 250 employees);United Kingdom;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;No;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;;No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Cyber risks;;Yes;;Yes;;Yes, for all AI applications;;;Edited_Responses_AI_White_Paper_Kings_final.docx
F529883;19-06-2020 14:53;English;Academic/Research Institution;Niek;BRUNSVELD;;"""Amsterdam: AI Technology for People”. We are a consortium of the knowledge institutes in Amsterdam and the City of Amsterdam. Together we form the largest science and innovation ecosystem in the Netherlands.";Unive7618063212;Large (250 or more);Netherlands;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;The Amsterdam AI consortium fully supports the 6 actions proposed under section 4. but points out that there is a hugely disproportionate imbalance between sections 4 and 5. This limited focus on AI excellence we also see in section 3 where the role of low-power electronics and quantum computing get far more attention than the excellence in algorithms, the core of modern AI. Also, it is vital to allocate much more attention to skills/skilled staff so as to help attract, train and retain talent.;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;The Consortium welcomes the development through the European data strategy and the investment through the Digital Europe Programme to support high-performance and quantum computing. However, there is a tangible need to coordinate policies on the further development of AI-relevant infrastructure such as a compute infrastructure with clusters of many GPU enabled nodes, a data exchange infrastructure to facilitate secure and fair sharing of data and a local knowledge sharing infrastructure.;3 - Neutral;5 - Very important;5 - Very important;Key focus in this section should be the lack of talent and on connecting existing AI research excellence centers, emphasizing each center’s key strengths (e.g. in Amsterdam: Deep learning, hybrid intelligence, and responsible AI), creating synergy through network events and exchange programs for talents at all levels, and jointly address the major challenges the world is facing by connecting the excellence in various fields of AI, each taking advantage of their own local ecosystem.;4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;In identifying national Digital Innovation Hubs, the EC should take into account the hub’s surrounding business environment which needs to consist of regional, international, SMEs, as well as knowledge and other AI-related institutes which have a close connection to AI excellence research centres. Indeed, the Amsterdam region builds on 3 decades of AI-research and offers an ideal environment here, in particular for the service industry, given its long tradition of public-private partnerships.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;The Amsterdam IA consortium argues that the fact that technology can result in concerns does not mean we should not develop it, but that we need to find ways of taking away these concerns, either through better technologies, or new legal safeguards. The consortium, therefore, reiterates the imbalance between sections 4 (ecosystem of excellence) and 5 (ecosystem of trust). The consortium argues for at least as much focus on section 4 as section 5 for Europe to truly become a world leader in AI.;There is a need for a new legislation;;Yes;;Yes;;A particularly high-risk concern associated with AI is that of function creep, the gradual widening of the use of a technology or system beyond the purpose for which it was originally intended. The risks are in the infringement, not the technology. Thus distinguishing between high risk and low risk AI applications is difficult. To provide legal certainty and a focused approach, a definition of high risk is required that matches existing definitions of risk in terms of severity and likelihood.;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;Creating permanent surveillance infrastructures is at odds with a commitment to strong fundamental rights, such as the right to privacy or to assembly, and invites abuse and function creep.;Rather not;As experience from e.g. the field of food labeling shows, a labeling system to be effective requires considerable investment in communication, standardization and consistency.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Combining ex-ante compliance and ex-post enforcement mechanisms for high risk applications allows innovation to thrive -in a trusted manner- and AI to be deployed in a timely way (without undue delay due to burdensome compliance procedures). At the early stages of AI development, it is often difficult to foresee the end result, and, thus, it is necessary to allow for secure piloting of AI applications before any conformity assessment, limiting EU researchers and SMEs to innovate and invest.;Mental health risks;Effective enforcement of product safety in complex product chains.;Yes;Many of the existing provisions depart from the idea of a ready-made product. As explained above, AI applications can be subject to changes also after the product or application have been deployed. More flexible and re-occuring risk assessment procedures should cater to that fact.;Yes;As this is a fast moving technology with continuously new applications, we believe that what is needed is continuous monitoring of the existing rules and their application to AI application, to be able to ascertain that the current frameworks still hold. One way of doing so, is by finding shorter and more agile development cycles, also for the law.;Yes, for all AI applications;;As this is a fast moving technology with continuously new applications, we believe that what is needed is continuous monitoring of the existing rules and their application to AI application, to be able to ascertain that the current frameworks still hold. One way of doing so, is by finding shorter and more agile development cycles, also for the law.;Amsterdam_AI_Technology_for_People_POSITION_PAPER.pdf
F529882;19-06-2020 13:57;English;;;;National;;;;;The feedback can be published in an anonymous way;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;We would like to stress that our contribution will be submitted in the form uploaded below.;DK_Specific_Comments_on_the_AI-White_Paper.pdf
F529881;18-06-2020 23:59;English;EU Citizen;Frederike;Kaltheuner;;;;;Germany;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;4 - Important;5 - Very important;5 - Very important;3 - Neutral;4 - Important;4 - Important;"There are applications and uses of AI systems that are fundamentally incompatible with fundamental rights.

While AI is not always accurate, the technology also has inherent limitations (that cannot be mitigated through safeguards), which means its use is not always justified or even appropriate.";There is a need for a new legislation;;Other;"Current EU legislation leaves gaps, especially when it comes to non-discrimination law (see CoE report). Where current legislation is  applicable in theory, it is often not in practice due to (1) a lack of enforcement and (2) specific characteristics of AI technologies. To address (1), regulators need sufficient funding, technical expertise to enforce existing laws. New rules are needed to ensure that AI (2) is responsible, auditable & accountable, especially for fundamental rights violations.
";;;"The current definition of high-risk is too narrow, as it leaves out all ""high-risk"" applications of AI in ""low-risk"" sectors (AI-driven consumer products, data brokers and Ad-Tech industry, personalisation & recommender systems used by onlien platforms). Discrimination is a risk in all of these applications. For certain groups of people any application of AI, not just those considered “high-risk” comes with an inherent risk of discrimination and exclusion.";5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;"The use of such systems involves the processing of special category data, yet its deployment in public spaces makes explicit consent impractical, if not impossible. Further EU-level guidelines would be welcome, given that many EU Member States have introduced derogations with respect to restrictions on processing biometric data. Guidance on particularities of semi-public spaces (spaces that function like public spaces but are privately owned) would also be welcome.
";Not at all;"Given that the current definiton of high-risk is exceptionally narrow and excludes high risk applications in low-risk sectors, voluntary labeling is unsufficient. Another shortcoming of labelling is the connected, dynamic and open nature of many AI applications, combined with the fact that they  are  often integrated  in complex  IoT  environments where  many  different connected devices  and  services interact. The burden cannot be on the consumer to fully understand and mitigate risks.

";A combination of ex-ante compliance and ex-post enforcement mechanisms;;Ex-post enforcement is crucial because of the connected, autonomous and data-dependent nature of AI systems. AI applications can be updated, or changed after they’ve been purchased or deployed. AI can priduce unexpected outcomes, if the environment is deployed in is substantially different from the one it has been trained and tested on.  And finally, AI systems  can  also  ‘improve’  their  own  performance  by  learning  from  experience (with unintended or unexpected outcomes).;Mental health risks;"The report on the safety and liability implications of Artificial Intelligence, the Internet of Things and robotics only talks about mental health risk in the context of humanoid robotics and other forms of embodied AI. It doesn’t address the risks associated with the use of AI in the design of addictive or manipulative (online) environments. 
";Yes;The report assumes that it is only (and exclusively) the self-learning feature of AI products and systems enables the machine to take decisions that deviate from what was initially intended by the producers and consequently what is expected by the consumer. There are many other reasons why AI systems and applications can produce unintended ourcomes, for instance if application environment is different from testing and training environment.;Yes;;;;;Frederike_Kaltheuner_-_Submission_to_the_Consultation_on_the_AI_White_Paper.pdf
F529880;18-06-2020 20:25;English;Business Association;Helen;SMITH;;IMPALA;12383069253-19;Micro (< 10 employees);Belgium;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;IMPALA_contribution_-_White_Paper_on_AI_.pdf
F529879;18-06-2020 19:21;English;Company/Business organisation;Olga;Afanasjeva;;GoodAI Research;;Small (< 50 employees);Czech Republic;The feedback can be published with your personal information;No opinion;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;The WB admits Europe has a weak position in b-2-c. Nevertheless the b-2-c, not b-2-b applications, have paramount influence on our societies. The EC needs to support high potential start-ups towards development, testing and deployment of AI algorithms for scalable b-2-c solutions. The administrative burden needs to be reduced drastically. Otherwise high potential start-ups will not apply since the administrative burden dilutes their focus on the core research and deployment activities.;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;4 - Important;"If Europe aims for excellence in AI it needs not only to fund the fundamental research, promote take-up of AI solutions and support innovative start-ups using AI, but to move one step forward to support and fund research done by businesses seeking to develop and deploy high potential AI solutions. 
To assist in testing is crucial for AI safety, we STRONGLY support establishment of testing facilities. They should also allow for assessment of impacts as we outline the attached statement.";4 - Important;4 - Important;4 - Important;;3 - Neutral;5 - Very important;4 - Important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;4 - Important;4 - Important;3 - Neutral;4 - Important;"The problem definition is based mainly on qualitative description of potential risks. Data and evidence on each of identified risks should be added. This is also important in order to understand the potential negative impact related to identified risks.
The Commission should propose support structures in particular for SMEs which would assist in the assessment of impacts of AI and develop related methodologies. This would help the EC to identify, monitor, assess and analyse related risks.";Current legislation may have some gaps;;Yes;;Other;"Yes, but. The definition of significant risks needs to be further specified. This refers in particular to ""legal or similarly significant effects for the rights of an individual or a company"".
Moreover, the methodology for assessing ""impact on the affected parties"" for the purpose of risk level assessment needs to be detailed. A potential way forward is to use standards on well-being impact assessment of AI (IEEE P7010) and human rights impact assessment.";"AI systems that intentionally or unintentionally abuse human biases leading to manipulation (negative impact on human agency)
AI systems with impacts at a societal level, incl. social cohesion and democracy
AI systems causing addiction in particular in children, increasing loneliness and with other impacts on physical and mental health.
More attention needs to be devoted to testing the output in terms of impact assessment and to the causal links between the observed impacts and AI system. ";No opinion;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;No opinion;Biometric identification systems pose significant risks due to possible abuse. However, due insufficient knowledge of this legal area we are not in a position to comment. ;No opinion;Voluntary labelling systems pose an important opportunity to test various approaches to impact assessment and ways to provide information to users. However, due to high complexity of carrying out such assessments and establishing causal links between the AI systems and observed impacts, and also due to the high likelihood of trade-offs among various ethics requirements, voluntary labelling poses significant risks, e.g. related to “ethics-washing”.;;;;Mental health risks;;Yes;"As ""self-learning"" may lead to important changes of the systems over their lifetime, there may need to be developed a new approach how to clearly define boundaries in terms of uses, functions and other properties of AI systems.";No opinion;;No opinion;;;Response_to_the_Consultation_on_the_WP_on_AI.pdf
F529878;18-06-2020 18:13;English;Public authority;;Vasamo-Koskinen;National;Ministry of Economic Affairs and Employment of Finland;;Large (250 or more);Finland;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Finland supports the Commission's human-centred approach which emphasises trust, in which European artificial intelligence is based on the values of freedom, human dignity and privacy. Finland attaches importance to competitiveness and broad-based competence development, as demonstrated by the Elements of AI online course and AuroraAI program. Due attention should be given to risks associated with fundamental rights, such as protection of personal data and discrimination.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"Finland supports the Commission's objective of defining a common European approach that takes into account the interests of citizens, companies, municipalities and society, with the aim of avoiding the fragmentation of the internal market. 

Finland supports the policy instruments presented in the White Paper promoting investments in research and innovation and strengthening expertise. European data spaces should provide solid background for ethical and secured AI development.";5 - Very important;5 - Very important;5 - Very important;Finland supports the proposal to promote the establishment of centres of excellence and testing to increase synergies and networks between AI research centres, with the aim of promoting excellence, attracting and keeping the best researchers and developing the level of technology. In addition, efforts should be made to create and support flexible network-based structures based on cooperation between higher education institutions and working life.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;In addition to technological innovation, the DIHs should be able to promote business model innovation.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"AI systems must be developed and deployed in a human-centric manner. Equality of rights and prevention of exclusion has to be taken into consideration. 

It is important to take into account national security, national defence and the role of the Defence Administration in society as well as the special needs related to this role with regard to data management and production. The main differences and the links between the military and civilian sectors must be taken into account.";Other;"Current legislation may have some gaps;Regulatory environment must encourage innovations,support development and application of new technologies,business and services,comply with principles of human rights,rule of law and democracy,principle of better regulation and proportionality,ensure up to date,needbased,predictable and technology-neutral,top down approach,must define the roles,accountability and responsibilities and must not impose unnecessary regulatory burden on businesses and consumers";Yes;;Other;Finland supports in general the idea of defining the possible risks related to AI applications and managing those risks accordingly. Finland has reservations about the plan to appoint sectors that apply high risk artificial intelligence (such as transport and health) in advance. Neutral criteria should be drawn up for taking risks into account. Comprehensive framework should be established to address these risks, regarding the principle of proportionality and other principles of good governance.;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);The scope of possible new regulation must be clear. Responsibilities and requirements of different actors, powers of the authorities and adequate means for preventing and combating threats must be guaranteed.  It is necessary to carefully examine the operating area of artificial intelligence and to specify requirements, for example as regards facial recognition. Investigation of crimes and cyber security of critical functions in society must be secured.;Much;Finland welcomes the possibility of using self-regulation and sharing best practices such as voluntary labelling system.;Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;The global competitive situation requires that EU regulation is favourable and promotes the responsible development and application and investment of AI. It is likely that there already exists basic necessary procedures for many systems/applications that apply AI. New regimes for technical requirements might be needed, as well as testing and evaluation procedures.;;;;;;;;;Current liability regimes might serve quite well after establishing the roles of the new players (especially developers and deployers of AI systems) as well as the new roles of old players (like humans as end-users) and the accountability/responsibilities attached to these roles. On the EU level, it might be necessary to evaluate e.g. the Product Liability Directive.;
F529877;18-06-2020 17:45;English;Non-EU Citizen;Sara;Jordan;;;;;United States;The feedback can be published with your personal information;3 - Neutral;5 - Very important;4 - Important;5 - Very important;3 - Neutral;3 - Neutral;;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;4 - Important;4 - Important;;4 - Important;4 - Important;4 - Important;;3 - Neutral;4 - Important;4 - Important;3 - Neutral;3 - Neutral;;4 - Important;4 - Important;4 - Important;5 - Very important;3 - Neutral;5 - Very important;;Current legislation may have some gaps;;No;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Risks related to the loss of connectivity;;Yes;;No opinion;;Yes, for all AI applications;;;
F529876;18-06-2020 12:10;French;NGO (Non-governmental organisation);INSTITUT MACONNIQUE EUROPEEN;GLFF;;INSTITUT MACONNIQUE EUROPEEN / GRANDE LOGE FEMININE DE FRANCE;yes;Micro (< 10 employees);France;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;4 - Important;Imaginer un réseau d’observation et d’analyse IA sur les valeurs UE. Encourager le secteur public à la transparence et l’éthique. Développer l’esprit critique aux usages, bénéfices/risques. Dialogue usagers, pouvoirs publics, industriels, fournisseurs de contenus. Associer CivicTech. Evaluation systèmes.Usages libres, résultats PPP. Formation, renforcement capacités Femmes.Porter attention aux valeurs et stratégiesIA partagées.Règlement conflits dans le respectde l’humainet son environnement.;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;Harmonisation politiques  sociales, développement autonomie  recherche et pratique IA  dans le respect des libertés.Réduction fractures numériques. Création surveillance éthique type CNIL-UE, CR  régulier CE. Définition politiques communes plans agriculture, santé, culture, éducation, transport, défense et environnement, droit social et fiscal. Suppression biais cognitifs des algorithmes. Intégration éthique dans formation des   programmateurs. Interdiction actionnariat privé.;5 - Very important;5 - Very important;4 - Important;Rencontre annuelle UE avec acteurs IA. Garantir autonomie de la recherche fondamentale sur infrastructures IA avec flexibilité et implication directe des chercheurs. Promouvoir modèle Open data avec partage de résultats. Révéler conflits d’intérêts et promouvoir transparence générale dans les appels d’offres, les financement de projets et tests programmes IA. Prévoir brevet européen de protection du secteur public. Encourager la responsabilité des développeurs. ;3 - Neutral;4 - Important;4 - Important;5 - Very important;5 - Very important;Eviter confusion entre PME et Start-ups. Souplesse PME: y introduire IA selon besoins. Accès information PME aux pôles spécialisés IA pour dégager des applications Etats UE. Développer flexibilité en termes d’organisation et de changement des compétences. Favoriser mixité public/privé, TPE/Start ups/GE dans une UE régulatrice et nonfinanceur. Norme évaluation-validation. Gouvernance UE multi décisionnaire-ajustements IA avec société civile participative.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Face à l’essor IA-UE, anticiper réaction GAFA et Licornes. Mode d’emploi construction et usages IA ne garantit pas accès à tous. Information-formation IA femmes et jeunes. Suivi des inégalités territoriales et sociales. Encadrer pratiques et droit par une puissance juridique UE-IA reconnue par ONU.Prévention  risques déshumanisation  programmée, manipulation politique,exploitation vulnérabilités,modification comportements pour conformisme,atteintes liberté de pensée,orientations sexuelles. ;There is a need for a new legislation;;Yes;;Yes;;Attention à la hiérarchisation des critères cumulatifs IA sans alterner par secteur/application, à l’absence de contrôle et sanctions contraires aux valeurs démocratiques, à la création de lois sécuritaires dans un Etat d’exception banalisé, à une augmentation de capacités de rendement IA illimitée au plan sociétal et environnemental, à un code de déontologie sans exigences obligatoires hors domaines à haut risque. Il y a nécessité d’une Déclaration UE des droits de l’IA.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;En cas de guerre ou terrorisme dans les Etats sous régime d’exception et/ou de dérives portant préjudice aux libertés publiques. Lier reconnaissance faciale à la garantie des libertés et des sécurités. Obligation pour IA de rester un outil d’amélioration des rapports humains. Le traitement de données biométriques soulève des questions et génère de la défiance. Il faut susciter la confiance en renforçant le cadre juridique et éthique IA en vue de son appropriation citoyenne UE.;Very much;Label de conformité et de traçabilité pour applications liées aux services et produits hors UE mais utilisés dans UE type Label de confiance IMT de Paris. Attention label-argument marketing: toute validation d’éthique, de transparence, de maîtrise de fonctionnement dans le domaine de la recherche est illusoire. Exigence label et code juridique pour programmateurs IA et applications. Id Label de suivi Formation et Education à l’IAE en vue d’une solidarité plus accrue.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Surveillance externe neutre.Instaurer normes de respect conformité objectifs et finalités d’applications. Interdire spéculations en IA. Justifier avec IA réduction inégalités, consommation judicieuse énergie et ressources naturelles. Pédagogie sur enjeux et critères de choix IA pour une société durable.Créations: Agence UE de contrôle de conformité accessible à tous les citoyens «Whistle blowing», Code de justice européen IA, Agence citoyenne de validation mesures IA. ;Personal security risks;Risques: aggravation des inégalités, diffusion rapide fausses informations, risques de préjudices liés aux compétences et à l’emploi, sécurité sociale, au domaine médical, reprogrammation appareils domestiques, emprise cognitive, perte du libre arbitre, marchandisation du corps, rupture du lien social...Il faut définir une approche dommage-réparation. Etablir un code de justice avec contrôle en aval et amont. Protection acteurs et usagers transition IA.;Yes;Evaluation tout au long du cycle de vie du produit ou du robot. Etablissement d’une procédure adaptée à la nature et à la finalité des outils IA. Instauration d’une taxe éthique. Instaurer une veille permanente des institutionsUE face à la réactivité des outils IA et leur rapidité d'évolution. Renforcer au sein de l’UE la vigilance, la  protection, et l’indépendance totale face aux puissants lobbies. Instaurer une gouvernance d’encouragement d’échange de bonnes pratiques IA dans l’UE.;Yes;La question de la souveraineté des données et d’une réglementation UE complexe et parfois opaquese se pose. Il faut une mise à niveau des compétences et des moyens sur l’IA. Promouvoir l’éducation pour développer un esprit critique. Remettre l’humain au centre des décisions. Développer en permanence la communication, la formation et l’information sur l’IA.;Yes, for all AI applications;;Extension RGPD à tous usages IA. Consentement citoyen. S’assurer compétence juridique du CEPD et encadrer saisine usagers et consommateurs. Analyse risque centralisation UE/bureaucratisation et risque de puzzle national/perte sécurité.Inverser principe de subsidiarité acteurs IA hors UE avec compétence unique CJCE.Gouvernance UE sous contrôle ONU. Impératif élargissement mission TPI aux contentieux environnementales en prévention guerre de suprématie IA aux conséquences économique et sociale.;
F529841;18-06-2020 10:57;English;Company/Business organisation;Agnès;Guerraz;;Skopai;;Small (< 50 employees);France;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"The biggest challenge to the AI is to have the ability to set up data sets large enough to build AI algorithms. Deep learning needs huge set of data to reach efficient results. The ability to build efficient data sets with enough data but also the quality requested to set up AI algorithms constitutes one of the actions to be considered at European level.
For a detailed answer, please see attached submission.";4 - Important;3 - Neutral;4 - Important;5 - Very important;4 - Important;5 - Very important;Education to promote a larger understanding of the legal features around AI should be another area of interest to make the Europeans knowledge better on those subject and avoid unnecessary fears about new services based on AI.;4 - Important;5 - Very important;5 - Very important;"Several EU Member States have adopted strong measures in order to support excellence in AI Research. The European Union could support these national measures and help establish a network of existing AI research excellence centres. The EU should also ensure that these AI research excellence centres work in cooperation and harmony and help create strong synergies among them.
For a detailed answer, please see attached submission.";3 - Neutral;3 - Neutral;3 - Neutral;5 - Very important;5 - Very important;"The funding in digital companies and AI companies in Europe is limited in comparison with the funding of equivalent companies in North America or in Asia. The difference in level of investment in these companies can be double or triple. Investment above series B and C is rare.
Main tasks of specialised Digital Innovations Hubs should be optimizing the financing of such companies and enhancing pan-european co-investements.
For a detailed answer, please see attached submission.";3 - Neutral;5 - Very important;5 - Very important;4 - Important;3 - Neutral;2 - Not important;"It is necessary to focus on misuses of AI by humans.
The questionnaire focuses probably too much on “AI” and almost personifies it. Let’s not forget the human beings are behind. An AI system could be extremely good at accomplishing specific goals. But its (mis)use could deviate from our society rules and ethics.
For a detailed answer, please see attached submission.";Current legislation may have some gaps;;Other;"A risk-based approach is welcome. However, it is necessary to better define high-risk and the methodology to assess it. Poorly defined categories might deter or delay investment and be detrimental to European companies in relation with their counterparts in other parts of the world.
For a detailed answer, please see attached submission.";;;From lethal autonomous weapons systems (sometimes called “killer robots”), developed by militaries and manufacturers around the world with little or no control, to sophisticated surveillance and human manipulation/control systems based on AI, there are several risks of misuse of AI that require attention and regulation.;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;Other special requirements in addition to those mentioned in the question above should be imposed (please specify);"A research project initiated recently between Skopai and the MIAI Chair on the Legal and Regulatory Implications of Artificial Intelligence has already identified more than 160 startups working in the field of facial recognition and biometrics. These startups develop very diversified products and services including many that could be “used in public spaces”. If one adds to this all the products and services developed by other companies, including big tech companies, we understand the number and diversity of biometric systems that could be deployed in public spaces by private or public actors for verification, identification, surveillance or other purposes.
There is today in Europe an important number of rules, starting with the GDPR and the Law Enforcement Directive, permitting to regulate the use of Facial Recognition Technologies (FRTs). However, a careful analysis is needed in order to assess whether the “technologically neutral” rules of European instruments or the specific provisions concerning biometric data are sufficient and permit to address all the risks related to this particularly sensitive sector.
The initial work of the MIAI Chair on the Legal and Regulatory Implications of Artificial Intelligence shows that there is a lack of transparency concerning the existing and intended uses of FRTs in public spaces. It also shows different attitudes in European States in relation with several issues, including the question whether Data Protection Authorities are systematically consulted or not before the deployment of FRTs in public spaces. A risk of fragmentation exists in the interpretation of the GDPR and the Law Enforcement Directive by different States, DPAs and other regulatory or oversight bodies. The adoption of guidelines and new rules might be useful in order to provide a better framework for FRTs, fix the red lines, determine in which cases and under which specific conditions (including necessity and proportionality) their use is acceptable in public spaces and help deal with the important issue of the composition and management of associated databases.";Rather not;"We are skeptical with the idea of a voluntary labelling system.
A voluntary labelling system would quickly become mandatory as people would mainly use labeled applications putting aside non-labeled ones.
Also, putting in place a labelling system would bring new actors that would offer to help companies to make their AI get the labelling. This would add significant costs to SMEs and favor large groups.
For a detailed answer, please see attached submission.";A combination of ex-ante compliance and ex-post enforcement mechanisms;;"These requirements and controls should be subject to several conditions and caveats. For instance, conformity assessment requirements already exist in EU law in relation with a large number of products and it is necessary to avoid duplication. The joint ex-ante/ex-post control mechanism should be implemented in a wise way to avoid creating unnecessary hurdles for European AI industries.
For a detailed answer, please see attached submission.";Personal security risks;;Yes;;Yes;"As a matter of principle the EU and its Members States should avoid overburdening European companies with new AI liability rules as this could hamper innovation in our continent and expose European companies to a competitive disadvantage in relation with their counterparts in other continents.
For a detailed answer, please see attached submission.";Yes, for specific AI applications;;Please see attached submission.;Skopai_Submission_EU_AI.pdf
F529840;18-06-2020 10:34;English;Company/Business organisation;Stuart;Holland;;Equifax;232484027628-74;Large (250 or more);Spain;The feedback can be published with your personal information;4 - Important;4 - Important;5 - Very important;4 - Important;5 - Very important;4 - Important;On skills, it is important that the institutions regulating AI have sufficient technical expertise and skills in AI. ;4 - Important;4 - Important;5 - Very important;4 - Important;4 - Important;5 - Very important;;4 - Important;4 - Important;5 - Very important;;5 - Very important;5 - Very important;4 - Important;5 - Very important;3 - Neutral;Digital innovation hubs can provide resources and information to encourage organisations to deploy AI innovations that are shown to be safe and effective. This will promote the uptake of trustworthy AI so that European consumers and the economy can realise its benefits.;5 - Very important;4 - Important;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;Please see our paper attached for how to improve explainability, fairness and accuracy when using AI in credit risk. One of our recommendations is that consumers who are declined credit should always get an accurate explanation they can reliably act upon to increase their chance of accessing credit in future. We also identify data that creditors, public bodies and consumers could share to improve the predictiveness of credit risk models, regardless of whether they use AI.  ;Other;Current legislation is sufficient for credit risk. The sector is well regulated and firms have strong controls in place for algorithmic decision making. Financial regulators, including the EBA and central banks, are mitigating risks by setting guidelines and running sandboxes. These also give firms confidence to develop and deploy trustworthy AI products. Please see our attached paper for more information. ;Yes;;Yes;;Credit risk is not a high risk application. The sector is well regulated and firms have strong controls in place for algorithmic decision making. Financial regulators, including the EBA and central banks, are mitigating risks by setting guidelines and running sandboxes. These also give firms confidence to develop and deploy trustworthy AI products. Please see our attached paper for more information. ;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;3 - Neutral;No opinion;;Much;Organisations that develop products that use AI should be closely involved in deciding the voluntary labelling system standards for each use case.;Other enforcement system;"1. Within credit risk, enforcement is best delivered within existing legislation through clear guidelines and expectations from financial supervisors, such as the EBA and central banks. 
2. The uptake of trustworthy AI credit risk models that outperform traditional models should be promoted to increase financial inclusion and reduce the incidence of non-performing loans. ";;;;;;;;;;;Consultation_on_the_White_Paper_on_Artificial_Intelligence_-_Equifax_supplementary_paper_14.6.20.pdf
F529839;18-06-2020 00:20;English;NGO (Non-governmental organisation);Dace;LUTERS-THÜMMEL;;European Women Lawyers Association, Brussels;ID: 730379225729-59;Medium (< 250 employees);Belgium;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;Upskilling people as foreseen in the Digital Education Action Plan seems focussed on young people only. What is planned for upskilling the older generation, elderly women in particular? Also employers neglect the group aged over 50. Relying on the private sector only to improve IT proficiency is insufficient. EUROSTAT shows lacking IT proficiency of elderly people and also a gender divide between older men and older women.The rural public sector: rural areas are inhabited mostly by older people.;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;Upskilling should not exclude elderly people (widespread ageism, only in the Nordic countries basic digital skills exist among the older population).;3 - Neutral;5 - Very important;3 - Neutral;Public procurement could be combined with a diversity requirement regarding the developers’ team (and training data sets) to give incentives for more women involvement.;4 - Important;4 - Important;4 - Important;3 - Neutral;4 - Important;Digital Innovation Hubs should foresee special offers for female SMEs and female startups as they by average only get one tenth of financing in comparison to their male counterparts.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Violations of fundamental rights also concern gender equality.;There is a need for a new legislation;;No;;;;Decisions of AI applications impacting human rights without human oversight.;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;Too much intrusion into the privacy of citizens.;Much;It would rather be important to give a clear set of criteria to be observed by all AI systems, not just for participants of a voluntary system.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;AI systems may evolve (depending on the data sets they get fed with) into an unintended direction, human oversight should be ensured in regular intervals.;Mental health risks;Risks related to the interaction of different (AI) systems. Opacity of decisions made by neural networks.;Yes;;Yes;;Yes, for all AI applications;;;EWLA_Statement_White_Paper_AI_final_14.06.2020.pdf
F529875;17-06-2020 18:59;English;Non-EU Citizen;Valerie;Hudson;;;;;United States;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;Please see written response.;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;5 - Very important;No opinion;Please see written response.;5 - Very important;5 - Very important;5 - Very important;Please see written response.;3 - Neutral;5 - Very important;3 - Neutral;3 - Neutral;3 - Neutral;Please see written response.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Please see written response.;There is a need for a new legislation;;No;;;;Weaponry.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;Please see written response.;Very much;It should be mandatory.  Please see written response.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Please see written response.;Mental health risks;Please see written response.;Yes;Please see written response.;Yes;Please see written response.;Yes, for all AI applications;;Please see written response.;EU_Commission_AI_White_Paper_Consultation.pdf
F529874;17-06-2020 18:44;French;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;4 - Important;5 - Very important;Nos réponses s’inscrivent dans la continuité de celles de l’UER. Les ressources de l’IA permettront aux médias d’améliorer les outils de production, recherche, vérification et diffusion de l’information, recommandation et l’accès aux contenus, etc. Etant donné leur rôle essentiel dans la société, les médias, en particulier publics, doivent pouvoir s’appuyer sur un écosystème européen de la donnée, large, fiable et accessible, contribuer aux recherches l'IA et accéder aux financements européens.;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;Dans la continuité de la position exprimée par l’UER, France Télévisions tient à souligner l’importance d’infrastructures européennes, qui permettrait de réduire la dépendance aux technologies développées par des opérateurs puissants, essentiellement non européens, développer l’accès à d’importants volumes de données, indispensable à l’apprentissage des technologies d’IA, de manière fiable et éthique. A cet égard, la collaboration entre secteurs public et privé devrait être encouragée.;4 - Important;4 - Important;4 - Important;Les plateformes collectent un volume considérable de données issues des services des entreprises qui les utilisent pour accéder à leur public. Une évolution réglementaire semble nécessaire afin de faciliter l’accès à ces données et rétablir un juste équilibre entre acteurs. Les partenariats entre public et privé devraient être encouragés et facilités, y compris avec les start-ups, handicapées par leur taille et la modicité de leurs ressources pour l’accès à certains marchés et financements.;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;France Télévisions est très favorable au développement de partenariats entre secteurs public et privé, qui favoriseraient l’essor d’une IA fiable, performante et éthique. De tels modèles collaboratifs ne pourront toutefois se développer sans prendre en considération, pour y remédier, les contraintes de structures aux statuts et ressources disparates.;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;"Le développement de l’IA est étroitement lié à l’existence de bases larges et fiables de données personnelles et non personnelles. La protection des personnes sera assurée par le développement d’infrastructures et de technologies européennes maîtrisées, dont l’accès sera facilité pour les acteurs vertueux; la transparence dans la collecte et l’usage des données;la réaffirmation des droits humains dans l’exploitation de l’IA, du droit à la maîtrise des données personnelles, l’existence de recours";Other;La législation européenne doit lever les obstacles auxquels les entreprises sont confrontées dans l’accès aux données générées par leurs services, collectées, exploitées et conservées par les plateformes. Les règles de concurrence devraient être adaptées au secteur numérique, dans le respect du droit des personnes. La législation doit envisager les nouveaux risques liés à l’IA (« deep learning », « deep fake », bulles informatives, décisions automatiques sans supervision humaine, etc.).;Other;En accord avec l’UER, France Télévisions considère que l’introduction d’exigences obligatoires nouvelles devrait être limitée aux applications à haut risque pouvant entraîner des conséquences dommageables pour les individus. La transparence doit toutefois être encouragée. Dans le secteur des médias l’usage de l’IA devrait être subordonné au respect de valeurs communes telles que l’impartialité, l’indépendance, l’universalité et l’équité.;;;Toute intelligence artificielle chargée d’analyser des données sociales doit être considérée comme pouvant mener à des discriminations. Les usages de l’IA sur les réseaux sociaux apparaissent préoccupants. Les phénomènes de « bulle de filtre », n’offrant pas aux individus l’accès aux opinions contraires, de « deep fake », permettant la création de fausses vidéos, sont dangereux dans la mesure où ils fragilisent l’accès des citoyens à une information fiable, diverse et de qualité.;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;No opinion;L’utilisation de systèmes d’identification biométrique à distance dans l’espace public n’entre pas dans les usages de France Télévisions.;Very much;"France Télévisions est favorable à l’existence d’un système de label volontaire, dans la mesure où celui-ci serait ambitieux et supervisé.

Des labels différents pourraient être envisagés en fonction du secteur d’activité, afin de tenir compte des spécificités. Ces labels devront être établis au niveau européen pour avoir du sens et susciter de la confiance dans l’IA, en particulier en assurant la transparence des systèmes, le respect de valeurs communes et du droit des personnes. 
";A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Cyber risks;Les risques liés à la cybersécurité figurent parmi ceux les plus susceptibles de toucher le secteur des médias (la modification malintentionnée d’algorithmes peuvent détourner les utilisateurs des contenus initialement proposés (ex : un enfant amené vers un contenu inapproprié), endommager les bases de données utilisées par l’IA.;Yes;Les applications considérées comme étant à haut risque devraient faire l’objet d’évaluations obligatoires au cours de leur existence afin de tenir compte d’éventuelles modifications de leur fonctionnement lors de leur apprentissage. ;No opinion;;No opinion;;;
F529873;17-06-2020 18:22;English;Business Association;Jan;LOHSTROH;;ARTEMIS Industry Association;310026619668-24;Micro (< 10 employees);Netherlands;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;AI is the new opportunity as well as the new challenge in Europe. Technological leadership in AI requires mastering of new digital technologies, in particular new computing architectures, software algorithms and semiconductor production. This calls for a deep, sustained partnership on Key Digital Technologies (KDT) beyond the current levels of cooperation up and down the value chain. The KDT partnership proposal will be available on the EC website on “European Partnerships in Horizon Europe”.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Keep up with developments in the future KDT partnership;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
F529872;17-06-2020 17:37;Swedish;Public authority;Patrick;Eckemo;National;DIGG - Agency for Digital Government;;Medium (< 250 employees);Sweden;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;"Skapa, samordna och intensifiera samarbeten och kluster både nationellt och internationellt för att öka användningen av AI samt skapa ett internationellt konkurrenskraftigt ekosystem. Fokus för Sverige förutom att vara en allmän AI nod är att fokusera på hållbar AI utifrån ett perspektiv på samhällsförändring. Viktigt att beakta livslångt lärande utifrån ett helhetsperspektiv - inte bara på forskningsnivå.
Det bör finnas AI-rådgivare och förändringsledare för offentlig sektor.
 
";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"Viktigt att alla människor får rätt förutsättningar att förstå möjligheterna och riskerna med AI för att skapa en öppen samhällsdialog. Det innebär att vi behöver etablera förutsättningar på alla nivåer i samhället, från grundskola till universitet samt på alla organisationsnivåer. Satsa på inter- och multidisciplinär forskning med AI i fokus.
En förutsättning för att etablera en fungerande styrning är en grundläggande förståelse för nivåer av styrning i olika kulturer
";5 - Very important;4 - Important;4 - Important;Viktigt med gemensam målbild för ett digitalt EU så vi kan bygga rätt förutsättningar, AI är ett viktigt medel i att realisera rätt förutsättningar. Viktigt med inter- och multidisciplinära samarbeten mellan offentliga och näringslivet med tydliga incitament. Etablera ett AI center baserat nya och existerande satsningar i Sverige. Utveckla och nyttja AI4EU plattformen som gemensam plattform för utbyte av erfarenheter, information, kod, modeller, best practises, principer etc.;4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;"Viktigt med gemensamma AI förmågor som alla behöver stöd ifrån. Ex. teknisk kompetens, arkitekturkompetens, . Men det bör också finnas specialiserade hubs som har specialistkompetens inom specifika AI eller tillämpningsområden. Ex. kan det i Sverige handla om hållbar omställning, rymd, IoT och 5G. Viktigt med en tydlig integration av lighthouse och DIH samt förutsättningar för samarbeten med SME.
 
";5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;Vi måste öka förståelsen för AI i relation till hur en människa arbetar och tänker. Det handlar om hur vi bygger AI acceptans och tillit. Förklarbar AI är viktigt för att skapa tillit och ett område som behöver fokus. Ta fram en digital AI-guide med principer och riktlinjer för utveckling, köp och användning. Forum för dialog med medborgare.;Current legislation may have some gaps;;No;;;;All AI kan teoretiskt användas för onda avsikter oavsett utvecklarens intention. Därför bör ansvaret delas upp för deklarationen.;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Very much;Det borde vara fokus på KVALITE som en CE-märkning för AI, dvs inte primärt utifrån RISK.;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;Det bör stöd och riktlinjer för hur man utvecklar enligt kraven, men det bör också finnas en ansvarig funktion som hanterar godkännande och tillsyn;Personal security risks;Tydliggöra det juridiska ansvaret vid automatiserat beslutsfattande;Yes;AI kan förändras över tid. Frågan är hur klassificeringen av en AI tillämpning ändras över tid kopplat till en given riskklassning och ev. kvalitetsklassning. Den garantin gäller endast oförändrade system. Detta är komplicerat eftersom data kan förnyas och ersättas vilket indirekt påverkar beteendet för AI.;Yes;AI är föränderligt och mera komplext som produkt och ramverket bör ses över och eventuellt anpassas.;Yes, for all AI applications;;;Summering_av_AI_Vitboken_4_-_inskick.docx
F529871;17-06-2020 16:14;French;Public authority;Camille;Darche;National;Comité national pilote d'éthique du numérique;;Small (< 50 employees);France;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;cf. document joint ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;cf. document joint ;3 - Neutral;5 - Very important;5 - Very important;cf. document joint ;4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;cf. document joint ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;cf. document joint ;Current legislation may have some gaps;;Other;cf. document joint ;;;cf. document joint ;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Other special requirements in addition to those mentioned in the question above should be imposed (please specify);cf. document joint ;Much;cf. document joint ;A combination of ex-ante compliance and ex-post enforcement mechanisms;;cf. document joint ;Mental health risks;N/A;Yes;N/A;Yes;cf. document joint ;Yes, for specific AI applications;cf. document joint ;cf. document joint ;CNPEN-contribution-consultation-IA4EU-2020-06-14.pdf
F529870;17-06-2020 15:57;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;;4 - Important;4 - Important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;CEPI is an association of national associations of Independent producers of film/TV and animation (most of them SMEs) in Europe. The AI element is crucial for the animation sector- for example, to replicate animated part that otherwise should de designed over and over again each time to re-create the environment where content is developed and/or edited in the post production phase. We believe that with the right guidance via the DIHs, more production companies can take advantage of AI.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;AI applies throughout the value chain, from content generation to distribution, search and selection, and quality evaluation. First of all, we need to ensure use of AI does not breach intellectual property rights as content provides key inputs for AI. Also, the use of AI algorithms in AV can be a threat- this is why is important to be managed considerably by platforms. AI for scripts needs human intervention. It also can create lack of transparency, or difficulties to define liability.;Current legislation may have some gaps;;Yes;;Yes;;;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;No opinion;;No opinion;;No opinion;;;
F529869;17-06-2020 14:55;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;1 - Not important at all;1 - Not important at all;Set clear red-lines for impermissible use, ensure democratic oversight, and include the strongest possible human rights protection.;4 - Important;No opinion;1 - Not important at all;2 - Not important;4 - Important;2 - Not important;The coordinated plan and member state strategies should include a section on human rights, societal impacts of AI/ automation, and how to ensure democratic oversight.;3 - Neutral;4 - Important;1 - Not important at all;Public interest should set the priorities of research centers and research partnerships. Research priorities should include the human rights and societal implications of the development and use of AI, fairness design, discrimination risks and transparency.;1 - Not important at all;2 - Not important;3 - Neutral;3 - Neutral;No opinion;DIHs and other innovation incentives for SMEs must not provide exceptions from fundamental rights. ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;AI used in sensitive areas (public services) without democratic oversight, transparency or sufficient evidence to justify the need/ purpose. Increasing use of opaque, privately owned tech. Conscious obsfucation of accountability for harms. AI poses collective harms which cannot be addressed in the anti-discrimination or data protection frameworks• ‘Innovation’ invoked to justify trials without safeguards. Machine learning can lead to unauthorised or secondary use and function creep.;Other;Legislation must strengthen, not replace, GDPR. The current law does not address use of non-personal data, and collective impact of AI, such as furthering overpolicing, surveillance and inequality.;Other;New rules are necessary to determine the criteria for when it should be legal to develop and deploy AI- standards for scientific and policy evidence- burden of proof is on the developer/deployer and not on impacted groups- mandatory democratic oversight before deployment of AI in public sphere. Mandatory fundamental rights impact assessments for all uses• The EU should proactively ban AI applications in areas where the fundamental rights and societal implications are too great to risk.;;;The use of AI to determine delivery of essential public services, predictive policing, autonomous lethal weapons, identification/ analysis of emotion and identity traits, and indiscriminate bio-metric surveillance, are incompatible with fundamental rights and should be banned by default. Determining  ‘risk’  should  be  rights  and  outcomes  focused,  not  sectoral. ;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces; use   of   biometrics   for   remote   identification   in   publicly-accessible   spaces   sig-nificantly   contributes   to   unlawful   mass   surveillance   so   should   never   be   deployed.;Rather not;We should exercise caution deciding that some applications are inherently low risk and therefore do not require oversight to guarantee fundamental rights. Don't incorporate  voluntary,  self-regulatory  and  ethics  based  approaches  in  AI  regulations.  Such  approaches  provide  scope  to  circumvent  accountability  and  soften  fundamental  rights  obligations.  And they  reduce  certainty  and  impede  access  to  justice for those harmed.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Need mandatory, ex ante human rights impact assessments (which include an assess-ment of collective and social harms posed by applications, with review  at the stages of design, development, testing and deployment. Clearly enforced consequences should applications fail to meet certain standards, including the potential to halt deployments.;Mental health risks;Heightened risks of discrimination, in particular with reference to online products and services using data for targeted advertising. This poses risks of differentiated pricing,  discrimination financial detriments, the risk of creating filter bubbles and interference in the democratic process, based on sensitive inferences or associations.;Yes;Data Protection Officers under GDPR should be included and and asked for advice.;Yes;Liability  should  be  centered  around  accountability  and  the  extent  to  which  faults  can  be  re-dressed by users of technology.;Yes, for all AI applications;;AI applications are covered both by copyright and database rights protections, which prevent users from assessing their quality and limit their ability to redress issues that have been observed.;
F529868;17-06-2020 14:45;English;Company/Business organisation;sophie;MULLER;;Thales SA;91711831031-23;Large (250 or more);France;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;"Address the whole public and private ecosystem (not only SMEs) globally.
Foster cross-fertilization between academia and industry. As examples:PhD curriculum or post-docs in industry, alternate periods for students in Industry and AI academic centers, with incentives for companies having their HQ in Europe.
Reinforce the attractiveness of AI (technologies and applications) by promoting beneficial impacts andadapting education programs accordingly, both on AI and humanities impacted by AI.";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;3 - Neutral;5 - Very important;5 - Very important;In order not to drain the skills from where they are and to foster public-private cooperation Europe-wide, a supported/managed network of existing centers of excellence would be preferable to the creation of a dedicated Lighthouse Research Center. Beyond creating links/inspiring the whole community with examples of AI use-cases/ways of exchanging data for mutual benefits, setting up a Public-Private Partnership would foster transfer of academic research towards industry and the whole society.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;;4 - Important;4 - Important;4 - Important;3 - Neutral;3 - Neutral;3 - Neutral;"Among major risks, some could arise from malevolent actors (AI-based deep fakes, AI-assisted hacking, rogue autonomous systems used to harm people or infrastructures, cyberattacks…) against which a regulatory framework would be uneffective. Concerns do not mean no solutions. In other respect, for example, as regard safety criticality in
transportation, the regulatory framework already exists, with a high level of requirements, and should not necessarily be revisited due to the use of AI Tech.";Current legislation may have some gaps;;Other;Some adaptations could be brought out depending on classes of use-cases. It is worth noting that in a given sector which could be considered “high-risk” according to section B of the White Paper, all its AI based applications are not systematically “high-risk”.;;;;3 - Neutral;3 - Neutral;5 - Very important;4 - Important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);"Monitoring and control are required to prevent from unwanted uses.
Concerning the previous question ( « Information on the purpose and the nature of AI systems »), we have chosen to rate it as ""very important"", although it is not clear what is meant by « the nature of AI systems ».";No opinion;Such a voluntary labelling system might be useful in some cases, provided it is certified by a national third independent party, in compliance with a EU label recognized by all member states, and that can be obtained at national level.;Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;The ex-ante compliance, including safety and security, should keep being self-assessment with certification and regulation bodies, in safety-critical domains as transportation, for example.;Personal security risks;To minimize risks of misuse or rejection (sometimes observed when a new technology is introduced), the purpose, the scope and the regular use of the system/application, as well as the possible misuses, have to be clearly stated. In addition, with AI and continuous-learning systems, the user should be warned in case an operational drift along time is observed possibly leading to an abnormal behaviour.;Yes;The answer has to be weighted by what is meant by “important changes” and how such changes may occur.;No opinion;;No opinion;;"A high emphasis on compensation specific to AI-based systems rather that on risk assessment (prior to usage) might upstream alter confidence and social acceptance.

As a general remark concerning this EU consultation, the emphasis seems to be put more on concerns than on opportunities. Highlighting examples of beneficial impact and added-value would be appropriate in order to further foster societal acceptance.";
F529867;17-06-2020 13:19;English;Company/Business organisation;Shafiq;Urréhman;;CEVT AB;;Large (250 or more);Sweden;The feedback can be published with your personal information;4 - Important;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;;4 - Important;3 - Neutral;4 - Important;5 - Very important;4 - Important;5 - Very important;;There is a need for a new legislation;;Yes;;Yes;;;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;;Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F529866;17-06-2020 09:41;English;Trade Union;Jan Peter;Brauburger;;industriAll European Trade Union;358284014848-82;Small (< 50 employees);Belgium;The feedback can be published with your personal information;No opinion;5 - Very important;5 - Very important;5 - Very important;No opinion;5 - Very important;industriAll Europe believes that the general framework regulating AI should be defined by legislation. When introducing AI to the work place, worker's representative bodies and trade unions must play an active role in the decision-making process within in the framework thus defined, including via collective agreement, social dialogue, and co-determination. The European social partners' capacity building activities should further be supported by promoting European social dialogue.;5 - Very important;4 - Important;4 - Important;No opinion;5 - Very important;5 - Very important;"Upon introducing AI to the work place, forward planning of employment and skills, lifelong learning and upgrading of workers' skills are important in terms of the anticipation and preparation for changes within companies, which must be underpinned by an individual worker's right to training, irrespective of their age or statute, preferably guaranteed by collective agreements. In addition, a ""European transition fund"" is need to support those workers and regions negatively impacted by AI.        ";No opinion;No opinion;No opinion;Specific funding should be allocated to research on bias and how to mitigate against it's consequences. AI relies on the idea that decisions impacting the future should be based on the past experience embedded in data. It risks reproducing the status quo forever, including any discrimination bias present in our societies, and thus in teaching data. Policy should therefore support research on means to introduce innovation, experimenting, change and creativity in the operations of ML systems.;No opinion;5 - Very important;5 - Very important;No opinion;No opinion;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;With AI systems, it becomes possible to supervise all workers permanently, and to detect all occasions of non-compliance with prescriptions in real time. This has the potential to significantly reduce the space of worker autonomy and workers' contributions to innovation based on their professional skills and experiences. The collective voice of workers must play a determining role in ensuring AI and ML systems are used in their interest and in a balance with that of employers.;Other;"Works councils should be provided the means to hire the competencies of software engineers or data scientists to support them in the discussions on the application of AI and ML systems at the workplace. Effective negotiations on their application must be ensured. Policy should mandate the creation of a position of data accountant in companies, whose duty is to control and report annually on the use of AI systems, in the way a financial accountant controls and reports on the financial situation
";No opinion;;;;Application of AI in HR. Workers should be consulted regarding all automated tools used by management to supervise work, manage the workforce in HR processes, or profile workers, i.e. anticipate their performance at work. Management should thus report to, consult and reach agreement with worker's representative bodies on this. Policy should refuse to consider individual consent as enough to ensure that it is “freely given” (GDPR, Art. 4(11)) and the human must stay in command.                   ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;;No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;"Mandate that any ML software taking decisions regarding humans and specifically workers or embedded in a safety-critical system be explainable - and prohibit its use if not the case.
Minimse the ""unexplained"" fraction of predictive software based on neural networks. Model  all that can be modeled based on known equations, explicit calculations and parameters that can be estimated using standard statistical tools, leaving for the unexplainable black box only a small fraction of the modelling. ";Mental health risks;"Profiling in HR could be used to detect or predict sensitive personal data, as defined by Art.9, GDPR. This regulation prohibits to describe the present situation of a person, but it does not, however, prohibit guessing or detecting this sensitive information using indirect data sources or predicting the person's evolution in this matters. The usage of ML systems that anticipate or detect the health status of workers or ""any category of personal data"" should be prohibited.";Yes;"We support strong policies to anticipate and manage the social consequences of industrial change, such as those brought by AI. We demand 1: anticipation of change, in order to act before the restructuring takes place; 2: re- and upskilling, and indeed AI literacy of all workers. Regarding anticipation of change, workers must not be left with contributing exclusively to managing the social consequences of AI but must proactively contribute to shaping a world of work in which AI would play a role.";Yes;Define clearly the liability in accidents and incidents involving AI systems. The current general rule, whereby the employer is by default liable for any accident in the workplace (in the absence of any wrongdoing by the worker) should remain, and workers victims of such an accident should be compensated swiftly and without delay. This solid liability regime should be maintained, even when the behavior of a ML system keeps evolving after purchase. ;Yes, for all AI applications;;;636849754506900075_Policy_Brief_-_Artificial_Intelligence.pdf
F529865;17-06-2020 09:13;English;Business Association;Lanfranco;BENEDETTI;;SEA Europe;SEA Europe 009214311424-03;Micro (< 10 employees);Belgium;The feedback can be published with your personal information;4 - Important;4 - Important;5 - Very important;4 - Important;5 - Very important;;Develop and strengthen academy and industry partnerships to guide the AI development towards industrial applications is very important for the waterborne sector as a whole both to further enhance design and shipbuilding practices and for application of AI when sailing at sea with potential benefits to the safety of navigation and increased efficiency of the maritime traffic with relevant reduced environmental impact.;5 - Very important;5 - Very important;5 - Very important;No opinion;;4 - Important;"The role of the public sector in fostering and creating a business environment for the development of  common test cases in critical sectors and for the spin-off of starts-up should be further exploited and reinforced. 

While probably in the naval domain, in the short term the development of AI technologies will still be driven by military applications (such as target identification, underwater signature analysis and voice control operation, AUV ASV), shipbuilding and maritime industry as";4 - Important;5 - Very important;4 - Important;"The theoretical foundation of AI principles cannot certainly be underestimated; this work is normally best carried out by academia and research centers, however for the application cases, the partnership with industry is of the outmost importance. 

EC programs such as Horizon 2020 and Horizon Europe are certainly creating a suitable environment to develop such collaboration also for the Waterborne sector. The AUTOSHIP project in Horizon2020 is first of this kind, creating a solid experience ";5 - Very important;4 - Important;4 - Important;4 - Important;;At such early stage of AI development, raising awareness of the technology potential is certainly of relevance to the least to build a common understanding knowledge basis in the industry and society. ;4 - Important;5 - Very important;4 - Important;3 - Neutral;4 - Important;4 - Important;For maritime industrial applications the development and enforcement of an international regulatory framework is of essence. While AI applications are growing in shipyards design and building process (Industry 4.0, often couple with IoTs and other digital technologies), the AI role in autonomous shipping could develop in a paradigm shift of the industry. In that respect, without such set of rules developed at IMO level the deployment of autonomous surface ships would not be possible. ;There is a need for a new legislation;;Other;"In the shipbuilding sector, again two distinct domain of potential application of AI technologies can be identified: the shipbuilding design and process and the use of AI systems at sea for navigation aid.

While for the first category of problems the requirements should follow and adhere to the relevant and applicable legislation for the safety and wellbeing of workers and to the efficiency of the technologies for a sustainable and profitable business, for the second category a regulatory fra";;;"The waterborne sector is highly regulated at both European and International level and respond to detailed standards, norms and international conventions where the presumption of conformity is not applied. Such regulations and rules are mostly developed by solid and quantitative risk-assessment principles. 

Therefore for what the safety of products is concerned (and for the ship as a whole), each application of the AI in whatever element, system, system-of-systems has to be verified.";4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Risks connected to AI are only partially predictable: some risks will rise only ex-post and need to be properly addressed. The risks identified ex-post should be limited and addressed before they are acted.;Risks related to the loss of connectivity;"For applications of AI at sea during navigation, all regulations should be developed by strictly following risk-assessment procedure as already stipulated at international level in the IMO rule-making process. 
For AI and robotics applications in shipbuilding, relevant legislation should be adapted to take into consideration the new potential risks raising from the application of these technologies.
";Yes;As explained, maritime industry has a solid and long track record of developing regulations based on risk-assessment frameworks. Overall these procedures are grouped under the Formal Safety Assessment rule making process at the IMO. This is quantitative analysis carried out whenever a new set of rules need to be developed. It has some similarities with the Better Regulation of the EC, when coupled with a Cost Benefit Analysis.;Yes;;Yes, for specific AI applications;"Applications having high autonomy levels, where the AI based system can take decisions unsupervised or supervised but without a strict control but also high-risk situations needs to be regulated. 

For the waterborne sector liability regime is established at international level at the IMO, given the global character of maritime shipping. The liability rules should be updated internationally to allow for safe deployment of autonomous applications to ship sailing at sea.
";;AI_Consultation_Summary_01.docx
F529843;16-06-2020 23:51;English;Public authority;Pilar;Vega;National;Comisión Nacional de los Mercados y la Competencia (CNMC) and Autoritat Catalana de la Competència (ACCO);;Large (250 or more);Spain;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;CNMC_S_AND_ACCO_S_JOINT_CONTRIBUTION__TO_THE_PUBLIC_CONSULTATION_ON_AI.pdf
F529842;16-06-2020 22:58;French;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;4 - Important;"Approfondir ce que l'on appelle l'IN (acronyme signifiant ""Intelligence Naturelle"". Le processus démarre in Utero et se poursuit tout au long de la vie. Les premiers artifices interviennent en milieu scolaire et sont cause de nombreuses distorsions ultérieures. L'école doit permettre de faire germer les concepts nouveaux et leur laisser le temps de grandir discrètement. Une stratégie IA en appoint à l'enseignant doit permettre de fonder le Lifelong Knowledge Stack de chaque petit Européen.";5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;"Améliorer la communication d'Homme à Homme, Brain à Brain (B2B), et ensuite l'inferface Homme-Machine. Passage obligé par l'école. Prendre le temps d'enseigner. ""A School is a seamless process for transfering & experimenting newly acquired knowledge"".   Construire ""un espace européen de l'information"": une idée de Jacques Delors en 1993 !!! Voir Livre Blanc de décembre 1993.";4 - Important;5 - Very important;5 - Very important;"Il faut susciter des vocations; Mieux communiquer sur les conditions et implications personnelles des scientifiques: le fameux S1 décrit par  Gerald Holton ""Thematic Origins of Scientific Thought"". Infrastructure de soutien aux jeunes sur smartphone, avec service confidentialisé. Voir programme OLILOKSE.";4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;"Il faut recentrer l'IA sur l'humain: c'est lui qui est intéressant et qui doit apprendre à mieux se connaître. Promouvoir le partage des modèles mentaux entre concepteurs et utilisateur d'information. Voir article joint:  ""Ergonomie cognitive du logiciel » présenté en 1993 à Georges Metakidès, Directeur du Programme ESPRIT. Voir document en annexe.";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;L'IA est un outil, une technologie. Elle doit être utile et légitime. Elle n'a pas été inventée hier. La sémantique probabiliste induite par l'IA sur Big Data est normative. Pas applicable partout, notamment dans l'innovation. Au-delà de la logique, il y a l’analogique, le disruptif propre à la créativité, le malentendu permanent et la fécondité du doute. Einstein a remporté le prix Nobel pour une ligne de texte. Alfred Korzybski et Gerald Holton expliquent comment et pourquoi.;There is a need for a new legislation;;Yes;;;;Les plate-formes: les réseaux sociaux, achats en ligne. L'utilisation: protection des Données privées (problème légiféré depuis 45 ans déjà, et de plus en plus critique). Plateformes d'achat et de publicité. Transactions financières.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Other special requirements in addition to those mentioned in the question above should be imposed (please specify);Il faut une certification, via service en ligne sécurisé, stipulant les clauses d'agrément entre service et utilisateur.;Very much;;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;Utilisation du schéma « AI Online content moderation workflow », de Cambridge Consultants (2019). Mise en service via certification, et suivi des mises à jour logiques du dispositif (n° de version des composants+dossier justificatif).;Mental health risks;l'utilisateur doit disposer de la liste des sociétés ou intermédiaires auxquelles ses données ont été communiquées.;Yes;tout site est responsable des fraudes occasionnées par détournement d'enseigne publicitaire (phishing, entre autres). La charge de se retourner contre tout intermédiaire lui revient.;Yes;;Yes, for all AI applications;;L'IA est un concept flou. Toutes les applications classiques comprennent des éléments de décision automatisée qui ne sont pas toujours explicites et changent souvent. Même au niveau de l'Administration. En particulier, le détournement d'information à des fins de croisement avec d'autres sources est pratique courante et contraire à la règle n°1 de protection des données privées, à savoir que l'information donnée ne peut être utilisée que pour les fins du service offert. Autre usage: anonymiser!;200217_AI_Liability_and_PLD_Reform_-_Letter_to_Chair_Petra_De_Sutter_-_FINAL_-_AI_White_Paper__Comment_on_.docx
F529844;16-06-2020 18:18;English;;;;National;;;;;The feedback can be published in an anonymous way;;;;;;;See attached document;;;;;;;;;;;;;;;;;;;;;;;;see attached document;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;White_paper_on_Artificial_Intelligence_-_public_consultation.docx
F529864;16-06-2020 15:42;English;Trade Union;Leigh;Meechan;;The Educational Institute of Scotland;;Medium (< 250 employees);United Kingdom;The feedback can be published with your personal information;5 - Very important;3 - Neutral;;4 - Important;1 - Not important at all;3 - Neutral;"The Institute would refer to its response to the White Paper contained in the Memorandum of Comments, uploaded at the end of this document. 
Whilst ratings have been given in terms of the importance of the six proposed actions above, these ratings must be read in light of the comments and qualifications made in that document.
";5 - Very important;No opinion;2 - Not important;No opinion;5 - Very important;No opinion;"The Institute has outlined its response to the White Paper in the Memorandum of Comments which is uploaded at the end of this document.  
Whilst ratings have been given in terms of the importance of aligning policies and strengthening co-ordination, these ratings must be read in light of the comments made in that document and the concerns raised around partnerships between the private and public sectors.  
";3 - Neutral;4 - Important;1 - Not important at all;"The Institute has outlined its response to the White Paper in the Memorandum of Comments which is uploaded at the end of this document.
Whilst ratings have been given to the importance of the three actions proposed above, these ratings must be read in light of the comments made in that document and the concerns raised around the proposed public-private partnership approach.
";No opinion;No opinion;No opinion;No opinion;No opinion;The Institute has outlined its response to the White Paper in the Memorandum of Comments which is uploaded at the end of this document and would refer to that document in responding to this question. ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;The Institute has outlined its concerns about AI in the Memorandum of Comments which has been uploaded at the end of this document.;There is a need for a new legislation;;No;;;;The Institute has outlined its concerns about the application and use of AI in its response to the White Paper.  The details of these concerns can be read in the Memorandum of Comments which has been uploaded at the end of this document. ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion; ;Very much;"The Institute has outlined its position in relation to the regulatory framework, the legal test and the enforcement system which should be applied in the Memorandum of Comments which has been uploaded at the end of this document and would refer to that Memorandum in response to this question.  

";Other enforcement system;The Institute has outlined its position in relation to the regulatory framework, the legal test and the enforcement system which should be applied in the Memorandum of Comments which has been uploaded at the end of this document and would refer to that Memorandum in response to this question.  ;The Institute would refer to its response to the White Paper, contained in the Memorandum of Comments uploaded at the end of this document.  ;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F529863;16-06-2020 13:41;English;EU Citizen;Moritz;Christoph;;;;;Germany;The feedback can be published with your personal information;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;NGOs with Focus on Human Rights / Democracy / Computer Science (like Chaos Computer Club, Democracy International,...);5 - Very important;4 - Important;4 - Important;3 - Neutral;5 - Very important;3 - Neutral;;4 - Important;5 - Very important;4 - Important;Continue to work with the concept of excellence center like it is already done for other topics. For all public-private partnerships: make sure that the public governance doesn't need to follow economic pressure. AI is and will become even more a growing sector. Therefore strong and independent legislativ, judicativ and executiv pillars are mandatory to guarantee human rights and democracy.;2 - Not important;4 - Important;5 - Very important;2 - Not important;2 - Not important;"Enable a transparency: who supports which light house project - who receives which benefits (results of the research). Property rights and benefits shouldn't be contralized in the hands of a few market players. Creation of a ""common"" data  base or access so that start-ups can benefit from research and no concentration of market players is enabled.";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;The utilization of AI needs to be transparent to end-user. Example AI can apply filters by using datas from different sources for specific end-users => end-user doesn't receive holistic information on products or services and is not aware of this situation (even different price levels can be shown etc). In democratic processes the end-user needs to have access to all opinions and information => pre-filtered information can impact decision (vote) of end-users without any awareness.;There is a need for a new legislation;;No;;;;Rules for all AI applicatiosn are mandatory. No focus on high risk ones.;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;Other special requirements in addition to those mentioned in the question above should be imposed (please specify);Creation of digital twins / all ways to identify users ;Not at all;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;AI changes due to machine learning. I needs to be guaranteed that it is compliant throughout the life span of an application;Personal security risks;Produkthaftung - who uses AI in their products, are fully responsible for the outcome and need to justify any decision.;Yes;Internal supervisors who check and verify quality of AI system;Yes;Accountability and levels of transparency need to be defined;Yes, for all AI applications;;yes, but current liability rules should only be strengthened;
F529862;16-06-2020 12:00;Portuguese;Company/Business organisation;ALFREDO;OLIVEIRA;;Frilixa;;Medium (< 250 employees);Portugal;The feedback can be published with your personal information;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;4 - Important;5 - Very important;5 - Very important;;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;Current legislation may have some gaps;;No opinion;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);;Much;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Personal security risks;;Yes;;Yes;;Yes, for specific AI applications;;;
F529861;16-06-2020 10:47;English;;;;National;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;"•	Governance dimension: coordination and alignment with strategic European objectives, e.g. green deal.
•	Uptake SMEs in all sectors
";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"Skills: e.g. crash courses AI for SMEs’employees
Data: standardization on European level
";2 - Not important;5 - Very important;5 - Very important;Priority should be given to a network of excellence centers;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"Also focus on traditional SMEs and on providing info on ethical issues. The use of AI should be demistyfied.
Pro-actively reach out to SMEs. Benchmark tests, reference databases and methods for the assessment and curation of data are very important.  In this context, the development of standards and reference systems should be oriented towards the needs of end-users and should be carried out in close cooperation with SMEs
";4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;;There is a need for a new legislation;;No;;;;"Applications that can cause health damage.
Applications without human control.
Applications that rely on data without adequate data governance.
The list should be dynamic. 
";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Other special requirements in addition to those mentioned in the question above should be imposed (please specify);"In terms of the GDPR certain conditions apply to the use of biometric personal data. However, other conditions outside the realms of data protection should be taken into consideration and this requires further examination and potentially comprehensive guidelines.

Cases of national security and law enforcement.

Research on biometric identification systems should be subject to other requirements then biometric identification applications used in the economy or by governments. Research into these systems can provide us with new insights to apply them with respect for, among other things, privacy 
";Much;"A European labelling system is necessary. 

Should be based on an objective standardization
";A combination of ex-ante compliance and ex-post enforcement mechanisms;;For new products or services (or major overhauls of existing products or services) ex ante should be required. For incremental updates, ex post will probably suffice. Trustworthy AI is much more about stimulating the people who use/work with AI rather than setting up strict requirements. For “High-risk” applications, it is important that there is a good definition (not an arbitrary one) so that not all AI system has to undergo an ex-ante assessment.;Personal security risks;;Yes;"The risk of not using AI should also be taken into account
What about the responsibility in the event of a defect linked to a hacking of the application ?
";Yes;One possibility would be to lighten the burden of proof by presumption in the event of realization of a specific danger related to the product.Example of general rule: When the injured person establishes the plausibility of the causal link between a product which presents a specific risk and the realization of this risk, the product is presumed to be defective and the damage is presumed to result from this defective product, unless proven otherwise.;Yes, for all AI applications;;"The owner of an object should be liable for the damage caused by the default of this object. Furthermore, when the injured person establishes the plausibility of the causal link between an object which presents a specific risk and the realization of this risk, the object should be presumed to be defective and the damage should be presumed to result from this defective object, unless proven otherwise.
It is very important that there is legal certainty for AI developers and researchers
";
F529860;16-06-2020 10:25;Spanish;Academic/Research Institution;Arturo;Urrutia de Andrés;;CEU IAM Business School - Master in Legal Tech - J.A.L.A.;;Micro (< 10 employees);Spain;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;2 - Not important;5 - Very important;4 - Important;Colaboración con el sector privado.;4 - Important;5 - Very important;5 - Very important;FORMACION ESPECIFICA, UNIFICACION PROGRAMAS ACADEMICOS, FACIL HOMOLOGACION EN PAISES MIEMBROS.;3 - Neutral;4 - Important;4 - Important;5 - Very important;5 - Very important;DEBERÍA SER TAMBIÉN UN CENTRO FORMATIVO Y DE DESARROLLO/EXPERIMENTACIÓN.;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;;Current legislation may have some gaps;;No;;;;AQUELLAS QUE AFECTEN A LA INTEGRIDAD FÍSICA DE LAS PERSONAS.;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;Other special requirements in addition to those mentioned in the question above should be imposed (please specify);(Ver documento adjunto);Rather not;(Ver documento adjunto);Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Risks related to the loss of connectivity;Pensar riesgos y régimen de responsabilidad de cada punto.;Yes;;Yes;SEGUROS DE RESPONSABILIDAD CIVIL (Ver documento adjunto);Yes, for all AI applications;;Desarrollar requisitos para libertad voluntad entre partes, y cuándo se atribuye responsabilidad a una determinada parte. Garantizar acceso al responsable por parte del usuario final.;Propuesta_sobre_Libro_Blanco_IA.pdf
F529859;16-06-2020 10:00;English;Business Association;Enrique;Velazquez;;Association of Consumer Credit Information Suppliers;21868711871-63;Micro (< 10 employees);Belgium;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;"Data standardisation - without unjustified limitations as per data types and sources - and interoperability promote trust.
Promotion of sector-specific use of AI and education to C-suites is highly beneficial.
Incentives should be introduced to promote the use of scaled AI. 
Partnership with the private sector needs to maintain / promote competition.
Leadership belongs to the R&D community, but the regulator should promote it. 
Promote research in use of large sets of data (sandboxes).";5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;4 - Important;4 - Important;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;;Other;In considering rules for AI, the Commission should take into account the existing body of legislation and guidance which may already address some of the concerns mentioned above. This is the case for example in relation to the recently issued Guidelines from the European Banking Authority on loan origination and monitoring, which cover credit decision-making process that use automated models.;Yes;;Other;;"We support a risk-based approach to determine “high-risk” AI applications. In considering the introduction of additional compulsory requirements, the Commission should take into account (i) the depth & breadth of regulation / guidance in a given sector (e.g. EBA Guidelines on loan origination); (ii) the experience & track record in safe AI development and / or deployment in that sector; and(iii) the sectoral supervisory capabilities to monitor & mitigate potential AI-related risks.";4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;No opinion;;Much;"There must be clarity and consistency in the way a “quality label” would be awarded (common rules), to guarantee a level playing field and avoid national discretions / sensitivities. 

There must be clarity on the feedback loop/ mechanism of monitoring in a voluntary system. 

Organisations that use AI should be closely involved in deciding the standards in each use case.";Other enforcement system;Ex-ante compliance systems with applicable, general consumer and data protection rules and - where existing - sectoral legislation / guidance on AI, coupled with ex-post enforcement by existing sectoral bodies / regulators, should ensure that AI is trustworthy, secure and in respect of European values and rules. Additional mechanisms e.g. external conformity assessments to be developed by the industry could also be envisaged. ;European institutions can play an important role in highlighting examples of good practice to promote the benefits of AI for citizens and the economy, and in encouraging organisations to harness the benefits of compliant, trustworthy AI products. ;Risks related to the loss of connectivity;;No opinion;;No opinion;;No opinion;;;
F529858;16-06-2020 00:52;French;;;;;;;;;The feedback can be published in an anonymous way;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;;No opinion;No opinion;No opinion;;No opinion;No opinion;No opinion;No opinion;No opinion;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;;There is a need for a new legislation;;;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Not at all;;Other enforcement system;;;Mental health risks;;Yes;;No opinion;;Yes, for all AI applications;;;
F529857;15-06-2020 22:39;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;2 - Not important;2 - Not important;The protection of fundamental rights should always be put above business interests. AI should only be adopted in the public sector provided there are clear rules ensuring a safe use, an accountability mechanism, safeguards against discrimination and human oversight in place.;5 - Very important;4 - Important;2 - Not important;3 - Neutral;5 - Very important;4 - Important;As mentioned above, uptake of AI should only take place when risks are mitigated and controlled. The plan should take due account of fundamental rights and societal impacts of AI.;5 - Very important;5 - Very important;1 - Not important at all;;2 - Not important;4 - Important;4 - Important;3 - Neutral;No opinion;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;AI cannot capture nuances and take extraordinary circumstances into account. There is the risk that AI draws too simplistic conclusions, based on standard values when the norm is not applicable, leading to inaccurate and undesired results.;There is a need for a new legislation;;No;;;;predictive policing, biometric surveillance, emotion and identity detection/analysis, access to public services, social and financial rating systems, hiring systems, etc.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;Biometric identification systems in publicly accessible spaces lead to mass surveillance and pose threats to fundamental rights, such as the right to privacy, data protection, non-discrimination, freedom of expression and of assembly.;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;AI applications should undergo a human rights impact assessment.;Mental health risks;Online harms, risks to democracy, to freedom of expression, threats to privacy and data protection, discrimination;Yes;;Yes;;Yes, for all AI applications;;;
F529856;15-06-2020 18:35;French;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;Soutenir les institutions culturelles à produire davantage de données et à développer des outils et des services pour leurs chercheurs.;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;Soutenir les infrastructures existantes (Europeana, Dariah …);3 - Neutral;5 - Very important;4 - Important;Donner les moyens aux institutions culturelles de traiter leurs données grâce à l’IA afin de rendre de meilleurs services.;5 - Very important;4 - Important;5 - Very important;5 - Very important;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;;;No opinion;;No opinion;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;;Very much;;No opinion;;;;;No opinion;;No opinion;;No opinion;;;
F529855;15-06-2020 18:14;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;;5 - Very important;3 - Neutral;5 - Very important;3 - Neutral;3 - Neutral;3 - Neutral;;5 - Very important;5 - Very important;4 - Important;;No opinion;No opinion;No opinion;No opinion;No opinion;;4 - Important;4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;;Current legislation may have some gaps;;No opinion;;;;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;No further guidelines or regulations are needed;;No opinion;;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;;;;No opinion;;No opinion;;No opinion;;;EU_AI_White_Paper_-_Motorola_Solutions_comments_FINAL.pdf
F529854;15-06-2020 18:12;English;Company/Business organisation;Helena;Ursic Vrabec;;Palantir Technologies Inc.;553786035745-66;Large (250 or more);United States;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;Palantir_Statement_on_EC_Consultations_on_AI.2020.pdf
F529853;15-06-2020 17:57;English;NGO (Non-governmental organisation);Valerie;Thomas;;Regulatory Institute;;Micro (< 10 employees);Portugal;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;No opinion;4 - Important;No opinion;In focussing the efforts of the research and innovation community, some thought should be given as to how regulation will be used to manage the research risks and technology of the community. We have previously written about how regulation could contain risks linked to research projects (https://www.howtoregulate.org/regulating-research-technology-risks-part-i-research-risks/#more-248) and risks linked to the use of technologies (https://www.howtoregulate.org/technology-risks/#more-252).;5 - Very important;5 - Very important;No opinion;No opinion;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;In supporting a research and innovation community, the framework for the classification of research risks should be clear so that research of limited benefit for humanity is not inadvertently supported. We described (https://www.howtoregulate.org/classification-research-technology-risks/#more-296) how research and technology risks could be classified in regulation.;No opinion;No opinion;No opinion;No opinion;No opinion;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"We write about regulating AI here (https://www.howtoregulate.org/aipart2/#more-327), where government opts to use AI or some other automated decision in delivering public services or outcomes in support of the public interest, such use should be ""reviewable"" as per the principles of administrative law and certainly no such use should run afoul of such principles.";Other;Both current legislation has some gaps and there is a need for a new legislation. For example AI use in particular sectors may require amending existing legislation eg. migration legislation to be amended on the use of AI systems to support migration-related decisions. However, new legislation is required in areas dealing with oversight functions of AI development for example.;Yes;;Yes;;The issue raised about supply chain is pertinent, particularly around data used to train AI ie. company HQ in country X, data for training AI country Y and AI product sold in country Z.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Very much;We have written about best practices for labelling in our Handbook (https://www.howtoregulate.org/wp-content/uploads/2015/04/Handbook-INT-V1-3.pdf), pages 29 and 77 refers, particularly the importance of setting limits around own-brand labelling.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;AI research and technology that raises an existential risk should be prohibited;No opinion;Please see our article on risk classification for research and technology (https://www.howtoregulate.org/classification-research-technology-risks/#more-296), incluidng our suggestion for a prototype regulation (https://www.howtoregulate.org/prototype-regulation-research-technology/#more-298);Yes;;Yes, for specific AI applications;Applications that would directly affect a person's safety or liberty, possibly including financial loss eg decisions made by govt, judicial decisions, applications that could reinforce bias eg. public housing decisions contracted to third-party to manage.;;
F529852;15-06-2020 16:49;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;2 - Not important;4 - Important;5 - Very important;5 - Very important;5 - Very important;The European Union is leading in AI innovation. At the same time, it is lagging in the marketisation and industrial uptake of existing research. EU efforts should prioritise real-life applications of AI-based solutions, through the creation of partnerships with the private sector, while paying particular attention to SMEs. ;4 - Important;4 - Important;5 - Very important;4 - Important;4 - Important;5 - Very important;;5 - Very important;3 - Neutral;5 - Very important;;5 - Very important;4 - Important;4 - Important;4 - Important;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;;Other;Legislators need to define general rules and boundaries ensuring that AI systems are developed in a human-centric manner. Furthermore, the European Commission should explore verticals, where the application of AI systems may happen only under human oversight and define regulations for subsequent fields. The application of AI in healthcare (i.e. medical imaging), telecoms and the automotive sector, could serve as an example. ;Other;The distinction between high, low and no-risk AI systems should be based on mandatory Commission preliminary assessments that will cover AI applications in all verticals, unless a specific application field is already covered and protected by existing legislation. ;;;;5 - Very important;4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;No further guidelines or regulations are needed;The General Data Protection Regulation and the Law Enforcement Directive already provide a robust legislative framework. ;Much;A governance system is critical to guaranteeing the accuracy of voluntary labelling systems and ensuring high quality of datasets. ;A combination of ex-ante compliance and ex-post enforcement mechanisms;;AI systems should not be treated differently to any other product. Ex-ante assessments are necessary to guarantee the security of AI systems, while ex-post enforcement mechanisms ensure the consistent behaviour of AI-based systems throughout their lifecycle.  ;Mental health risks;;Yes;;No opinion;;No opinion;;Legislation should come out in a timely manner to ensure Europe’s global competitiveness. In the past, Audi abandoned its plans to introduce its autonomous driving technology to European markets due to the lack of an appropriate legislative framework. Furthermore, future AI legislation should avoid over-regulation striking a balance between business clarity, technological innovation, and market uptake of AI systems. ;HPE_Position_Paper_-_White_Paper_on_AI_June_2020.pdf
F529851;15-06-2020 15:34;English;Business Association;Adam;Roose;;The Alliance for AI in Healthcare;;Micro (< 10 employees);United States;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;Expanding on item E, in parallel to the push for adoption, we recommend an investment in education around AI systems to advance understanding, comfort, and willingness to use. Separately, there should be a focus on commercialization and reimbursement policy for AI enabled healthcare products which is not outlined;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;4 - Important;Streamline or standardize across members states the process for regulatory and commercial approval, implementation, and reimbursement for AI enabled healthcare products;4 - Important;5 - Very important;5 - Very important;Consider development and management of central, standardized data sets to provide access to reliable, quality data for EU consumers;3 - Neutral;4 - Important;5 - Very important;5 - Very important;5 - Very important;Promote data access and improve access to sources of RWE for therapeutic development by SME's;4 - Important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;5 - Very important;AI algorithms may not be trained appropriately and so can carry biases impacting developmental outcomes in the healthcare setting;Other;Regulation and legislation should strike the balance between setting a high bar for organizations to aspire to, while recognizing current gaps, challenges, and obstacles to companies attaining that bar. Example: Current datasets are not perfect, so a general EU expectation of perfection in datasets would move much of the innovation happening today away from the EU and toward jurisdictions with more nuanced regimes focused on case-specific risk-benefit;Other;New requirements should take into account the benefits of AI, as well as the benefits and risks of non-AI alternatives. The comparative benefits and risks should be determinative, not just the objective risks of the AI system.;;;The current proposal is good, but item 1 in the list of criteria based on sector of use does not do much to determine high-risk applications and so should be clarified. Further, the criterion laid out does not clearly state who is deemed the responsible party for ensuring explain-ability of AI platforms, key to determining liability.;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;4 - Important;No opinion;;Very much;"Could consider creation of different 'grades' of voluntary compliance; healthcare companies, e.g., may not be able to live up to all data quality regulations owing to intrinsic challenges in healthcare data, but said companies may wish to opt into this voluntary system as much as possible.";A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;Genomic privacy and self-identifying data sets have potential to fuel AI-driven innovation in healthcare to healthcare but are nuanced due to their inherit risk of revealing deeply personal information;Yes;;Yes;;Yes, for specific AI applications;For healthcare, diagnostics and clinical decision support;;European_Commission_-_AI_Whitepaper_Additional_Considerations_-_vF_14_JUN2020.pdf
F529850;15-06-2020 15:33;English;Other;Sören;LENZ;;Conference of European Churches;"Conference of European Churches (CEC)
Identification number:
55481528937-66
Registration date:
08/06/2012
Section:
III - Non-governmental organisations
8937 Conference of European Churches 55481528937-66 CEC Attendance...Parliament in Strasbourg";Large (250 or more);Belgium;The feedback can be published with your personal information;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;;3 - Neutral;3 - Neutral;3 - Neutral;;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;No opinion;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;;No opinion;;No opinion;;;Mental health risks;;No opinion;;No opinion;;;;;TG_SCI_White_Paper_final_15.06.2020.pdf
F529849;15-06-2020 15:26;German;Business Association;Anna;Dietrich;;Bundesverband Digitale Wirtschaft (BVDW) e.V.;479540331468-69;Small (< 50 employees);Germany;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;B2B Wissenstransfer;5 - Very important;3 - Neutral;5 - Very important;4 - Important;5 - Very important;5 - Very important;;2 - Not important;2 - Not important;5 - Very important;Zwingend gefördert werden muss die Zusammenarbeit zwischen Forschung und Unternehmen.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;2 - Not important;#NAME?;2 - Not important;2 - Not important;2 - Not important;2 - Not important;3 - Neutral;2 - Not important;"KI primär als Chance sehen. Risiken immer auch gegen den Nutzen abwägen.
KI ist nicht Ursache von Diskriminierung sondern macht diese in den Daten sichtbar.
Auch menschliche Entscheidungen sind nicht immer ethisch oder objektiv. ";Current legislation may have some gaps;;Yes;;Yes;;;5 - Very important;5 - Very important;4 - Important;No opinion;4 - Important;4 - Important;No opinion;;Not at all;"- Inflationäre Effekte bringen mehr Nachteile als Vorteile. 
- Alle müssten ihre verschiedensten z.T. auch bereits am Markt etablierte Produkte zertifizieren lassen.
- Gefahr des Missbrauchs, der dann ggf. massenhaft imageschädigend wirken könnte.
- Wer k";Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;"Anhaltspunkte, was als Unternehmen alles dokumentiert und wie lange aufbewahrt werden muss.
";;;No;KI, Machine-Learning, Software-Update etc. pp. trennscharf unterscheiden in der Risikobewertung.;No;;No;;;
F529848;15-06-2020 15:01;English;Academic/Research Institution;Haydn;Belfield;;By Haydn Belfield, José Hernández-Orallo, Seán Ó hÉigeartaigh, Matthijs M. Maas, Alexa Hagerty, Jess Whittlestone, at Universitat Politecnica de Valencia, Copenhagen University, Cambridge University, and Leverhulme Centre for the Future of Intelligence.;;Large (250 or more);United Kingdom;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"Keeping fundamental research and scientific diversity in AI is important to be ready for the next paradigm.
On skills: Possibly consider professionalisation or certification of AI engineers, like other engineers. ";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"Build trust and confidence with the other countries via technical collaborations e.g. via the 'lighthouse' and joint horizon-scanning and risk assessment efforts e.g. via the Global Partnership on AI.
Coordinate internally and with allies on incorporating AI technology inupcoming legislative action on export control and on a common framework for the screening of foreign direct investment (Reg 2019/452)";5 - Very important;5 - Very important;5 - Very important;"""Attract AI talent to Europe” with an ambitious AI doctoral programme.
Fund research into ethical & secure AI via Horizon Europe on e.g. interpretability, privacy-preserving machine learning & encrypted computation.
Increase funding of academic computing resources for a level playing field with industry.
Support bodies like the European Academies Scientific Advisory Council to develop publication & release norms & provide additional guidance to Horizon Europe project proposal appraisal.";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"Recommendation: Consider including security explicitly in the list of concerns. 
It would be appropriate to also include security as one of the concerns. “Safety” generally refers to proper internal functioning of an AI system and the avoidance of unintended harms, while “security” addresses external threats to an AI system and the malicious use of AI as a tool for attacks. 'Security' is emphasised throughout the White Paper, especially in (5.D.d). ";There is a need for a new legislation;;Yes;;Yes;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;We need a moratorium. Moratoria have been successful in other areas, from the Asilomar Conference on Recombinant DNA to gain-of-function experiments in disease research. Use of remote biometric identification systems is currently being reassessed by companies from Amazon to IBM, and from Axon to Microsoft.;Very much;A voluntary labelling system can promote trust and uptake. It should be specified that the labels are specified by the conformity assessment, and it is the decision to submit one’s AI system for assessment that is voluntary.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;"Recommendation: Keep the commitment to ex-ante, external conformity assessments by independent testing centre(s), supplemented with additional ex-post market surveillance and enforcement, for example for software updates and systems that keep learning during operation. Self-assessment cannot ensure trustworthy AI.
We make several other recommendations in the attaced document.";Mental health risks;;Yes;"Require periodic safety reassessments for systems that keep learning during operation. They should be resubmitted for assessment after a time-period determined by officials during the previous assessment.
Create an obligation to monitor systems that keep learning during operation for unanticipated harms. Both active and passive monitoring are established under German law as the ‘product monitoring obligation’. This is similar to the concept of pharmacovigilance or the EMA's EudraVigilance.";Yes;Resolve the ncertainty as to whether “stand-alone” software is a product or service, and restrict use of the 'development risk defence' and 'later defect defences' for liability protection, as recommended by the Expert Group on Liability and New Technologies;Yes, for all AI applications;;;Consultation_Response_White_Paper_on_AI_-_Belfield__Hern_ndez-Orallo____h_igeartaigh__Maas__Hagerty__Whittlestone.pdf
F529847;15-06-2020 14:47;Lithuanian;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;The administrative burden of funding should be very minimal, and funding terms must acknowledge that investments in innovation are inherently risky.;5 - Very important;4 - Important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;;4 - Important;4 - Important;4 - Important;4 - Important;3 - Neutral;4 - Important;;Current legislation may have some gaps;;Other;"1. Only products (application) and not technology, algorithms should be regulated with any new legislation. 
2. High-risk applications should be determined not only by the extent of the harm but also by the probability of occurrence of that harm. 
3. The new regulation should be introduced only if current legislation is not enough for a particular area of application and only if it is necessary";;;;1 - Not important at all;1 - Not important at all;4 - Important;4 - Important;3 - Neutral;3 - Neutral;No further guidelines or regulations are needed;;Rather not;;Other enforcement system;Separating technology development from products, combination of ex-ante risk assessment (where not only extext of a harm, but also the probability of occurance is taken into account), evalutation of existing horizontal and sector regulation and ex-post market surveillance would make regulations specific, smart and would not hinder the development and application of AI.;;Mental health risks;;Yes;;No;The current legislation is fully suficient and ensures technology-neutral approach;No;;;INFOBALT_analysis_of_EC_whitepaper_on_artificial_intelligence.pdf
F529846;15-06-2020 14:03;Dutch;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;4 - Important;;4 - Important;3 - Neutral;4 - Important;5 - Very important;5 - Very important;5 - Very important;;3 - Neutral;4 - Important;4 - Important;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;;There is a need for a new legislation;;Yes;;Yes;;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Not at all;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Cyber risks;;Yes;;Yes;;Yes, for all AI applications;;;
F529845;15-06-2020 13:40;French;;;;;;;;;The feedback can be published in an anonymous way;3 - Neutral;4 - Important;5 - Very important;3 - Neutral;5 - Very important;No opinion;Create a framework for the ethics of data;5 - Very important;3 - Neutral;No opinion;4 - Important;5 - Very important;3 - Neutral;Ethics of AI / Barriers of entry;5 - Very important;5 - Very important;4 - Important;Large framework for public & private partnerships;3 - Neutral;3 - Neutral;4 - Important;4 - Important;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;Impact of AI on employment / employability;Current legislation may have some gaps;;Yes;;Yes;;Employment / Health / Housing / Banking / Education / Citizen Ratings;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;Biais & Discrimination;Yes;;No opinion;;;;;
F530430;15-06-2020 12:17;German;Company/Business organisation;Alexander;Klier;;Beck et al. GmbH;;Medium (< 250 employees);Germany;The feedback can be published with your personal information;5 - Very important;4 - Important;3 - Neutral;5 - Very important;4 - Important;4 - Important;"Screening relevanter Aktivitäten in Nicht-EU-Ländern mit Hinblick auf thematischen Schwerpunkte der Maßnahmen.
Anknüpfungspunkte, Kollaboration, Synergien mit diesen programmatisch fördern - der technologisch-wissenschaftliche Kontakt besteht auf Eben der Einzelorganisationen ohnehin.
Monitoring der Entwicklungen von Start-ups innerhalb der EU, Kollaborationen fördern, die Einzel-Lösungen ingenieurmäßig zusammen binden.";5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;"Es fehlt die Forderung / Förderung der Exzellenz hinsichtlich der Umsetzung! Exzellente Forschung muss in exzellente Entwicklung und Einsatz münden, etwa wie es die Mission der Fraunhofergesellschaft mit ihren Instituten ist. Hier ist die EU-internationale Zusammenarbeit mit Ziel auf ""Exzellenz der Ergebnisse"" strategisch zu sehen.
Aufbau nicht nur eines europäischen Datenraums sondern einer oder mehreren Zusammarbeitsplattformen mit kollaborativ-offenem Zugang zu Entwicklungen und Tools.";3 - Neutral;5 - Very important;4 - Important;"Aufbau regionaler und thematischer (scientific) Communities, in die sich die jeweiligen Akteure niedrigschwellig einbringen können. Vorhalten einer entsprechenden Infrastruktur bzw. Plattform, um die Vernetzung problemlos zu ermöglichen.
Aufbau und Förderung einer organisatorischen Infrastruktur (statt Leitzentren), die die selbstorganisierte und ggf komplementäre Kollaboration unterstützt.";4 - Important;5 - Very important;3 - Neutral;5 - Very important;4 - Important;"Beck et al. als KMU hat langjährige Erfahrung mit dem Einsatz von KI-Methoden und -Produkten bei Kunden. Insofern ist zu trennen zwischen KMU's, die state-of-the-art KI-Entwicklungen verfolgen und kennen müssen, um damit ihren Kunden Mehrwert zu verschaffen, und solchen, die als potenzielle Anwender sich selbst um den Einsatz von KI - z.B. in der Produktion, Logistik etc. - kümmern sollten.
Es sollte ein (EU-weites) Kompetenz-Netzwerk von beratenden Unternehmen dazu initiiert  werden.";3 - Neutral;5 - Very important;4 - Important;3 - Neutral;4 - Important;3 - Neutral;KI in eingebetteten / autonomen Systemen entscheiden in Realtime unabhängig von Menschen.;There is a need for a new legislation;;Yes;;Yes;;Vorhandene geltende RV sollten explizit für KI-Bedenken interpretiert werden. Neue, KI-spezifische Bedenken sind in neuen RVs abzubilden.;5 - Very important;3 - Neutral;5 - Very important;4 - Important;4 - Important;4 - Important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Rather not;Es wird nur verpflichtend funktionieren.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Risks related to the loss of connectivity;;Yes;;Yes;Haftung für eingebettete KI-Produkte (Daten & Algorithmen) muss durchgängig verantwortet werden (ähnlich DSGVO).;Yes, for specific AI applications;Für eingebettete und autonome KI Systeme bzw. Systeme, die substanzielle Empfehlungen geben.;;Stellungnahme_KI_-_Wei_buch_EU_Kommission.pdf
F530429;15-06-2020 12:13;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;;4 - Important;4 - Important;4 - Important;5 - Very important;3 - Neutral;;4 - Important;5 - Very important;4 - Important;4 - Important;3 - Neutral;4 - Important;;Current legislation may have some gaps;;No;;;;;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;No opinion;;Yes, for all AI applications;;;
F530428;15-06-2020 11:38;English;Company/Business organisation;Harriet;Kingaby;;BoraCo;;Micro (< 10 employees);United Kingdom;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;4 - Important;;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;;There is a need for a new legislation;;No;;;;The use of facial and emotional recognition in public is highly concerning, particularly when 'clustered' and stored with location data, as in the case of smart TVs.;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;"We strongly advocate a change to a human rights based approach, with a sliding scale of ‘risk’ that encompasses: 
Applications of AI which should be banned. 
Applications which require a full, publicly available human rights impact assessment by third parties, which are repeated every 2 to 3 years. 
Applications which require impact assessments to be conducted every few years, by industry, and audited by third parties. ";Not at all;Voluntary labelling systems allow bad actors to exploit the system. We do not support their use.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;;;;"See above - unknown unknowns must be monitored and acted upon. More information is needed about the methodology for quantifying and categorising high risk.
";Yes;;Yes, for all AI applications;;;AI_consultation_template_BoraCo_1_.pdf
F530427;15-06-2020 11:29;English;Business Association;Angela;Lo Mauro;;FEDIL - The Voice of Luxembourg's Industry;286194516022-33;Small (< 50 employees);Luxembourg;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;3 - Neutral;4 - Important;4 - Important;4 - Important;3 - Neutral;4 - Important;;No opinion;;Yes;;Other;"we strongly recommend using a “Matrixial” approach, with a functional and technical level, to define what is high risk. In this process, our members are convinced that the determining factor should be the high level of the interaction between the AI application and human beings. The “risk score” should define what level of requirement will be called for, on a mandatory or voluntary basis, and according to the proportionality principle
";;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;4 - Important;No opinion;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Cyber risks;;No opinion;;No opinion;;No opinion;;;FEDIL_Position_Paper_on_Trustworthy_AI__final__no_scheme.docx
F530426;15-06-2020 10:52;English;Other;Secretary;GENERAL;;Big Data Value aisbl;042849916153-53;Micro (< 10 employees);Belgium;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;[Paper];5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Europe has already a relevant innovation ecosystem and the concrete opportunity to become a global leader. Relevant and focused investments will be necessary, as well as building on the excellence and best practices already in place, and developing tools and solutions to maximise AI applicability while being fully compliant with the regulatory, policy and ethical framework. The AI, Data and Robotics partnership will play an essential role in consolidating this European ecosystem and AI adoption.;3 - Neutral;5 - Very important;5 - Very important;"A coordinated and comprehensive effort is required to achieve a World Leading Centre in AI. The EC is investing strongly in this area through different programmes such as the AI on demand platform, the AI Network of CoEs, the AI DIHs, the AI TEFs, and the AI, Data and Robotics partnership. It is essential the alignment and synergies among all these instruments. In this complex landscape additional instruments such as the  “lighthouse centre of research.."" could lead to too much fragmentation.";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;There is a narrow definition of SMEs in the context of the AI whitepaper, as SMEs can be AI solution providers and/or AI solution consumers. Both viewpoints should be included. The support of DIHs to SMEs and start-ups should include not only the AI perspective, but also a data driven approach as a previous step. The Digital Innovation Hubs are already in place, ready to receive Data and ready to support AI systems;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;No opinion;3 - Neutral;;Current legislation may have some gaps;;Yes;;No;;It remains unclear which criteria will be used to determine whether the existing framework sufficiently addresses the risks created by AI systems. The distinction between high-risk and low-risk AI systems should thus be further clarified. Without a taxonomy of risk, self-assessment of “high risk versus low risk” and how close a product/service is to the threshold of “high risk” is not quantifiable.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;5 - Very important;No opinion;;Much;Voluntary certification can have several benefits, both for purchasers of the certified AI system as well as for its producer. Such certification increases the confidence of users in AI systems as it indicates the producer’s commitment towards higher safety and quality standards. At the same time, however, voluntary certification should be carefully addressed as it can result in a meaningless label and even increase non-compliant behaviour when there are no proper verification mechanisms. ;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;These prior assessments could include procedures for testing, inspection or certification. The introduction of certification schemes for AI-systems should be carefully addressed: what exactly can and will be certified (process vs. system), who will certify (cf. public/private bodies), impact on market access for new players and on competition, etc. Experiences from other sectors in which certifiers provide their services also illustrate that several (legal) challenges remain (cf. immunity, liabi;;;No opinion;;No opinion;;No opinion;;;BDVA_s_reponse_to_the_European_AI_whitepaper_-_May_2020_-_ed1.pdf
F530425;15-06-2020 08:59;German;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;4 - Important;4 - Important;4 - Important;3 - Neutral;4 - Important;Es muss auch eine ehrliche Diskussion darüber geben, welche Bereiche mit AI unterstützt werden sollen und welche nicht. Beispielsweise finde ich AI-gestützte Wettervorhersagen unproblematisch, die AI-gestützte verhaltensforschung individueller Personen hingegen sehr problematisch.;4 - Important;4 - Important;3 - Neutral;3 - Neutral;4 - Important;4 - Important;Meine Unterstützung für AI-Projekte ist stark von der Anwendung abhängig. Darum gebe ich keine generelle Unterstützung für Start-Ups und den öffentlichen Sektor ab.;4 - Important;4 - Important;3 - Neutral;;4 - Important;4 - Important;4 - Important;4 - Important;3 - Neutral;;5 - Very important;5 - Very important;4 - Important;4 - Important;3 - Neutral;2 - Not important;;No opinion;;No opinion;;;;Autonomes Fahren, da Fahrzeuge ein sehr hohes Risiko für die Insassen und Passanten tragen.;3 - Neutral;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;Es soll sichergestellt sein, dass ein Minimum an Daten und Metadaten generiert wird, wenn diese Systeme benutzt werden. Vor Allem darf keine Nachverfolgung möglich sein (effektiv soll sich die rechtliche und praktische Situation gegenüber von analogen Systemen aus Sicht der Benutzer nicht verschlechtern).;No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Vertrauen im Sinne von Prüfbarkeit, dass die Integrität des Systems nicht verletzt wurde, muss hergestellt werden. Es muss im besten Fall für jeden Benutzer möglich sein das System vor Benutzung auf Integrität zu prüfen.;Personal security risks;;Yes;;Yes;;Yes, for all AI applications;;;
F530424;15-06-2020 02:17;English;Company/Business organisation;John;Hasselmann;;ASME (The American Society of Mechanical Engineers);;Large (250 or more);United States;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"Related to bridging skills gaps (§4C), ASME stresses the importance of encouraging edu. institutions, at all levels, to prepare the next gen. of engineers to be competent in digitization, eg. coding. Across a wide range of industries, 500 engineers prioritized specific competencies in these areas: data management, analytics & web tech services; hands-on experience to validate models, especially in mfg. and robotics; & digital engineering mgmt., specifically business efficiency (report attached).";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;"ASME applauds the Commission for committing to invest in public-private partnerships and innovative research (as outlined in 2018's ""Coordinated Plan""). ASME is a trusted neutral convener of government, industry, and academia; we offer our support as the EC revises this Plan for 2020, especially toward AI solutions that can be deployed to readdress, improve, and revolutionize engineering systems and workflow. ";5 - Very important;5 - Very important;5 - Very important;ASME is a major supporter/partner of the Manufacturing USA Institutes, a network of public-private partnerships bolstered by federal funding, akin to Germany’s Fraunhofer Insts. As Horizon Europe details, research ctrs. that can both attract global talent & catalyze commercialization are important tools for tech development. From our standpoint as a neutral not-for-profit, effective execution of this end requires a level playing field that excludes favoritism toward any one company or industry.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;The U.S. maintains the lead in infrastructure, especially given AI development in Silicon Valley/Boston. And there are estimates that China will invest $100bn on AI. But in the context of COVID, the conversation needs to go beyond the regional innovation ecosystem that makes SV successful, when social distancing and a likely scattered workforce could change the model of Dig. Innovation Hubs in Europe. There needs to be a related re-imagination of environments conducive to success (and failure). ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Having consulted a network of US mfg. experts, ASME asserts that AI, a topic which seemingly eclipses all schools of thought, is often oversold as a technological solution. AI products/systems need secure override mechanisms to protect against risks this paper covers. E.g., automated/robotic systems incorporating AI need underlying override, real-time controller response to ensure physical safety when AI fails. This may require adjustments in existing legislation, standards and/or regulations.;Current legislation may have some gaps;;Yes;;Yes;;§5 rightly IDs uncertainties among surveillance/enforcement authorities as a means for AI to “increase or aggravate the risks.” A disjointed regulatory scheme furthers those risks. ASME is a global standards-devel. org., offering a continuously evolving portfolio of stds. for pressure tech, power plants, elevators, construction, nuclear & more. Uncertainties inherent to an unregulated AI scheme could be mitigated by using the best global stds., many which are not optimally used in the EU system.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);"Biometric identification systems are an example of technologies that, if developed effectively and implemented transparently, could both expedite and safeguard public processes. On the other hand, a February 2020 report from experts at Stanford and New York Universities called on the U.S. government to improve its internal AI expertise to leverage biometrics specifically. U.S. Customs and Border Protection (CBP), for example, employs facial biometrics but cannot explain rates of failure due to the proprietary nature involved. Thus, an example of a “certain condition” that must be met is meaningful accountability and disclosed options for public recourse. The CBP example underscores that private-sector AI development can often clash with government’s imperative to inform the public. This relates back to the fact that protections may require adjustments in existing legislation, standards, and/or regulations; and should apply to systems and products including both original and third-party/after-market AI additions/upgrades. ";Much;A voluntary labeling system would provide end users a nominal measure of AI trustworthiness for a product or system. Whereas CE (European Conformity) and UL (Underwriters Laboratories) certifications are mandatory, something akin to ENERGY STAR certification could serve as that validation. Without a consistent, standardized framework for accountability and reproach, the public would likely distrust these AI systems and feel insecure in a context without public recourse.  ;No opinion;;Not at this time.;Cyber risks;§5B rightly identifies cyber threats as major risks that EU legislation may not explicitly address. Across a wide range of industries, 500 engineers agreed that cyber attacks are a major area of ethical concern (ASME-YouGov report attached). Broadly, bolstering legal certainty in this realm would fall under the EU Cybersecurity Act’s recent strengthening of ENISA, which, from ASME’s perspective, is an important means of safeguarding mechanical engineering assets. ;Yes;Not at this time.;No opinion;Not at this time.;No opinion;;Not at this time.;ASME-YouGov_White_Paper_re_Digital_Engineering.pdf
F530423;15-06-2020 01:48;French;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;4 - Important;3 - Neutral;5 - Very important;3 - Neutral;Increase the financing of focussed collaborative projects for the research and innovation communities to apply and integrate AI-based functions in useful products or services in compliance to the seven requirements established by the EU expert group report on AI of Trust. ;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;"1/ Increase the financing of focussed collaborative projects for the research and innovation communities to apply and integrate AI-based functions in useful products or services in compliance to the 7 requirements established by the EU expert group report on AI of Trust. 
2/ Increase the financing of focussed collaborative projects for the research, industry and institutions to develop means of compliance to the above seven requirements for AI-based functions, products and services";1 - Not important at all;4 - Important;5 - Very important;Increase the financing of focussed collaborative projects for the research, industry and institutions to develop means of compliance for AI-based functions, products and services to comply to the 7 requirements established by the EU expert group report on AI of Trust. ;4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;;No opinion;No opinion;No opinion;No opinion;No opinion;5 - Very important;AI-based functions are like all other technology that presents a number of weaknesses and may cause some risks. Their use has to be conditioned to an agreed proof, based on a number of means of compliance, that such AI-based functions are compliant to the 7 requirements established by the EU expert group report on AI of Trust : robustness and safety, accuracy, transparency, equity, responsibility, social & environment impact, human oversight, human factor, as for example, in EASA regulations. ;Current legislation may have some gaps;;No;;;;I have personally no opinion to express about that. Some automatic financial trade systems and some personal data exploitation systems may be already using some AI-based technology and such data exploitation systems seem not to be regulated by data protection laws. It is not a matter of data protection.  Is it low-risk ? Risks must be assessed specifically for each and all applications.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Other special requirements in addition to those mentioned in the question above should be imposed (please specify);"Risks must be assessed specifically for each and all applications and risks must be mitigated and compensated according to the possible harm caused by the use of these functions. This should apply all the same to AI-based and non-AI-based functions. Risks should be assessed prior to the development of applications and not pre-supposed high or low. 
Systems (AI or not) must be compliant to the 7 requirements established by the EU expert group report on AI of Trust : robustness and safety, accuracy, transparency, equity, responsibility, social & environment impact, human oversight, human factor,  etc";No opinion;"Some level of proof or qualification is required for all the applications within each specific domain. Domain specific regulations must be adapted when necessary and applied to both AI-based and non-AI-based systems. The corresponding assurance is not ""just another label"" : it is a true methodology with regulations and tools applicable to demonstrate compliance to the requirements, not a label to show that there is AI-inside.
How do you define what is AI-based and what is not AI-based ?";A combination of ex-ante compliance and ex-post enforcement mechanisms;;The most appropriate way may depend on the domain of application. EASA is responsible of that in the Aeronautics domain and EASA develops a roadmap for regulations evolution : with ex-ante and ex-post means of compliance;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;Yes for all new applications, AI-based or not AI-based;Syst_mes___base_d_IA.pdf
F530422;14-06-2020 23:58;English;Academic/Research Institution;Camylle;Lanteigne;;Montreal AI Ethics Institute;;Micro (< 10 employees);Canada;The feedback can be published with your personal information;4 - Important;5 - Very important;4 - Important;3 - Neutral;4 - Important;2 - Not important;Please see the attached PDF for more details;3 - Neutral;2 - Not important;4 - Important;5 - Very important;5 - Very important;4 - Important;Please see the attached PDF for more details;3 - Neutral;5 - Very important;4 - Important;Please see the attached PDF for more details;2 - Not important;3 - Neutral;5 - Very important;5 - Very important;3 - Neutral;Please see the attached PDF for more details;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;Please see the attached PDF for more details;Current legislation may have some gaps;;No;;;;;4 - Important;5 - Very important;2 - Not important;5 - Very important;3 - Neutral;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;Please see the attached PDF for more details;Much;Please see the attached PDF for more details;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Please see the attached PDF for more details;Mental health risks;Please see the attached PDF for more details;Yes;;Yes;Please see the attached PDF for more details;Yes, for all AI applications;;Please see the attached PDF for more details;EC_AI_Whitepaper_-_Submission_by_MAIEI.pdf
F530421;14-06-2020 23:57;Hungarian;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;;3 - Neutral;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;;Current legislation may have some gaps;;Yes;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No further guidelines or regulations are needed;;Very much;;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;;Personal security risks;;Yes;;Yes;;Yes, for specific AI applications;;;
F530420;14-06-2020 23:53;English;Company/Business organisation;Yannis;Skoulikaris;;PatentMind Netherlands, consultancy and training in intellectual property matters;;Micro (< 10 employees);Netherlands;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;establish a scoping and monitoring service for intellectual property rights (like patents, which mean commercial monopolies) involving AI and affecting innovation in Europe;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;Other;applications should be categorized according to risk and kind of repercussions, and rules should be adopted accordingly;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;more legal certainly is needed in patenting AI applications, in order to make such patenting predictable and avoid unforeseen negative effects on innovation;Yes;creation and functioning of a specialized AI risk assessment agency with special authority;Yes;;Yes, for all AI applications;;;
F530419;14-06-2020 23:50;French;Academic/Research Institution;Guillaume;Avrin;;Laboratoire national de métrologie et d'essais;;Large (250 or more);France;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;No opinion;4 - Important;4 - Important;;5 - Very important;5 - Very important;4 - Important;No opinion;5 - Very important;5 - Very important;;3 - Neutral;5 - Very important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;5 - Very important;;Current legislation is fully sufficient;;No opinion;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;No opinion;;No opinion;;;Avrin_livreblanc_IA.pdf
F530418;14-06-2020 23:50;English;EU Citizen;simeon;de brouwer;;;;;Belgium;The feedback can be published with your personal information;4 - Important;4 - Important;4 - Important;3 - Neutral;1 - Not important at all;1 - Not important at all;"For a good ecosystem of excellence, AI should not be blindly promoted. AI applications should be promoted only when there is evidence that AI is a right response to a specific problem. Further, additional safeguards for fundamental rights in the lifecycle of AI applications are needed. This, even more in the public sector or public spaces.
Democratic oversight and transparency of impact assessments would also contribute to this ecosystem.";4 - Important;4 - Important;1 - Not important at all;2 - Not important;3 - Neutral;3 - Neutral;"The coordinated plan on AI should incorporate criteria (scientific and policy)
about how the EU will allocate its resources of AI, based on the EU's commitment regarding trustworthy AI and human rights, given the societal impacts of AI and automation. The plan should also put forward mechanisms of (public) oversight as well.";3 - Neutral;3 - Neutral;2 - Not important;"The White Paper puts forward an agenda of research which completely disregards the social, ethical and human rights area of research. These should especially be part of any research agenda on AI in light of all the issues that have been pointed out on the topic (discrimination, impact on communities, impact on wider society's infrastructure, etc.).

To receive EU funding for AI, any project must respect the EU's own ethical standards (cf. AI HLEG) and its laws (esp. fundamental rights laws).";1 - Not important at all;4 - Important;3 - Neutral;4 - Important;3 - Neutral;"Raising anyone's awareness about the potential benefits of AI should go hand-in-hand with awareness about the potentiall issues raised by AI. Otherwise AI will continue to be seen as a silver bullet for every social issue.

When funding AI applications developped by SMEs (or the private sector in general), there should be benefits for the public, e.g. in the form of free software, publicly available results, etc.";2 - Not important;5 - Very important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;"-The focus in public debates is sometimes misplaced: it may matter less that an AI application is *inaccurately* distributing outcomes than that it be distributing outcomes *at all*.
-People will often not even know that they have been 'harmed' (in any fo";Other;"-GDPR provides a solid basis, but needs a complement. It is too focused on the individual (vs societal impact), is not strong enough on affinity profiling, sensitive inferences, or the wide range of to-be sensitive data.
-AI applications that do not proce";No;;;;Facial recognition comes to mind first, but perhaps only because it has received most public attention and scrutiny. Others which are as problematic include scientifically dubious AI (e.g. lie-detector, detection of 'abnormal' behaviour, etc.), autonomous lethal weapons, predictive policing, or AI that delivers or shape services essential for the functioning of society and of the individual (e.g. in public spaces or sector).;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;"-The potential for function creep is too high, surveillance has been shown to be widespread and excessive even in the EU (cf. abuses by secret services in the UK).
-The other risks are too high, incl. for our fundamental rights to privacy, freedom of asse";Rather not;"As we have seen with the GDPR, a significant loophole is left open by allowing the data controller to determine whether a system poses a high risk and whether a data protection impact assessment is needed. We must therefore avoid a situation in which those responsible for deploying an AI system can shirk their responsibilities by ignoring risks.
In any case, though, if there is a mechanism for determining the level of risk, there ought to be mechanisms to contest the initial assessment.";Other enforcement system;"Avoid self-regulation and ethics
Ensure no loopholes in determining the safeguards required";There should be mandatory ex-ante human rights impact assessements, with regular ex-post updates by the company, but also by the regulator in case of suspicion or concern (e.g. reported by users) of issues and irregularities.;Mental health risks;"The assessment of the level of risk of a given use of AI should also be based on wider societal considerations, including the impact on the democratic process, the due process and the rule of law, the public interest, the potential for increased general surveillance, the environment and (concentrations of) market power.
Risks can also arise from the very act of delegating tasks to machines (here, AI) which were previously assigned to humans.";Yes;data protection authorities and officers should be involved, as their expericence in conducting thorough (data protection) impact assessments would be extremely valuable here -- whether or not the AI application in question processes personal data;Yes;Developpers and deployers of AI applications should be clearly liable for any harm created by their (use of the) application, whether it be discrimination, data protection, product liability, consumer protection, competition, etc.;Yes, for all AI applications;;Liability rules should be different for deployers/developpers who 'open the code of AI their application' for review by either independent certified auditors or the public at large, than for those who do not. The burden of proof should not be on individuals, and the existing imbalance of power should be reversed by requiring more transparency, giving more rights to individuals, as well as to NGOs to get access to non-public information, audit, and pursue collective redress;
F530417;14-06-2020 23:50;English;NGO (Non-governmental organisation);Milla;Vidina;;European network of equality bodies (Equinet);718971811339-46;Micro (< 10 employees);Belgium;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;4 - Important;1 - Not important at all;"AI adoption by both public and private sectors should be led by objectives related to equality and human rights and not be encouraged as an end in itself 
Better alignment between the two “ecosystems” with an equality and HR mainstreaming focus 
Accessible “digital literacy” education for the general public to stimulate victims-led recourse to justice and redress possibilities
Financing of R&D should be linked to results of equality and HR impact assessments";4 - Important;4 - Important;1 - Not important at all;2 - Not important;4 - Important;2 - Not important;"The CP should include clear and verifiable criteria based on impact on equality and human rights and translated into a scientific and policy framework for AI development
EU and national financing under the Plan should be conditional on compliance with equality and HR benchmarks
The proposal that the Plan incorporates societal and environmental well-being as a key principle for AI should use a clear definition of well-being that is grounded in common equality and HR legal standards";4 - Important;4 - Important;1 - Not important at all;"The priorities of research and innovation should be set by the existing relevant equality and human rights legal standards due to their proven value in protecting individual and collective interests in society 
For adjustments to equality and human rights standards - see Section 3 below
The focus of AI research should be on providing robust and verifiable evidence on which AI uses are beneficial to the individual and collective interests protected by equality and human rights standards   ";4 - Important;5 - Very important;5 - Very important;5 - Very important;No opinion;DIHs and other innovation promotion tools should be subject to mandatory equality and HR oversight without exceptions based on the size of the entity  ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;"Especially wide-spread risks for equality due to nature of AI systems – AI-enabled decision- making operates through differentiation and exclusion mechanisms, so that the end result is that some individuals and/or groups of individuals unjustly receive more favorable outcomes than others
Disproportionately negative impact on enforcement and redress possibilities for victims of equality and HR violations
Obstructing democratic control over all AI uses in sensitive areas of public interest ";Other;"Strengthening of existing equality and HR legislation, especially at the implementation and enforcement end, should be combined with new rules on AI  
New rules should impose universal mandatory requirements based on the HLEG ethical guidelines and be expressed through the legal frameworks of consumer protection and product safety law 
Careful alignment and harmonization of new rules on AI with existing equality and HR legal protections";Other;"Risk-based approach to AI - specific regulation is not adequate for protecting against the equality and human rights risks of AI
For the purpose of equality and HR protection, the widest possible range of AI uses should be subject to compliance checks based on the requirements proposed by the White Paper and the HLEG ethical guidelines 
Threshold for excluding AI applications from regulatory scrutiny for their effects on equality and HR should be set exceptionally high – see below
";;;"For equality and HR protection against AI risks, there should be no a priori risk definition, instead emphasis should be on strengthening end user redress possibilities

Risk-based exemptions from regulatory scrutiny over AI uses limited to only the most “patently harmless” uses, e.g. manufacturing belts' speed control, which produce beyond reasonable doubt no negative effects on equality and HR. Evidence requirements for assessing harmfulness in that context should be especially strict. 
";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;There should be more scientific evidence-gathering on the equality and HR implications of AI systems on end users in order to compile an accurate and comprehensive list of all AI uses, which are especially detrimental from an equality and HR perspective. Depending on the degree of danger, all such AI uses should be subject to stricter EU-level regulation, including possible bans.  Biometric processing that could lead to mass surveillance in public spaces should fall in that category of stricter regulatory control.;Rather not;"Regulatory oversight to safeguard equality and HR should be mandatory and not based on self-regulation. 
Voluntary control mechanisms in the context of AI and equality and human rights provide leeway to circumvent accountability and weaken legal protections. Furthermore, they undermine legal certainty and impede access to justice for those affected.
";A combination of ex-ante compliance and ex-post enforcement mechanisms;;"In general we welcome the White Paper proposal of introducing AI –specific enforcement bodies mechanism that control compliance with the requirements set by the White Paper 

Our emphasis is that these governance mechanisms, which should mainly focus on technical investigations and inspections of AI systems, should be carefully aligned with and coordinated with existing oversight mechanisms in the field of equality and human rights such as independent equality institutions.";Mental health risks;If the main legal framework for addressing AI-related risks is product safety and consumer protection law, then given the significant implications of AI uses for the protection of equality and HR, the concept of safety should be expanded to cover risks related to equality and HR. In parallel, existing equality and HR legal protections should be strengthened to adequately respond to AI-related risks. ;Yes;Inputs by independent equality and human rights specialised institutions, such as equality bodies, should be mandatory for assessing the equality and HR components of risk. Their contribution should be harmonized with inputs from other relevant authorities such as consumer protection and data protection bodies, who assess other aspects of AI-related risk. The entire coordinated multiple-actor procedure should render one comprehensive risk evaluation.  ;Yes;"Liability should be proportionate to the ability of different actors in the AI supply chain to contain risks, with those developing and deploying AI systems carrying greatest burden, while distributors and end users of AI carrying progressively less responsibility
Given unpredictability and autonomy of AI systems, there should be a legal obligation to conclude insurance, coupled with strict liability     
The burden of proof should be on the developer/deployer and not on those affected ";Yes, for all AI applications;;"Liability for equality and human rights violations by AI systems should not be circumvented through competing legal protections such as intellectual property rights and trade secrecy rules, which preclude the transparency required to access justice and seek redress against these violations 
Legal changes should address “function creep” and repurposing of AI systems by clarifying who is liable for any unexpected changes to an AI product or service once they have been placed on the market
";
F530416;14-06-2020 23:47;English;NGO (Non-governmental organisation);Digitale;Freiheit e.V. i. Gr.;;Digitale Freiheit e.V. i. Gr.;;Micro (< 10 employees);Germany;The feedback can be published with your personal information;No opinion;5 - Very important;5 - Very important;4 - Important;No opinion;1 - Not important at all;;4 - Important;3 - Neutral;No opinion;4 - Important;4 - Important;5 - Very important;;3 - Neutral;No opinion;No opinion;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;;4 - Important;5 - Very important;5 - Very important;4 - Important;No opinion;4 - Important;In the course of time it might undermine our capability to make self-determined choices. This could be prevented by continuously training the public in independent thinking.;There is a need for a new legislation;;No;;;;Remote biometric identification;5 - Very important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;4 - Important;Biometric identification systems should never be allowed in publicly accessible spaces;"Biometric identification systems in public spaces allow for mass surveillance in unknown dimensions. Live video analysis increases the psychological pressure put on citizens by CCTV systems. 

Combined with the possibility of being (wrongly) identified as a wanted person, it leads to a chilling effect. Citizens will refrain from the use of their fundamental rights to freedom of expression, assembly and association. Their right to privacy and respect for private life will be violated.

Mere regulation will legitimize the use of the technology, leading to the establishment of the necessary infrastructure. While in the beginning it will be reserved for exceptional cases, over time the application will presumably become broader. Once fully in place, this technology will be commonly proposed as the standard solution against most types of security threats. 

The EU Commission has identified these issues and, in its draft, contemplated a moratorium to learn more about the risks the technology poses to fundamental rights. It made headlines. German and international media were enthusiastic. Some misunderstood it as a plan for a ban without a time limit. However, an indefinite ban would be the truly ""European approach"" - protecting the fundamental rights of EU citizens, their sense of freedom and anonymity in public spaces. Several cities in the USA have issued a ban. Voices calling for a federal ban in the USA have become louder during the ""Black Lives Matter"" movement. 

Let us - the EU - set an example banning this dangerous technology.";Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Cyber risks;;Yes;;Yes;;Yes, for specific AI applications;;;
F530415;14-06-2020 23:35;English;Company/Business organisation;Tibor;Toth;;Numereco, Ireland;;Micro (< 10 employees);Ireland;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;;5 - Very important;4 - Important;5 - Very important;3 - Neutral;4 - Important;5 - Very important;;4 - Important;4 - Important;5 - Very important;;3 - Neutral;4 - Important;4 - Important;4 - Important;4 - Important;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;3 - Neutral;;There is a need for a new legislation;;Yes;;Yes;;;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;No opinion;;No opinion;;No opinion;;;Personal security risks;;Yes;;Yes;;Yes, for all AI applications;;;Numereco_submission_1.docx
F530414;14-06-2020 23:31;German;Other;Österreichische;Sozialversicherung;;Dachverband der Österreichischen Sozialversicherungen;685141118619-24;Large (250 or more);Austria;The feedback can be published with your personal information;4 - Important;4 - Important;5 - Very important;4 - Important;4 - Important;5 - Very important;Wir vertreten zwar eine neutrale Einstellung gegenüber der Zusammenarbeit mit dem privaten Sektor, dennoch ist eine Kooperation zwischen der Versicherung und beispielsweise den Arztsoftwareherstellern von höchster Bedeutung. Nur so kann eine State-of-the-Art-Technologie für die PatientInnen gewährleistet werden, die dann auch in weiterer Folge Wahlärzten zur Verfügung gestellt werden kann.;4 - Important;4 - Important;5 - Very important;4 - Important;4 - Important;5 - Very important;;4 - Important;4 - Important;4 - Important;;4 - Important;4 - Important;5 - Very important;4 - Important;3 - Neutral;Die Innovationszentren könnten beispielsweise einmal pro Halbjahr ein Get-Together für KMUs desselben Sektors anbieten, bei dem sie mit den derzeit am Markt bestehenden Technologien vertraut gemacht werden und die Möglichkeit bekommen zu Networken.;3 - Neutral;4 - Important;2 - Not important;3 - Neutral;4 - Important;5 - Very important;Ein weiterer Punkt wäre die Befürchtung, dass durch die Weiterentwicklung der KI Arbeitsplätze abgebaut und Menschen in gewissen Positionen überflüssig werden.;There is a need for a new legislation;;Other;Die Einführung von neuen verbindlichen Auflagen sollte so gestaltet werden, dass sie sämtliche Anwendungen neuer KI-Systeme abdeckt und sich nicht nur auf die besonders risikoreichen fokussiert. Bei einem hohen Risiko könnte ggf. ein Zusatz zu einem allgemein gültigen Paragraphen angedacht werden.;;;;4 - Important;3 - Neutral;4 - Important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;;Personal security risks;;Yes;;No opinion;;Yes, for all AI applications;;;2020-06-02_DVSV_Position_Digitalpaket.pdf
F530413;14-06-2020 23:24;English;NGO (Non-governmental organisation);Cecilie;Waagner Falkenstrøm;;MindFuture Foundation;;Micro (< 10 employees);Denmark;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;No opinion;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;No opinion;;Other enforcement system;;;Mental health risks;;Yes;;No opinion;;Yes, for all AI applications;;;2020.05.31_MindFuture_Foundation_Respond_to_the_Commission_s_public_consultation_on_the_white_paper_on_Artificial_Intelligence_Final.pdf
F530412;14-06-2020 23:17;English;Company/Business organisation;Kevin;Vindevogel;;IDEMIA;904218833588-78;Large (250 or more);France;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;IDEMIA_Consultation_on_the_White_Paper_on_Artificial_Intelligence_14JUN20.pdf
F530411;14-06-2020 23:17;English;;;;;;;;;The feedback can be published in an anonymous way;;;;;;;;;;;;;;;5 - Very important;No opinion;No opinion;;;;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;No opinion;;There is a need for a new legislation;;Other;A limitation to high-risk applications would be economical, but the qualification of an app as high-risk or non-high-risk should be done by an independent external organization.;;;;No opinion;No opinion;;No opinion;4 - Important;No opinion;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;The current situation (concerning safety) does not justify the potential risks (misuse) associated with biometric identification systems in public areas (principle of proportionality);Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;;;;;;;;
F530410;14-06-2020 23:16;English;NGO (Non-governmental organisation);Konstantinos;ALIGIANNIS;;EURORDIS - Rare Diseases Europe;93272076510-87;Medium (< 250 employees);France;The feedback can be published with your personal information;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;;5 - Very important;2 - Not important;No opinion;No opinion;No opinion;5 - Very important;;5 - Very important;5 - Very important;4 - Important;Importance should be given to the involvement of all relevant stakeholders in any initiatives proposed. Although industry plays a crucial role in the development of AI technologies, civil society organisations play an key role as well. In the area of health, patient organisations should be structurally involved on priority setting.;No opinion;No opinion;No opinion;No opinion;No opinion;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;As further explained in the accompanying document, it is problematic to draw a clear line between high and low risk AI systems. Keeping the assessment scheme robust and dynamic enough to respond to the pace of innovation and up to date will be costly. Voluntary self-regulation has not seemed to work for mHealth apps, so we would suggest allocating these resources to guarantee that AI applications comply with the mandatory legal requirements imposed by the sector-specific and horizontal measures.;Current legislation may have some gaps;;Other;It will not alleviate the burden for SMEs given that the vast majority of AI systems developed to be used in the healthcare sector will be considered high risk. At the same time, having clear criteria to define whether or not a certain product or service should be considered high risk has proven to be very challenging in the past. For more detail please see attached document.;;;All applications in the area of health;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Not at all;Keeping the assessment scheme (low/high riks) robust and dynamic enough to respond to the pace of innovation and up to date will be costly. Voluntary self-regulation has not seemed to work for mHealth apps and privacy, so we would suggest allocating these resources to guarantee that all, high and low-risk AI applications, comply with the mandatory legal requirements imposed by the sector-specific and horizontal legislative instruments. ;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;;;Yes;;No;;No opinion;;;AI_consultation_EURORDIS_FINAL.pdf
F530409;14-06-2020 23:13;English;NGO (Non-governmental organisation);Johannes;Blankenbach;;Business & Human Rights Resource Centre;;Medium (< 250 employees);Germany;The feedback can be published with your personal information;3 - Neutral;3 - Neutral;3 - Neutral;No opinion;1 - Not important at all;1 - Not important at all;"The White Paper lists these actions as they are deemed vital for the ""development and uptake of AI across the EU [...]"". This is no end in itself, however, and the decision whether and how to promote such technology should depend on a thorough and transparent assessment of benefits and risks, notably human rights risks; stake- and rightsholders, that is people/society, need to be meaningfully involved and empowered in this process.";3 - Neutral;No opinion;1 - Not important at all;No opinion;3 - Neutral;No opinion;"See above for general concerns regarding the approach; ""develop[ing] skills for AI and adapt[ing] existing training programmes"" or ""strengthen[ing] excellence in research"" should include building capacities for better AI risk understanding (including of what is 'beyond risk' and should be banned) and mitigation, from various perspectives (e.g. technological, legal, sociological...) and across sectors, including public administration, business, academia, civil society and the broader public.";3 - Neutral;4 - Important;1 - Not important at all;"Similar to above; there need to be human rights and ethical safeguards for AI research and innovation, and there should be more research on AI risks, on whether and how they can be mitigated, on whether and how AI can even serve to promote rights, etc.; there should conversely be no EU funding for AI research projects that pose a human rights risk.";1 - Not important at all;No opinion;No opinion;No opinion;No opinion;Again, partnerships, knowledge transfer and the development of AI expertise for SMEs must put human rights first to promote SMEs' awareness of risks and responsibilities related to AI.;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;"Restrictions on civic freedoms, such as use of AI for increased surveillance of demonstrations; adverse impacts on freedom of expression online, e.g. through AI-based content moderation, or on access to information, e.g. through AI-powered spread of disinformation
Negative impacts on labour rights, e.g. people losing their jobs because AI takes over, with impacts often on those already in the most precarious positions; surveillance of workers, targeting and dismissals of union members, etc.";There is a need for a new legislation;;No;;;;Surveillance through facial recognition, AI in essential service delivery, autonomous weapons, AI for criminal justice-related predictions (who will re-offend...) - these and other similar AI uses should be banned;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;Severe adverse impacts on human rights, including surveillance and discrimination risks;Not at all;;Other enforcement system;Effective mandatory human rights & environmental due diligence legislation is a key means to require companies to respect human rights, including companies creating & utilising AI tech. Due diligence is a risk management process that companies would then be legally required to follow to identify, prevent, mitigate, remediate and account for how they address their (potential) adverse impacts on people & the planet. The DG JUST-led legislative initiative is an important opportunity (see attached).;;Mental health risks;Human rights risks;Yes;;Yes;;Yes, for all AI applications;;;mHRDD_AI_Johannes_Blankenbach_BHRRC_14062020.pdf
F530408;14-06-2020 23:10;French;Public authority;Anne-Marie;Jean;National;Commission Supérieure du Numérique et des Postes (commission extra-parlementaire);;Micro (< 10 employees);France;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Il est souhaitable d’intégrer dans les instances de gouvernance les sciences humaines, politiques, juridiques et la société civile, mais également de veiller à une véritable diversité des experts. Cette diversité sera utile notamment pour éviter les biais (genrés, sociaux…) dans la conception de l’IA, favoriser son orientation vers des usages sociétaux et démocratiques et développer l’inclusion numérique.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"Une gouvernance trop éloignée de l’évolution des travaux menés par les centres de recherche serait un frein. Une meilleure intégration d’experts en IA dans les équipes de gouvernance est à étudier.
Le succès de la stratégie européenne IA est lié à la stratégie de la donnée : l’UE ne doit pas abandonner le marché de la donnée à ses partenaires internationaux.  L'UE doit développer ses ses propres data sets en collectant des cohortes de données dans un cadre protecteur des droits de ses citoyens.";5 - Very important;5 - Very important;5 - Very important;La stratégie européenne dans le domaine de l’intelligence artificielle doit trouver  une articulation avec les initiatives et les travaux du Groupe international d’experts sur l’intelligence artificielle (G2IA)  lancé lors de la conférence multipartite du G7 sur l’intelligence artificielle en décembre 2018.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;A défaut d’augmenter de manière substantielle les fonds européens dédiés au développement de l’intelligence artificielle, il faut améliorer les procédures de financement en les rendant plus flexibles et en s’inspirant de modèles ayant démontré leur efficacité dans d’autres écosystèmes étrangers. ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;Yes;;Other;Les biais peuvent être induits lors de la conception des algorithmes mais également par la collecte des données et leur provenance.  Il est donc important de trouver un cadre respectueux des droits fondamentaux qui permettrait la collecte de cohortes de données des citoyens européens nécessaire au développement d’une IA éthique.; ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;"Contrôle par des autorités indépendantes (AI) nationales ou européennes associant des représentants de la société civile. Ces missions pourraient être exercées par des AI existantes, comme celles en charge de la protection des données personnelles, si elles sont dotées des moyens leur permettent d’exercer un contrôle effectif.
Création d’un Commissaire aux algorithmes certifiant sur une base régulière les systèmes IA à l’instar des commissaires aux comptes certifiant la validité des comptes.";Risks related to the loss of connectivity;;Yes;;No opinion;;Yes, for specific AI applications;;;Avis_2020-08_du_12_juin_2020_-_livre_blanc_europ_en_intelligence_artificielle.pdf
F530407;14-06-2020 23:09;English;EU Citizen;Renata;PALEN;;;;;Poland;The feedback can be published with your personal information;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;4 - Important;4 - Important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;3 - Neutral;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;;Current legislation may have some gaps;;Yes;;Yes;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;Yes;;Yes, for all AI applications;;;
F530406;14-06-2020 23:09;English;EU Citizen;Arek;Skuza;;;;;Poland;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;;5 - Very important;3 - Neutral;5 - Very important;4 - Important;4 - Important;4 - Important;;5 - Very important;3 - Neutral;5 - Very important;;5 - Very important;4 - Important;5 - Very important;5 - Very important;3 - Neutral;;2 - Not important;4 - Important;4 - Important;1 - Not important at all;2 - Not important;5 - Very important;;There is a need for a new legislation;;Yes;;Yes;;;4 - Important;3 - Neutral;4 - Important;2 - Not important;3 - Neutral;3 - Neutral;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;No opinion;;No;;No opinion;;;
F530405;14-06-2020 23:07;English;Company/Business organisation;Krzysztof;Trynkiewicz;;Sukces Strony Krzysztof Trynkiewicz;;Micro (< 10 employees);Poland;The feedback can be published with your personal information;2 - Not important;4 - Important;4 - Important;2 - Not important;3 - Neutral;5 - Very important;;5 - Very important;5 - Very important;1 - Not important at all;3 - Neutral;4 - Important;5 - Very important;;5 - Very important;1 - Not important at all;2 - Not important;;1 - Not important at all;1 - Not important at all;2 - Not important;1 - Not important at all;2 - Not important;;5 - Very important;5 - Very important;1 - Not important at all;2 - Not important;4 - Important;2 - Not important;;There is a need for a new legislation;;No;;;;Safety endagerment;4 - Important;3 - Neutral;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F530404;14-06-2020 22:57;Italian;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;;4 - Important;3 - Neutral;5 - Very important;4 - Important;5 - Very important;5 - Very important;;2 - Not important;5 - Very important;5 - Very important;;3 - Neutral;4 - Important;5 - Very important;5 - Very important;4 - Important;Provide expert resources to help SME design and execute AI enabled projects;4 - Important;4 - Important;4 - Important;5 - Very important;No opinion;4 - Important;;Current legislation may have some gaps;;Yes;;Yes;;;5 - Very important;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;No opinion;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;No opinion;;No opinion;;;
F530403;14-06-2020 22:56;English;NGO (Non-governmental organisation);Bruno;Min;;Fair Trials;302540016347-29;Small (< 50 employees);United Kingdom;The feedback can be published with your personal information;;;;;;;See attached;;;;;;;See attached;;;;See attached;;;;;;NA;;;;;;;See attached;There is a need for a new legislation;;Yes;;Other;See attached regarding the use of AI for criminal justice purposes;Use in Criminal Justice;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;;No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;See Attached;;;;See Attached;No opinion;;;;;Regulating_Artificial_Intelligence_for_Use_in_Criminal_Justice_Systems_in_the_EU_-_Final.pdf
F530401;14-06-2020 22:55;English;Academic/Research Institution;Stefan;Torges;;Centre for the Governance of AI (Future of Humanity Institute, University of Oxford);;Small (< 50 employees);United Kingdom;The feedback can be published with your personal information;No opinion;4 - Important;No opinion;No opinion;No opinion;No opinion;;No opinion;5 - Very important;No opinion;No opinion;No opinion;No opinion;The European Union would benefit from having a coherent position on AI application standards in the context of international standards organisations like the ISO, the IEC, and the ITU. The Commission could coordinate a shared commitment among Member States to align policies and efforts in this domain.;4 - Important;No opinion;No opinion;;No opinion;4 - Important;No opinion;No opinion;No opinion;;5 - Very important;4 - Important;4 - Important;3 - Neutral;4 - Important;3 - Neutral;"AI applications may make recommendations or take actions that are not fully aligned with the interests of their users. Usually, algorithms further the commercial interests of their developers, which track the best interests of their clients only imperfectly. While this is not objectionable as such, the conflict of interest should be appropriately managed.

AI applications may harm public interests like the functioning of our democratic system or the environment.";There is a need for a new legislation;;Yes;;Other;Clarify the definition of risk as a function of the likelihood of harm and its severity. Consider incorporating harm to public interests in the assessment. Incorporate the scale of AI applications into the assessment. Applications interacting with millions should face more scrutiny than applications interacting with hundreds. Clarify the assessment of systems that can be used in multiple sectors or have capabilities that extend across sectors.;;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;;No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Conformity assessments will require testing, evaluation, verification, and validation (VVT&E), especially for robustness requirements. Unfortunately, the “current state of AI VVT&E is nowhere close to ensuring the performance and safety of AI applications” (Tarraf et al. 2019). Methods used for ensuring the safety and reliability of traditional software can currently not be applied to machine learning systems. Advancing this field will be crucial for an effective assessment scheme.;Mental health risks;;Yes;;No opinion;;No opinion;;;EU_White_Paper_Consultation_Submission_-_GovAI_Oxford.pdf
F530402;14-06-2020 22:55;English;EU Citizen;Moritz;Schleicher;;;;;Germany;The feedback can be published with your personal information;3 - Neutral;4 - Important;5 - Very important;4 - Important;2 - Not important;5 - Very important;Involving the civil society with its organisations but also individual citizens;3 - Neutral;4 - Important;4 - Important;3 - Neutral;5 - Very important;4 - Important;Promoting the usage of open data standards to guarantee access for research and CSOs;3 - Neutral;5 - Very important;3 - Neutral;Provide open data access, hub for exchange of data and algorithms between researcher in academic institutions and private companies;3 - Neutral;4 - Important;5 - Very important;4 - Important;4 - Important;;4 - Important;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;2 - Not important;Responsibilities become unclear, intransparent decision-making process;There is a need for a new legislation;;Yes;;Yes;;;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;Public surveillance is a major invasion in the personal privacy and freedom to move and should be usually avoided by all means, any step towards Orwellian systems have to be very well considered and restrictions here must be verified by a regular system of checks and balances, thus well justified.;Rather not;Preference of open and accessible data;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;Let the civil society take part within an process dominated by a maximum of transparency in data, algorithms and mechanism, as done for the development of tracing applications during Covid-19 in Germany, no national but European Agency;Mental health risks;risks of monopolisation in certain areas;Yes;;;;Yes, for all AI applications;;;ideasonai_moritzschleicher.pdf
F530400;14-06-2020 22:54;English;Academic/Research Institution;Jan;GULLIKSEN;;KTH Royal Institute of Technology;;Large (250 or more);Sweden;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"Invest in and facilitate lifelong learning in AI. Develop general AI educations and professional training in AI.
Focus on leading transdisciplinary research on and with AI to drive both disciplinary and AI research forward.
Ensure availability of data and skills.
Focus on incentives for innovation and creating more business (SME and large) and application-oriented AI. 
Establish test beds for applied AI.
Investigate new business models and value-based procurement for AI";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"Transdisciplinary research with and for AI. 
Increase digital insight and inclusion to enable residents to take part in digital
services and services through popular education efforts.
Establishing an AI advisor and change manager for the public sector and for different industries. 
Encourage conversations between associations, trade unions and civil society on issues of AI and the impact of automation on society, working life and the required transition.";5 - Very important;5 - Very important;5 - Very important;Strengthen AI research funding collaborations as a separate research area among research funders. Create conditions and incentives for shared employment between companies and academia to strengthen knowledge transfer and ensure access to skills. Build infrastructure for industrial research in AI and applied AI. Develop national test beds. Mandate authorities to structure and make data available in collaboration with industry so that it is accessible for further use and innovation.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"Building skills
Access to research and researchers
Specialized knowledge in AI, cybersecurity and HPC";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"Ensure that citizens have sufficient knowledge and understanding of AI and its consequences so that they can be active and critical members of society. Develop  guidelines and practices that support the efficient and sustainable development and use of AI solutions in public administration and business.
Facilitate AI companies collaboration with public sector and government through innovation platforms. ";There is a need for a new legislation;;No opinion;;;;Ensure legislation adapted to AI as a natural part of the society;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;Develop guidelines and best practices to apply AI in a legally secure manner. The legal challenges are many – data protection, patents, legal liability, product safety, etc. – and create great uncertainty that can hamper the implementation of AI. Guidelines and best-practices could significantly increase usage. Produce simple and clear examples of how personal data can be handled in a legally secure way.;Much;AI infrastructure/ applications with common standards, where data and services can be shared with ensured privacy and data protection.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;Bias;No opinion;;No opinion;;Yes, for specific AI applications;Personal security;Respecting everybody's equal rights and value, understand and follow decisions and be able to appeal ex government decisions;
F530399;14-06-2020 22:47;English;Company/Business organisation;Florian;DAMAS;;Nokia;35167875358-33;Large (250 or more);Finland;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"In addition to the above, European and global cooperation will ensure the European Union contributes, as well as  reaps the benefits of trustworthy AI (inclusive growth, sustainable development and well-being; human-centred values and fairness; transparency and explainability; robustness, security and safety; and accountability), encompassing the privacy and security of data in multiple sectors and education.";5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;Most areas are equally important to align policies and strengthen coordination among research, private and public sectors. European data infrastructure and data space as well as ubiquitous broadband connectivity are the main pillars for interoperable data exchange between a wide range of sectors, abiding by EU standards in data protection and security.;5 - Very important;5 - Very important;5 - Very important;A coordinated network among existing AI research excellence centres should be prioritised to create a leadership structure to ensure coordination and coherent operation, agree on a vision regarding the focus and priorities beyond national borders and provide continuous financial investment. AI funding should be prominent in Horizon Europe, for core AI research and components supporting distributed and federated learning, providing data training access and allowing access to multiple data sets.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;Specialised Digital Innovation Hubs should support SMEs and start-ups in their developing, accessing and using AI. Their variety and divergence in terms of digital literacy, sector of activity and size are important for creating different needs. Digital Innovation Hubs should provide points of contact, as well as services and tangible support in SMEs’ transformation, including helping them assess which technologies to adopt and how to implement them plus training with allocated time to GPU/HW.;5 - Very important;5 - Very important;4 - Important;1 - Not important at all;1 - Not important at all;1 - Not important at all;The concerns listed above can easily be addressed by providing evidence of design quality and by demonstrating performance, as well as by reassuring the general public that AI, while technically complex, is really no different than other similar technologies which are widely accepted. Actually, for purposes of assessing the impact and increasing the trustworthiness of Artificial Intelligence, the opposite of these concerns is missing from the list above.;Other;Evidently, there is a need for new domain specific, sector-based regulation where AI is likely to have a significant impact, to encourage technological evolution However, such regulation should be appropriately timed and carefully calibrated to the predictable use cases and impact so as to prevent premature intervention that adversely affects innovation and investment. In addition (and equally important) there is even a need for deregulation within the existing legislative framework (see paper).;No;;;;It is worth pointing out that an AI application in itself carries no risk. Rather, it is the use of and AI application that carries a higher or lower level of risk. For example, a benign function like face recognition is harmless in providing access to a building, but if is scaled for mass surveillance, it will most certainly cause ethical and legal issues. Therefore, certain mandatory requirements for development and deployment should be formulated that would guarantee the use of such apps.;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);"The mass scoring and surveillance in public places should be restricted to benign use cases. If a risk is foreseen, it should be accurately specified. Please also refer to our answer under question 2.5.
Even when allowed, the use of AI systems in this context should be regularly assessed and the system shall follow the GDPR rules on personal data.";Rather not;More important than a voluntary labelling system are the sector-based approvals subject to a number of specific requirements, e.g. for medical device categories and autonomous cars. Risk is best defined in the specific context of use cases. Following an impact assessment, it should be possible to determine if users of certain AI systems fulfil certain conditions or have certain skills necessary at a given moment and in a given use case. Such approach would be more valuable than applying a label.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;The assessment of compliance with recommendations is best done through surveys and checklists, which can be both generic, as well as domain specific. Compliance with the generic requirements can be addressed through the tool being developed by the HLEG. As for compliance with any regulatory requirements, we recommend attention be given to taking a multi-disciplinary approach, with involvement of a sufficiently diverse specialisms, to ensure a technically correct and balanced review.;;An expansion of risks does not seem to be necessary at this time. The current product safety framework is technology neutral and has proven to be fit for purpose, even for technologies that were not in existence at the time when that framework was set up. Further technological developments (including aspects such as software as product, connectivity and even automated decision making) were addressed already through sector-specific legislation.;No;New risk assessment procedures are not necessary, rather fine-tuning existing, proven risk assessment processes (such as the data protection impact assessment under the GDPR) in line with technological developments seems more suitable. While it may seem tempting to work with pre-defined concepts such as “high-risk AI and “low-risk AI”, it is important to give due consideration to the fact that low-risk AI deployment could have distributive effects.;No;The existing basic legal structure already offers victims appropriate means for the recovery of damages caused by AI applications. Rather than adapting the product liability directive, promoting and expanding concepts and principles such as privacy-by-default and by-design, security-by-design and “trustworthiness-by-design” and incentivising those designers and producers/manufacturers that implement these into the AI solutions they sell, seem to be better approaches.;No;;Victims have currently various possibilities to obtain compensation for damages. Contractual allocation of liability as well as insurance coverage allow for recovery of damages and can be applied in addition to tortious liability. In addition, adapting the approach of impact assessments as described above could help uncover issues to be addressed before the occurrence of harm. Transparency of AI systems should be perceived as additional guarantee for allocation of responsibilities.;20200613_-_AI_White_Paper_Response_and_Feedback_NOKIA_.pdf
F530398;14-06-2020 22:44;English;Company/Business organisation;Tomas;Ruseckas;;Avanade (UK);;Large (250 or more);United States;The feedback can be published with your personal information;3 - Neutral;4 - Important;5 - Very important;4 - Important;5 - Very important;4 - Important;A tangible definition of the ethical framework for EU citizens.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;An internationally agreed upon framework / standard for legal penalties and breaches related to AI.;4 - Important;4 - Important;4 - Important;Ensuring that all research into AI considers diversity and inequality at its core.;4 - Important;5 - Very important;4 - Important;5 - Very important;4 - Important;Ensuring appropriate and diverse representation by making sure that specific consideration is given to underrepresented communities.;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;4 - Important;3 - Neutral;;There is a need for a new legislation;;Yes;;Other;A more detailed and intuitive framework is required.;Autonomous systems controlling critical national infrastructure, e.g. water supply, power utilities etc.;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);Immediately required for addressing COVID19 pandemic. The recognition that AI-enabled technologies must appropriately consider trade offs between the social good and individual civil liberties.;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;Make sure to continually monitor and control for potentially discriminatory decision-making by AI-enabled technologies.;Yes;A transparent standard for how to legally enforce punishment for unlawful AI use.;Yes, for specific AI applications;Applications which pose high risks to security & safety, privacy, mental health etc.;;Article_by_Avanade_s_SMEs.docx
F530397;14-06-2020 22:39;English;NGO (Non-governmental organisation);Markus;Brueckner;;European Judicial Training Network;;Small (< 50 employees);Belgium;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;3 - Neutral;;No opinion;No opinion;No opinion;No opinion;No opinion;These questions goes beyond our mission as a training provider.;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;;Current legislation may have some gaps;;No opinion;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;This question goes beyond our mission as a training provider.;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;No opinion;This question goes beyond our mission as a training provider.;No opinion;This question goes beyond our mission as a training provider.;No opinion;;This question goes beyond our mission as a training provider.;
F530396;14-06-2020 22:37;English;Other;Achim;Luhn;;"EIT Digital, EIT Climate-KIC, EIT Health, EIT Manufacturing, and EIT Urban Mobility, all Knowledge and Innovation Communities (KICs) supported by the EIT and members of the cross-KIC Activity ""Artificial Intelligence""";;Large (250 or more);Belgium;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;support the access to funding for start-ups and scale-ups active in the area of AI, e.g. by a special booster programme.;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;strengthen the role of public procurement in the adoption of AI, stronger use of pre-commercial procurement for AI based solutions (and streamline that programme);5 - Very important;4 - Important;5 - Very important;Actions need to strengthen and integrate the value chain for the commercialization of research results: from enabling employees and retaining talent, bridging the gap between research results and marketable products, to fostering business creation and business growth and public acceptance of AI.;5 - Very important;4 - Important;5 - Very important;5 - Very important;4 - Important;Consider a thematic focus of the Digital Innovation Hubs for AI. Not all AI Hubs in all countries can or should cover the full breadth. For that, consider local research excellence and industry strengths;5 - Very important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;5 - Very important;"Energy consumption (mainly in the training phase of ML) may be a concern in public acceptance;
For specific use cases, e.g. co-bots: misalignment between AI and human on the goals to be achieved and the actions taken to achieve the goals;
General impact on personal lives and the society at large by ever increasing permeation of AI ";Current legislation may have some gaps;;Yes;;Yes;;"AI in Health; 
Mobility and autonomous driving; 
Autonomous devices in production and logistics when interfacing with humans; 
The know-how, tacit knowledge of industrial workers and supervisors must be protected from unauthorized use especially by data and analytics companies that develop AI applications utilizing such data.";5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;No opinion;We consider this a specific topic that, while partly enabled by AI technology, is much more related to a general privacy and security discussion and should not be mixed into this consultation;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Special provisions should be in place to not hold back piloting new applications enabled by AI in field tests.;Risks related to the loss of connectivity;;Yes;For any framework, care needs to be taken to strike a balance of not impeding product improvements but at the same time  avoiding essentially new products/services entering the market without proper assessment (as more and more product features are defined by easily upgradable software);No opinion;;No opinion;;The EC needs to prevent market fragmentation and ensure a common regulatory regime across Europe;Addendum_for_EC_Consultation_Response.pdf
F530395;14-06-2020 22:34;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;Yes;;No;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;AI_Commentary_.pdf
F530394;14-06-2020 22:31;French;Public authority;Valérie;Fontaine;National;French Defender of Rights;;Medium (< 250 employees);France;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;"Les biais algorithmiques doivent pouvoir être identifiés puis corrigés en amont lors des phases d’expérimentation et avant la mise sur le marché donc. Les auteurs de décisions discriminatoires issues de traitement algorithmiques doivent pouvoir être sanctionnés.
";;;;;;;;;;
F530393;14-06-2020 22:29;English;EU Citizen;Joanna;BRYSON;;;;;Germany;The feedback can be published with your personal information;5 - Very important;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;AI is becoming a standard software engineering technique and as such should not be mystified. The main issues associated with AI are mandating sufficient transparency that accountability can be maintained, holding corporations liable for manufacturing devices providing services, and grasping this moment to innovate better governance for facilitating new agile sustainable economies.;4 - Important;2 - Not important;4 - Important;3 - Neutral;4 - Important;5 - Very important;"Evidently research is important (thus I'm an academic), and I also see the advantage of the EU developing expertise and authority in certifying digital systems, though I worry ""testing facilities for AI"" may get bogged down in definitions. But the main challenges before us are jobs/social engagement, security, and, and governance. If we get these right, the other things will follow. Secure local compute (AI is more about computation than data) and data stores are critical though.";2 - Not important;2 - Not important;1 - Not important at all;"I share Mariana Mazzucato's skepticism re: Public Private Partnerships.
Everyone in the EU always wants to build the EU MIT, but many US states have institutes of technology. MIT got to be MIT through honest competition, though it did with CMU and Stanford get significant investment as a reward for WWII time service / importance.
It's the information age–AI people can network themselves.
I would pump money into the national hubs and require these to network/fund leading national universities.";2 - Not important;4 - Important;5 - Very important;5 - Very important;3 - Neutral;Supporting partnerships between SMEs, and between SMEs and Academia are absolutely critical to the EU's security. We need to move away from the model of large corporations with market dominance and coercive powers, and rather create consortia across sectors to represent those sectors and pool resources for innovation. Academia should provide all of training, expertise, and high-risk blue sky research innovation, while marketable ideas should be explored and developed by agile corporations.;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;"These questions are ridiculously anthropomorphised. It's not about AI; People/corporations mystify AI to excuse bad practice in development, documentation, and accountability. Enforce the documentation recommendations by the EU HLEG, make developers liable for the manufacture of products that deliver services, and require member states to have regulatory agencies able to enforce good practice and transparency. If you can't tell who's fault it is, it's the developer's for inadequate transparency.";There is a need for a new legislation;;No;;;;Social media, though it is also a powerful way for generating novel ideas, it necessarily bleeds private data to those who will store it. We need therefore also regulatory bodies to proactively look for evidence of misuse of stored private data.;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;No opinion;I have opinions to be honest, but I also know the EU has fantastic expertise in this area to whom I defer.  But let me say that the two most important things here are pumping money into and networking the SMEs and academia through the innovation hubs, and using the above documentation to maintain accountability and transparency. Those and cybersecurity.;Much;It would be useful but I prefer it was not entirely voluntary, e.g. it was necessary for the SMEs to have access to protection of the state. Rather than categorising what is or isn't high risk, let the developers decide whether they are becoming sufficiently exposed that they seek state assistance in validating their systems, but encourage them to do so with special protections.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;"Please do not focus on ""high risk"", see above. ALL software systems, whether they are considered AI or not (don't argue definitions!) must have transparency documentation making it clear whether damages are due to poor construction, negligence in testing, deliberate actions, third party cyber security violations, or owner/operator actions assumed to be deliberate. If lines of accountability are not clear, then the developer is negligent and should be liable. Let them assess their own exposure.";Mental health risks;"liability for the manufacture of products providing services
pseudo identity theft through coercion / nudging / manipulation";Yes;These must be ongoing. This should be part of what the network of national regulatory bodies (plus other organisations like the OECD) should consider at least annually. There should be bodies doing proactive and responsive research and modelling to track both accidental / emergent risks and breaches of cybersecurity or personal privacy.;Yes;liability for the manufacture of products providing services;No opinion;;;BrysonJJ_EU_AI_Consultation_June_2020.pdf
F530391;14-06-2020 22:18;English;Company/Business organisation;Fabian;SCHMIDT;;Software AG;253418726919-22;Large (250 or more);Germany;The feedback can be published with your personal information;5 - Very important;5 - Very important;4 - Important;3 - Neutral;5 - Very important;4 - Important;The definition of SMEs must not be too narrow. Many German SMEs, which have a high potential in the implementation of AI, do not fall under the EU definition of SMEs, but need support as well.;5 - Very important;4 - Important;5 - Very important;4 - Important;4 - Important;3 - Neutral;The EU Commission should back up the Coordinated Action Plan with as concrete, measurable targets as possible in order to be able to monitor and evaluate the impact of the individual measures.;4 - Important;4 - Important;5 - Very important;The EU Commission should specifically promote research topics with strategic importance for industry. Particular emphasis should be placed on the topic of Edge AI and the interplay between AI and cybersecurity.;4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;The definition of SMEs must not be too narrow. Many German SMEs, which have a high potential in the implementation of AI, do not fall under the EU definition of SMEs, but need support as well.;4 - Important;4 - Important;3 - Neutral;4 - Important;4 - Important;4 - Important;The concerns mentioned above have to be seen in relation to other Technologies.  Positive aspects must also be highlighted. Compared to other technologies, AI can e.g. increase security and counteract discrimination by human decisions.;Current legislation is fully sufficient;;Yes;;No;;Any application with a black-box algorithm, that is self-learning and makes automated decisions with high impact on human rights.;No opinion;1 - Not important at all;5 - Very important;4 - Important;4 - Important;5 - Very important;No further guidelines or regulations are needed;;Rather not;"Voluntary labelling could rather confuse the user. Furthermore, if an AI application does not pose an increased risk, voluntary labelling does not create any additional added value. It is also questionable whether ""horizontal"" voluntary labelling can do justice to the large number of specific applications. More important than voluntary labelling are clear, transparent rules based on international standards.";A combination of ex-ante compliance and ex-post enforcement mechanisms;;For many high-risk applications, extensive conformity assessments are already taking place. Within this framework, it should be checked whether the overall system meets the corresponding requirements, regardless of whether AI is used or not.;Cyber risks;We reject an additional safety regime for the application of AI-based technologies. Safety regimes should be set up in a technology-neutral framework and in the further development of vertical regulatory frameworks.;No;"We would advocate for a careful approach with regard to a potential update on the legislative liability framework. According to our assessment, we consider the current liability framework as sufficient and balanced. Products based on AI are adequatly addressed. 
";No;Liability regimes should be as technologically neutral as possible. Existing sector-specific regulatory frameworks must be taken into account in this question.;No;;We reject an AI-specific liability regime. There is no need to leave the path of a technology-neutral liability regime. Liability regimes should be as technologically neutral as possible. In the medium and long term, EU harmonisation should be pursued in a technology-neutral framework.;
F530392;14-06-2020 22:18;German;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;1 - Not important at all;5 - Very important;5 - Very important;1 - Not important at all;1 - Not important at all;Vertrauen der Bevölkerung auf Basis von Transparenz ist Grundlage für erfolgreichen, europäischen Weg. Jede Maßnahme auf EU-Grundrechte-Charta + EU-KI-Ethik-Leitlinien überprüfen. Gewährleistung demokratischer Aufsicht, Einbeziehung Zivilgesellschaft + Betroffener. Massive Förderung vorhandener Forschungseinrichtungen (gerne gezielte Förderung von Kooperationen) statt Leitzentrum. 20 Milliarden für Investitionen. Nur so wird EU Rückstand einholen. Digitalkompetenzen ab Grundschule aufbauen.;5 - Very important;3 - Neutral;1 - Not important at all;5 - Very important;5 - Very important;4 - Important;"Gemeinsame Strategien sollten Regelungen beinhalten zu:
Transparenz, 
auch Offenlegung aller Finanzierer der jeweiligen Forschung,
demokratische Kontrolle, 
Passung mit EU-Grundrechte-Charta UND EU-KI-Ethik-Leitlinien, 
Verantwortung, ethische Leitlinien, Grundrechte, Sensibilität für gesellschaftliche Auswirkungen und Auswirkungen auf die Demokratie als einer der Ausbildungsschwerpunkte und als Voraussetzung jeglicher Förderung/Geldmittel.";1 - Not important at all;5 - Very important;1 - Not important at all;Das Gemeinwohl sollte Prioritäten bestimmen.Transparenz, auch Offenlegung aller Finanzierer der jeweiligen Forschung, Passung mit EU-Grundrechte-Charta UND EU-KI-Ethik-Leitlinien, Aufmerksamkeit für gesellschaftliche Auswirkungen und Auswirkungen auf die Demokratie sollte allen Maßnahmen und Förderungen zugrunde liegen.;1 - Not important at all;4 - Important;5 - Very important;1 - Not important at all;5 - Very important;"100 Millionen sind viel zu wenig. 
Gemeinwohl, Transparenz, Passung mit EU-Grundrechte-Charta UND EU-KI-Ethik-Leitlinien, Aufmerksamkeit für gesellschaftliche Auswirkungen und Auswirkungen auf die Demokratie als Fördergrundbedingungen - keine Ausnahmen für KMU. 
Förderungsschwerpunkt für Freie-Software-Lizenzen: KMU, Forschungseinrichtungen und fachkundige Bürger*innen können in Open-Source-Projekten kooperieren, dadurch enormer Schub an Innovationen und Stärkung europäischer Unternehmen.";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"KI in sensiblen Bereichen ohne Transparenz (hinsichtlich Finanzierer, Funktionsweise, Zweck, Auswirkungen für Gesellschaft, Demokratie und Einzelne), ohne demokratische Aufsicht, ohne ausreichende Regulierung und Rechts-Durchsetzung.
„Innovation“ als Rechtfertigung für Risiken, mangelnde Regulierung und Rechts-Durchsetzung.
Mangelnde Hinterfragung von KI (Einzel-Ergebnisse im Kleinen und Entwicklungen im Großen) aufgrund Technikgläubigkeit. 
Bewusste Verdunkelung von Verantwortung.";Other;"KI-Rechtsvorschriften dürfen die Europäische Datenschutz-Grundverordnung nicht ersetzen, sondern sie müssen sie ergänzen und stärken.
KI-Risiken müssen konkret adressiert werden (durch viele Beispiele, keine Allgemeinplätze, keine „Gummi-Paragraphen“).
Das geltende Recht befasst sich nicht mit der Verwendung nicht-persönlicher Daten und den kollektiven Auswirkungen der KI. Es verbietet nicht Diskriminierung aus nicht geschützten Gründen.";Other;Eine differenzierte Risikobewertung ist erforderlich. Das ist mit Hoch - Mittel - und Niedrig nicht erledigt!;;;"Autonome Waffen, Analyse von Emotionen, Identitätsmerkmalen, insbesondere biometrische Erkennung, Verhaltensvorhersage, auch prädiktive Polizeiarbeit, Bewegungsdaten, Gesundheitsdaten im Sinne der DSGVO, Einsatz von KI zur Bestimmung der Bereitstellung grundlegender öffentlicher Dienstleistungen.
Die Bestimmung des ""Risikos"" sollte rechte- und ergebnisorientiert sein, nicht Sektor-/Branchen-bezogen.";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;Biometrische Fernidentifikationssysteme im öffentlichen Raum sind ein typisches Merkmal von Diktaturen. Ihr Einsatz ist in Demokratien entschieden abzulehnen. Sie verwandeln öffentliche Räume in Orte ständiger Überwachung und beschädigen irreversibel wesentliche Grundrechte sowie Elemente und Funktionen der Demokratie.;Not at all;"Generell sollte Abstand davon genommen werden, KI-Systeme als risikoarm zu betrachten und daraus zu schließen, sie bräuchten keine Aufsicht zur Gewährleistung der EU-Grundrechte-Charta und der EU-KI-Ethik-Leitlinien.
Freiwillige, selbstregulierende Ansätze in KI-Regelungen bieten Spielraum für  eingeschränkte Rechenschaftspflicht, lockern Grundrechtsverpflichtungen, verringern die Sicherheit und erschweren den Geschädigten die Durchsetzung von Ansprüchen.";Other enforcement system;Bewertung der Passung zur EU-Grundrechte-Charta UND zu den EU-KI-Ethik-Leitlinien, Folgeabschätzung für gesellschaftliche Auswirkungen und Auswirkungen auf die Demokratie in den Phasen des Entwurfs, der Entwicklung, der Erprobung und des Einsatzes. Schnell durchsetzbare Konsequenzen für den Fall, dass Anwendungen diese Standards nicht erfüllen, einschließlich der Möglichkeit, KI-Einsätze zu stoppen.;Die Einhaltung von Regulierungsmaßnahmen sollte durch externe und unabhängige Stellen erfolgen. Selbstregulierung sollte vermieden werden. Es sollte sichergestellt sein, dass es keine Schlupflöcher beim Schutz der Grundrechte und Ethik-Leitlinien gibt.;Mental health risks;"Profiling und Scoring – mit den Gefahren der Manipulation und Diskriminierung. 
Profiling und Scoring insbesondere auch auf Basis der Zusammenführung von Daten aus verschiedenen Quellen.";Yes;Mögliche Änderungen sind in Risikobewertungsverfahren einzubeziehen, insbesondere in eine Datenschutz-Folgeneinschätzung.;Yes;"Verstärkte Haftungsregelungen sollten für Hersteller von KI bestehen, 
die den Quellcode (einschließlich ihrer algorithmischen Modelle/Datensätze) nicht offenlegen und/oder 
nicht zügig für Korrekturen von ihnen zur Kenntnis gebrachten Problemen sorgen.";Yes, for all AI applications;;Die Regeln der Transparenz, Verständlichkeit und Vollständigkeit von Datenschutzerklärungen und weiteren Risikodarstellungen von KI-Systemen für Privatnutzer*innen müssen den Anforderungen an Packungsbeilagen für Medikamente entsprechen.;
F530390;14-06-2020 22:17;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;4 - Important;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;The AI strategy should comprise the Sustainable Development Goals. The strategy should also focus on education and professionals and the general society. Also, the strategy should consider the social impact of AI ensuring the evaluation frameworks (see attached document for further details).;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;The design of the data space is critical, it should be oriented to reduce potential bias in the application of AI (see attached document for further details).;5 - Very important;5 - Very important;5 - Very important;The academy should become part of the partnerships. The civil society should be also be involved in the configuration of AI application to industry (see attached document for further details).;4 - Important;5 - Very important;4 - Important;5 - Very important;4 - Important;Education is very important and should be oriented beyond technology, social aspects are critical for the right development of AI in Europe. It is necessary to create networks to promote collaboration  (see attached document for further details).;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Geographical inequalities in development of AI (among EU countries). Increase of digital gap in the society because problems in the representativeness of data (see attached document for further details).;There is a need for a new legislation;;No;;;;Risk depends not only on the application, but also who creates these applications and how they area managed (see attached document for further details).;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;Biometric applications have deep consequences in freedom and their use should be very limited. There should be a very strong need to promote social wellbeing to adapt these solutions for public use. The availability of this technology is not sufficient reason to deploy them in public spaces because the risks are very high.;Very much;Labelling and benchmarking should be necessary for most uses of AI, not just voluntary  (see attached document for further details).;A combination of ex-ante compliance and ex-post enforcement mechanisms;;As mentioned, the outcomes of AI may not predictable. The evaluation of AI has to be evaluated also in the environment of use. Incremental evaluation is required  (see attached document for further details).;Mental health risks;Mental health risk are frequently overseen and they need more attention.;Yes;Small changes can lead to large impact because of the scalability of AI-driven platforms and therefore, any change has to be evaluated  (see attached document for further details).;Yes;It is necessary to expand these frameworks, but specific frameworks are required to be aligned with previous frameworks;Yes, for all AI applications;;;
F530389;14-06-2020 22:17;English;Academic/Research Institution;Theodore;CHRISTAKIS;;Chair Legal and Regulatory Implications of Artificial Intelligence, Multidisciplinary Institute on Artificial Intelligence (Université Grenoble Alpes);;Medium (< 250 employees);France;The feedback can be published with your personal information;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;Please see Attached Document;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;Please see Attached Document;3 - Neutral;5 - Very important;4 - Important;Please see Attached Document;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;Please see Attached Document;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;3 - Neutral;Please see Attached Document;Current legislation may have some gaps;;Other;Please see Attached Document;;;Please see Attached Document;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Other special requirements in addition to those mentioned in the question above should be imposed (please specify);Please see Attached Document;Much;Please see Attached Document;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Please see Attached Document;Mental health risks;Please see Attached Document;Yes;Please see Attached Document;Yes;Please see Attached Document;Yes, for specific AI applications;Please see Attached Document;Please see Attached Document;AI_Regulation_Submission_EU_AI.pdf
F530388;14-06-2020 22:13;English;NGO (Non-governmental organisation);Sherwin;Siy;;Wikimedia Foundation;596597913132-95;Large (250 or more);United States;The feedback can be published with your personal information;4 - Important;3 - Neutral;4 - Important;4 - Important;3 - Neutral;3 - Neutral;"Skills should include, if not emphasize, citizens’ and regulators’ understanding of how AI systems work, not merely for job skills, but to improve accountability of systems to citizens and regulators and ensure trustworthy systems.
 
Public adoption of AI should not be encouraged as an end in itself, but only when the technology is the appropriate tool for the public sector task and if necessary safeguards for fundamental rights can be provided.
";5 - Very important;4 - Important;1 - Not important at all;2 - Not important;4 - Important;5 - Very important;"Supporting open source software & open data enables both public accountability & technical innovation. Increased openness in funded research gives SMEs access to expanded resources, and also increases AI’s public accountability.
The EC should be aware that data sets are not static; AI systems rely upon continually updated data, which implicates accuracy and human rights. A European data space will benefit from strong open data policies and Commission support for free data sets like Wikidata.";No opinion;No opinion;No opinion;;2 - Not important;4 - Important;4 - Important;4 - Important;2 - Not important;"Partnership efforts should ensure that network effects, economies of scale, or closed systems associated with AI and machine learning do not create anticompetitive results or lock researchers, auditors, and SMEs out of existing technological infrastructure.
";5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;"AI accuracy can have either trivial or catastrophic effects, depending upon its application and the confidence invested in its use in that application. Accuracy should not be the goal for malicious or discriminatory uses.
Automated or bureaucratic decisions affecting individuals’ rights must be auditable and explainable, whether or not AI is used.
For our further concerns on discriminatory outcomes and also the intersection of IP and AI; please see our attached comments for more.
";Current legislation may have some gaps;;Other;Broad categories of risk may be less useful than requirements that are addressed specifically to the type of information used, the subjects affected, the potential results of the AI system's determination, and the finality of the decision. Baseline requirements for opaque systems should apply with limited exceptions, but different applications can raise risk in different ways. Requirements for those higher-risk scenarios should account for the type, and not just the severity, of the risk.;;;"There are too many potential uses for AI to name any one as highest-risk. Risk depends not only upon the use or application, but the speed and finality of its decisions, and the mechanisms for review and accountability available before, during, and after its use.
Certain baseline requirements should apply to AI systems with significant effects on individuals’ rights, even if those systems may not be specifically labeled “high-risk.” 
Specific high-risk uses may require specialized rules.";4 - Important;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;"Many uses of biometric ID systems bear severe consequences for the people they target. Furthermore, systems with unproven and questionable accuracy are being marketed and deployed. Subjecting the public to these systems’ data-gathering, and judgment without mechanisms of meaningful consent is a severe privacy risk.
There may be extremely limited scenarios in which such a use is justified, but defining and enforcing their narrow scope likely requires far more in-depth consultations in the future. 
";No opinion;Systems that provide the public with more information, in a comprehensible format, are typically helpful. However, labeling systems risk assuring consumers of qualities that the labels can not guarantee. Meeting standards for transparency, for instance, does not mean that the results are accurate. Furthermore, label requirements and their enforcement could allow entities to game a labeling system that is too reliant on rigid benchmarks, especially in a rapidly developing area of technology.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;;The White Paper notes the possible gaps regarding stand-alone software and services. Without commenting directly on software liability generally, we note that entities should not be able to evade liability by mischaracterizing a product as a service, or integrated software as stand-alone software.;Yes;;No opinion;;No opinion;;;
F530387;14-06-2020 22:06;English;NGO (Non-governmental organisation);Dominik;Dahlhaus;;ThinkTech e.V.;;Small (< 50 employees);Germany;The feedback can be published with your personal information;5 - Very important;4 - Important;4 - Important;No opinion;4 - Important;No opinion;"Promoting ""ethical AI"" as unique selling proposition (ensuring that experts on ethical AI have a critical say in the research, development, and implementation of the other action points; observing guidelines for ethical AI is required for any public funding). 
For a more detailed elaboration, see open statement. ";4 - Important;5 - Very important;No opinion;No opinion;5 - Very important;4 - Important;"Fostering the promotion of ""AI and data ethics"" topics on existing platforms s.a. AI4EU";4 - Important;5 - Very important;4 - Important;;2 - Not important;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;#NAME?;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;3 - Neutral;"- can undermine democratic processes (deep fakes, microtargeting)
- inequality
- wealth and prosperity
- environment
- security

For a more detailed elaboration, see open statement. ";There is a need for a new legislation;;Other;"We agree that, for the introduction of new compulsory requirements, high-risk applications should be prioritized. It should, however, not be limited to that; for a more detailed elaboration, see open statement. ";;;;5 - Very important;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;"As a first step while no specific legislation is in place, a voluntary labelling system could prove beneficial. It should, however, not refrain from developing regulation regulation medium- and low-risk applications; for a more detailed elaboration, see open statement. ";A combination of ex-ante compliance and ex-post enforcement mechanisms;;"- regular audits to ensure that development processes in companies comply to certain standards
- regular ex-post compliance benchmarks

For a more detailed elaboration, see open statement. ";Mental health risks;;Yes;"- regular audits to ensure that development processes in companies comply to certain standards
- regular ex-post risk benchmarks

For a more detailed elaboration, see open statement. ";Yes;#NAME?;Yes, for specific AI applications;"- AI systems that change their behaviour due to learning
- Autonomous systems
(- High-risk systems)
(- open source)";;ThinkTech_Statement_final.pdf
F530386;14-06-2020 21:55;English;;;;National;;;;;The feedback can be published in an anonymous way;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;;No opinion;No opinion;No opinion;;No opinion;No opinion;No opinion;No opinion;No opinion;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;No;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;No opinion;;No opinion;;;
F530385;14-06-2020 21:54;English;Company/Business organisation;Julia;Lamb;;Booking.com;146537115285-34;Large (250 or more);Netherlands;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;;3 - Neutral;5 - Very important;4 - Important;Please see the attachment to this submission;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;Please see the attachment to this submission;4 - Important;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;It is vital to consider the context in which AI is used and how these factors might arise (e.g. the importance of inaccuracies/discriminatory outcomes will vary significantly depending on the situation use case).;Current legislation may have some gaps;;Yes;;Yes;;Please see the attachment to this submission;;;;;;;;;Much;Please see the attachment to this submission;;;Please see the attachment to this submission;;;;;;;;;;Booking.com_AI_public_consultation.pdf
F530384;14-06-2020 21:49;English;EU Citizen;Angeles;Manjarrés;;;;;Spain;The feedback can be published with your personal information;4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;;4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;;3 - Neutral;4 - Important;4 - Important;;3 - Neutral;5 - Very important;4 - Important;4 - Important;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;See attached document;There is a need for a new legislation;;No;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Not at all;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;Manjarres_Comments_White_Paper_14Jun20.pdf
F530383;14-06-2020 21:48;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;;5 - Very important;3 - Neutral;4 - Important;4 - Important;5 - Very important;4 - Important;Research needs to be broad, not simply by excellence limited to a few. ;1 - Not important at all;2 - Not important;5 - Very important;Support a broad research movement towards AI, remember Linux was created by a student, not a research center. Innovation should focus on small and fast teams.   ;4 - Important;3 - Neutral;5 - Very important;1 - Not important at all;3 - Neutral;Set strategic goals, provide funding based on achievements.;4 - Important;5 - Very important;3 - Neutral;5 - Very important;4 - Important;5 - Very important;AI may make getting recourse more difficult, how to prove you were wronged.;There is a need for a new legislation;;Other;High-risk application should be determined though individual assessment, not in law text.;;;Any automated decision-making that significantly impact humans. This is not limited to AI!;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Not at all;AI requires continuous supervision, a label shows a historic snapshot.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;AI requires continuous supervision, ex-post must be a requirement.;Personal security risks;"1) The definition of AI is highly problematic from a system perspective, does a system contain an AI or is a traditional rule-set based decision-making software used? Any trained machine learning network can with relative ease be converted into a rule-based system. Thus, circumventing the legislation is quite possible!
2) Autonomous systems will have to individually learn from their interactions. This may introduce hard to test systems.";Yes;This is very important! Ensure an auditable trail of taken decisions. ;Yes;The behavior (decision space) of the AI system is not fully known before deployment. If a product e.g. starts to drift how should that be handled? Do we have the right to an updated AI for how long? ;Yes, for all AI applications;;;
F530382;14-06-2020 21:45;English;Other;Melanie;KING;;TECHNGI Project, Loughborough University;;Large (250 or more);United Kingdom;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;See uploaded document for additional comments.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;See uploaded document for additional comments.;4 - Important;5 - Very important;3 - Neutral;See uploaded document for additional comments.;3 - Neutral;5 - Very important;3 - Neutral;5 - Very important;No opinion;See uploaded document for additional comments.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;See uploaded document for additional comments.;There is a need for a new legislation;;Yes;;Yes;;Red-lining or Blue-lining in insurance. Un-explainable or un-interpretable decisions.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;See current actions by IBM and Amazon in the USA.;No opinion;See uploaded document for additional comments.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;See uploaded document for additional comments.;Mental health risks;See uploaded document for additional comments.;No opinion;See uploaded document for additional comments.;;;Yes, for all AI applications;;See uploaded document for additional comments.;SUBMITTED_Formal_response_to_the_European_Commission_V5.pdf
F530381;14-06-2020 21:39;English;Company/Business organisation;Ina;SEBASTIAN;;Infineon Technologies AG;10751968675-85;Large (250 or more);Germany;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;"- Investment in and promotion of key technologies in the field of trustworthy AI, especially cybersecurity.
- promotion of international technical standards, especially in the area of trustworthy AI";4 - Important;3 - Neutral;5 - Very important;5 - Very important;5 - Very important;4 - Important;The European Commission should back up the Coordinated Action Plan on AI with as concrete , measurable targets as possible in order to be able to monitor and evaluate the impact of the individual measures. The EC should develop and continuously collect appropriate indicators in cooperation with science and industry.;2 - Not important;2 - Not important;3 - Neutral;"- The EC should specifically promote research topics with strategic importance for industry, such as robotics, low-data AI or hybrid AI. Particular emphasis should be placed on the topic of Edge AI and the interplay between AI and cybersecurity.
- Demysti";5 - Very important;4 - Important;3 - Neutral;5 - Very important;4 - Important;- The definition of SMEs must not be too narrow. Many German SMEs, which have a high potential in the implementation of AI, do not fall under the EU definition of SMEs, but need support as well.;4 - Important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;;"AI must be ""demystified and reasoning to be done on factual level. Most problems and concerns do not originate from AI itself but from inappropriate utilization by ruthless (service) providers or product vendors. This can be handled.";Current legislation may have some gaps;;No;;;;Any application with a black-box algorithm, that is self-learning and makes automated decisions high impact on human rights.;5 - Very important;5 - Very important;4 - Important;4 - Important;3 - Neutral;4 - Important;No further guidelines or regulations are needed;;Not at all;"Voluntary labelling could rather confuse the user. Furthermore, in an AI application does not pose an increased risk, voluntary labelling does not create any additional added value. It is also questionable whether ""horizontal"" voluntary labelling can do justice to the large number of specific applications. More important than voluntary labelling are clear, transparent rules based on international standards.";Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;"AI will be considered as trustworthy in case the following criteria are applied to:
- in accordance with GDPR
- Security (e.g. through encoding of data)
- Responsible use (clear stated, what for are the data used)
This can be checked by defining certain processes where adherence can be applied and verified.";Cyber risks;Misuse of personal data without consent of the user. Default is opt-in, not opt-out.;Yes;Updates will be frequent, plus learning in the field.;No;;No;;Documentation of training data sets should be mandatory.;
F530380;14-06-2020 21:33;English;NGO (Non-governmental organisation);Floor;van Holsteijn;;Nederlands Juristen Comite voor de Mensenrechten (NJCM);;Micro (< 10 employees);Netherlands;The feedback can be published with your personal information;5 - Very important;No opinion;No opinion;No opinion;No opinion;No opinion;;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;;No opinion;No opinion;No opinion;;No opinion;No opinion;No opinion;No opinion;No opinion;;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;A human rights impact assessment should be executed, with every use of algorithms, where personal data and human rights are being involved. We highly recommend this assessment in order to protect EU values in regard to human rights (also see sections 3 and 6);Current legislation may have some gaps;;No;;;;There should not be an approach to seperate “high-risk” and “low-risk” AI applications. Every application could possibly contain a human rights violation and therefore we do not agree with these two separate categories (also see section 6).;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Not at all;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure and a Human Rights Impact Assessment is needed to.;;;Yes;;;;Yes, for all AI applications;;;Outline_reactie_internetconsultatie_AI__2_.pdf
F530379;14-06-2020 21:17;English;NGO (Non-governmental organisation);Richard;Benjamins;;OdiseIA - Observatory of the social and ethical impacts of Artificial Intelligence;;Micro (< 10 employees);Spain;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Many stakeholders need to be better informed and educated about i) what AI can and cannot do, ii) the difference between personal data and anonymized and aggregated data. Stakeholders include policymakers, governments, media (press, etc) and the public at large. Currently, fundamental misunderstandings about AI limits ;5 - Very important;4 - Important;5 - Very important;Avoid the typical European fragmentation that hinders excellence and scale.;3 - Neutral;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;2 - Not important;Try to be not too alarmistic regarding those point. Other concerns: future of work, LAWS, relation people-robots;Current legislation may have some gaps;;Yes;;Other;Assume you mean 5C. A risk-based approach is good, but there are too many undefined aspects on what constitutes high risk. A transparent definition is needed. Maybe the 5-level approach of the German Ethics Data Commission is fairer while still practical.;"Mass surveillance with AI could have severe negative effects on societies and human rights.
It is not only a question of high-risk AI applications, but also whether AI-based decisions improve current human (biased, black box, ...) decisions. ";4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;Use only for national security and health emergencies and only when proper EU regulation is in place.;Much;But don't turn voluntary guidelines into obligations. Let the market decide whether voluntary guidelines are appreciated by consumers and governments. ;A combination of ex-ante compliance and ex-post enforcement mechanisms;;This decision is especially important for Europe's position vis a vis AI products & services from other regions of the world. All AI services and products that operator in Europe should be governed by the same rules.;Mental health risks;;Yes;;Yes;Only for cases where the behavior of the AI system changes during its lifetime due to self-learning capabilities. ;Yes, for specific AI applications;Self-driving cars.;;Feedback-AI-whitepaper-EC.docx
F530378;14-06-2020 21:12;English;Academic/Research Institution;Jesus;Salgado;;Universidad Politecnica de Madrid;20647 Universidad Politecnica de Madrid 555819220647-67 UPM;Large (250 or more);Spain;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Increase public awareness on AI technology, benefits from early education.;4 - Important;5 - Very important;4 - Important;3 - Neutral;4 - Important;5 - Very important;;5 - Very important;4 - Important;4 - Important;;4 - Important;5 - Very important;3 - Neutral;5 - Very important;2 - Not important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;A document signed by more than 70 reserachers and practitioners is uploaded. There 9 main concerns described in that paper that should be addressed further in the EU AI approach.;There is a need for a new legislation;;No;;;;"I contend the question. ""High risk"" or ""risk"" should not -and is no, in the domain of risk management- be subject to individual perception. A multi-dimensional risk assessment methodology should be proposed based on the severity of potential harm to an individual, but also the number of individuals affected, the impact to common goods and the probability of occurrence. In this sense, the proposed assessment of “high risk” applications should be refined and not limited ""a priori"" to some sectors.";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Rather not;Could be useful for B2B applications. For end-users, regulation should be complete. Otherwise it would be just a cost or a barrier for new or small companies.;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;"Regulation should include TECHNICAL PROTOCOLS to be followed and easy to report and monitor, so enforcement is NOT the bottleneck. This is a problem for regulation that is not very specific on implementation. Like in GDPR, cookie policies could be easily enforced if there was a specific protocol of consent mandatory for all browsers and easily reported to user or external auditing entity. 
";Mental health risks;The VERY high risk of user giving uninformed consent (99% of todays consent). Do NOT leave the final decision to a consenting user. We know what happens: users do not have the time, the will or the information to take these decisions.;Yes;There is a very broad field of practise on risk assessment and management. The white paper provides a very superficial look into this.;Yes;"Specific on-line services like e-commerce, content provision, social media have AI systems that exploit some phychological vulnerabilities that should be subject to product liability. They are not ""products"" in the classical sense (most of them are free), but are widely used and providers should be liable.";Yes, for all AI applications;;;Comments_White_Paper_14Jun20.pdf
F530377;14-06-2020 21:02;English;Academic/Research Institution;Gloria;GONZALEZ FUSTER;;Law, Science, Technology & Society (LSTS) Research Group at the Vrije Universiteit Brussel (VUB).;;Large (250 or more);Spain;The feedback can be published with your personal information;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;;No opinion;No opinion;No opinion;;No opinion;No opinion;No opinion;No opinion;No opinion;;No opinion;5 - Very important;5 - Very important;No opinion;5 - Very important;5 - Very important;Please see the uploaded contribution, on AI and gender.;There is a need for a new legislation;;No;;;;;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;Other special requirements in addition to those mentioned in the question above should be imposed (please specify);Please see uploaded contribution.;No opinion;;No opinion;;Please see uploaded contribution.;;;No opinion;;No opinion;;No opinion;;;feedback_White_Paper_AI_ggf_14june2020.pdf
F530376;14-06-2020 20:58;English;;;;National;;;;;The feedback can be published in an anonymous way;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;SCA_Reply_to_the_Public_Consultation_AI.pdf
F530375;14-06-2020 20:57;English;Company/Business organisation;Jacomien;van den Hurk;;PwC IL;60402754518-05;Large (250 or more);Belgium;The feedback can be published with your personal information;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;"All are important - a prioritisation mechanism could be by planned monetary efforts/resources for the specific action areas. Focus on needs in AI adoption for organisations by size: for large enterprises, focus on robust governance and risk management. For the SMEs and the public sector, stimulate AI adoption knowledge, expertise and financial support.
Collaboration across member states will enable better coordination of research efforts.";4 - Important;3 - Neutral;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;"Coordination of countries initiatives; share best practices 
Identify a priority portfolio of investment for all member states to pool resources
Explore private-public partnership model to help with designing governance
Build EU knowledge repository for SMEs 
Having access to multiple data platforms and flexibility can circumvent any issues. Building an EU data space will be costly";5 - Very important;5 - Very important;5 - Very important;"Meaningful knowledge exchange, common agenda for research with priorities and challenges set up collectively to ensure research is focus on areas of interests and relevance 
Foster the innovation and research culture within Europe's industry with incentives like a  local tax break. The tax is offset by European Investment fund transfer to member state
Focus on creating  new talent in research and provide the right incentives for them to continue to work in research";4 - Important;4 - Important;5 - Very important;5 - Very important;3 - Neutral;"Set up a european market place where ideas and innovations should be established. 
Support also early education/experimentation activities for school-aged children focusing on Ai and Ethics
Use Mission-Oriented Innovation approach to define policy to boost innovation aimed to  tackle societal and technological challenges.";5 - Very important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;4 - Important;"Explanation must be tailored to the use case and its stakeholder
The concept of accuracy is difficult to define in different forms of AI and may not be the right measure -- there is no common measure for performance, as it is context-specific
Models are stochastic -- most processes in regulation and risk management are structured for deterministic systems. Processes need to adjust accordingly";Current legislation may have some gaps;;Yes;;;;"Define and quantify harms/unintended consequences on 3 level: individual, organisations and society
As a general rule safety critical applications should be highest risk. E.g. midair collision avoidance systems
Engage with industries  to define and align the high risk applications";5 - Very important;4 - Important;5 - Very important;4 - Important;3 - Neutral;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);"An independent oversight body to over the adoption and use that allows citizen reporting, queries or explanations 
Until there is a regulation in place, cases should be limited to only public safety, not efficiency reasons, and be explicitly authorized by the EU. Except if people are explicitly informed about and have a choice of using biometric identification or not (e.g. airport gates).";Much;Raise public awareness for AI risks, so that large companies adopt the voluntary labels to increase customer satisfaction and user adoption. This could also be a basic requirements for receiving financial support for AI innovation developments.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;"Firms can voluntarily engage with independent parties (accredited individuals or entities) to provide assurance against a recognised framework or standards. It may fall short of a certification until there is a standard to go along the voluntary labelling
Frameworks can guide local certification. It should be aligned with global initiatives on that. The EU should promote CEN to play an active role in the relevant ISO committees, as ISO is the primary source for technical certification standards";Mental health risks;"Business continuity
Loss of explainability but from a non-technical perspective, may lead to loss of control/decision making/choice";Yes;"Clear definition of AI within the context of risk mitigation, users, Risks, harms building on the EU definition of AI 
Define the impact assessment to cover aspects such as people and mandated early in the life cycle. This would inform actions need across areas such as Safeguards, Business Continuity, alternative/back up, risk mitigation 
Consider governance structures to monitor and respond to change in risk appetite, risk profile, likelihood etc,  a command centre approach or similar";Yes;"It  should be the starting point, but needs to be adjusted  as best as possible 
A new approach for EU legislative framework for liability on AI products should be considered as well in line with the fact  Many risk management processes are built for deterministic systems. AI is probabilistic -- regulations and risk management should adjust accordingly";Yes, for all AI applications;;Clear definition of liability in context of probabilistic systems is needed ;
F530374;14-06-2020 20:41;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;5 - Very important;3 - Neutral;4 - Important;5 - Very important;Cross collaboration between SMEs, larger corporates and public sector has high potential to increase speed in areas where needs are common. Reskilling, education, certification within AI, improving ease of movement between academia, public and private sector would increase AI adoption. ;5 - Very important;2 - Not important;4 - Important;4 - Important;5 - Very important;3 - Neutral;The definition of AI in the white paper is too broad, the current wording can be seen to include all software. Creating a clearer data regulation would facilitate the sharing of important data sets between private and public sector. ;2 - Not important;5 - Very important;4 - Important;The lighthouse idea is ok but it is very unclear how it could be implemented, taking into account political differences.;4 - Important;2 - Not important;4 - Important;5 - Very important;3 - Neutral;;4 - Important;4 - Important;5 - Very important;2 - Not important;3 - Neutral;2 - Not important;"There is a need for certification for individuals building AI systems in a non-research context. The ""human-in-the-loop"" approach should always be ensured. In finance the legislation already covers many of the potential risks with AI systems. Banks already monitor model risk based on a low to high business impact measurement.";Current legislation may have some gaps;;Yes;;No;;Remove open-ended statements that create legal uncertainty from the white paper, to allow for safe innovation. The current wording is very broad and vague.;4 - Important;4 - Important;3 - Neutral;4 - Important;3 - Neutral;4 - Important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);Biometric applications that directly harm the integrity of EU citizens or challenge the rights of EU citizens should be inspected, approved and monitored by a central instance. ;No opinion;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;"The definition of ""high risk"" will need to be assessed on a sector level together with corporate representatives to ensure an appropriate level of surveillance and assessment, to mitigate the risk of hindering innovation in industries that are central for the EU. ";Mental health risks;"Creating expert centres around ethical and fundamental rights questions, where eg. SMEs can get information and education on ""best practices"", analyse the threats and risks in their systems and products and get guidance on how to mitigate those. ";Yes;;No opinion;;No opinion;;;
F530373;14-06-2020 20:35;English;Public authority;Zlata;Strakova;National;The Office of the Government of the Czech Republic;;Large (250 or more);Czech Republic;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"Socio-economic implications of building an ecosystem of AI excellence should be also
mentioned. We appreciate the focus on digital skills. Implications for the labor market and
work environment should not be ommited. The results of the work of the High-Level
Expert Group on the Impact of the Digital Transformation on EU Labour Markets could be
used as a basis. The important role of DIH network to promote investments to SMEs and
help them recover after the pandemic should be also addressed.";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"Besides skills for AI, the broader impacts on the labor market should be considered. The
results of the work of the Expert Group on the Impact of the Digital Transformation on EU
Labour Markets should be taken into account. Also, investments should be provided for
fast growing scale-ups and other companies as well. Coordination among all stakeholders
should be ensured. The clear idea of CoE in AI research should be given. We prefer the
network to be based on regional specialization.";2 - Not important;5 - Very important;5 - Very important;"Attracting and retaining research talents; compensation for use of data for training
purposes, defining the role of the different actors in the AI ecosystem (business,
academia, DIH, RTEF, CoE etc.) and their coordination is crucial (e.g.: best practice
ECSEL JU). We prefer the network of regionally specialized Centers of Excellence instead
of one Lighthouse center. Clarification and better diferentiation between digital skills and
AI skills should be provided.";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"Coordination with other elements of the AI ecosystem such as Centers of excellence and
reference testing facilities. There is also a need to support big companies that are in fact
small compared to gigants such as Google, Amazon, etc. Attract, promote and facilitate
investments to SMEs automation and modernization to help them survive post-covid crisis
and maintain their competitiveness on European and global market.";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"Misuse of biometric recognition, social credit scoring systems, manipulation of users
emotions, new types of personal data (psychological)&medical, disinformation spread (biased prioritisation of information). We cannot presume risks that will prove as most threatening the fundamental rights/society in the future. Risk assessment should weigh the potential improvements against drawbacks. There should not be any binding legal definition of AI that can easily misguide potential regulation. ";Current legislation may have some gaps;;Yes;;Yes;;"Critical infrastructure (energy sector, telecoms, hospitals), some health,
transport, judicial applications/predictive policing, financial and public social services
applications (e.g social credit scoring to obtain a loan, social benefits distribution), military,
media sector, AI for personalization, digital twin of a person. Clear methodology to determine high-risk applications is needed, some factors to determine it are too broad (avoidability) and some are not clearly delineated.";5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);"Facial recognition as a technology for potential abusive misuse should be assessed and
consulted with stakeholders first. In the CZ non-paper we suggest a time-limited
moratotium to be considered instead of any unlimited ban on facial recognition
technologies. Sound approach and methodology is needed to assess that these systems
are free of bias and discriminatory decisions. Based on the available data, the technology
must be assessed for whether it is proportional and whether it will fulfill an additional
function. It should not be allowed for private subjects unless there is a
specific reason (such as protection of nuclear facilities, etc.). The use of remote biometric
identification systems needs to be subject to supervision of the national regulatory body
(Data Protection Office). On the other hand, cautious use of AI systems for e.g. antifraud, decryption or big data analysis could be considered. The actual risks of remote biometric identification should be evaluated because the use cases will be quite diverse. While the use of remote biometric identification in some sectors might require additional harmonized conditions above the limitations imposed by the GDPR, the use cases falling under the scope the Law Enforcement Directive should be left to possible further regulation by relevant Member States.";Very much;CZ very much supports the voluntary labelling system and asks to consider voluntary approach also in the case of prior conformity assessment for the high risk applications. The introduction of prior conformity assessment would require a broad range of experts that might not be available now. Consequences for liability in the form of a restricted liability should be considered. Costs and impact of voluntary labelling on SMEsshould be examined and self-regulation taken into account;Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;"We prefer the pro-market self-regulatory measures voluntarily imposed by the private
companies themselves, similar to code of conduct adopted for free speech on internet.
The form of the assessment needs to be subject of further debate. The form could involve
certification, testing procedures etc. Also, support of regulatory sandboxes would speed
up the development of the technology.";Mental health risks;"CZ is of the opinion that any possible regulation should be the result of a thorough
scrutiny and assessment and considered only in case of serious gaps in the legal system.
We suggest focusing on AI systems that develop over time and their functioning depends
on one or more users regarding liability and legal certainty. Also, there should not be any
binding legal definition of AI that can easily misguide any potential regulation of this very
dynamic technological field.";Yes;"There is a need to think through the possibilities of the prior conformity assessment.
Certification or testing procedures could be a way forward. Prior conformity assessment
should be based on voluntarity. Also, the influence of the prior conformity assessment on
the liability should be considered.";Yes;The PLD is an appropriate instrument to resolve the legal uncertainty regarding liability issues related to AI applications. The PLD should resolve the question on fair distribution of liability based on factual control over AI. Questions of immaterial damage and updates need to be clarified. We support sectorial approach, limiting liability when a user does not update a system, and promotion of self-regulation.;Yes, for specific AI applications;;"Cautious, targeted and limited adaptations of national rules regarding prove of the fault by
the victim could be considered. The national liability rules should not slow down the use of
AI, i.e. we should not wait for the regulatory framework to be put in place first.";CZ_nonpaper__EU_regulatory_framework_for_AI.pdf
F530372;14-06-2020 20:25;English;Academic/Research Institution;Marietje;Schaake;;Stanford Institute for Human Centered AI, Stanford Cyber Policy Center;;Medium (< 250 employees);Netherlands;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;2 - Not important;4 - Important;;5 - Very important;5 - Very important;3 - Neutral;4 - Important;4 - Important;5 - Very important;;5 - Very important;5 - Very important;3 - Neutral;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;Yes;;Yes;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;;Yes, for specific AI applications;;;StanfordWonks_TechiesEUAIWhitePaper.pdf
F530371;14-06-2020 20:08;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;4 - Important;4 - Important;1 - Not important at all;1 - Not important at all;1 - Not important at all;"AI is based mainly on Big Data. This is implies at least the risk of collecting unnecessary amount of sensible data with all the related risks for the individuals and the society. 
I miss an explicit task to protect us and to use e.g. the ""Privacy by design"" approach.

The use of AI in any application should be under control and as citizen I should be informed about it resp. any decision relevant for me which was taken with such a software. AI should not be used only because it is AI...

";4 - Important;;1 - Not important at all;1 - Not important at all;4 - Important;4 - Important;What about the social impact? ;3 - Neutral;4 - Important;1 - Not important at all;"Again, for applications impacting humans the red lines should be established. This is not static ... it means need of transparency and involving of a political control (primate of transparency above e.g. property protection). 

The support should request ethical standards and not support projects which pose e.g. a risk of Mass Surveillance.

Projects should not be supported because they use AI but because the propose an real added value ...";1 - Not important at all;4 - Important;4 - Important;1 - Not important at all;1 - Not important at all;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;All points are very important. For me everywhere skilled humans (controlled by the society) should be in the process with a maximum on information about the used algorithms. This includes the knowledge that any result could be incomplete or even incorrect and is also influenced from the inputs given by the programmers or users. There are a lot of risks due to correlations performed by the AI for all groups of minorities or also individuals. More data is not a solution ...........................;Other;"I think the protection of (e.g. personal) data has continuously to be improved. Also the definition of ""red lines"" has to be regularly reviewed. Also the information/ right to get information about stored data and the decisions taken e.g. by AI systems should be reviewed with the increased knowledge about the used AI systems. ";Other;"I do not think that it is possible to define high-risk applications. Any AI used at the ""wrong"" place could introduce a high-risk for people. Yes, I think that new rules for all AI systems are necessary.";;;"mass surveillance, any decision taken by a AI impacting the live of peoples e.g. selecting people to be employed, health insurances, policing, 

 Any correlation with biometric data for e.g. the questions above should be forbidden.";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;This is creating not needed biometric data and is a step to mass surveillance. This is reducing our freedom and at the end the democratic live!;Rather not;Such a system gives no valuable added value because it is not under a real public control it could even confuse people with finally commercial (miss-)information.;Other enforcement system;This could be valid for any AI system (the risk is difficult to determine and can even change during the deployment). ;It should be possible to perform at any time (mainly ex ante) a human rights assessment by any adequate external entity. ;Mental health risks;;Yes;complexity and opacity are increasing and have to be handled;Yes;complexity and opacity are increasing and have to be handled;Yes, for all AI applications;;Priority of these liability rules of copyright rules - to get a real information....;
F530370;14-06-2020 20:08;French;Company/Business organisation;Kenza;Zaz;;La Française des Jeux;14184846326-23;Large (250 or more);France;The feedback can be published with your personal information;4 - Important;4 - Important;5 - Very important;3 - Neutral;4 - Important;5 - Very important;Ces actions doivent s'accompagner d'une réflexion sur la souveraineté des données et la responsabilité des centres (secteur public ou privé). La diversité des participants et leur représentativité sont cruciales pour tirer le meilleur de ces actions. Les questions éthiques devraient en particulier être ouvertes à l’ensemble de la société civile, aux ONG, universitaires (et non aux «professionnels de l’IA ») afin de ne pas limiter le débat à ses aspects technologiques ou économiques.;4 - Important;3 - Neutral;4 - Important;3 - Neutral;4 - Important;4 - Important;"Si le développement des startups est encouragé, il semblerait adéquat d’évaluer si celles-ci détiennent réellement une technologie avant de leur accorder des financements supplémentaires.
Dans le cadre de la construction d’un espace européen des données, la question de la valeur commerciale de ces données doit être envisagée.";3 - Neutral;5 - Very important;3 - Neutral;La création d’un réseau des centres d'excellence dans le cadre de partenariats publics-privés associant des universités pourrait être encouragée.;4 - Important;4 - Important;No opinion;5 - Very important;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;3 - Neutral;Toutes ces préoccupations sont de grande importance pour les citoyens. Afin d'instaurer la confiance, il pourrait être recommandé que les applications d'IA soient utilisées uniquement comme des aides à la prise de décision des personnes. ;Current legislation may have some gaps;;No;;;;Les applications ayant des conséquences sur les droits et libertés fondamentales, notamment dans le domaine médical peuvent susciter de fortes préoccupations pour les citoyens et les entreprises qui détiennent des données.;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;No opinion;Sur ce sujet, un socle de règles communes aux États Membres pourrait permettre d'assurer l'égalité de la protection des citoyens européens.;Much;Un label unique sur l'éthique, revu à un cycle régulier, pourrait permettre d'assurer une certaine lisibilité et ainsi la confiance des citoyens.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Une combinaison de ces mécanismes permettrait d'expliquer les choix proposés ou réalisés par le biais de l'IA, et d'assurer une transparence propice à la confiance.;;;No opinion;;No opinion;;No opinion;;;Contribution_FDJ_IA_2020_Final.pdf
F530369;14-06-2020 20:07;French;NGO (Non-governmental organisation);Jeanne;Bretécher;;Social good Accelerator EU;971676535322-14;Micro (< 10 employees);France;The feedback can be published with your personal information;4 - Important;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;Partnership with Social Economy and civil society to accelerate social & environmental transition, fairness, diversity and vocational rehabilitation;5 - Very important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;A better integration of Social, political and law Sciences in Research on IA;4 - Important;5 - Very important;4 - Important;A better integration of Social, political and law Sciences in Research on IA and excellence center, a focus on Design and final uses for social and environmental transition;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Special programmes dedicated to Social Economy and cilvil society organisations, Digital hubs inspired by Territorial Economic Cooperation Points (TEPs) with a diversity of organisations from classic corporations/start-ups and social corporations/start ups;3 - Neutral;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;Not sur-invest on AI, bet on pluridisciplinarity and a shared governance in between organisations seeking for different purposes in order to reach a balance, avoid bias, inequality and unfairness and ultimately make the right decision ;Other;"The establishment of an independent public/private body, again diverse and multidisciplinary, which would rate algorithms and technologies on the basis of consumer/citizen feedback and testing (retro engineering). Also, a EU label on Responsible AI ... We support the proposals of the European EESC to create a ""European Trusted Company Certificate for AI""";No;;;;Health, Mobility, Intimate Data;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;;Very much;We recommand a Label on AI social & environmental responsability;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Mental health risks;;Yes;;Yes;;Yes, for specific AI applications;;;
F530368;14-06-2020 20:03;English;NGO (Non-governmental organisation);Guillermo;Beltra;;Open Society European Policy Institute;8557515321-37;Small (< 50 employees);Belgium;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;1 - Not important at all;1 - Not important at all;The top priority must be to protect human rights throughout the AI lifecycle. The promotion and uptake of AI, particularly in the public sector, should not be a policy goal or value in itself, and public institutions should use AI only where benefits are proven, and with proper impact assessments and risk mitigation safeguards. A plan is needed to systematically include the voices of groups that are most discriminated against in all policy and legislative processes. ;4 - Important;3 - Neutral;1 - Not important at all;2 - Not important;4 - Important;2 - Not important;"A revised coordinated plan should strive to ensure the affordability and accessibility of ADMs that are critical to accessing basic services, supporting people in vulnerable situations; and ensure personal data obtained from people is not re-purposed for law enforcement or immigration purposes. The plan should also include criteria on how to allocate EU resources on AI based on human rights, the societal impacts of AI and ADMs, and the democratic oversight of AI systems.";3 - Neutral;4 - Important;1 - Not important at all;"All EU-funded R&I must:
-Primarily aim to address the challenges faced by the most discriminated against and racialized groups
-Be made conditional upon respect for fundamental rights
-Involve and consult civil society, particularly those groups who are most affected by discrimination, throughout the design, implementation and evaluation phases
-Include provisions to make the outputs (hardware, software, data, publications) available to the public under free, open licenses.
";1 - Not important at all;2 - Not important;3 - Neutral;3 - Neutral;3 - Neutral;DIHs should engage with civil society, especially with marginalized groups to build models of innovation and AI governance that safeguard democratic values and fundamental rights. As a rule, civil society should be included in decisions on funding for technology. DIHs should prioritise projects to enhance the access to digitalized public services for people in vulnerable situations, with the aim of closing the digital divide and avoiding an increase in inequities.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;"AI: 
-is used by the public sector without sufficient visibility, transparency and oversight 
-threaten accountability by making public and private uses and actors more difficult to monitor, evaluate and control
-can disrupt democratic processes and public debate
-can exacerbate existing and persistent discriminatory, harmful situations 
-can provoke and worsen anticompetitive behavior
-can entail harmful collection and exploitation of personal data
-can be inappropriately repurposed";Other;The Commission must ensure EU law is fully applicable where AI is involved, and assess if laws on liability, safety, anti-discrimination and equality require review. Ambitious enforcement, especially of the GDPR, is necessary. Any legislative intervention must reinforce existing rights, not undermine them. Additional measures will be necessary to regulate and, in some cases, prohibit the use of AI in highly sensitive public functions (e.g. policing, surveillance, immigration or social welfare). ;Other;Mandatory rules should not be limited to high-risk use, because that would leave important uses and potential transfers to other sectors out of scope and miss an opportunity to address existing harms. Instead, the regulatory framework should encompass all types of uses that present risks to people. The public sector should be regulated through a dedicated regime, due to the critical nature of certain functions as well as states’ obligations to protect human rights and provide essential services.;;;The use of AI by public authorities requires a specific governance framework given state obligations to observe human rights and the high-risk nature of the services provided. A special risk-assessment, register and public audit framework is necessary. More generally, any AI use that risks harming fundamental rights, creating or exacerbating discrimination, or endangering democratic accountability of public and private actors, are to be considered always high-risk. ;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;The most harmful uses of AI, such as the use of biometrics for identification purposes (1:n), will likely require an outright ban, due to the severity of the threats they pose to human rights, in particular to the rights to privacy, equality and non-discrimination, and to peaceful assembly and association. The EU regulatory framework should include a list of prohibited uses, and the Commission should consult stakeholders and representatives of the most affected groups specifically on such list. ;Rather not;The EU could impose a labelling system for state-deployed ADMs as a mechanism to open access to data and systems and to enhance accountability, while respecting rights. Mandatory labelling could include information on training and modeling data, model cards and oversight notes. Beyond that, voluntary, self-regulatory or ethics-based mechanisms have demonstrated to be ineffective governance mechanisms in the digital economy and should only complement, never replace, regulatory obligations. ;Other enforcement system;A successful governance framework for AI will require both clearly defined and ambitious ex-ante rules as well as a structured, efficient ex-post market surveillance and enforcement framework. Independent human rights impact assessments must be mandatory and undertaken before entry into market.Enforcement procedures should include a regular review and possible sunset clauses, and mechanisms of remedy and redress to correct and/or compensate natural and legal persons if their rights are violated.;Especially if the high-risk approach is followed, it should never be for the providers of AI/ADM applications to self-assess compliance with the established requirements and provisions. Member States breaching the legal framework should be sanctioned. Assessment and enforcement activities will require multidisciplinary and multi-sectorial expert advice, particularly from civil society and human rights experts.;Mental health risks;;;;;;;;;OSEPI_recommendations_to_AI_HLEG_on_Public_Sector.pdf
F530367;14-06-2020 19:59;German;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;;4 - Important;5 - Very important;3 - Neutral;3 - Neutral;5 - Very important;5 - Very important;;3 - Neutral;4 - Important;3 - Neutral;"many centres, networks, partnerships (not just ""a""); diversity of gender, origin, but also science-disciplines (not just boards of men, whites or bwl/vwl)";2 - Not important;4 - Important;4 - Important;3 - Neutral;4 - Important;creating an eu-server;3 - Neutral;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;programming and algorithmising with neutral values is impossible (programmers will always put their education, value system, prejudices, etc. in it);There is a need for a new legislation;;Other;for all apps;;;algorhitmic based evaluation of humans (e.g. face-recognition, predestination, behaviour-prediction);5 - Very important;5 - Very important;4 - Important;3 - Neutral;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;suspicion of all destroys democracy;Not at all;"""voluntary"" and KI-risks are incompatible";A combination of ex-ante compliance and ex-post enforcement mechanisms;;ex-ante compliance and ex-post enforcement: external AND internal;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;"discussing ""product liability"" completly new";
F530366;14-06-2020 19:51;Italian;Business Association;francesca;giordano;;Confindustria Digitale;746723218152-94;Large (250 or more);Italy;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;;5 - Very important;;5 - Very important;5 - Very important;;5 - Very important;4 - Important;4 - Important;"La sfida che può costituire un cambiamento significativo del posizionamento europeo nello “scacchiere” dell’intelligenza artificiale è senza dubbio quella di creare un centro di eccellenza di livello mondiale, in grado di attirare talenti da tutto il mondo e rafforzare la competitività europea; questo non deve avvenire con un approccio centralizzato, ma valorizzando e potenziando le esperienze efficaci di sviluppo della ricerca nel campo dell’AI già presenti nel sistema.";5 - Very important;;5 - Very important;5 - Very important;;;No opinion;5 - Very important;5 - Very important;5 - Very important;4 - Important;No opinion;;Current legislation may have some gaps;;Yes;;Yes;;Riteniamo che dei criteri indicati per i progetti ad alto rischio dal libro bianco (settori e modi di utilizzo) quello sui settori possa essere di difficile gestione in quanto che l'AI sarà sempre più trasversale e presente in tutti i settori della società e la lista potrebbe diventare di difficile gestione e richiederebbe degli aggiornamenti continui.;;5 - Very important;5 - Very important;;4 - Important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);"L’utilizzo dei sistemi di riconoscimento facciale è importante in molte applicazioni di AI già in fase di utilizzo; tuttavia le preoccupazioni rispetto ad una sua adozione negli spazi pubblici in ordine al rispetto del diritti fondamentale alla privacy dei cittadini hanno motivazioni importanti, che devono condurre ad una rigida regolamentazione dei casi eccezionali in cui questo possa essere ammesso, in linea con i principi e le norme dl diritto dell’Unione";Very much;"E’ interessante per ecosistemi frammentati perchè consente di mediare le esigenze all’interno di un medesimo scenario. La proposta è: approccio bottom-up e PP per definire gli std minimi da adottare; improntato alla massima semplicità; basato su nuova sussidiarietà per concentrare messaggi e servizi comuni a livello di ecosistema e su strumenti per valutare il livello di compliance. Intendiamo agire da luogo di confronto sul tema per identificare l’approccio più adatto alle diverse realtà";Other enforcement system;"Il favore con cui meccanismi di selfassessment sono accolti dall’industria in questo caso deve tenere in considerazione il fatto che i sistemi di IA saranno pervasivi e andranno ad intervenire in settori con caratteristiche e regole molto diverse tra loro; dovranno quindi poter essere utilizzati tutti gli strumenti più adatti a rinforzare affidabilità e fiducia nei sistemi di IA, quindi sarà opportuno effettuare analisi e valutare meccanismi di compliance diversi per i diversi ambiti";;Personal security risks;;Yes;;Yes;;Yes, for all AI applications;;"L’utilizzo di sistemi di AI potrà essere favorito da chiarezza normativa rispetto alle responsabilità anche con riferimento al tema dei risarcimenti, quindi può darsi che sia opportuno integrare la normativa rispetto a tale aspetto; di fondamentale importanza però che una simile integrazione avvenga con una approfondita analisi di quelle che sono le reali responsabilità decisionali sul funzionamento dei sistemi, che potrebbe condurre anche a non adottare ulteriori norme.";
F530365;14-06-2020 19:47;French;NGO (Non-governmental organisation);Didier;COEURNELLE;;Asssociation Française Transhumaniste - Technoprog;;Micro (< 10 employees);France;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;2 - Not important;2 - Not important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;2 - Not important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;2 - Not important;;2 - Not important;2 - Not important;2 - Not important;4 - Important;2 - Not important;;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;;Current legislation may have some gaps;;No;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;No opinion;;Yes, for all AI applications;;;R_ponse_AFT__Technoprog_Consultation_on_the_White_Paper_on_Artificial_Intelligence_-_A_European_Approach.pdf
F530364;14-06-2020 19:35;English;NGO (Non-governmental organisation);Sarah;Andrew;;Avaaz Foundation;475565317526-24;Medium (< 250 employees);United States;The feedback can be published with your personal information;4 - Important;4 - Important;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;Partnership with and consultation of civil society organisations such as Avaaz, the world’s largest online civic movement with 62 million members. Avaaz has been investigating and campaigning on the threat posed by disinformation, organized and distributed at scale by AI on social media platforms and has detailed legislative and regulatory solutions backed by evidence in over 8 international and EU-focused reports;4 - Important;4 - Important;3 - Neutral;4 - Important;3 - Neutral;3 - Neutral;Partnership with and consultation of civil society organisations such as Avaaz, the world’s largest online civic movement with 62 million members. Avaaz has been investigating and campaigning on the threat posed by disinformation, organized and distributed at scale by AI on social media platforms and has detailed legislative and regulatory solutions backed by evidence in over 8 international and EU-focused reports;4 - Important;4 - Important;3 - Neutral;The key tool researchers need is access - to the data, coding and outputs of AI. The restrictions social media platforms place on access to this data frustrates civil service and crucial research for example the Royal College of Psychiatrists in the UK has called on social media companies to share data with researchers to measure the mental health impacts on young people of microtargeting, filter bubbles, and advertising.;No opinion;No opinion;No opinion;No opinion;No opinion;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;"AI amplifies the spread of disinformation and other harmful content at scale, and fails to identify and moderate it. Platforms using AI to disseminate content must invest more in its ability to learn and flag this content. Disinformation has an adverse effect on fundamental rights; health and the ability to make free, informed, independent decisions and contributions to the community which is essential to the functioning of democracies. We explain the mechanisms in the attachment";There is a need for a new legislation;;No;;;;AI used in the dissemination, recommendation and selection of media content is most concerning, particularly on social media platforms and search engines, both for the fundamental rights it affects including but not limited to freedom of thought, opinion, information and expression, and its role in distributing, moderating and/or correcting disinformation, please see attachment for further details.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;;Rather not;The threat and risks of algorithms being used to amplify dis/misinformation are too large to be left to voluntary self-regulatory systems. While voluntary labelling systems may be useful in some cases, for example in keeping younger users safe and helping them understand their data use at the point of use, they are insufficient to deal with the major threats AI can pose. Mandatory labelling systems and adequate regulation and enforcement are key.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;"Robust ex-ante compliance and ex-post enforcement mechanisms are vital. Given the freedom of thought, opinion and speech concerns that ex-ante mechanisms can raise, the ex ante mechanisms we have included in our paper are systemic: relating to the design of the algorithm, its learning models and investment in data sets not to particular pieces of content being flagged ex ante. There should be no prior censorship for legal content, but freedom of speech does not equal freedom of reach.
";Mental health risks;"Yes, the information processing capacity of AI in the dissemination, recommendation and selection of media content has the capacity to undermine the fundamental right to freedom of thought, from which flows freedom of opinion, expression and information. Immaterial harms not dealt with in the framework include loss of privacy, the undermining of democratic processes, limitations on the rights to dignity and freedom from discrimination.
";Yes;"AI-based products are always evolving, so continuous risk assessments are crucial. We have suggested an algorithmic audit framework to assess the impact on fundamental rights, identify mitigation strategies for harms, and reduce the spread of harmful content and misinformation. There should be a two step process: planning and audit of the design of the AI and its code in the context of its likely usage, and then periodic audit of its output.
";Yes;"People having suffered harm due to AI must enjoy the same level of protection as people having suffered harm caused by other technologies. But under the current EU product safety and liability framework, it may be hard for victims to get redress because it’s often difficult to prove defect, damage, and a causal link. Amending the framework to cover material and immaterial risks due to AI will provide EU citizens with an avenue for redress where this may not be available right now.
";Yes, for all AI applications;;We believe this issue calls for a comprehensive EU framework to avoid a fragmented field of national liability rules;Avaaz_Foundation_submission_to_to_EU_AI_White_Paper.pdf
F530363;14-06-2020 19:25;English;Business Association;Monika;Magyar;;Association of Commercial Television in Europe (ACT);18574111503-28;Micro (< 10 employees);Belgium;The feedback can be published with your personal information;4 - Important;4 - Important;;;5 - Very important;;Promoting AI compliance by design (transparency, explainability, internal and external auditability) to ensure AI applications respect and uphold existing IP and media rules and freedoms. Ensure AI applications do not reinforce the market dominance of certain players, notably in the online environment.;4 - Important;;4 - Important;;;5 - Very important;;;;;;;;;4 - Important;;;No opinion;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;The AV sector increasingly uses AI tools at several points of the creative, production and distribution chain. See examples attached in annex to this response. The core concern for commercial televisions on AI focuses on intellectual property rights (IPRs) and editorial integrity. Any legislation on AI should ensure existing IPRs are maintained and enforced whilst maintaining strong editorial integrity to ensure high trust in news media.;Other;Any new legislation regarding AI should be aligned with existing IP and media law in order to ensure that IPRs are maintained, contractual freedom is not undermined and existing media laws and freedoms are upheld. ;Other;A low or high risk assessments should also account for whether the AI application may be at risk of undermining existing legal provisions and protections. AI applications are a means and not an end. As such the ACT would recommend an approach that is compliant by design (transparency, explainability, internal and external auditability). ;;;Where AI applications use proprietary data to create works without due compensation/prior authorisation of the right holder. AI applications that automatically generate “news” content without due human supervision. AI applications used to create deep fakes with the intention to disinform or mislead the general public. It is important to make sure monitoring and crime prevention does not endanger journalistic sources, source material and journalistic research. Please find full answer attached.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;Where AI applications are compliant with existing legal protections, including GDPR, they can be a useful tool to enhance the creation and distribution of content as well as viewer experience. For example, emotion-tracking can be used in the media sector as a means of enhancing viewer experience.  Biometric identification in public spaces pose major risks to journalistic sources.;Very much;;;;"ACT would suggest a by design approach in the development of AI applications that includes the relevant provisions of existing EU laws governing IP and media (e.g.; Copyright directive, IPRED) to ensure ex-ante compliance. (transparency, explainability, internal and external auditability).  For any monitoring of publishing (e.g. AVMS) ex-post mode must remain the rule. Further ex-post verification measures should be in place to ensure the compliance requirements are present and effective. ";;"Breach of intellectual property
Breach of editorial integrity
Breach of the safety of journalistic sources
";Yes;Risk assessments should be updated with new legal provisions in place;Yes;Concerning high risk getting ex ante control and no risk getting ex post control, the specifics of media publishing needs to be addressed. Any kind of media publishing must be regulated via ex post control when it comes to utterances. However, when it comes to protection of data, especially in relation to journalistic sources, publishers deal with high risk information and system safety used for this purpose needs ex-ante control.;Yes, for specific AI applications;New AI applications are continuously being developed. It is important that any new national or European rules governing AI and liability ensure that existing liability rules are upheld so as to ensure proper compensation of right holders and reflect existing rules. This should be applicable to all AI applications    ;The ACT is concerned that certain reports have outlined the need to reevaluate existing IPR protections in the light of AI applications. AI applications are a means and not an end in themselves. As such we see no need or justification for new exemptions to existing IP rules. As such AI applications should by design respect existing rules, notably in the field of IP and media law, whilst upholding the principle of contractual freedom. ;200614_ACT_-_AI_consultation_-_final.pdf
F530362;14-06-2020 19:22;German;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;1 - Not important at all;KI ist kein Selbstzweck. Wir können KI nicht dem Rest der Welt überlassen, aber wir müssen sie immer eindeutig mit unseren Werten verknüpfen. Besonders die öfftl. Hand darf nicht zu früh für Experimente -die es lange noch bleiben- benutzt werden. Der Schwerpunkt muss auf der Überprüfung der KI-gebildeten Kriterien liegen. Jedes Anlernen sorgt für Bias und der muss kontinuirlich dynamisch im Blick bleiben - damit können wir anderen voraus sein. DSGVO erweitern!;5 - Very important;5 - Very important;1 - Not important at all;4 - Important;5 - Very important;5 - Very important;"Mir fehlt der wichtigste Aspekt: der auf gesellschaftliche Implikationen. KI muss ganzheitlich betrachtet werden und hier läge auch die besondere Stärke der EU-Initiative. Haftung, demokratische Prozesse, Ethik und Menschenrecht müssen einbezogen werden. Letztlich profitiert davon der Output und dann hätte EU-KI DAS Alleinstellungsmerkmal. 
Vorrang für KI zu Themen Klima/Umwelt und Folgenbekämpfung";4 - Important;5 - Very important;2 - Not important;Intransperenz wie bei iBorderCtrl darf es nicht geben. Einbeziehung von anderen Disziplinen zur Transparenz-Steigerung, Rechtewahrung von Betroffenen und Vermeidung von Diskriminierung. PPP nur mit strikten Spielregeln;2 - Not important;4 - Important;5 - Very important;4 - Important;4 - Important;"KMUs als unser 'Rückrad' zu fokusieren ist sehr wichtig. Die EU muss aber auch direkt über die 'Pflichten' aufklären und effiziente Tools für die Einhaltung bereitstellen. Das Debakel mit der Einführung der DSGVO darf sich nicht wiederholen.
Vorrang für KI zu Klima/Umwelt - dort wird sie dringend benötigt.";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;Alles genannte wird sich definitiv nicht vermeiden lassen. Umso wichtiger ist, dass der Output jederzeit von unabhängigen Stellen bewertet werden kann. Black Boxes darf es nicht geben. Open Source!;Other;Current legislation is fully IN-sufficient;No;;;;Der gesamte öffentliche Sektor darf nur unter strengsten Auflagen und mit größter Transparenz KI einsetzen. Bürger brauchen ein Widerspruchsrecht.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;"selbst mit den besten Absichten bleibt das absolut inkompatibel zu freiheitlichen Gesellschaften! 
Gerade verlässt sogar IBM aus ethischen Gründen dieses lukrative Geschäft.";Rather not;wo so ein Label in Frage kommt sollte es eher eine Zertifizierung mit Kennzeichnungspflicht geben. (Das stärkt dann nebenher auch die EU-KI);A combination of ex-ante compliance and ex-post enforcement mechanisms;;obligatorischer interdisziplinärer (inkl. Ethik, Recht), externer Prüfvorgang ex-ante und jederzeit die Möglichkeit anlassbezogen ex-post.;Mental health risks;KI ist systembedingt IMMER diskriminierend (Bias)! Die EU muss sich darin auszeichnen, diese Schwäche sehr gut zu adressieren.;Yes;Es braucht sehr gute Werkzeuge und Fachpersonal, um eine erweiterte DSGVO effizient umzusetzen und um nachträgliche Anpassungen, auch ex-post, vorzunehmen und zu belegen.;Yes;"keine Verantwortlichkeit, ""der Computer ist schuld"" darf es nicht geben. Transparenzpflicht, Anreize für OpenSource";Yes, for all AI applications;;eine (europäische?) verpflichtende KI-Haftpflicht.;
F530361;14-06-2020 19:18;English;NGO (Non-governmental organisation);Emma;Traoret;;European Society of Radiology;478131313572-40;Medium (< 250 employees);Austria;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;No opinion;5 - Very important;5 - Very important;Actions should be undertaken to enhance data availability and secure access in a common database. In healthcare, EHRs and an EU data space should be advanced for a federated access to health data to support personalised medicine, research and coordination between national initiatives. EU guidelines based on expert work should be developed for identical machine-readable anonymised data despite the origin, to enable cross-sector flows and reusability.;5 - Very important;5 - Very important;5 - Very important;No opinion;5 - Very important;5 - Very important;The ESR believes that Member States and the EU Commission should cooperate and align regulations regarding disclosure of data which may be prohibited by the Member States in which the data is located, and thus preventing efficient implementation of GDPR and of the free flow of non-personal data regulation for research uses. The EU Commission should also push for an exchange of best practices and synergies between member states built on national initiatives and strategies.;5 - Very important;5 - Very important;5 - Very important;Varied high-quality health data is valuable for research and their availability should be increased. The EU should incentivise Member States to foster interoperability in their systems and to raise awareness citizens’ shared data potential to improve R&D. A framework should enable all EU data to be in the same machine-readable format. AI in daily use requires adequately trained professionals and investments in education and training to complement the Digital Europe Programme’s skills pillar.;No opinion;No opinion;No opinion;5 - Very important;3 - Neutral;;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;AI in healthcare risks widening inequality gaps between hospitals in countries with different economic developments, this can be tackled with EU funding (Digital Europe Programme, Structural & Cohesion Funds). AI also needs EU guidance in health to steer national developments for a new approach to pricing and service reimbursements. The “black box phenomenon” poses liability concerns of computers overcoming human medical skills and decreasing quality of care for patients left not fully informed.;Current legislation may have some gaps;;Yes;;No opinion;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;;Much;The voluntary labelling system for trustworthy AI-enabled products and services can be a useful tool to build user confidence, to enhance human oversight and support the safe use of AI applications. However, it should be set up and harmonised at EU level, based on expert opinion and with agreed standards so that operators can increasingly relate to it. Self-assessment should be considered to create a flexible and decentralised mechanism with built-in ex-post assessments to ensure compliance.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;The ESR’s further suggestion to the assessment of compliance is for the audit processes to be fulfilled by a European governance structure, to ensure that the external assessment procedure be carried out by bodies with adequate resources and expertise. Besides, the ex-ante assessment should not be centralised as it may cause delays in patients’ access to innovative technologies.;Personal security risks;An additional risk from the use of artificial intelligence stems from the “black-box phenomenon” of which decisions are made solely based on the AI application, which creates liability issues. Moreover, a lack of ample varied datasets to train the artificial intelligence system may create biases, widen socio-economic disparities and wrongly influence policy or funding decisions.;Yes;;Yes;To uphold EU standards and legal certainty, the Product Liability Directive should consider existing sectoral legislation (eg. Medical Device Regulation). Legislation should be mapped, adapted to technological innovation and leave further room for harmonisation. A risk-based approach for AI ought to be pursued. Besides, any framework should enable the protection of patients using AI applications and treated with AI-operated devices but also cover practitioners using said devices.;Yes, for all AI applications;;The ESR believes that national liability rules should be adapted to consider the risks resulting from the emergence of new technologies. Any cross-sectoral approach at EU-level should result in more adequate national liability rules that protect citizens against potential damages from the use of AI applications that put at risk ethical, societal, liability and privacy standards.;ESR_Statement_for_Public_consultation_Artificial_Intelligence_June2020.pdf
F530360;14-06-2020 19:18;English;EU Citizen;Alessandro;Lofaro;;;;;Italy;The feedback can be published with your personal information;5 - Very important;3 - Neutral;4 - Important;5 - Very important;5 - Very important;5 - Very important;"The ""skills"", and in general the initiatives, are too much focused on the economic side and on the ""create enthusiasm"". Must be given broader understanding to broader base of citizens, enthusiasm must come from understanding, not from ""pink glasses"". Also, EU organisations are giving the bad example about sharing public data,  I know agencies holding tight to ""their"" data, sharing should be mandatory by default, not sharing should be the exception (e.g. EUROPOL cannot share police data)";4 - Important;3 - Neutral;5 - Very important;5 - Very important;4 - Important;5 - Very important;;2 - Not important;5 - Very important;4 - Important;Trying to create a monster centre, is only going to create a bureaucratic academic centre of power. It is better to create a diffused pool and network, guarantees more resilience of the whole system, more spreading of the results, and more space for creativity, while allowing still to have and promote excellence;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;"See above the comment about the ""lighthouse research centres"", the digital innovation hubs have often become something similar in practical terms";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;Use and abuse by private interest, public authorities, and third countries. Other concerns (e.g. criminality, military use) are explicitly and diplomatically (subsidiarity is about the legislation, not discussing the subject) excluded  from the consultation. Third countries includes USA and e.g. India (see their tender for a face identification system, using even pictures taken from the Internet);Current legislation may have some gaps;;No;;;;"Use by public authorities and private sector, typically with the approach/expectation ""is a computer, it cannot be wrong"" - in ""AI"" (mostly, really means ""machine learning"" and ""deep learning""), that is not true, given the way it works and learns. It is not about calculating an fixed equation  ";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;"The use of images captured, e.g. by police, through image analysis, could be allowed, under strict controls and limitations, and is already used (often with poor results, police does not understand the systems, and they ""sold"" as perfect from the lab but give poor results in reality) but the general use of real-time biometric recognition systems acting on a public area, like face recognition of everybody passing in the street in an everyday situation, should not be used.  ";Not at all;It risks to create a false sense of security and trust. Below the high-risk, there are a number of situation were there can still be a damage, it should not be given the false impression that nothing can go wrong;Other enforcement system;"Adapting the rules about civil and criminal responsibility, to avoid ""manufacturer""/developer, seller and business using them to hide away using the 'it was an autonomous ""AI""' excuse ";Existing authorities, like the EDPS and data protection authorities, should still be able to do their own evaluation, assessment and enforcement.;Mental health risks;Liability towards third parties.;Yes;Again, focus on the potential outcome, and consider the need to check, and possibly recertify (depending on the extent of the changes)  when changes occur. I know it is more complicated than a checklist and standardised testing procedure, but these can be dynamic systems (if the system is retrained in production), thus requiring a dynamic approach;Yes;;Yes, for all AI applications;;;
F530359;14-06-2020 19:09;English;NGO (Non-governmental organisation);Ioanna;Psalti;;European Glaucoma Society Foundation;720911016370-40;Micro (< 10 employees);Switzerland;The feedback can be published with your personal information;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;"Establish framework for validation of AI applications within Europe & across member states;
Incentives for employers to upskill their workforce;
Target sectors for improve awareness of changing markets and the potential for new technologies;
Engage with ordinary citizens when defining what is a good AI society and integrate the operating principles & potential of AI as a new civic skill that covers all groups of citizens.
";4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;Funding for organisational and social innovations are needed for the successful utilisation of new technologies and for innovations that generate new types of production, as they create new tasks and jobs.;4 - Important;3 - Neutral;5 - Very important;Incentives to encourage multidisciplinary and versatile research linking basic research and cooperation between universities, companies and public organisations.;4 - Important;5 - Very important;4 - Important;5 - Very important;4 - Important;;4 - Important;4 - Important;5 - Very important;4 - Important;3 - Neutral;5 - Very important;"Increasing AI supply in healthcare may increase demand/unwarranted concerns & fear at patient level,precipitating overdiagnosis and increasing costs at service/system level.
AI may have negative health impact e.g. other than the use of data analytics there is no proof that wearables alter health trajectories;
Increasing reliance on AI for clinical decision-support may lead to unintentional deskilling of medical workforce/challenging cognitive capabilities/impact on quality/safety of care.
";Other;"Sector-specific legislation needed e.g. healthcare;additional guidance regarding GDPR implementation to remove barriers in health research & create evidence;
Responsibility across supply chain of AI solutions for: physical and psychological health of consumers;data protection/protection of privacy; addressing existing structural inequalities in society; prevention of injustice/damage/suffering to individuals & groups of people.
Algorithm transparency &code of conduct (value of metrics used,etc";Other;Sector specific requirements depending on risk for society. Huge ethical questions emerge regarding young & healthy people inventing algorithms/devices for other young healthy people which however make them sick such as evidence shows on influence of social networks & gaming in promotion of self-harm/suicide.  There is a need for an independent regulator body with enforceable powers & additional codes of practice for social networks, gaming and internet companies to address such issues.;;;Healthcare as accountability demands traceability (data integrity used as ground for AI making decisions).Accuracy of AI may be overestimated when compared with what is achievable in daily practice due to case?control design e.g. well?defined groups of healthy & glaucoma eyes are selected in nearly all studies, rather than the use of imaging tests in series of patients at risk of glaucoma as in real world.Screening programmes need mapping of patient flow across whole clinical pathway (ref);5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;"There is need for clear guidance on: assessing impacts of biometric ID systems, risk management measures; assigning accountability and who will audit such guidance at national and EU level..
Otherwise structural inequalities may persist with uneven distribution of benefits and risks to social cohesion.";Much;"Voluntary labelling system with overlooking European monitoring group to ensure promotion of ethical values & monitoring of compliance with such principles including assessment of practices in the context of defining responsibilities when a machine is making decisions autonomously.
Such system to promote trust, appreciation & cooperation and consider the European societal heterogeneity and diversity in educational backgrounds, languages, ethnic groups, genders and age groups.";A combination of ex-ante compliance and ex-post enforcement mechanisms;;"Rigorous assessment regarding quality of evidence before AI is made available for public use;
Mandatory algorithmic transparency requirement with ethical obligations imposed on platforms as it is in the GDPR; 
Additional scrutiny for algorithms which may distort or restrict competition; 
Ethical rules for secondary use of social welfare and healthcare data and for more extensive use as part of the health sector growth strategy;";Cyber risks;Before AI product in healthcare are placed in the market, there should be high quality evidence on benefit & relevancy to clinical problem they claim to solve & the current clinical practice as in development of medicinal products. Ophthalmology suffers greatly from  lack of responsibility & accountability in cases of harm. In absence of any regulations for diagnostic tests or medical devices which can be inserted in the eye, diagnosis & treatment proceed with no evidence on improving outcomes.;Yes;"There is particular concern for software as a medical device and the need for software modifications guidance that focuses on  risks to users/patients resulting from the software modification which significantly affects device performance/safety/effectiveness; new indications for use, modifications introducing a major change to algorithm/new clinical effects, or significant technology modifications that affect performance characteristics.";Yes;Harmonisation across frameworks regarding safety of products.;Yes, for specific AI applications;Sector-specific legislation for sectors where safety and security is critical such as in the healthcare sector where specific issues for compensation for damage arise due to vulnerability to hacking/malicious behaviour. Liability in screening programmes in public health when not only accuracy of diagnostic test is important but also mapping patient flow during whole clinical pathway. https://www.cochranelibrary.com/cdsr/doi/10.1002/14651858.CD008803.pub2/full?highlightAbstract=glaucom%7Cglaucoma;"Transparency, responsibility & societal benefit to inform AI technology use & application & clear strategy & action plan to achieve a societal vision of AI but there is a pressing need for detailed definitions at national/EU level of what these mean in practice & how they relate across the several components of AI ecosystem & the different regulatory systems per sector of application. Responsibility also for structural inequalities, uneven distribution of benefits and risks to social cohesion.
";
F530358;14-06-2020 19:04;English;Company/Business organisation;Alexa;VELLER;;T-REGS bv;;Micro (< 10 employees);Belgium;The feedback can be published with your personal information;3 - Neutral;5 - Very important;2 - Not important;3 - Neutral;1 - Not important at all;3 - Neutral;;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;3 - Neutral;4 - Important;;5 - Very important;5 - Very important;3 - Neutral;With respect to the lighthouse research centre: the White Paper narrows the AI definition down to “data” and “algorithms” which is too wide. COM(2018) 237 defines it as “[…] systems that display intelligent behaviour” which is circular. The High Level Expert Group narrows AI down to machine-learning type applications and does not seem to cover applications in e.g. autonomous agents research (robotics). The lighthouse research centre should be opened to the widest possible interpretation of AI.;3 - Neutral;5 - Very important;3 - Neutral;3 - Neutral;1 - Not important at all;;4 - Important;5 - Very important;4 - Important;5 - Very important;4 - Important;4 - Important;;Current legislation may have some gaps;;Yes;;Yes;;;3 - Neutral;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;Remote biometric ID systems should not be put in place until a specific guideline or preferably EU level legislation is in place. Recent events have shown that even the companies offering these applications (to e.g. law enforcement) have become wary of the way in which these systems can be (ab)used, with a temporary discontinuation of their offer to specific agencies, or even a blanket withdrawal of their system from the market. ;Not at all;The voluntary labelling system does not contribute much to the safety of AI products and services.Such labelling is often a high cost burden, especially for SMEs. Software requires regular updates. Refreshing the label with every iteration is impracticable. AI labelling of products and services aimed at consumers is questionable, given that it is unlikely that citizens would readily be able to understand the implications of AI being part of a product or service.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;No new AI regulatory authority is required. In stead, one AI expertise centre to advise all sectoral (such as banking, telecom, health, food, product safety, etc.) regulatory authorities on AI specific matters is appropriate. The combination of sector-specific ex-ante regulation, combined with ex-post enforcement mechanisms, plus the application of competition law, has worked  towards continually liberalising and broadening other markets and ecosystems. ;;;Yes;;Yes;;No opinion;;;
F530357;14-06-2020 19:02;German;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;4 - Important;5 - Very important;5 - Very important;2 - Not important;5 - Very important;"Die Entwicklung und Förderung von AI Skills sollte allgemein verfügbar sein und auf keinen Fall auf eine elitäre Gruppe von z.B. sog. ""Exzellenz-Unis"" beschränkt sein.";5 - Very important;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;No opinion;"section 4.A trifft keine Aussage über ""European data space"", daher kann die Frage nicht beantwortet werden";2 - Not important;5 - Very important;2 - Not important;"Die Entwicklung und Förderung von AI sollte allgemein verfügbar sein und nicht auf ein ""lighthouse centre"" oder eine elitäre Gruppe von z.B. sog. ""Exzellenz-Unis"" beschränkt sein.";4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;Current legislation may have some gaps;;No;;;;Gesichtserkennung (oder jede andere Form der automtischen Erkennung einer Person anhand persönlicher Eigenschaften), autonome Waffensysteme;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;"""Biometric identification systems"" verändern durch ihre bloße Anwesenheit das Verhalten der Menschen im öffentlichen Raum und verhindern dadurch eine freie Entfaltung der Persönlichkeit.";Not at all;"Selbstregulierung funktioniert fast nie. Ein ""labelling system"" sollte verpflichtend sein";A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F530356;14-06-2020 19:00;English;Company/Business organisation;Ken;Cassar;;Umnai ltd;;Micro (< 10 employees);Malta;The feedback can be published with your personal information;3 - Neutral;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;In AI the vast majority of excellence is still to be invented. The ability to recognise and nurture the sources of breakthrough excellence is likely to be useful.;4 - Important;3 - Neutral;4 - Important;4 - Important;4 - Important;4 - Important;;3 - Neutral;3 - Neutral;4 - Important;;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Within 3 to 5 years AI development and deployment is likely to be no more technically challenging than typical programming today. Pervasive deployment may be better aided by supporting entrepreneurship ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;These are all currently true but may not be soon. A forward looking regulatory framework should anticipate what will be different lest the regulation become outdated before its implementation ;Other;As new disruptive technologies take hold they will require a fresh look and gap analysis of legislation;Other;Rules are designed to mitigate harm not risk and the individual suffering harm will subjectively grade that harm. New technologies will likely change the cost and complexity of avoiding harm, removing the need to trade off perceived lower harms;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Very much;Emerging technologies are likely to normalise and de-cost risk assessment and audit of AI systems at which point all systems can be trust marked if appropriate or valuable.;Other enforcement system;Emerging technologies will likely trivialise the cost and complexity of validating compliance with standards and regulation which will enable new concepts of oversight.;;;;Yes;See attached;Yes;;Yes, for all AI applications;;;Umn_ai_Ltd_-_EU_AI_Consultation.pdf
F530355;14-06-2020 19:00;English;Other;Richard;Jones;;Institution of Occupational Safety and Health (IOSH);913858710558-02;Medium (< 250 employees);United Kingdom;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;IOSH, the Chartered body for occupational safety and health (OSH) professionals, advocates improved multidisciplinary working to ensure safe and healthy AI-enabled applications in the workplace, involving OSH professionals, human resources professionals, designers, employers and workers. This could help generate more trust and engagement in this challenging field. We welcome the White Paper’s stakeholder-inclusive approach in helping facilitate communication with interested parties.;5 - Very important;3 - Neutral;4 - Important;3 - Neutral;5 - Very important;3 - Neutral;IOSH believes it is essential that technological advancements positively impact workers’ OSH and working conditions and that public policy supports responsible development and use of AI. We advocate more focus on strengthening multidisciplinary research funding, evidencing the impacts on work and workers of AI-enabled applications in working environments. We also suggest wider engagement of relevant expertise, such as professional bodies, academic institutions, think-tanks and foresight units.;4 - Important;5 - Very important;4 - Important;To strengthen the research and innovation community, IOSH suggests prioritising EU funding initiatives like Horizon 2020 Marie Sk?odowska-Curie actions, which can fund Individual Fellowships, where fellows engage through secondments in companies, and also seeking talent from different disciplines including occupational health, safety, wellbeing, security, sustainability and social science.;5 - Very important;3 - Neutral;4 - Important;5 - Very important;3 - Neutral;IOSH believes AI technologies must have a human-centred and ethical focus, which prioritises occupational safety and health (OSH) and process safety. It is therefore important for Digital Innovation Hubs to extend their approach and engage the expertise of professional bodies such as IOSH and OSH professionals in AI design and development work. They can also contribute to the implementation stage of AI-driven technology, helping identify, prevent and mitigate any unintended consequences.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;IOSH welcomes the White Paper’s focus on principles for AI based on respect of fundamental rights, human dignity, non-discrimination and protection of privacy and personal data. We advocate risk assessment and ongoing review of new technologies and digital transformation (including AI adoption and man?to?machine communications). In addition to those listed, our concerns include potential harm to physical and mental health at work, due to any inappropriate algorithmic decisions or tracking.;Other;IOSH stresses the need for the revised Machinery Directive to consider AI, robotics, the Internet of Things, etc. Also, extension of existing legislative focus on safety risk at product-launch, to include AI lifecycle changes, with designers and adopters required to control potential OSH risks. Lifecycle risk assessment and competent human oversight are needed for autonomous behaviour of certain AI systems. We advocate considering rights to disconnect, explanation and AI education at work.;Other;The proposed high-risk approach has a welcome focus on the protection of safety, consumer rights and fundamental rights. However, IOSH believes the system for AI must also protect physical and mental health at work and should potentially apply to moderate risk applications too. Overall, it is critical to maximise the many potential occupational safety and health benefits of AI applications, while minimising any potential threats.;;;"While recognising the OSH benefits of ethical and socially responsible use, examples of concern include potential misuse of AI-enabled workplace sensors leading to tracking of all aspects of worker activity; insufficient collision control where AI-enabled robotic devices and workers share space; and insufficient clarity on responsibility for AI-enabled decision support systems.";3 - Neutral;3 - Neutral;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;;Very much;A labelling system in addition to the legislation, will need to involve designers, developers and deployers and other professions such as OSH and human resources professionals. These can help address the potential effects on work and workers before, during and after an AI-enabled system/technology is implemented. Labelling systems will need to be reviewed and updated as new technology, evidence and standards develop.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;IOSH advocates regulatory oversight and checks of AI applications at work, including by OSH departments and authorities. Focus and training should prioritise OSH assistive and collaborative AI. Workers must be consulted whenever new technologies are integrated at work, applying a worker-centred ‘human in command’ approach. As AI is increasingly used for workplace decision-making and assisted work, it is essential that evidence is built and that OSH benefits are maximised, and any risks minimised;Mental health risks;;Yes;IOSH advocates socially responsible adoption of AI technologies. We believe that before any AI-enabled devices or systems are introduced into a workplace, a thorough OSH review of their benefits and risks should be performed. OSH professionals, researchers, employers and workers must consider how AI-enabled applications in the workplace will impact the workforce, local community, supply chain and others affected by the organisation’s activities. This requires adequate AI education at work.;Yes;IOSH believes that a central premise of European regulation should be to ensure that there is clarity, visibility and traceability over who is responsible for what, to ensure they take the necessary actions. For enforcement purposes, it is essential to know who is responsible if complex AI technologies go seriously wrong. Regulators and duty holders will need to address any challenges in investigating serious failures when AI and complex algorithms are implicated.;Yes, for specific AI applications;IOSH advocates a comprehensive and protective AI governance framework. This can help ensure joint planning to prevent, manage and mitigate system failures and OSH harm that can arise due to human error or cyberattack. Safety critical systems failure, as on major hazard sites, may have serious and substantial OSH consequences and effective emergency planning is vital. IOSH believes that regulatory focus should primarily be on preventive protection of workers’ OSH and wellbeing, to avoid harm.;"The use of socially responsible technology can reduce workers’ exposure to serious workplace hazards, ensuring OSH is designed-in at inception stage (prevention through design). However, there are also challenges that must be addressed and IOSH believes that the EC’s ethical requirements of trustworthy AI (lawfulness, ethics and robustness) must be met, including ensuring human agency; safety; governance; transparency; fairness; societal wellbeing; and accountability.";IOSH_references_for_EC_AI_White_Paper_consultation_Jun_20.pdf
F530354;14-06-2020 18:53;Hungarian;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;;3 - Neutral;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;;Current legislation may have some gaps;;Yes;;Yes;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Very much;;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;;Personal security risks;;Yes;;Yes;;Yes, for specific AI applications;;;EUMIFeherkonyv_MI_Koalicio_eszrevetelek.pdf
F530353;14-06-2020 18:47;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;4 - Important;4 - Important;5 - Very important;3 - Neutral;5 - Very important;4 - Important;Financing is not the only obstacle for start-ups innovating in AI. It is more important to strengthen the general development of the entrepreneurial environment in the Union. ;5 - Very important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;The Digital Innovation Hubs should be set up as sandboxes in which any interested party can innovate with very limited regulation without creating significant risks for the broader economy and community;5 - Very important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;2 - Not important;;No opinion;;;;;;;4 - Important;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Cyber risks;;Yes;;No opinion;;No opinion;;;AI_consultation_-_Georgi_Katanov.pdf
F530352;14-06-2020 18:46;English;Company/Business organisation;Janne;ELVELID;;Facebook Ireland Limited;28666427835-74;Large (250 or more);Ireland;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;;4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;A regulatory framework for AI should focus not only on harms, but also on the benefits of AI. By focusing also on the positive effects of AI, one should also take into account the risks of not using AI in a particular context, along with the beneficial uses and applications of AI, such as their potential to reduce or mitigate discrimination, increase human autonomy, improve safety and strengthen fundamental rights.;Current legislation may have some gaps;;Yes;;Other;"We highlight 3 key recommendations:
1. Ensure precision and clarity around the definition of sectors. 
2. Remove “immaterial damage” from the definition of high-risk uses and, consistent with GDPR Art.22, focus on the legal and similarly significant effects of an AI system on people.
3. Remove the “exceptional instances” clause. vague exceptions risk eliminating legal certainty and causing unintended regulatory overreach.
For more details, see our comments. ";AI applications that risk direct physical injury or death.;4 - Important;3 - Neutral;4 - Important;4 - Important;4 - Important;4 - Important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);"Processing of biometric data for purposes of a biometric identification system should be optional and work on an opt-in basis. For example, on Facebook, our face recognition setting requires users to provide affirmative express consent. People who have the face recognition setting on can also turn it off at any time; when they do, we delete their face template and are no longer able to recognise them.";Rather not;We support an approach to AI governance that considers and leverages multiple instruments to achieve regulatory goals, including voluntary labelling systems. However, we do not believe it is proportionate to condition such labels for lower risk AI systems based on compliance with strict requirements suited to higher risk, as is proposed in the White Paper.;Other enforcement system;;The ex-ante self-assessment could be carried out via ADIAs as described above. When the ADIA process identifies residual high risks for which appropriate mitigations are not reasonably available or have not been identified, the organisation could engage in a prior consultation with the regulator. Any external audit requirement could be shifted to the ex post enforcement phase, reserved for exceptional cases. ;;Facebook seeks further clarity as to whether expanding existing safety regulation is appropriate or necessary to address any of the above risks, and whether and how AI implicates those risks. There is increased risk that any proposed expansion will create, rather than resolve, areas of legal uncertainty. ;No opinion;Without more in-depth consideration of whether existing regulation and risk assessment procedures are adequate, there is an increased risk that any proposed modifications will not provide clarity, but instead will create areas of legal uncertainty. E.g, AI developers could be subject to varying and inconsistent definitions of “AI,” “harm,” or “risk” arising from different risk assessment procedures and/or Member State enactments, resulting in additional uncertainty and potential overlap of rules;No opinion;Existing safety and product liability laws are currently drafted in a way that is technology neutral.  As such, it is unclear whether expanding existing regulation is appropriate or necessary to address potential challenges arising from AI, without more in-depth consideration to mitigate against legal uncertainty and increased risk that AI developers could face conflicting and concurrent liability under a patchwork of various regulatory regimes and local tort laws.;No opinion;;To the extent that the Commission proposes modification or expansion of existing liability frameworks to AI, Facebook questions the application of strict liability principles to AI systems that do not implicate abnormally dangerous activities or product defects resulting in physical injuries without further consideration of whether such systems warrant such an inflexible approach. Such an expansion would likely significantly impact innovation and investment in AI, and should be given careful con;FB_EC_AI_Response.pdf
F530351;14-06-2020 18:42;English;Company/Business organisation;Petra;Wilson;;Health Information Management Systems Society/Personal Connected Health Alliance;431651836022-33;Medium (< 250 employees);Belgium;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;"
•	A clear definition of AI should be adopted and regularly reviewed to ensure its up to date with the state of the art.
•	 It is paramount that TRUST in AI be developed through robust use of transparency and explainability of AI based solutions and services. 
•	Healthcare Professional Training should be included in the coverage of AI.
•	 Private partnership and public partnership are important for pilot project on AI to assess AI through new technologies.
";5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;;To achieve the proposed ecosystem of excellence, the EU MS could help by developing and providing guidance how to evaluate AI partnership opportunities. In parallel, they should create the right incentives to accelerate the adoption of AI solutions, promote the uptake of AI and put a strong focus on skills to fill competence/skill shortages.;5 - Very important;5 - Very important;5 - Very important;AI research and innovation community should associate with hospitals and industries to test AI solutions in real operational environments. Research is needed to develop high-quality datasets and environment for a wide variety of AI applications and to enable responsible access to good datasets, testing and training resources. ;4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;For AI to thrive, users must trust that their personal and sensitive data is protected and being handled appropriately. EU MS policy makers should assess whether there are gaps in existing regulation and seek to fill these gaps with sensitive legal guardrails that prevent harm and foster trust.;Current legislation may have some gaps;;Yes;;Yes;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;Any EU level legislation adopted in this area should be developed in close consultation with expert data scientists and subject to frequent formal review.;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;Risks of bias generally. In the medical field risk of bias that may in rare cases exclude a patient from treatment, as well as risk of a missed or false diagnosis.;Yes;;Yes;;Yes, for specific AI applications;in health care - all applications that can have a direct impact on patient care planning delivery.;;PCHA-HIMSS_White_Paper_on__EU_Data_Strategy_and_AI.pdf
F530350;14-06-2020 18:34;English;Business Association;Frédérique;FAGES;;Fédération Bancaire Française;09245221105-30;Medium (< 250 employees);France;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Feed national/EU AI ecosystems to promote medium/long term fundamental R&D. Ensure EU autonomy and sovereignty by building up own libraries of AI open source codes (EU standards). Develop transversal processes, such as natural language processing, shared datasets in languages other than English. Raise awareness and retain talents through incentives, public/private cooperation. Tax system facilitating investments made by banks in research. Give SME critical mass in data-enabled model.;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;Create reference datasets and benchmark industrial problems. Ensure better communication between industries and AI researchers. Significant investments in terms of research and development, methodologies to industrialize and implement. Regain EU independence on AI algorithms in the long terms facing large IT companies and excessive concentration of the market. Necessary AI pedagogy: positive communication, based on pedagogy and training, with public authorities and private actors.;4 - Important;5 - Very important;5 - Very important;"Creation of a harmonized EU research framework: at the national level, enhanced collaboration in the transverse fields, such as processing of the languages other than English; at EU level, a homogeneous framework and a level playing field for retaining talent and counterbalancing non-European players. An ecosystem: governance and steering structures; network of AI research centres; material infrastructure; legislative and regulatory framework and tax system without distorsion of competition.";4 - Important;4 - Important;5 - Very important;4 - Important;4 - Important;Innovation requires experimentation with new IT solutions and business models, without lowering regulatory standards (financial stability, consumer protection). Not in favour of Anglo-Saxon “sandbox” model (regulatory with prior authorization). For a fair and harmonized regulatory framework for banks and new entrants (level-playing field), avoiding any regulatory arbitration by supervisory authorities. No specialised innovation hubs, but open to all actors for use cases on eligibility criteria.;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;Risks often related to the service as such than to AI itself. Risk assessment criteria linked to applications and use cases, not to underlying technology. No real AI autonomy: bias linked to (involuntary) partial human judgment pre-existing at the adoption of AI. ?Specific processes for risks prevention and minimization: model monitoring, back testing exercises. Concerns not all relevant to the heavily regulated financial sector: risk management framework in place to ensure effective mitigation.;Other;"Relevant EU authorities to work with industry and society to clarify validation criteria, develop guidance on how to apply existing requirements to new AI use cases.
 Ensure regulators’ capabilities to provide oversight and supervision: mitigate potential risk, reduce uncertainty to foster innovation. Principle of technological neutrality to guarantee innovation. Compliance with GDPR and data minimisation principle. Banks’ regulatory obligations allowing to adapt risk model to implement AI.";Other;Risks with serious consequences only: physical/psychological risk, manipulation of opinion, democracy. No new rules needed for the banking sector which is not at high risk. AI management done through internal policies and governance. Guidance on AI issues produced collaboratively by competent authorities for all sectors to help apply obligations under different regulatory regimes. If a horizontal regulation, risk-based approach focusing on high-risk applications with technology neutral principle;;;"In any sector, any actor using AI with high risks or systemic risks must be required to act within an internal and external control framework, inspired by the best international standards. Banking regulation is already very comprehensive and could inspire all actors, particularly those who, without being banks, carry out ""financial"" or related activities. In any case, it seems not understandable that an entire sector such as finance can be considered at risk,  establishing a special status.";3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;No opinion;"The current legal framework seems sufficiently robust to grasp the main risks linked to the use of AI, especially from the consumer's point of view, and to avoid any risk of excess or slippage. ?However, it seems to us necessary to launch a EU debate: gradual approach proposed by Thierry Breton; call for in-depth analysis and political choices by French CNIL on facial recognition aiming at adapting data protection level to the new uses of technology (case-by-case basis, proportionality of the means deployed and special protection for children, consent collection and control  over data).";Rather not;"What is labelled – an application or all the activities of a firm? Even non-mandatory, labels are difficult to set up, likely to complicate development of scalable, self-learning and encapsulated AI: new burdens of certification, additional rules and procedures. Risk of discrimination if no label/ ""AI washing"". Create a supportive scheme: label, not for products, services or specific algorithms, but the ability to set up a up and comply with an appropriate “algorithm governance"" & risk control.";A combination of ex-ante compliance and ex-post enforcement mechanisms;;"Ex-ante conformity: ensure AI compliance with regulatory frameworks within a governance model; guarantee quality of training data and compliance with GDPR and other laws. Respect of model-risk management procedures in compliance with ECB and FED requirements. Ex-post enforcement mechanisms: any prior conformity assessment should be without prejudice to monitoring compliance and ex post enforcement by competent national authorities for all AI applications. Global bank control.";Cyber risks;;No opinion;Bank sector aware of the consideration to be given to risk assessment procedure for services, undergoing significant changes during lifetime. However, our AI services should not be submitted to new horizontal legislative framework for security. Sector extremely supervised:  consumers, national banking and liability laws, civils rights, GDPR, and originally driven by permanent risk control governance to ensure the safety of the services. Strong cooperation with Authorities.;No opinion;AI services provided by the Bank sector should not be submitted to a new horizontal legislative framework for liability, as the sector is subject to multiples levels of liability regimes: GDPR, national civil liability laws, all specific banking liability rules as for instance the consumers protection framework. Multiple current legal regimes of responsibility already cover the issues of responsibility in terms of AI services. Therefore, concerns should focus on sectors not yet covered.;No;;"We consider that the national liability rules (in France) are suitable and sufficient for the operation of AI to ensure proper compensation for damage and a fair allocation of liability. However, if changes were to be considered, it could only be part of a European framework.  
";FBF-AI_Position-Paper_for-release_20200614.pdf
F530349;14-06-2020 18:11;English;Business Association;Hugues;Ribiere;;Fédération Française de l'Assurance (FFA - French Insurance Federation);5149794935-37;Medium (< 250 employees);France;The feedback can be published with your personal information;5 - Very important;3 - Neutral;5 - Very important;3 - Neutral;4 - Important;3 - Neutral;For insurers, the environmental impact of artificial intelligence and the standardization of data should be considered to build a European ecosystem of excellence that promotes green AI and to fuel AI in various contexts and industries.;3 - Neutral;4 - Important;4 - Important;3 - Neutral;5 - Very important;5 - Very important;Within the framework of the Coordinated Plan revision, members States should align policies and strengthen coordination on the development of global labels or certifications in the level of data protection or level of privacy protection, etc. To go further the European data space build, it would be appropriate and to set up a data register allowing annotations by the users in order to facilitate the detection and understanding of these data.;3 - Neutral;4 - Important;5 - Very important;The EU should have a more ambitious approach to AI research and be able to retain or attract experts in this field. It could be useful to learn from the practices of other countries, to further develop European AI laboratories, etc.;2 - Not important;4 - Important;4 - Important;5 - Very important;3 - Neutral;DIHs may promote and focus on use cases (being customer minded and business oriented to help move faster from R&D to practical and industrialized applications), promote customer centric and ROI focus and acculturate employees to demystify, understand and evaluate the valuable contributions of AI. Furthermore, to develop a European sovereignty in terms of artificial intelligence, specialised Digital Innovation Hubs should facilitate the creation of shared European AI models. ;3 - Neutral;4 - Important;4 - Important;3 - Neutral;3 - Neutral;4 - Important;Insurers point out as important the following concerns about AI: responsibility, acculturation, need for transparency throughout the entire AI manufacturing chain, data, human factor, bias, not to promote the AI singularity (cf. Attached annex for further details).;Other;The current EU legislation allows to respond to many issues related to the use of AI, but it is difficult to predict its relevance on future cases of AI at the present time. For French insurers, some clarification or in some rare cases an extension of a current legislation might give a better framework to implement and operate safely AI, keeping in mind that double regulation should be avoided whenever possible. (cf Annex);Other;French insurers agree that the notion of “high risk” needs to be defined beforehand to assess whether these AI systems should be subject to new rules. To date, it is not obvious that there is a need to introduce such rules. An entire sector such as finance should not be considered at risk:  this would wrongly establish a special status for all models in the same sector. (cf Annex);;;For insurers, all the applications that can interfere with our freedoms and may have an impact on human life or health are the most likely to be qualified as by “high risk” AI applications.;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);"Those use cases have not to be prohibited but supervised in their use, especially in the collect of data. The right balance must indeed be found between individual freedom and the common good. The regulation must include an obligation to provide information on the areas under surveillance. Consent must remain mandatory except in imperative situations such as the fight against terrorism regulated by ad hoc legal disposition. According to our members, there is today a problem linked to the request for consumer consent. A legal basis for data processing is the consent of the involved user. However, this request for consent is often very vague as to the primary and secondary purposes for which the data could be used (e.g.: the resale of this data).  
Most often, a form of pressure is carried out on the consumers: either they give access to their personal data, or they benefit from a degraded user experience being only entitled to very reduced products functionalities less suitable offers due to the lack of knowledge of the customer’s profile.  
The EU must establish strict rules on this topic, otherwise the request for consent would be meaningless.";Much;"The voluntary labelling system may include a normalized description method of AI systems under control of a European independent body to allow a clear and high-speed information of natural persons or citizens.  
Labelling the presence of a “shutdown button” to disconnect an AI system at any time, whatever its level of risk, could also be interesting and clearer for the user.";A combination of ex-ante compliance and ex-post enforcement mechanisms;;For instance: Ex-ante compliance to guarantee ethic AI, anticipate ex-ante issues in the development of AI and establish clear frameworks for liability in the case of ex-post harm done by AI systems. Expost enforcement mechanisms to minimize involuntary effects developed during AI lifetime. ;Personal security risks;No;Yes;"The current applicable EU legislations on product safety only cover products at the time they are placed on the market. However, AI systems have the particularity to evolve over time.  
Thus, it seems necessary to adapt existing procedures to potential changes in order to comply with the safety principle. According to FFA's members an adapted governance could be relevant as it will evolve along the AI system lifetime. ";No;In our experience, current liability framework is fit for purpose. It implements a well-balanced system of liability, providing a high level of protection to consumers while taking into account manufacturers’ legitimate interests, thereby encouraging economic growth and technological innovation. Any change to this balance could negatively impact both cost and availability of insurance. Guidance documents on key definitions may prove useful, addressing certain perceived AI-risks.  ;No;;"According to French insurers, the current liability framework is fit for purpose. FFA believes that the EU should remain within this dynamic of re-using existing rules so as not to hinder the development of new technologies while guaranteeing the users’ protection and the legal certainty. 
However, rules on liability could be studied at a future stage with regard to new challenges and technological developments especially under international law or in the event of a conflict of laws.";2020_06_14_FFA_position_paper_on_the_EC_White_Paper_on_AI_and_Annex_of_the_consultation_response.pdf
F530348;14-06-2020 18:11;English;Business Association;Maria;BACKLUND;;BIL Sweden;;Small (< 50 employees);Sweden;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;"- Investment in and promotion of key technologies in the field of ""trustworthy"" AI are welcomed.
- A strong collaboration between large European companies and public sector should be considered, as this could foster the uptake of AI in Europe.
- Looking a";5 - Very important;4 - Important;4 - Important;3 - Neutral;5 - Very important;4 - Important;#NAME?;3 - Neutral;5 - Very important;4 - Important;"- Set up large scale EU projects that will mobilize industries' efforts.
- A wide minimum knowledge on AI should be ensured to increase trust in AI technologies.";4 - Important;4 - Important;4 - Important;4 - Important;No opinion;- While it is important to support the development of AI expertise for SMEs, there is a need to clarify what is meant by knowledge transfer.;4 - Important;4 - Important;4 - Important;4 - Important;3 - Neutral;3 - Neutral;"- These concerns shouldn't be underestimated and therefore it is highly important to identify how those areas of concerns are already regulated.
- Any AI outcome must be legally compliant. AI has to be so accurate that applicable laws and traffic rules ar";Other;"Current legislation may have some gaps that need to be addressed, but since technology is emerging fast any new legislation need to be principal-based and technology-neutral.
To ensure that an AI framework doesn't duplicate/invalidate existing certification requirements/regulatory frameworks, it should be carefully considered where these and industry standards are better instruments to address gaps.";Yes;;Other;"From the white paper it can be interpreted as autonomous vehicles are per se defined as ""high-risk"" AI. The high-risk sector approach and the classification of transport sector as high-risk are inadequate, this may hamper AI development and innovation in the sector, thus exclude many of the potential benifits of AI; variety of AI applications within transport is too wide.
A clear approach to risk assessment should identify all high-risk cases without the need to list high-risk sectors or areas.";One suggestion for identifying high-risk AI would be to focus on learning rational AI systems (self-learning systems), with possible disproportionate impact on humans and/or the environment. High risk AI should be limited to learning rational AI systems since those systems fall outside the scope of existing regulations like the product safety or product liability directives. Rational AI systems should fall out of the high-risk definition and the scope of new compulsory requirements.;3 - Neutral;4 - Important;3 - Neutral;4 - Important;2 - Not important;4 - Important;Other special requirements in addition to those mentioned in the question above should be imposed (please specify);"Biometric identification systems are covered by the General Data Protection Regulation (GDPR) and the processing of such data for uniquely identifying purposes is forbidden pursuant to Article 9(1) of GDPR. Facial recognition can only take place if it falls under the scope of one of the exemptions listed in the article.
Biometric identification systems, ex face recognition, may be needed in autonomous vehicles for safety reasons by determining the intention of road users as pedestrians and other drivers.";Not at all;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;"Assessment of compliance should be carried out ex-ante, preferably through third party assessment and approval. The existing third party type approval system in automotive industry works well and could be applied here.
Care should be taken to ensure the process is feasible, especially in terms of documentation requirements,so as to avoid discouraging innovation or putting an undue burden on industry.";;In the automotive sector, cyber security and connectivity loss will be tackled in other regulations.;Yes;"Carrying out a new risk assessment should only be required when there has been a significant change to the functionality of the product.
In Automotive sector, when OEMs introduce new SW and HW in vehicles on the field, those should undergo the procedures of Software Updates (or new approval if considerable changes that affects performance covered by current approval). Third party SW or HW changes affecting safety functions must be prohibited.";No;The current PLD remains fit for purpose, being both effective and technology neutral. The PLD provides both legal certainty and compensation to consumers.;No;;"The current liability regime is based on a fair distribution. It is important that liability continues to be adequately allocated and that national law is governed/influenced by European law. There is neither any reason to have specific compensation policy as long as the system meets its requirements.
A third party that upgrade or make changes of a product or service after it has been placed on the market need to have strict liability of the product or service they change.";
F530347;14-06-2020 18:10;English;Other;Olga;Hamama;;V29 Legal - Duve Hamama Rechtsanwälte PartG mbB;;Micro (< 10 employees);Germany;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;"A strong emphasis with regards to research and financing should be placed specifically on explainable AI as one of the key aspects of trustworthy AI.

In addition, initiatives should be aligned with the Sustainable Development Goals as well as the Green Deal of the EU. “AI for Good” should play a key role in such areas as, for example, tackling of climate change and fight against hunger.";5 - Very important;5 - Very important;5 - Very important;Supporting interdisciplinary initiatives to foster development of market ready technologies and accelerate European development of AI technologies.;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;In addition to specific risks, the overall impact of AI on society, eg increased unemployment, related decrease of human input and value, over-reliance on technology and decline of cognitive abilities of humans.;Other;V29 Legal believes that a new framework for AI needs to be established at EU level to harmonize certain principles for the use of AI (e.g. in relation to transparency and monitoring) and set European standards with regards to ethics and trustworthiness of the AI systems. At the same time, certain sector-specific rules, depending on use cases and related risks, will have to be adapted as well.;No;;;;The high-risk examples cited so far in the White Paper tie in particular to the fundamental rights relevance of the respective application. V29 Legal supports such a risk assessment, but also advocates that applications that can lead to particularly serious macro-economic consequences - such as a stock market crash – or systematic risks should be classified as high-risk applications.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);"V29 Legal considers that as a basic principle, biometric identification systems should only be allowed when exceptionally required by law and only insofar as the principle of proportionality is observed. 
Third-party performance tests should be required by law as well as disclosure of information on potential error rates and data safety standards.";Much;V29 Legal considers that voluntary labelling may be an option at the lowest level of intervention. Labelling/monitoring as such can be performed by third parties based on criteria defined at EU level, e.g. like confirmation of compliance with accounting principles by independent private players – accounting firms;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Compliance and enforcement mechanisms should also be subject to a gradual approach in five stages depending on the intensity of the intervention and related risks. At the lower risk levels an inspection by independent third parties can be carried out, but from the third risk level onwards, additional information and inspection rights should be granted by the state. From the fourth level of intervention onwards, ex-ante authorization controls could be considered.;Mental health risks;;Yes;V29 Legal suggests risk assessment procedures take into account the possibility for an AI system to evolve over time. Given that new vulnerabilities might arise, risk assessment procedures will need to be repeated once a product is already placed on the market. ;Yes;V29 Legal submits that a liability regime based on but regulated separately from the Product Liability Directive should be created on EU level specifically for AI-related products to better reflect the peculiarities of such products.;No opinion;;;V29_Legal_s_Contribution_Paper.pdf
F530346;14-06-2020 17:49;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;;4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;;5 - Very important;4 - Important;4 - Important;;4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;Yes;;Yes;;;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F530345;14-06-2020 17:42;English;;;;;;;;;The feedback can be published in an anonymous way;No opinion;No opinion;5 - Very important;5 - Very important;1 - Not important at all;1 - Not important at all;AI should not just be promoted for the sake of it - use cases should be carefully considered to assess their ethical and other risks. The EU should include actions around strengthening democratic oversight, transparency, ethical governance, human rights safeguards, and risk assessment for the use of AI.;;;1 - Not important at all;;;;The coordinated plan on AI should include a section on ethical risks, human rights, transparency and democratic oversight by citizens.;No opinion;5 - Very important;1 - Not important at all;Research and innovation in this area should be governed by a strong ethics code and framework to ensure that it fully respects fundamental rights, including clear ethical reporting and transparency standards in this area.;;;;;;Given the high ethical and privacy risks posed by misuse of AI, SMEs should receive a high level of support in this area to ensure that all AI activity is legal and respects fundamental rights.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;Other;It is important that existing legislation (e.g. GDPR) is not undermined by new AI regulation. In addition, as existing frameworks (e.g. around fundamental rights) were developed before the potential of AI was understood, there is a risk they will fall short in ways we cannot predict. It is important that there is ongoing rigorous and transparent discussion about AI ethics and use.;Other;I am concerned that classifying some AI systems as 'low risk' may serve to undermine scrutiny and oversight before the impact of such systems can be fully understood. All AI applications should be understood within a rights-based framework.;;;Facial recognition and associated technologies, e.g identification of emotions;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;Risks to fundamental rights including the right to privacy, non-discrimination, data protection, freedom of assembly, and other rights.;Rather not;Voluntary labelling may be misused. As above, the use of a 'low risk' designation may also  introduce loopholes which allow applications with an impact on fundamental rights to be subject to lower levels of scrutiny. ;Other enforcement system;"Independent ex ante assessment of potential ethical and fundamental rights risks; transparency and reporting requirements.";Compliance mechanisms should be independent, rather than relying on self-reporting by providers;Mental health risks;Risks of discrimination by AI systems against certain groups of citizens;Yes;;Yes;;Yes, for all AI applications;;;
F530344;14-06-2020 17:40;Dutch;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;4 - Important;;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;4 - Important;4 - Important;4 - Important;No opinion;3 - Neutral;3 - Neutral;;Current legislation may have some gaps;;Yes;;No opinion;;;No opinion;4 - Important;No opinion;No opinion;3 - Neutral;4 - Important;No opinion;;No opinion;;No opinion;;;Cyber risks;;Yes;;Yes;;Yes, for all AI applications;;;
F530343;14-06-2020 17:26;Italian;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;4 - Important;5 - Very important;3 - Neutral;5 - Very important;4 - Important;integrated approach to AI, BigData and HPC, one without the other is much less effective;4 - Important;3 - Neutral;4 - Important;4 - Important;5 - Very important;5 - Very important;;3 - Neutral;4 - Important;5 - Very important;AI should be more and more combined with simulation of accurate models, which need to be boosted as well, AI alone will not be accurate enough in the long run, Where there is an accurate model, AI should aim ad improving it, not replacing it.;4 - Important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;;3 - Neutral;5 - Very important;4 - Important;3 - Neutral;4 - Important;2 - Not important;;There is a need for a new legislation;;Yes;;Yes;;;4 - Important;4 - Important;5 - Very important;4 - Important;3 - Neutral;4 - Important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);I suggest to have an alert system (for threats)  for each public space, with different level, e.g. green, Yellow, red. Above a certain threshold (to be defined) identification should be allowed.;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;create a market of certification bodies (public and private) to certify the assesment.;Personal security risks;;No opinion;"define bwtter ""important chenage"", tuning parameters is as impotant as changing the topology? New assesmet should be needed when the product is proposed for new application (e.g. from turcks to cars).";Yes;;Yes, for specific AI applications;;uniformity avross Europe is of fundamental importance, in order to avoid competition between member states, with the risk to lower safety.;
F530342;14-06-2020 17:12;English;Other;Prakriti;Pathania;;Arthur's Legal B.V.;510448420031-72;Micro (< 10 employees);Netherlands;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;There is a need for a new legislation;;No;;;;;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Other special requirements in addition to those mentioned in the question above should be imposed (please specify);;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;Yes;Arthur's Legal has participated in the consultation that was launched by the European Commission and submitted a detailed report on potential gaps in the Product Liability Directive in light of the technological advancements that have been made. Accordingly, a list of detailed recommendations were also submitted, ;Yes, for all AI applications;;;
F530341;14-06-2020 17:09;English;NGO (Non-governmental organisation);Carlotta;Besozzi;;Civil Society Europe;520775919740-63;Micro (< 10 employees);Belgium;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;3 - Neutral;1 - Not important at all;1 - Not important at all;We believe that it is fundamental to addressing the fundamental rights impact of AI and to ensure partnership with civil society organisations in the governance of AI.;4 - Important;4 - Important;1 - Not important at all;3 - Neutral;5 - Very important;4 - Important;Fundamental rights including civic rights, societal impacts of AI/ automation, ensuring democratic oversight through a governance mechanism with strong involvement of civil society organisations. AI not to be promoted in the public sector without a careful assessment of its consequences in areas such as e-governance, justice, but also in services used daily by citizens such as transport, telecoms. European data space to comply with GDPR.  Fundamental rights guidelines in public procurement of AI;4 - Important;4 - Important;1 - Not important at all;"it is fundamental to develop Research on the human, ethical and fundamental rights impact of the development and use of AI, to be monitored through AI independent centres of expertise, to ensure participation of civil society organisations in the governance of AI including by providing skills building on AI.Horizon 2020 and Horizon Europe to implement ethical principles in the definition of research priorities and allocation of funding.
";3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;Fundamental rights impact assessment & GDPR compliance must be also considered as priority;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Human oversight over work processes in AI systems must be ensured.Humans interacting with AI systems must be able to keep full and effective self determination/autonomy, and be able to take part in the democratic process.   AI systems must not create asymmetries of power or information, must not endanger either people, their fundamental rights or civic space.AI used in public services without democratic oversight, transparency or evidence based with opaque private technologies is concerning.;There is a need for a new legislation;;No;;;;The use of AI to determine delivery of essential public services, predictive policing identification/ analysis of emotion and identity traits, and indiscriminate biometric surveillance, are incompatible with fundamental rights and civic space and should be banned . Determining ‘risk’ should be rights and outcomes focused, not sectoral. Major concern with systems which impact fair trial, in migration control and policing, and systems which may lead to discrimination in employment.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;Considering the high risk of abuse, discrimination and violation of fundamental rights to privacy and data protection, as well as freedom of expression and assembly, such systems should not be allowed.;Not at all;"Clear rules and enforcement mechanisms must be put in place and cannot be replaced by voluntary labelling especially in a context where understanding by the public is limited and with issues linked to an evolving machine learning system. 

";Other enforcement system;Ensure compliance also through public procurement legislation. See EU cities that have developed trustworthy AI principles. Also for EU institutions procurement.;"Fundamental rights compliances must be ensured at all stages.Any assessment, audit, certification, market surveillance activities have to cover the evolving nature of the AI system. For this, access to the AI system 
algorithms, codes and data sets must be ensured to understand and assess the risks.

";Mental health risks;"Risk to freedom of assembly, expression and interference with democracy
Risks of discrimination
Risks related to minorities such as persons with disabilities
";Yes;evolving nature of AI systems must be addressed in  risk assessment procedures through continuous assessment;Yes;As the EC assessment of product safety and liability legislation shows, there are gaps in present legislation and new AI related aspects require new legal provisions, especially for enforcement purposes.;Yes, for all AI applications;;AI applications are covered both by copyright and database rights protections, which prevent users from assessing their quality and limit their ability to redress issues that have been observed. Openness should be ensured.;
F530340;14-06-2020 17:08;English;Other;Nikolaos;Thomopoulos;;"Cost Action 16222 ""Wider Impacts of Autonomous and Connected Transport"" (WISE-ACT)  which is a network of more than 150 experts with multi-disciplinary background in 41 countries representing academics and practitioners.";;Medium (< 250 employees);United Kingdom;The feedback can be published with your personal information;4 - Important;4 - Important;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;"In our view, there should be wider public deliberation about desired futures with AI. Informing and educating not only experts and large businesses, but also SMEs and the wider public is essential. Past WISE-ACT activities such as the ""Idea Jam 2019"" or the ""2019 Autonomous and Connected Transport Training & Education"" workshop have contributed in this respect, so it would be useful to organise more such activities at a European, national, regional level (e.g. Survey, Glossary, Focus Groups).";5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;Central to ideas of Digital Single Market is the emphasis on the public good and competitive value of safeguarding certain, particularly critical, data as open access resources. This initiative is founded upon the recognition that certain data have particularly high value, in that they may function as infrastructural resources for other (data-based) activities. Additionally, the creation of regulatory sandboxes in order to allow the testing of AI systems is crucial for various sectors.;5 - Very important;4 - Important;4 - Important;It is crucial for the EU to build up top level research skills and capabilities from all disciplines, including social sciences.The R&I community needs to be supported both via existing and new initiatives/infrastructure to develop AI relevant research capabilities e.g. testing/experimentation sites such as Living Labs and sandboxes. The private sector, particularly SMEs and extra-EU R&I collaborations are valuable. EU should encourage public deliberation and abiding to RRI principles. ;3 - Neutral;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;"SMEs form the backbone of the EU economy, so they should be supported to join the AI evolution, particularly in light of COVID-19 and the Green Deal. Since data is the currency of the 21st century (Floridi 2012, Thomopoulos et al 2015; Hogan et al 2019) and SMEs possess a lot of expertise in their sectors (e.g. about privacy), they should be invited to enhance equity (e.g. gender, age) and decrease the digital divide. ";4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;The impact of AI and digitisation on employment and the workforce is of very high importance as also highlighted in the WISE-ACT 2019 workshop on “Autonomous and Connected Transport Training & Education”. Changing or degrading job roles (e.g. into a mere “AI helper” completing less meaningful tasks) are important especially for SME/white-collar jobs, but more important are job losses particularly in the short-term. So fundamental rights of today and tomorrow should be the top EU priority.;There is a need for a new legislation;;No;;;;The use of AI in transport as outlined in this WISE-ACT inter-continental mapping review (https://www.sciencedirect.com/science/article/pii/S2543000920300081).AI and Big Data offer capabilities to re-identify originally anonymous data,challenging key GDPR rights e.g.Right to be forgotten in an accident fatality. Therefore, anonymised data should be the foundation of data used to train AI algorithms according to EU regulations.Also:What will be the legal status of this AI based Artificial Agents?;5 - Very important;5 - Very important;4 - Important;5 - Very important;3 - Neutral;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;"5D: Since data are invaluable for the EU economy(Hogan etal 2019, Thomopoulos et al 2015)and we should avoid another social media failure(e.g.2016 US elections)before regulating AI, accuracy needs to be high before AI introduction.This is particularly relevant to transport(e.g.AVs)since most people,especially their children,would not fly if it was not 100% accurate/reliable.
Publicly accessible spaces should be perceived as “common public/private spaces”: https://euobserver.com/coronavirus/148387
";Rather not;Non-High-Risk self-assessment is difficult to enforce and to maintain as proven in other industries to date and there is no need for another Black Swan (Taleb,2010) before taking action. Existing legislation does not cover a lot of existing/emerging AI activities, so more is needed. Open AI could offer safeguards,but limited public deliberation, hence questionable benefits.Algorithmic auditing (AV Ethics Report)could be helpful if coupled with effective monitoring and enforcement EU Regulations.;Other enforcement system;Both ex-ante and ex-post evaluation are essential and should be used in tandem. Blockchain (DLT) may also assist and its use could be at European or national level, co-ordinating activities and regulations internationally. Developing common standards at inter/national level is essential as ICT standards have already proven(e.g. ETSI, SAE-J…).Application and enforcement should be Transparent, Accountable, Explainable, adhering to clearly defined ethical, privacy,social principles.;Assessment of compliance should acknowledge the diversity of AI applications and offer options to include all stakeholders. Incorporating (AI) ethics (e.g. van Wee, 2011), developing a composite indicator for transport (e.g. SUMINI - Thomopoulos and Grant-Muller, 2013) or for MaaS more specifically (Pangbourne et al, 2019) would increase both ex-ante and ex-post compliance. Living Labs, Sanboxes and trials (e.g. for AVs) should be essential components of any compliance assessment.;Personal security risks;Personal risks are slightly more important given the low level of awareness of the general public about AI benefits/risks. So the public should be informed/educated, building up research capabilities for EU researchers and providing opportunities for SMEs and large corporations in Europe to compete at a global level. Incorporating the user perspective may be a unique opportunity for the European Commission, member states and European businesses to differentiate themselves from global competitors;Yes;Yes, this is important particularly due to the constantly change in meanings and values, which constitutes itself an emerging area of research with future implications for AI business. Risk Assessment procedures should include safety, Privacy and Ethics by Design principles, avoiding discrimination and reducing the digital divide e.g. reviewing/updating MFA/2FA practice is an existing case raising concerns for diverse age groups and device users, which in turn could be an AI business opportunity;Yes;Liability in the AI context is a major concern and AVs are a sector significantly affected, including various stakeholders such as insurance companies. Numerous actors are involved in the development and delivery of AI applications (e.g. developer, data provider, algorithm, trainer, user, regulator/authority), therefore legislation needs to be reviewed and amended reflecting emerging needs via an appropriate mechanism/platform.;Yes, for all AI applications;;This is crucial, particularly regarding cross-border EU markets and services learning from e.g. mobile phone services as highlighted in this book chapter: https://www.sciencedirect.com/science/article/pii/S2543000920300081 and the relevant WISE-ACT Data Reports. For more details and the WISE-ACT perspective, please visit: https://www.wise-act.eu/reports-publications;GDPReraAV2020.pdf
F530339;14-06-2020 16:45;English;Academic/Research Institution;Niek;Brunsveld;;"""Amsterdam:  AI Technology for People”. We are a consortium of the knowledge institutes in Amsterdam and the City of Amsterdam. Together we form the largest science and innovation ecosystem in the Netherlands.";Unive7618063212;Large (250 or more);Netherlands;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;The Amsterdam AI consortium fully supports the six actions proposed under section 4. but points out that there is a hugely disproportionate imbalance between sections 4 and 5. This limited focus on AI excellence we also see in section 3 where the role of low-power electronics and quantum computing get far more attention than the excellence in algorithms, the core of modern AI. Also, it is vital to allocate much more attention to skills/skilled staff so as to help attract, train and retain talent;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;The Consortium welcomes the development through the European data strategy and the investment through the Digital Europe Programme to support high-performance and quantum computing. However, there is a tangible need to coordinate policies on the further development of AI-relevant infrastructure such as a compute infrastructure with clusters of many GPU enabled nodes, a data exchange infrastructure to facilitate secure and fair sharing of data and a local knowledge sharing infrastructure.;3 - Neutral;5 - Very important;5 - Very important;Key focus in this section should be the lack of talent and on connecting existing AI research excellence centers, emphasizing each center’s key strengths (e.g. in Amsterdam: Deep learning, hybrid intelligence, and  responsible AI), creating synergy through network events and exchange programs for talents at all levels, and jointly address the major challenges the world is facing by connecting the excellence in various fields of AI, each taking advantage of their own local ecosystem. ;4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;In identifying national Digital Innovation Hubs, the EC should take into account the hub’s surrounding business environment which needs to consist of regional, international, SMEs, as well as knowledge and other AI-related institutes which have a close connection to AI excellence research centres. Indeed, the A'dam region builds on 3 decades of AI-research and offers an ideal environment here, in particular for the service industry, given its long tradition of public-private partnerships. ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;The Amsterdam IA consortium argues that the fact that technology can result in concerns does not mean we should not develop it, but that we need to find ways of taking away these concerns, either through better technologies, or new legal safeguards. The consortium, therefore, reiterates the imbalance between sections 4 (ecosystem of excellence) and 5 (ecosystem of trust). The consortium argues for at least as much focus on section 4 as section 5 for Europe to truly become a world leader in AI.;There is a need for a new legislation;;Yes;;Yes;;A particularly high-risk concern associated with AI is that of function creep, the gradual widening of the use of a technology or system beyond the purpose for which it was originally intended. The risks are in the infringement, not the technology. To provide legal certainty and a focused approach, a definition of high risk is required that matches existing definitions of risk in terms of severity and likelihood.  ;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;Creating permanent surveillance infrastructures is at odds with a commitment to strong fundamental rights, such as the right to privacy or to assembly, and invites abuse and function creep.;Rather not;As experience from e.g. the field of food labeling shows, a labeling system to be effective requires considerable investment in communication, standardization and consistency.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Combining ex-ante compliance and ex-post enforcement mechanisms for high risk applications allows innovation to thrive -in a trusted manner- and AI to be deployed in a timely way (without undue delay due to burdensome compliance procedures). At the early stages of AI development, it is often difficult to foresee the end result, and, thus, it is necessary to allow for secure piloting of AI applications before any conformity assessment, limiting EU researchers and SMEs to innovate and invest.;Mental health risks;Effective enforcement of product safety in complex product chains.;Yes;Many of the existing provisions depart from the idea of a ready-made product. As explained above, AI applications can be subject to changes also after the product or application have been deployed. More flexible and re-occuring risk assessment procedures should cater to that fact.;Yes;As this is a fast moving technology with continuously new applications, we believe that what is needed is continuous monitoring of the existing rules and their application to AI application, to be able to ascertain that the current frameworks still hold. One way of doing so, is by finding shorter and more agile development cycles, also for the law.;Yes, for all AI applications;;As this is a fast moving technology with continuously new applications, we believe that what is needed is continuous monitoring of the existing rules and their application to AI application, to be able to ascertain that the current frameworks still hold. One way of doing so, is by finding shorter and more agile development cycles, also for the law.;Amsterdam_AI_Technology_for_People.pdf
F530338;14-06-2020 16:37;German;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;4 - Important;5 - Very important;4 - Important;3 - Neutral;5 - Very important;;4 - Important;4 - Important;4 - Important;No opinion;4 - Important;No opinion;;2 - Not important;3 - Neutral;4 - Important;;5 - Very important;4 - Important;5 - Very important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;loss of control;No opinion;;Yes;;No;;;4 - Important;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;No opinion;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Mental health risks;;Yes;;No opinion;;No opinion;;;
F530337;14-06-2020 16:32;English;Academic/Research Institution;pierre;pozzi belforti;;Worldstone Ventures SARL;;Small (< 50 employees);Belgium;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;First, establish an EU global vision and strategy on AI - then develop an Action Plan.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;All the above urgently needs an overall framework with a vision and a related regulatory framework, to focus on the targets and avoid dilution of ressources in uncoordinated or unwanted outcomes.;5 - Very important;5 - Very important;5 - Very important;"The strategy and its implementation is to be ""framed"" top-down, so that bottom-up efforts and ressources are ""streamlined"". An EU central AI Authority should be set up.";4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"Criteria for these Hubs should be clear, simple and very selective, to aim to the highest outcome possible from invested ressources. We should chose as an example the ""best-in-class"" from other countries that are very far ahead than the EU on AI working efforts (China, USA), and aim reaching the same ambitions of AI independence and sovereignty within the framework of our own moral values.";5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;With Machine Learning growing, Human Brain and Machine interface (ex: Neuralink.com) becoming reality, advanced Deep Learning, and one day Quantum Computing (5 to 10 years), there is a severe risk that Mankind might lose its supremacy over computers. The EU should take the lead given its already existing record in laying the foundations for protection of fundamental human rights and always striving to develop, innovate and lead in this specific area.;Other;"GDPR has been the first regulation on Privacy applied e.a. to social platforms in the EU and well beyond, but massive data harvesting is still pursued by big tech actors for short-term gains. Current legislation on AI is nascent and has been long overtaken by the speed of new developments in AI.  With AI the game is much more ""letal"" and threatens Ultimate Mankind Sovereignty if not properly and timely regulated by the EU.";No;;;;"ALL applications of AI in which Mankind, Privacy, and democratic freedom can ultimately be put at risk by misuse, ill-fated use, intentional manipulations. The ""selection"" should be between (a) the principle of protecting and ALWAYS ENSURING the ""TRIO of Ultimate Sovereignty of Mankind, of Democratic values, and of Personal freedom""  and (b) AI being neutral or not thereatening the preceding TRIO principle. It's a deep, long-term vision of moral and ethical values that only the EU can defend.";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.; Some major tech companies like IBM, Amazon, etc have just suspended providing Face Rec databases to police in the US, but it's a unilateral decision due to other reasons. The reality is that the current prohibition is not fully respected by each EU country: national governments have conflicting views, in some countries local authorities adopt Face Rec with no understanding of implications, and private companies like e.g. retailers adopt it as well. Most Smartphone apply Face Rec, social media collect million of faces, and none are not stopped by authorities. ;Not at all;This solution is not credible. When there are huge economic gains in view, the industry players will try to protect themselves with self-regulation. We see it with social media platforms. Not much has happened in spite of all scandals and leakages of databases of private information. Facebook, Clearview, and tens of companies have been hacked and billions of Face images have been stolen, but the technology is still been pushed forward by its sponsors through new features and improvements.;Other enforcement system;Independent EU central Audit and Compliance Authority on AI. Just like with companies listed on Euronext, AI suppliers and users should be audited annually, to ensure respect of law and be held responsible for breaches. Company Annual Reports should include a chapter on AI transparency and governance, just like the information provided with Tax and CSR regulations.;Like with EMA for medicines and EU Patent Office for IP registration, AI should be screened and approved BEFORE being marketed. It should comply with regulations to be adopted by the EU. Please read above on Audit and Transparency.;Personal security risks;Main risks are identifiable as either a software problem, an intranet problem, a mobile connection problem, a device/support problem, or a combination of some/all. These can be detected by Court experts after the event. Penalties and compensations can/will be defined by law and jurisprudence, and can ultimately be insured on the private market. Where legal certainty needs to be defined ASAP is the risk of computers over-riding human control. This needs to be banned by law, with heavy penalties.;Yes;Every 3 years there should be a full review of new or modified/enhanced AI applications, as software upgrades take place several times per year, as much as algorithms are modified every day or week. Regulations need to be adapted and/or updated fast.;Yes;The EU should promote a private sector insurance market that covers the risks in this huge and fast-expanding industry, with appropriate rules, regulations, and independent verifications.;No;;National liabilities make no sense anymore. Ex: I have a car accident with my BMW whilst on self-driving mode, on a road in Italy. The car is registered and insured in Belgium, it's a German brand, the embarked AI used by BMW for self-drive is French, the BMW embarked mobile hardware is Finnish, the country in which the accident takes place is Italy, the injured person is Estonian, and I am ensured with a Belgian insurance company...This is clearly an EU matter.;Recent_media_interviews_on_Europe__AI_and_Indentity_2020.pdf
F530336;14-06-2020 16:32;French;Business Association;Anne-Claire;Le Bodic;;REIF - Représentation française des institutions de sécurité sociale auprès de l'Union européenne;493485518698-13;Micro (< 10 employees);France;The feedback can be published with your personal information;5 - Very important;4 - Important;5 - Very important;;4 - Important;5 - Very important;;;;;;;;;;;;;;;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;Other;REIF believes that a strong, compulsory framework should be applicable to all sectors. Additional measures should be considered for high-risk sectors, such as social security.;;;;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;;;Not at all;;;;;;;;;Yes;;;;;Position_REIF-_Donn_es_et_IA_-_Mai_2020_-_final.pdf
F530335;14-06-2020 16:31;English;Business Association;Angela;Gleason;;American Property Casualty Insurance Association;;Medium (< 250 employees);United States;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;20-06-14_-_APCIA_Comments_-_European_Commission_AI_White_Paper.pdf
F530334;14-06-2020 16:18;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;The availability of data and access to data to train AI models and run AI applications is a prerequisite. The existing legal framework (e.g. GDPR) should be assessed thoroughly for restrictions and barriers.;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;Cybersecurity for AI;Current legislation may have some gaps;;Yes;;Yes;;;5 - Very important;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;Other special requirements in addition to those mentioned in the question above should be imposed (please specify);"A clarification of what is considered a “public space” would be needed. Facial recognition applications may for example be used by vehicles (e.g. to unlock the car, to monitor driver awareness/drowsiness, etc.). So the question arises in which cases a vehicle might be considered a ""public space"" (public transport vehicles likes buses, private vehicles, company cars, carpooling, taxis).";No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Cyber risks;"It should be assessed whether an obligation to comply with cybersecurity requirements could become part of liability regimes. If, for example, a product manufacturer can prove having applied ""state of the art"" security hard- and software, liability could be reduced.";No opinion;;No opinion;;No opinion;;The current EU legislation on liability is effective and does not need to be fundamentally changed for artificial intelligence. The Product Liability Directive in particular, in combination with national regimes, already provides a sound legal basis to address consumer protection. Therefore, any revision of the current EU legislation should be assessed carefully. ;
F530333;14-06-2020 16:17;German;Environmental Organisation;Maria;Bossmann;;Deutscher Naturschutzring;;Small (< 50 employees);Germany;The feedback can be published with your personal information;5 - Very important;No opinion;5 - Very important;3 - Neutral;2 - Not important;3 - Neutral;"Das Thema Umwelt- und Klimaschutz wird nicht hinreichend aufgegriffen. Es wird in den allgemeinen Beschreibungen allenfalls vage erwähnt und ist in der Operationalisierung durch weitere Kriterien nicht zu finden. Ob eine KI aus gesellschaftlicher Perspektive
wünschenswert ist, hängt davon ab, ob sie zu einer nachhaltigen Energie-, Verkehrs-, Agrar- und Ressourcenwende, zur Klimaneutralität bis 2050 und sozialer Gerechtigkeit beiträgt. Dies muss sichergestellt werden.";3 - Neutral;3 - Neutral;3 - Neutral;4 - Important;3 - Neutral;4 - Important;Ziel jeglicher KI-bezogener Wirtschaftsförderung muss es sein, Produkte und Dienstleistungen zu fördern, die zur Lösung sozialer und/oder ökologischer Herausforderungen und dem Erreichen der entsprechenden Ziele der EU - insbesondere der völkerrechtlich verbindlichen - beitragen. Digitalunternehmen. Die Vorteile von Innovation sollen genutzt werden unter Berücksichtigung des Vorsorgeprinzips. Ein auf dieser Grundlage klug gesetzter rechtlicher Rahmen gibt Planungs- und Investitionssicherheit.;3 - Neutral;3 - Neutral;;"Die Forschungsförderung darf nicht die Unabhängigkeit der Forschung einschränken. Die einzelnen Staaten müssen weiterhin die Möglichkeit haben, eigene Belange und Nachhaltigkeit zu priorisieren. Die Exzellenzbemühungen dürfen nicht zur exklusiven Förderung einiger weniger großer Institutionen führen. Insbesondere ist die Förderung von Nachhaltigkeitsthemen im Zusammenhang mit KI zu intensivieren.
Öffentlich-private Partnerschaften sind zu hinterfragen.";3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important; KI-Systeme haben während ihrer Erstellung, des Trainings und der Anwendung einen mitunter erheblichen Ressourcen- und Energiebedarf. Hier besteht die Gefahr, dass  diese Effekte die positiven Effekte überwiegen. Auch im Sinne des EGD muss sichergestellt werden, dass KI-Systeme nachhaltig sind. Hierfür braucht es einen ordnungspolitischen Rahmen.;There is a need for a new legislation;;No;;;;Der Einsatz von KI für militärische Zwecke.;4 - Important;4 - Important;5 - Very important;4 - Important;5 - Very important;4 - Important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);Die Fehlerquote bei biometrischen Indentifikationssystemen ist nach aktuellem Stand noch viel zu hoch als das sie zuverlässig bei polizeilichen Ermittlungen und zur Wahrung der allgemeinen Sicherheit eingesetzt werden könnten.;No opinion;Um Vertrauen in KI-Systeme zu erzeugen, bedarf es als erstes einer grundsätzlichen Kennzeichnungspflicht für Systeme, die KI einsetzen, mit Angabe, welche Arten von KI eingesetzt werden. Eine derartige Kennzeichnungspflicht könnte die Akzeptanz von KI-Systemen erhöhen und ist notwendig für eine sinnvolle Interpretation, der von den Systemen gelieferten Werte.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Die Konformität muss einklagbar und sanktionsfähig sein.;;Es gilt, Umwelt und Klima als relevante Belange, die einen verbindlichen Regelungsrahmen nötig machen, fest im Weißbuch zu verankern. Bisher sind diese Belange unzureichend verankert, als oberflächliche grüne Rhetorik im Einleitungsteil und als hauptsächlich freiwillige Maßnahmen. Durch einen entsprechenden gesetzlichen Rahmen und klare Vorgaben, die bereits bei der Entwicklung von KI-Anwendungen ansetzen, können Entscheidungen von vornherein klimafreundlich gemacht werden.;Yes;Algorithmen können beabsichtigt oder unbeabsichtigt bestimmte gesellschaftliche Gruppen diskriminieren, indem bereits vorhandene Muster von Diskriminierung oder Vorurteile übernommen und im schlimmsten Fall noch verstärkt werden. Zudem darf KI darf aufgrund der Gefahr von Verzerrungen nicht zur Beurteilung von individuellen Menschen angewendet werden. KI muss digitaler Diskriminierung und Sicherheitsrisiken begegnen und den Grundsätzen der DSGVO entsprechen.;Yes;Es bedarf einer Softwarehaftung, damit Software-Hersteller die Verantwortung für die entstehenden Risiken bei Sicherheitslücken tragen, statt die Qualität ihrer Software dem Profit zu unterwerfen. IT-Sicherheit ist die Grundlage einer nachhaltigen digitalen Gesellschaft. Für die Datensicherheit ist wesentlich, dass Unternehmen verpflichtet werden, Quellcodes offenzulegen, wenn sie keine Sicherheitsupdates für eine Software bereitstellen. ;Yes, for all AI applications;;Durch einen gesetzlichen Rahmen und klare Vorgaben, die bereits bei der Entwicklung von KI-Anwendungen ansetzen, können Entscheidungen von vornherein klimafreundlich gemacht werden. Nur so kann das volle Potenzial von KI für den Klimaschutz, das im Weißbuch hervorgehoben wird, ausgeschöpft werden. Ein klarer gesetzlicher Rahmen fördert nicht nur das Vertrauen in KI, sondern gibt Planungs- und Investitionssicherheit und wird sich für die EU zukünftig als entscheidender Wettbewerbsvorteil zeigen.;Stellungnahme_KI_Weissbuch.pdf
F530332;14-06-2020 16:16;German;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;2 - Not important;5 - Very important;;1 - Not important at all;1 - Not important at all;"Vertrauen der Bevölkerung auf Basis von Transparenz ist Grundlage für erfolgreichen, europäischen Weg.
Jede Maßnahme auf EU-Grundrechte-Charta UND EU-KI-Ethik-Leitlinien überprüfen.
Gewährleistung demokratischer Aufsicht, Einbeziehung Zivilgesellschaft und Betroffener. 
Massive Förderung vorhandener Forschungseinrichtungen (gerne gezielte Förderung von Kooperationen) statt „Leitzentrum“. 
Digitalkompetenzen ab Grundschule aufbauen";5 - Very important;3 - Neutral;1 - Not important at all;5 - Very important;5 - Very important;4 - Important;"Gemeinsame Strategien sollten Regelungen beinhalten zu:
Transparenz, 
Offenlegung aller Finanzierer der jeweiligen Forschung,
demokratische Kontrolle, 
Passung mit EU-Grundrechte-Charta UND EU-KI-Ethik-Leitlinien, 
Verantwortung, ethische Leitlinien, Grundrechte, Sensibilität für gesellschaftliche Auswirkungen und Auswirkungen auf die Demokratie. Erweiterung des Schulunterrichts um entsprechende Informatik- und Mathemtikinhalte
Vereinfachte Gründung von Startups";1 - Not important at all;5 - Very important;1 - Not important at all;"Das Gemeinwohl sollte Prioritäten bestimmen.Transparenz, auch Offenlegung aller Finanzierer der jeweiligen Forschung, Passung mit EU-Grundrechte-Charta UND EU-KI-Ethik-Leitlinien, Aufmerksamkeit für gesellschaftliche Auswirkungen und Auswirkungen auf die Demokratie sollte allen Maßnahmen und Förderungen zugrunde liegen.
Maßnahmen zum Aufbau einer europäischen HW- und SW-Industrie initiieren, um Abhängigkeiten zu GB, USA und China zu reduzieren.
";1 - Not important at all;4 - Important;4 - Important;1 - Not important at all;4 - Important;"100 Millionen sind viel zu wenig. 
Gemeinwohl, Transparenz, Passung mit EU-Grundrechte-Charta UND EU-KI-Ethik-Leitlinien, Aufmerksamkeit für gesellschaftliche Auswirkungen und Auswirkungen auf die Demokratie als Fördergrundbedingungen - keine Ausnahmen für KMU. 
Förderungsschwerpunkt für Freie-Software-Lizenzen: KMU, Forschungseinrichtungen und fachkundige Bürger*innen können in Open-Source-Projekten kooperieren, dadurch enormer Schub an Innovationen und Stärkung europäischer Unternehmen.";5 - Very important;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;3 - Neutral;"KI in sensiblen Bereichen ohne Transparenz (hinsichtlich Finanzierer, Funktionsweise, Zweck, Auswirkungen für Gesellschaft, Demokratie und Einzelne), ohne demokratische Aufsicht, ohne ausreichende Regulierung und Rechts-Durchsetzung.
„Innovation“ als Rechtfertigung für Risiken, mangelnde Regulierung und Rechts-Durchsetzung.
Mangelnde Hinterfragung von KI (Einzel-Ergebnisse im Kleinen und Entwicklungen im Großen) aufgrund Technikgläubigkeit. 
Bewusste Verdunkelung von Verantwortung.";Other;"KI-Rechtsvorschriften dürfen die Europäische Datenschutz-Grundverordnung nicht ersetzen, sondern sie müssen sie ergänzen und stärken.
KI-Risiken müssen konkret adressiert werden (durch viele Beispiele, keine Allgemeinplätze, keine „Gummi-Paragraphen“).
Das geltende Recht befasst sich nicht mit der Verwendung nicht-persönlicher Daten und den kollektiven Auswirkungen der KI. Es verbietet nicht Diskriminierung aus nicht geschützten Gründen.";Other;Eine differenzierte Risikobewertung ist erforderlich. Das ist mit Hoch - Mittel - und Niedrig nicht erledigt!;;;"Autonome Waffen, Analyse von Emotionen, Identitätsmerkmalen, insbesondere biometrische Erkennung, Verhaltensvorhersage, auch prädiktive Polizeiarbeit, Bewegungsdaten, Gesundheitsdaten im Sinne der DSGVO, Einsatz von KI zur Bestimmung der Bereitstellung grundlegender öffentlicher Dienstleistungen.
Die Bestimmung des ""Risikos"" sollte rechte- und ergebnisorientiert sein, nicht Sektor-/Branchen-bezogen";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;Biometrische Fernidentifikationssysteme im öffentlichen Raum sind ein typisches Merkmal von Diktaturen. Ihr Einsatz ist in Demokratien entschieden abzulehnen. Sie verwandeln öffentliche Räume in Orte ständiger Überwachung und beschädigen irreversibel wesentliche Grundrechte sowie Elemente und Funktionen der Demokratie.;Not at all;"Generell sollte Abstand davon genommen werden, KI-Systeme als risikoarm zu betrachten und daraus zu schließen, sie bräuchten keine Aufsicht zur Gewährleistung der EU-Grundrechte-Charta und der EU-KI-Ethik-Leitlinien.
Freiwillige, selbstregulierende Ansätze in KI-Regelungen bieten Spielraum für  eingeschränkte Rechenschaftspflicht, lockern Grundrechtsverpflichtungen, verringern die Sicherheit und erschweren den Geschädigten die Durchsetzung von Ansprüchen.";Other enforcement system;Bewertung der Passung zur EU-Grundrechte-Charta und zu den EU-KI-Ethik-Leitlinien, Folgeabschätzung für gesellschaftliche Auswirkungen und Auswirkungen auf die Demokratie in den Phasen des Entwurfs, der Entwicklung, der Erprobung und des Einsatzes. Schnell durchsetzbare Konsequenzen für den Fall, dass Anwendungen diese Standards nicht erfüllen, einschließlich der Möglichkeit, KI-Einsätze zu stoppen.;Die Einhaltung von Regulierungsmaßnahmen sollte durch externe und unabhängige Stellen erfolgen. Selbstregulierung sollte vermieden werden. Es sollte sichergestellt sein, dass es keine Schlupflöcher beim Schutz der Grundrechte und Ethik-Leitlinien gibt.;Mental health risks;"Unklare Verantwortungs- & Haftungsstrukturen innerhalb von Behörden, Unternehmen und sonstige Strukturen, die KI anwenden. 
Fehlende Strafverfolgung von Unternehmen in ein paar Ländern in Europa.
Uneinheitliche Regulierung & Haftung & Strafverfolgung in den Mitgliedsstaaten.
Profiling und Scoring – mit den Gefahren der Manipulation und Diskriminierung.
Auswirkungen der Datenverarbeitung von verschiedenen Datenquellen (BSpw Haushaltsgegenstände, Implantate, Onlineverhalten usw.) 
";Yes;Mögliche Änderungen sind in Risikobewertungsverfahren einzubeziehen, insbesondere in eine Datenschutz-Folgeneinschätzung.;Yes;Mögliche Änderungen sind in Risikobewertungsverfahren einzubeziehen, insbesondere in Datenschutz-Folgeneinschätzungen.;Yes, for all AI applications;;Die Regeln der Transparenz, Verständlichkeit und Vollständigkeit von Datenschutzerklärungen und weiteren Risikodarstellungen von KI-Systemen für Privatnutzer*innen müssen den Anforderungen an Packungsbeilagen für Medikamente entsprechen.;
F530331;14-06-2020 16:01;Italian;EU Citizen;Simone;VENTURA;;;;;Italy;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;5 - Very important;;5 - Very important;4 - Important;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;3 - Neutral;;3 - Neutral;2 - Not important;2 - Not important;2 - Not important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;No;;;;;4 - Important;5 - Very important;4 - Important;5 - Very important;3 - Neutral;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Not at all;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;;No;;;Simone_Ventura_Consultazione_pubblica_sul_Libro_bianco_della_Commissione_europea_sull_intelligenza_artificiale___La_gestione_dell_intelligenza_artificiale_come_servizio_economico_di_interesse_generale.pdf
F530330;14-06-2020 15:59;Spanish;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;4 - Important;Fostering Universities to investigate;;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;4 - Important;;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;;4 - Important;4 - Important;3 - Neutral;3 - Neutral;2 - Not important;2 - Not important;;There is a need for a new legislation;;Yes;;Yes;;;3 - Neutral;3 - Neutral;4 - Important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;Ethics Risks;Yes;;Yes;;Yes, for all AI applications;;;
F530329;14-06-2020 15:56;English;Other;Anna;DROZD;;Law Society of England and Wales;38020227042-38;Large (250 or more);United Kingdom;The feedback can be published with your personal information;4 - Important;No opinion;5 - Very important;No opinion;5 - Very important;5 - Very important;Making sure that public authorities have enough skills in-house to be able to interrogate the way in which an AI application works.;No opinion;No opinion;5 - Very important;5 - Very important;5 - Very important;No opinion;We would like to add that the AI applications used in the justice system need to be safe and their use cannot breach fundamental rights.;No opinion;No opinion;No opinion;None;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No further comments.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Risks to rule of law and fundamental rights, procedural justice, access to justice and the ability to challenge a decision.;Other;There may be gaps but further analysis could be necessary.;No opinion;;;;From our perspective, some AI applications that are used in the criminal justice system are capable of creating significant adverse effects for an individual if not used under an appropriate framework.;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;;No opinion;;Other enforcement system;A combination of robust ex ante mechanisms (conformity assessment) and ex-post market surveillance with enforcement action when needed;;;No opinion.;No opinion;None;No;We agree that it is vital that safety and liability frameworks provide users of AI systems and applications with adequate protections. However, we consider that the current legal framework is mostly sufficient to address potential harms arising from the use of AI systems and applications. Before any new legislative instrument is proposed, there needs to be more evidence of possible gaps where the existing legal framework doesn't provide for adequate protection.;No;;We are concerned that without a gap analysis, introducing such measures as strict liability regime, restricting available defences, mandatory insurance, reversal of burden of proof or unlimited liability will create unjustifiably heavy burden on this nascent industry, limiting competitiveness of EU-based enterprises working in this field and having negative effect on further innovation in AI systems and applications.;
F530328;14-06-2020 15:55;Italian;EU Citizen;Natascia;Arcifa;;;;;Italy;The feedback can be published with your personal information;4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;Identify limits on the actions and on the uses of AI systems to avoid the violation of human rights. We need to learn how AI works and create a new culture on it. It is also necessary to better determine the boundaries of accountability:  who has the responsibilities on the uses of the AI systems? ;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;"Create a programme to teach the ""right use"" of AI to every citizen.";2 - Not important;4 - Important;5 - Very important;;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;;4 - Important;5 - Very important;4 - Important;5 - Very important;4 - Important;3 - Neutral;AI may be used by people that don't know how to use it.;There is a need for a new legislation;;Yes;;Yes;;Lethal autonomous weapons ;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;EU needs to avoid that biometric identification can be used against the citizens.;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;Responsibilities on the uses of AI systems ;Yes;AI system is going to be better every day, of course EU need to consider also how it will become ;Yes;EU need a new legislation on accountability ;Yes, for specific AI applications;In health systems and in warfare systems. Besides in every application by citizens to avoid discrimination.;;
F530327;14-06-2020 15:50;English;Business Association;sagar;singamsetty;;International Road Union (IRU);41802525291-86;Medium (< 250 employees);Belgium;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;A missing section is Support for and Collaboration with Undertakings in strategic sectors, such as transport (other than and beyond public transport). The AI White Paper focuses on building trust and protection of individuals only. Businesses such as transport operators, increasingly exposed to AI, need policy consideration similar to individuals, from a customer/user perspective.;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;3 - Neutral;The current adoption rate of AI applications in network planning and route design, customer and call centre support, and safety management is relatively low. Supporting organisations of all size to develop the skills to make responsible use of AI is crucial. Regarding the European Data Space, transport operators' trust to cooperate cannot be achieved without an EU legal framework providing clear rights to businesses regarding the data they generate, and responsibilities for data collectors. ;4 - Important;4 - Important;4 - Important;Consulting businesses on deployment aspects even from the research phase can boost trust and help a smooth deployment of research outcome. Businesses such as transport operators can be similar to individuals in the use of AI as customers and should be given the chance to understand and participate in the AI lab from a user/potential deployment perspective. ;5 - Very important;4 - Important;5 - Very important;5 - Very important;4 - Important;Innovation Hubs can help ensure that every company, small or large, can take advantage of digital opportunities. Investing in hubs at EU level could boost interoperability, uniform standards, and EU-wide digital administrative solutions that create a business-friendly environment for SMEs to easily access new markets. The lack of uniform standards, APIs/data formats, is an issue that makes interoperability between platforms difficult and increases the risk of lock-in with first comer platform.;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;The fundamental foundation of AI technology is in data. Key to balancing the data environment is an EU legal framework that will provide clear rights to businesses regarding the data they generate, specifically, in relation to data access, data use and re-use. Specifically regarding connected and automated vehicles, regulatory measures should be adopted to secure them against cyberattacks, in particular regarding large commercial vehicles and their cargo/passengers.;There is a need for a new legislation;;No;;;;Large commercial vehicles transporting goods or people can be particularly vulnerable if relying on AI. Future legislation should consider wider operational context when assessing risk and should expressly consider the protection of business users and not only of private users of AI technology. Distinctions between low/high risks can be artificial, difficult to prove and subjective and, hence, should be avoided. ;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);Special rules regarding storage, use and re-use of data collected by biometric identification systems should be enacted. In addition to challenges for fundamental human rights, in the case of drivers, for example, driver data may at the same time involve business sensitive information for the storage, use and re-use of which business operators should also consent. The use of biometric recognition should be limited to very special cases such as border crossing.  ;Rather not;A labelling system risks placing a significant burden on SMEs to comply. This would indeed favour large players who can afford to meet the requirements whilst delivering minimal benefits to consumers. In addition, as mentioned above, distinctions between low/high risks can be artificial, difficult to prove and subjective and, hence, should be avoided.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Risks related to the loss of connectivity;Risks for businesses should be given an equal foot to personal security risk when considering protection against AI risks. Undertakings using AI can be exposed to risks threatening the confidentiality of their business secrets and even the profitability of their business. For example, a transport operator dependent on a platform which uses AI to rank transport options faces the risk of business failure if the criteria of the algorithm are biased and favour certain other operators.;Yes;Technical standards to operate autonomous vehicles need to be harmonised and interoperable. Technology must be solid to ensure seamless functioning in various climates and traffic conditions. During the transition phase, trials should take place in a controlled or specific area and at specific times should be encouraged, and involve professional transport operators. Further work is needed on a system of technical controls, including roadside checks.;Yes;The scope of the Product Liability Directive should be expanded to software and all AI applications, i.e., that anyone involved in making an AI system should be held liable provided it can be proved that the said problems are not related to the installation of products on vehicle. ;Yes, for all AI applications;;Clear lines of responsibility including how liability shifts from one party to another should be defined, in particular when considering software and network failures and cyberattacks. Legal provisions should secure connected and automated vehicles against cyberattacks, with special cybersecurity provisions for commercial vehicles and their cargo/passengers. Transport operators should be consulted and made aware of changes to autonomous vehicles’ functionalities if software updates planned.;
F530326;14-06-2020 15:44;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;4 - Important;4 - Important;3 - Neutral;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;5 - Very important;4 - Important;3 - Neutral;"Ideally, I'd say building something like a website, based on a knowledge base, that allows you to have a top-down view of all the different research going on and to see possible synergies would be useful. This ties in with the ""Network of existing AI research excellence centres"".";5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;3 - Neutral;;Other;I think there are very likely gaps in the current legislation, but I'm no expert in legal law, etc. to know for sure.;No;;;;People can already design very harmful systems using, e.g., face recognition and drones, etc.. In my opinion, any HARDWARE system imbued with even a basic level of intelligence, is potentially very harmful. Such dangers are present even without AI systems, of course. But AI could make it easier for humans to carry out dangerous acts. As I said before, it's very hard to say which technology would actually be highest risk.;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;No further guidelines or regulations are needed;;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Risks related to the loss of connectivity;;Yes;;Yes;;Yes, for all AI applications;;;ec_ai_white_paper_response.txt
F530325;14-06-2020 15:37;English;Other;Per;BOQVIST;;Transportföretagen;78965944361-93;Medium (< 250 employees);Sweden;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;t is important to work closely with the business community in a wide sense, both as a matter promoting AI, but also to define problems and possibilities. Catering to the specific needs of SME is complementary to this overarching priority which is not clearly brought up in the White Paper.. ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;4 - Important;5 - Very important;4 - Important;4 - Important;The innovation hubs may be useful as centres for benchmarking/exchange of information and best practice regarding development and use of AI. ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"It is important not to forget practical aspects of contractual rights and obligations as well as extra - contractual liability issues linjed to the use of AI.

High priority and adequate resources should be devoted to addressing this issue and the possible need for EU wide legislation/guidelines.   ";There is a need for a new legislation;;No;;;;;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;"Obviously biometric indentification can be a useful tool in forinstance criminal investigations, search for missing persons. But as the importance of safeguarding personal integrity  must be weighted against that of pursuing an enquiry access to biometric data should only be granted for enquiries concerning serius crime or disappearances  under suspicious circumstances  and following a Cour Order.

As a matter of transparency and legal security it appears important that similar rules should apply in the entire EU.    ";Very much;;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;;Mental health risks;"While issues related to the product in a wide sense are well covered by the current EU legislation this is not so regarding service provision and responsibility issues arising from external factors such as inadequate internet coverage, problematic reception conditions, faulty or  negligent handling of the equipment .

Some of these will already today be covered by national legislations but it seems obvious that similar rules at least EU - wide would add to legal security     ";Yes;For instance in the for of a check - upp procedure at regular intevals or following significant modification to thje equipment or software. ;Yes;See answer to the first qurstion in this section.;Yes, for all AI applications;;Options and added value of an EU wide legislation on specific issues could be considered.  Similar rules would provide significant added value for the business communitgy. ;
F530324;14-06-2020 15:31;English;NGO (Non-governmental organisation);Dorothee;WILDT;;Deutscher Anwaltverein e.V. (German Bar Association) (DAV);87980341522-66;Medium (< 250 employees);Germany;The feedback can be published with your personal information;3 - Neutral;4 - Important;4 - Important;4 - Important;3 - Neutral;4 - Important;"The DAV supports the idea to promote the adoption of AI by the public sector; as far as the justice system is concerned, under the prerequisites mentioned in our position paper. This should be accompanied by funding for bar associations to provide lawyers with the necessary training and possibility to exchange knowledge in this field.";4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;;3 - Neutral;3 - Neutral;3 - Neutral;;4 - Important;5 - Very important;4 - Important;3 - Neutral;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"-	AI may lead to impartial binding (court) decisions
-	AI may hinder access to justice
-	AI may lead to staff reductions in court systems
";There is a need for a new legislation;;No;;;;"-	Legally binding court decisions that are solely based on AI 
-	Legally binding decisions based on biometric identification schemes
";4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);"As a basic principle, biometric identification systems should only be allowed when exceptionally required by law and only insofar as the principle of proportionality is observed. 
Third-party performance tests should be required by law as well as disclosure of information on potential error rates.
";Very much;Criteria, requirements and the responsible agency or third-party actors assigning and auditing these labels should be defined on EU level to enhance the applicability and uniformity of such system.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Risks related to the loss of connectivity;;Yes;;Yes;The DAV is of the opinion that a liability regime based on but regulated separately from the Product Liability Directive should be created on EU level specifically for AI-related products to better reflect the peculiarities of such products.;No opinion;;;2020-06-14_DAV-Position_Paper_AI.pdf
F530323;14-06-2020 15:04;English;Business Association;Danny;Van Roijen;;COCIR;05366537746-69;Micro (< 10 employees);Belgium;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"High quality data - Support the compilation and curation of high quality data sets for scientific research and development as well as for training and validation
Resources - Provide investment in and access to infrastructure and computing technologies
Access to data - Facilitate access to data while not imposing data localisation restrictions
International cooperation - Cooperate with international organisations (OECD, UN, WHO, WTO,...) and consider regulatory alignment where possible";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"Regulatory sandboxes may be crucial for some applications. Testing and experimentation facilities should offer wide support, including data generation, AI training/validation, workflow integration or product certification, where possible in a live or simulated setting.  

Support is needed for federated learning, training of AI systems and innovations on health data, cross-border and with trusted non-EU parties, and multi-language semantic interoperability to pool resources across EU MS.";4 - Important;5 - Very important;5 - Very important;Innovation-friendly policies/instruments to translate R&I into new opportunities. Specific healthcare mission in Horizon Europe aimed at integrated care. A level playing field through EU funding for companies of all sizes. Risk-proportionate, predictable and science-based policies and regulations that do not hamper competitiveness and innovation. Promotion of uptake of AI by enabling trust, combatting misinformation, putting risks into appropriate context and communicating benefits to society.;4 - Important;5 - Very important;4 - Important;4 - Important;No opinion;;4 - Important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;There are concerns of ‘over trust’ and 'under trust' related to potential benefits, threats and risks of AI. Trust should be balanced and proportionate with the evidence for validation of AI tools. All concerns need to be taken seriously and addressed properly. AI applications need proper validation rather than exact explanation of the algorithmic functioning. There is a concern about possible inhibitors of AI innovation by being over-cautious. Regulatory sandboxes may help in overcoming this.;Other;The EU has a strict legal framework for healthcare, as mentioned in the White Paper. The existing provisions of EU law (Medical Devices Regulation, GDPR, etc) as well as EU guidelines for the implementation of the MDR and GDPR also cover and apply to the AI. If any, additional measures should be risk-based and duly justified based upon a prior impact assessment, ensuring coherence and consistency with existing (sector-specific) frameworks and requirements.;Other;This might depend on how high-risk applications are defined and how new compulsory requirements at horizontal level will move beyond sector-specific requirements (e.g. the Medical Devices Regulation). There might also be consideration of a more differentiated risk-based framework like proposed by the German Data Ethics Commission (cross-sector) or the International Medical Device Regulators Forum (healthcare).;;;Self-learning and autonomous decision-making may be concerning if not supported by appropriate pro-active measures and human oversight. They need to be carefully considered in high-risk applications.;4 - Important;5 - Very important;4 - Important;5 - Very important;4 - Important;4 - Important;No opinion;;Rather not;Other means of communication may be more effective, particularly in B2B settings or with trained professionals. In case of labelling, this can only have value as a European system that is built upon industry standards. The success of such a scheme will however be highly dependent on the qualifying criteria, the costs and mechanisms for validation and oversight, as well as general acceptance by the public and private sector.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;In the health sector, the regulatory approach is risk-based (e.g. medical devices). Risk classification and ex-ante conformity assessment (also during the entire lifecycle of the device) are the fundamental principles and will even be reinforced under the MDR. These rules apply to software (which incorporates AI) as a medical device. GDPR also applies to AI (Art. 22) and to any processing of data through algorithms. This has been confirmed by the EDPB. ;;No;No;Risk assessment procedures should be coherent with and where possible refer to existing frameworks like the Medical Device Regulation, which applies to .AI-based medical devices and Software as a Medical Device, and build upon international or European accepted standards.;No;The Product Liability Directive has been drawn up to be technology-neutral, and in that sense is well-equipped to handle most risks. There is potential for improvement by setting more harmonised rules that would resolve some of the complexities and fragmentation brought on by the national liability regimes.;No;;"The current liability landscape is too fragmented and complex. National liability regimes should be harmonised as much as possible to provide equal protection for all EU citizens and to ensure legal certainty for both developers and deployers/users of AI applications.

AI applications should not be given legal personality, as concluded by the Report on Liability for AI and other emerging digital technologies (November 2019).";
F530322;14-06-2020 14:58;English;Company/Business organisation;Ansgar;Koene;;Ernst & Young Special Business Services CVBA;04458109373-91;Large (250 or more);Belgium;The feedback can be published with your personal information;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;"- Regarding Skills, it will be important to focus on practical application of AI through integration with other skills, such as business, manufacturing and service provision disciplines.
Additional actions include
- Accelerating the harmonization of the D";5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;"The establishment of testing facilities and access to data through the European data space will provide significant support infrastructure to ensure that SMEs have the resources to develop competitive AI innovations. 

In the context of establishing Europe as a leader in Trusted and Human-Centred AI, it will be important to ensure that AI skills and training programmes include ethics and social impact assessment training in addition to data science STEM skills.";4 - Important;5 - Very important;5 - Very important;To support the development of trusted human-centric AI it will be important for AI innovators to have access to stakeholder groups that are representative of a wide range of European society, and who can help with societal impact assessments. Importantly, co-development with stakeholders will also increase the ability of innovators to focus their attention of applications with real relevance.;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"In order to be of value to the SMEs, the awareness raising about potential benefits of AI will have to be details and specific to the application domain relevant to the SMEs. Generalized abstract information about benefits of AI is already available through many other channels.
The Digital Innovation Hubs should provide support for SMEs to identify and use relevant industry standards (e.g. ISO, IEC, IEEE, etc standards). This would help SMEs achieve compliance with industry best-practice.";4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;#NAME?;Current legislation may have some gaps;;Other;"We agree that compulsory requirements should be limited according to the risk posed by an AI application. 
However, we do not support the binary model that distinguished only between ""high-risk"" and ""not-high-risk"". 
We believe that AI developers (e.g. the business community) as well as society (especially vulnerable groups) will be better served by a more granular approach distinguishing three or perhaps up to five levels of rights.";;;"The AI applications that are most concerning are public sector applications that have significant impact on fundamental rights (e.g. resource allocation such as social benefits and health; uses in the criminal justice system or policing) and are attempting to infer or predict human behaviour or intentions from data sources for which we have no scientific rationale to support the models that are inferred by the AI.";3 - Neutral;4 - Important;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;"To establish a strong Trustworthy AI brand around the voluntary labelling scheme it will be necessary to fund work on AI benchmarking.

There will need to be an accreditation system for the certification and voluntary labelling schemes in order to guard against ‘white-washing’.";A combination of ex-ante compliance and ex-post enforcement mechanisms;;"By introducing a risk assessment framework that accommodates intermediate risk-levels, external ex-ante conformity assessment can be required of high-risk applications; self-assessed ex-ante compliance assessment can use required of medium risk application and ex-post market surveillance can suffice for lower risk applications.";Mental health risks;;Yes;Any risk assessment is conditional on a specific scope of capabilities, vulnerability and application context of the AI application. Any important changes to the AI that alter this scope should require a new risk assessment.;Yes;In the case that lack of transparency of the role of an AI application, or lack of explainability of the AI decision model, poses a significant obstacle to existing liability assessments, there may be a need to amend and clarify the liability assessment procedure.;No opinion;;;
F530321;14-06-2020 14:58;English;Business Association;Benjamin;LEDWON;;Bitkom Federal Association for Information Technology, Telecommunications and New Media;535 183 0264-31;Medium (< 250 employees);Germany;The feedback can be published with your personal information;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;All six actions are important. However, Bitkom believes that the research and innovation community should focus on the transfer of knowledge to the economy and not only on improving the integration of science and economy.;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;With regard to the question of excellence, compare the comments under 1.2. With regard to the testing centres, it is very important to link them to existing sectoral institutions and structures. With regard to skills for AI, it is very important that classical MINT skills continue to form the basis for achieving this goal. While we support the basic idea of European dataspaces, we want to emphasize that an open and non-protectionist design of these dataspaces is central.;4 - Important;5 - Very important;5 - Very important;"With regard to the question of the establishement, compare the comments under 1.2 and 1.4.

Overall we fully support the goal to establish a lighthouse centre of research in Europe. We would suggest to establish a structure in which the lighthouse centre has a coordinating role in the European research and innovation community. The lighthouse center must be clearly linked to existing structures of excellence, such as CLAIRE and ELLIS (see also our comprehensive commentary on this issue).";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;With regard to the question of access to testing and reference facilities, compare the comments under 1.2, 1.4 and 1.6;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;"With the selection of the box ""Important"" we want to express that the different concerns should all be considered. We explicitly do not want to say that the described concerns create dangers. Rather, we are saying that the modes of action and their effects must be analysed in detail and the counterfactual, reference scenario ""Not using AI"" must be considered.";Other;Gaps need be to addressed in existing sectoral framework. Sectoral adjustments can better take into account sectoral circumstances if additional EU legislation is actually needed. Please also consider our comprehensive comments attached regarding this point.The role of standards as self-management of the economy, entirely in the sense of subsidiarity, must also be considered in the question of whether additional legislation is necessary at all.;Yes;;Other;"General approach of focussing “high-risk” AI applications is proper.
Actual application of AI should weigh more heavily in the assessment and more granularity is needed within sectors, while consulting relevant stakeholders. Other criteria may also be considered (e.g. likelihood, level of oversight). Existing definitions of “harm” & “risk” that may differ in various sectors must be taken into account for the relevant sectoral application. Further details are given in the commentary attached. ";;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);"- In principle, the use of digital technologies is to be advocated where this will streamline processes, save costs and facilitate efficiencies, e.g. in cross-border traffic.

- It is always important to adhere to all security and data protection regulati";Rather not;"We support the goal of voluntary labelling, the generation of trust. However, we have different views and risks as to whether voluntary labeling can achieve these goals.
In our opinion, the proposed approach oversimplifies the concept of trustworthiness which will be more effectively built by brands and determined by the alignment of incentives and whether the performance of AI systems is meeting consumers’ expectations. See the attached comment for more details.";Other enforcement system;"•	Dialogue and exchang with the enonomic operators to find optimal mix of ex-ante and ex-post controls.
•	Ex-ante controls should be limited to self-assessement or should follow existing sector specific mandatory requirements and enforcements mechanisms.
•	Furthermore, when designing the enforcement system/framework it is very important to link it to the existing regulatory framework (especially to AI applications that are already regulated in this respect.";"It must be ""operationally easy to handle""  criteria, which can be used to decide whether AI applications and  systems meet the relevant requirements. AI applications trained with non-European data must be treated in the same way as systems trained with European data in this context.  Disproportionate protectionist restrictions on non-European data must be prevented.";;We reject an additional safety regime for the application of AI-based technologies. Safety regimes should be set up in a technology-neutral framework and in the further development of vertical regulatory frameworks.;No;We see no need for new superordinate  risk assessments and safety regulation. The so-called important changes are too vague, there is no need for additional regulation. Existing sector-specific regulatory frame-works must be taken into account in this question and adapted where necessary.;No;Liability regimes should be as technologically neutral as possible. Existing sector-specific regulatory frameworks must be taken into account in this question.;No;;We reject an AI-specific liability regime. There is no need to leave the path of a technology-neutral liability regime.  Liability regimes should be as technologically neutral as possible. In the medium and long term, EU harmonisation should be pursued in a technology-neutral framework. ;Bitkom_EUCOM_Consultation_White_Paper_on_AI.pdf
F530320;14-06-2020 14:57;German;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;4 - Important;5 - Very important;3 - Neutral;1 - Not important at all;1 - Not important at all;"There shoud be clear, published reasons to justify the use of AI und we need scientific evidence that the technology works.

Where the technology will play an important role in determining people’s access to vital services  or to enjoy their fundamental rights and freedoms, the public should have a say in whether or not AI can be acceptably used in a democratic society.";5 - Very important;4 - Important;1 - Not important at all;3 - Neutral;4 - Important;3 - Neutral;"Your plan should include a section on human rights ond societal impacts of AI and automation. 
Ist should also explain how to ensure democratic oversight for the application of AI systems.";3 - Neutral;4 - Important;1 - Not important at all;The EU’s own ethical standards for AI and fundamental rights laws should be respected. EU funds, such as the Horizon2020 fund, should comply these standards. The iBorderCtrl must be stopped.;1 - Not important at all;2 - Not important;;4 - Important;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;"AI systems must not have negative effects on certain groups like migrants, people of colour ... 
Positive effects & no (major) risks of AI systems must be scientific proven.
AI Usage in public sector only in combination with democratic oversight, transparency or sufficient evidence to justify the purpose (or need).
";Other;"AI regulation should not provide loop-holes to data protection legislation (GDPR!), or other frameworks, like discrimination law.
AI can lead to discrimination on financial status & other grounds which are usually not protected in discrimination law!
AI may have huge collective impacts, such as furthering overpolicing, surveillance, in-equalities – all not addressed in existing legal frameworks but are still major issues.
";No;;;;"The simple approach of high-risk and low risk AI ist to narrow. The health system should be a high-risk AI sector. 
But more important: fundamental  rights  and  the  human impacts should be the determining factors, rather than sector. ";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;"Their use in public spaces will lead to mass surveillance. 
This will irreversibly limit our fundamental rights to privacy, freedom of assembly, expression, non-discrimination, data protection, dignity and the right to a fair trial, creating societies of suspicion.
Even uses which do not contribute directly or indirectly to mass surveillance in public spaces still pose significant threats to privacy, data protection, non-discrimination, and dignity.  
";Rather not;"Self-labelling systems can be confusing for people and may give a false sense of security since it is the same company that develops a product the one saying that it is safe.

We believe that the high/low risk distinction is overly simplistic and could very well allow for loop-holes for systems with potentially very significant impacts on peoples’ safety and rights. This is especially so if ‘low risk’ systems are only voluntarily controlled.";Other enforcement system;All systems should undergo a mandatory ex-ante human rights impact assessment from an external body.; The compliance assessment should be external.;Mental health risks;"In particular, the use of AI in online products and services requires collection and use of  data lending toward discrimination in many fields related to targeted advertising.  

This poses risks of differentiated pricing, discrimination or financial detriments, the risk of creating interfences in the political process, all based on sensitive inferences or associations.";Yes;;Yes;"I think that AI developers and deployers should be accountable for harm generated by their products. 
Products developed using  AI should not  enjoy exceptions to  any  EU laws, whether it be discrimination, data protection, or product liability.";Yes, for all AI applications;;The EU should address copyright and database protections which prevent proper oversight of AI applications. Liability rules should provide incentives for openness.;
F530319;14-06-2020 14:54;English;Company/Business organisation;Barry;O'BRIEN;;IBM;7721359944-96;Large (250 or more);United States;The feedback can be published with your personal information;5 - Very important;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;Partnerships need to be open to broad industry participation, and not become a vehicle for protectionism.;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;4 - Important;;4 - Important;5 - Very important;4 - Important;Cross-border and international cooperation between academic and industry researchers is a key strength for European R&I, exposing European researchers and companies to world-class research and technologies.  This has been driven by consistent investment in EU research funding programmes for many years. The EU should continue to promote wide participation in programmes such as Horizon Europe.;5 - Very important;4 - Important;4 - Important;5 - Very important;3 - Neutral;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;"Safety and security are also important concerns.
Even though AI is not always accurate, this may or may not be an important concern depending on the use case and on the overall robustness of the system and risk mitigation measures.
";Other;In many cases existing legislation will be sufficient, but may need further clarification/guidelines to address how it applies for AI.  In high-risk AI applications there may be need for expansion/clarification of existing legislation (for example in sectors like medical devices where there is already extensive regulation around high-risk applications.);Yes;;Other;We believe there should be a single risk assessment framework to identify high-risk applications regardless of sector, and without lists of exceptions.;IBM believes the use of facial recognition technology for mass surveillance, racial profiling, or violations of basic human rights and freedoms is a particularly concerning application, and we oppose it. ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);There needs to be a public dialogue on whether and how facial recognition technology should be employed by law enforcement agencies.;Rather not;While voluntary labelling systems can be helpful to consumers or end-users in some markets, we do not believe such an approach could be effective across such a broad field as AI applications;A combination of ex-ante compliance and ex-post enforcement mechanisms;;In sectors where established conformity assessment mechanisms already exist, these should also cover high-risk AI applications in those sectors. In other sectors, we believe an appropriate level of compliance for high-risk applications can be achieved with a combination of ex-ante self-assessment and ex-post auditing and enforcement.;;The current broad concept of product safety is sufficient.;Yes;Updated risk assessments are justified when significant changes in product functionality or risk profile are made during the life of a product, e.g. through software updates. It is important to note that not all AI systems continue to learn or adapt their behaviour once deployed.;Yes;Any amendments should focus on applications that involve risk of significant harm to the consumer, or to the public at large;No opinion;;;IBM_Submission_on_the_EC_AI_White_Paper.pdf
F530318;14-06-2020 14:50;English;NGO (Non-governmental organisation);Eleftherios;Chelioudakis;;Homo Digitalis;611935634267-89;Micro (< 10 employees);Greece;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;1 - Not important at all;1 - Not important at all;"-Strong collaboration with the civil society community, which focuses on the interplay between the protection of human rights and the design/deployment/development of new technologies within the EU.  
-Meaningful consultation with users and or groups of p";5 - Very important;3 - Neutral;1 - Not important at all;3 - Neutral;5 - Very important;4 - Important;The development and deployment of AI systems must respect human rights. Therefore, the European Commission shall ensure that the policies will be built on existing values enshrined in the Treaties, the EU Charter of Fundamental Rights and the ECHR. When EU bodies or the Member States fail to enforce their legal duties, they shall be held responsible, while clear remedial routes shall be available to the individuals affected;4 - Important;4 - Important;No opinion;"-Break ""data silos"" and stimulate sharing, re-using and trading of non-personal data assets
-Involve human rights organization in the activities of the research and innovation community related to the design and development of AI tools. 
-Provide for the ";3 - Neutral;4 - Important;3 - Neutral;No opinion;No opinion;-Any innovation hub could develop AI tools that lack conformity with the applicable legal human rights framework. Thus, there should not be in place lower standards aiming at innovation, such as sandboxing, without ensuring at the same time the protection;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;"Policies should provide for Inclusion of human rights organizations in the design and development of AI intended for use in public services. Such inclusion creates transparency and boosts trust, since when civil society orgs can actively participate, pose questions, express opinions, and receive clarifications, they would have more confidence in the decisions agreed upon.
-EU bodies when give operational support or advice to EU institutions shall follow a human rights-based approach.";Other;Existing EU instruments such as the ETIAS Regulation or the PNR Directive foresee the use of algorithmic tools. However, the deployment of such systems for predictive purposes comes with high risks on human rights violations. Introducing ethical guidelines for the design and deployment of these tools is welcome, but not enough. Instead, we need MSs and the EC to ensure compliance with the applicable regulatory frameworks such as the EU Charter, the ECHR, the GDPR, the LED and Conv.108;No;;;;"-Use of AI tools for mass surveillance, such as the use of facial recognition technology for identification and categorization purposes. 
-Use of AI tools in predictive policing, such as algorithmic profiling and/or police stops, crime forecasting etc. 
-";5 - Very important;No opinion;5 - Very important;4 - Important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;The use of facial recognition for identification and categorization purposes in public spaces is fundamentally in conflict with the essence of human dignity and the protection of fundamental rights and freedoms in public spaces, such as the rights to privacy, data protection, freedom of expression and freedom of assembly. The risks for increasing authoritarian societal control is too high for any alleged “benefits” that AI developers/companies promise to LEAs from the use of these technologies. We call the European Commission and the MSs to follow EDRi's approach published in May 2020 on this matter and to ban any use of remote biometric identification systems.;Not at all;The voluntary labelling system for no-high risk AI applications presented in the AI White Paper is unfortunately inefficient for the users of these AI services and products. The idea is interesting, but without structural changes in the labelling scheme, the latter would unfortunately be nothing other than a publicity/marketing stunt for the AI developers.A good approach would be for the developers to provide precise information regarding the AI techniques incorporated in a product.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;"We do not agree with the term ""trustworthy AI"" used. Nevertheless, possibly, some new legal frameworks  - sector oriented - could be established to codify new requirements, obligations, rights, oversight bodies, regular reviews, and remedial routes. Lastly, maybe is some cases new legal frameworks are not necessary. Instead, revising the existing frameworks and complementing them with provisions on the use of AI tools could be a more efficient option.";Mental health risks;"-Risks related to discrimination
-Risks related to intellectual isolation and radicalization arising from filter bubbles created by algorithmic targeted advertising, personalized searches and personalized news-feed
- AI designers/developers shall take int";Yes;The European Commission and MSs shall ensure that the relevant supervisory/ oversight bodies in EU and national level are equipped with the necessary resources (human and financial) and authority to investigate, oversee and co-ordinate compliance with their relevant legislative and regulatory framework. ;Yes;A key aspect of future AI policy framework is the choice of the liability regime for damages caused by AI. As CEPS underlines, there are 3 main aspects on this issue. The first is related to the scope of the liability, the second to the type of remedy, and hence the type of liability rule to adopt, and the third revolves around problems of attribution or appointment of liability. Finally, the future EU liability regime will also have to be designed with a suitable insurance framework.;Yes, for specific AI applications;In general, AI tools must be adapted to the existing legal systems, rather than the other way around. Nevertheless, some of the existing legal systems have to be revised in order to contemplate new, AI-enabled, ways of providing goods and services, organizing production, and channeling social interaction.;Issues related to data ownership: The rapid expansion of the data economy in the field of AI raises questions about who has the ownership on data generated by AI products and services, as well as what  such data “ownership” entails in terms of exclusive rights. The concept of data ownership raises important questions about how to strike a balance between the rights of the AI developers and the society's interest in accessing and reusing these data as part of the public domain.;
F530317;14-06-2020 14:40;English;Company/Business organisation;Adrian W.;Pasieka Ikane;;PressXAI University Poland;;Micro (< 10 employees);Poland;The feedback can be published with your personal information;5 - Very important;5 - Very important;;3 - Neutral;5 - Very important;1 - Not important at all;"Maximizing the funding efficiency by taking a concrete, real measures AFTER a funded projects are completed, not only allowing them to ""look nicely on a paper"", to get funding reconciled. ";5 - Very important;5 - Very important;1 - Not important at all;3 - Neutral;3 - Neutral;5 - Very important;Building the AI projects real value;5 - Very important;3 - Neutral;4 - Important;;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;5 - Very important;"In Poland the startups are WASTING money, because all EU funding goes through the public institutions which are 100% POLITICIZED, and the money is given to the poor companies, which after receiving funding do nothing, but are seeking the IT subcontractors without getting them paid. After two years of a ""hard research"" the project is closed, money is wasted.";3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;I think that the EU shall focus on the R&D efficiency, and when we have a REAL PRODUCTS and SERVICES to adjust the societal safety to it. ;Other;As above, the AI societal use is in the Government hands, we need to build EXCEPTIONAL AI solutions first, and to regulate it, not the opposite.;Other;Let's stop regulation, let's start CREATING the REAL EUROPEAN AI.;;;;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;3 - Neutral;Other special requirements in addition to those mentioned in the question above should be imposed (please specify);It is nothing wrong with a face recognition, we have to only make sure that the data is not ABUSED.;Much;;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;Let's build the BEST AI in the World first. We can regulate it easily.;Cyber risks;;Yes;;No;;Yes, for all AI applications;;"In Poland the AI money is almost COMPLETELY WASTED.

More and more projects DO NOT HAVE any second stage verification. An ""independent commissions"" are like God, without any hope for the objectivity. The Government is financing their own very poor companies, because the EU money is ""free"".

It is not free, Poland has to repay it - also if the EU will waste this money we will lose to China/USA/Russia, and with the useless AI there is no future for Europe.
";
F530316;14-06-2020 14:32;English;NGO (Non-governmental organisation);Student Task Force;Digital Health;;EMSA;;Large (250 or more);Belgium;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;Lots to say about the “common European approach” Would be interesting in the concept of data sharing. Will there be a push for open-source future or a sort of behind the scenes competitiveness? The involvement of teams with individual from various disciplines would be vital when it comes to design, testing, etc of AI systems. This is particularly important in healthcare.;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"The security and quality of data is vital, the privacy applied to the AI technologies are very important, and should be considered a priority for the EU commision. 

AI systems that are evidence-based, alongside trustworthy, equitable and non-discriminating are needed, these could be achieved with Governmental and Independent  non-governmental oversight.  ";4 - Important;4 - Important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"Use of AI may be abused, and use in unethical and non-transparent ways.

The placement of liability regarding AI creators and the end-users is not clear, expansion is required.

The use of health data, and it incorporation into AI systems would introduce bias, and the data is not perfect and there are numerous biases present in health data and thus not reliable. 

The use of health data in AI systems to improve AI can infringe on medical confidentiality. ";There is a need for a new legislation;;Yes;;No;;"All AI systems involved with healthcare need to be considered as high risk, as wrong decisions/advice can impact health and be life-threatening e.g triage and mental health services.

The use of health data must comply with GDPR, and the privacy of patients is at risk!";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Very much;This is ideal, however, what precise parameters would be employed to determine that the AI is deemed to be safe, this would need to be very robust in order to maintain user trust.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;"Data security - due to sharing of data and its de-anonymised data 
Physical health - due to inaccuracy and/or flaws
Risk of Discrimination -narrow scope of data used
reduction of autonomy - due to over reliance on AI systems";Yes;;Yes;;Yes, for all AI applications;;;
F530315;14-06-2020 14:07;English;Academic/Research Institution;Justin;Bullock;;Governance of AI Research Group;;Micro (< 10 employees);United States;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;See written response;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;See written response;5 - Very important;5 - Very important;5 - Very important;See written response;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;See written response;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;See written response;There is a need for a new legislation;;Yes;;Other;See written response;Social media, criminal justice, law enforcement, ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;See written response;Yes;See written response;Yes;;Yes, for all AI applications;;See written response;Governance_of_AI_Research_Group_EU_Commission_AI_White_Paper_Consultation.pdf
F530314;14-06-2020 14:03;English;Academic/Research Institution;Jatinder;SINGH;;University of Cambridge;420093131813-97;Large (250 or more);United Kingdom;The feedback can be published with your personal information;;;;;;;;5 - Very important;5 - Very important;;;;;;5 - Very important;4 - Important;4 - Important;;;;;;;Making clear the RISKS of using AI for SMEs;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"* AI systems may interact, where systems may impact each other. Risks may be harder to foresee in highly complex systems.

* Trust in those providing the AI, and AI infrastructures.
";;;;;;;;;5 - Very important;5 - Very important;;;;;;;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;;Risks regarding the increasingly interconnected nature of systems. Systems become a complex assemblage, of many components, processes and indeed organisations. Liability, accountability, responsibility aspects require attention.;Yes;;;;;;;EU_AI_Whitepaper_response.pdf
F530313;14-06-2020 13:59;English;Business Association;Edward;Haynes;;American Chamber of Commerce to the European Union (AmCham EU);5265780509-97;Small (< 50 employees);Belgium;The feedback can be published with your personal information;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;AI uptake in the private sector should be another top priority for action, since AI Adoption still remains low in Europe. Public-private partnerships must be open to broad industry participation and should be open to all companies, regardless of where they are located. Regarding skills, the action should focus on upskilling and reskilling schemes to help workers adapt to the uptake of new AI technologies.;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;Governments should stimulate demand and promote the uptake of AI solutions by the private and public sectors. In particular, Member States should implement an Adopt AI programme linked to other EU funding streams, eg, the European Regional Development Fund, the Cohesion Fund and Digital Europe, to fund the ‘path to AI’. It should also complement the Tallinn Ministerial Declaration on eGovernment to help governments exchange practices and develop guidelines.;2 - Not important;5 - Very important;3 - Neutral;Setting up a specific EU lighthouse research centre would detract funding and skills from existing centres of AI excellence across Europe. The Commission should keep supporting and amplifying existing centres of excellence rather than trying to create new ones. The public-private partnership for industrial research should be open to all companies regardless of the location of their headquarters.;5 - Very important;4 - Important;4 - Important;5 - Very important;3 - Neutral;;5 - Very important;5 - Very important;4 - Important;4 - Important;3 - Neutral;3 - Neutral;The liability framework should be risk-based and consider the numerous operators in the production chain. Sensible government approaches to regulation should be dynamic and able to evolve with the technology space. This is important to build trust and ensure that AI is used responsibly, while encouraging innovation. It is important that a proportionate, risk-based and use case dependent approach is taken, balancing potential harms with the social and economic benefits created by AI.;Other;In many cases existing legislation will be sufficient, but may need further clarification/guidelines to address how it applies for AI. In high-risk AI applications there may be need for new legislation in some sectors, or expansion/clarification of existing legislation (eg, like medical devices where there is already extensive regulation around high-risk applications). We encourage the Commission to strive for providing legal certainty in any and all EU legislation for AI systems.;Yes;;Other;We are supportive of the 2-level risk-based approach (low/high risk), but regulation should be proportionate, targeted, and provide legal certainty. Risk assessments must take into account the context, as an AI application will pose different risks depending on the way it is integrated into business operations. The focus should always be on the specific use case, not the broad class of application or technology.;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);Biometric identification systems should be allowed but guidance is needed to give people confidence in them. Requirements for remote biometric identification systems should follow the Commission’s risk-based approach, with specific requirements applying only to high risk applications.;Rather not;A label might be helpful to increase trust in AI systems, however the proposal is vague and it is difficult to see how it would be implemented in the near-term. Questions remain on who would be the oversight authority, what if problems occur obtaining a label, and would this create a blanket conformity requirement for all AI systems? More work needs to be done in a multi-stakeholder setting to get to a place where all parties feel confident about labelling.;Other enforcement system;We would support a combination of ex-ante self-assessment, followed by ex-post market surveillance. Ex-ante conformity assessment models can hold back innovation and add huge burden to businesses, which deters companies from developing and launching new services in Europe. A more balanced approach is to make the expectations clear for risk assessment processes, and allow for self-checking prior to launch, with ex-post investigations carried out where problems are suspected.;It is essential for the governance framework to enforce any future rules on AI in a coordinated, harmonised and simple way. In order to achieve this objective, we strongly recommend further consultation with the private sector regarding the establishment of the governance framework. We tend to think that, and as is the case in highly regulated sectors (such as medical/air transport/financial services), the existing regulatory bodies are best placed to make ex-post conformity assessments.;;According to the Report on the Safety and Liability Implication of AI, IoT, and Robotics, the existing safety legislation framework already covers the full range of risks that may be implicated by the use of AI and other emerging technologies. The Report notes that the current product safety framework recognises an 'extended concept of safety'. ;No;;No;The use of AI systems, and therefore any resulting liability, is context-specific: the focus of risk should lie on a specific application and the context of its use. There is often a complex chain of various producers and intermediaries involved. Having more than a single operator who is liable or introducing joint liability would not be workable, and it would not make sense for anyone involved in making an AI system to be liable for problems they had no awareness of or influence over.;No;;;AI_supporting_document_final.pdf
F530312;14-06-2020 13:52;English;Business Association;Takenobu;Kurihara;;Japan Business Council in Europe;68368571120-55;Micro (< 10 employees);Belgium;The feedback can be published with your personal information;;;;;;;;;;;;;;"The Commission’s view that regulatory intervention on AI should be proportionate and that a horizontal framework should aim at not being excessively prescriptive to avoid creating a disproportionate burden on companies.
For the EU’s AI framework to be a success, JBCE believes that any new rules on AI must be clear, concise, well-justified and feasible for companies to implement.";;;;;;;;;;;;;;;;; AI applications can also be associated to a set of risks, associated to fundamental rights, personal data and privacy protection, safety, and liability-related issues.;;;;;;;;;;;;;;;"-Human oversight is needed to avoid adverse effects that could be caused by AI, but excessive oversight could hinder innovation and diminish the benefits of AI.
-Companies should not be required to keep records of their data-sets used to test and train AI";;Any voluntary labelling approach should not become a de-facto market entry requirement for AI products and services in Europe. For this labelling system to properly function and be widely used in the market, it must be aligned with international AI standards.;;;It is just as possible to have significant fairness and diversity issues with models trained in the EU, on data collected in Europe and compliant with European laws and ethics, as with data collected and trained elsewhere. In fact, restricting AI models so they only use limited data-sets, could lead to discrimination and lower quality systems.;;Some of the potentially high-risk sectors (e.g. transport) or AI uses (e.g. automated driving vehicles) are already subject to strict ex-ante rules. Finally, we believe that requirements for high-risk applications need to be sector specific. It will be crucial to thoroughly assess whether new sets of requirements for certain sectors should be included into any existing framework.;;Certain AI products will be subject to important changes in their lifetime due to their selflearning capabilities after they have been placed on the market. Any risks posed by self-learning should be addressed through the New Legislative Framework (NLF).;;The New Legislative Framework (NLF) procedures that take place prior to placing products on the market could be broadened through the adoption of new standards to cover important and foreseeable changes.;;;"The European institutions, Member States and all stakeholders should work together to create ""An Ecosystem of Trust"" where AI & ethics can have a key guiding role in the development of new solutions.";JBCE_Position_Paper_on_the_White_Paper_on_AI.pdf
F530311;14-06-2020 13:45;French;Other;eric;parize;;"European Expertise and Expert Institute
IL s'agit d'une association";807004837808-45;Micro (< 10 employees);France;The feedback can be published with your personal information;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;;No opinion;No opinion;No opinion;;No opinion;No opinion;No opinion;No opinion;No opinion;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Comment trouver des preuves irréfragables dans des systèmes à base d'intelligence artificielle, que ce soit a priori dans un logique de labellisation ou de certification ou a postériori, dans une logique de recherche en responsabilité ?;Current legislation may have some gaps;;Yes;;Yes;;La difficulté est d'inscrire dans une réglementation ce qui est à haut risque ou pas, sachant que les usages vont évoluer plus vite que la réglementation, et les risques seront découverts a postériori, leur gravité également. Bien sûr sont à haut risque tous les systèmes qui peuvent mettre en jeu des vies humaines, systèmes de pilotage des avions, voitures autonomes, certaines applications en médecine....;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;;Rather not;"Les systèmes de labels n'offrent aucune garantie sur la réalité qui est derrière. Ce sont des opérations commerciales pour ceux qui les attribuent et de marketing, d'image, pour ceux qui les affichent. Des labels qui auraient été attribués et dont on pourrait prouver a postériori qu'ils n'étaient pas mérités, pourraient constituer une infraction supplémentaire dans le cadre d'une recherche en responabilité.
A contrario, des labels vérifiés pourraient participer à accroitre la confiance.
";Other enforcement system;"Une évaluation externe a priori n'aura aucune valeur, car elle ne certifiera pas que le système en usage au moment d'un incident était bien le système qui a été évalué.
Une évaluation a postériori, sera difficile et couteuse. Difficile car aujourd'hui tous les systèmes sont en fait des systèmes de systèmes, utilisant chacun différentes IA, interagissant entre eux. Si les systèmes sont opérés dans le cloud dans différents pays il sera très difficile d'accéder à des preuves. ";En complément au paragraphe ci-dessus, nous tenons à signaler que les efforts pour essayer d'évaluer le comportement a postériori, d'un système à base d'IAs, seront très importants et ne pourront se justifier que si le préjudice est très élevé. Il faudra constituer une équipe avec des spécialistes de différentes disciplines et avoir le concours du fournisseur.;Cyber risks;;Yes;;No opinion;Les recherches en responsabilités sur un système de systèmes à base d'IA seront très longues et très couteuses.;No opinion;;;r_ponse_de_l_EEEI___la_consultation_de_la_CE_sur_le_livre_blanc_sur_l_IA.pdf
F530310;14-06-2020 13:31;English;Company/Business organisation;JOSEPH;MCHALE;;Bloomberg L.P.;214140728444-40;Large (250 or more);United States;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;3 - Neutral;Set appropriate and realistic metrics and goals. A number of milestones would need to be achieved to achieve the goal of democratizing AI access. Whatever regulatory frameworks, guidelines or initiatives are put in place need to be able to be adapted at a  rapid pace, or risk being made less relevant by technology. Strong incentives ought to be in place for open access and reproducibility of research, to avoid concentration of resources.;5 - Very important;3 - Neutral;4 - Important;4 - Important;5 - Very important;3 - Neutral;;5 - Very important;4 - Important;5 - Very important;A more standardized framework for public-private and/or academic-industry research partnerships and funding arrangements would help broaden the range of partnerships. ;4 - Important;3 - Neutral;5 - Very important;5 - Very important;; ;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;The realistic capabilities of AI should be well understood and socialized, to avoid use of AI in applications where simpler methods will suffice. A data driven AI pipeline is fundamentally much more complex system than a “traditional” software system. Whereas model checking, correctness proofs and other tools to certify traditional software systems exist, applying them to machine learning and AI systems is still an area of research. Note the issues surrounding adversarial examples as case study.;Other;The legislation must be carefully thought out to consider future developments and not to stifle innovation, or avoid creating misaligned incentives. For ex. under GDPR personally identifiable data can only be used for the purpose it was collected, which may be appropriate in context of passive collection of personal data (e.g. by mobile apps) but not for discoveries and progress arising in an unanticipated fashion. There ought to be mechanisms, allowing reuse subject to review and anonymization.;Yes;;Yes;;Medical diagnosis, self-driving cars, criminal justice and predictive policing. ;3 - Neutral;5 - Very important;4 - Important;5 - Very important;4 - Important;4 - Important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;;;Yes;;No opinion;;Yes, for specific AI applications;Medical diagnosis;;Bloomberg_L.P._annex_to_the_European_Commission_consultation_on_the_White_Paper_on_Artificial_Intelligence.docx
F530309;14-06-2020 13:30;Polish;Company/Business organisation;Przemys?aw;Króliszewski;;iTechnologie Sp. z o.o.;;Small (< 50 employees);Poland;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"We should focus on AI creativity. This is the key factor for generating advantage. If we can successfuly teach AI to design machines, or design constructions then we will be able to lower costs and massivly increase speed of performing that tasks. 
I am writing i.e. about agricultural devices, developed by AI, that will be small, efficient, eco-friendly and cheap. ";5 - Very important;3 - Neutral;No opinion;5 - Very important;4 - Important;5 - Very important;We should consider the impact that the AI will do on society at all. In the long term it will develop the society of no-workers, because all low competency tasks will be no longer need to be performed by humans. ;5 - Very important;5 - Very important;5 - Very important;;4 - Important;3 - Neutral;3 - Neutral;4 - Important;5 - Very important;Currently almost all hardware solutions for AI are designed and produced somewhere in Asia & USA. This causes the EU to be outside of the main stream. We should have our own answer for hardware used in Neural Networks implementations. If not, we are fully dependend on external supplies, which may be not stable or even consciously lowered by theirs owners. ;4 - Important;5 - Very important;No opinion;1 - Not important at all;3 - Neutral;3 - Neutral;;There is a need for a new legislation;;Yes;;No opinion;;"The AI may be a tool for taking the freedom of humans if not used right. It may replace humans in so many areas that in fact, humans will be not ""necessary"" anymore. I can imagine that the AI that is controlling AI's will take a decision to ""disable"" the humanity at all - it is a little bit science fiction but still possible scenario. ";3 - Neutral;3 - Neutral;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);We should think about scoring system performed automaticly on all levels of the society. The people with good score will have more benefits than those with bad score. In the way of incoming revolution on job market it will be easiest way to distinguish people ( that is cure in monetary oriented approach) between those legal-oriented and not-legal-oriented.;No opinion;;Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;;Mental health risks;;No opinion;;No opinion;;No opinion;;;Sztuczna_inteligencja_projektuje.docx
F530308;14-06-2020 13:18;English;Company/Business organisation;Carolina;Brånby;;Confederation of Swedish Enterprise;;Medium (< 250 employees);Sweden;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;The Commission has rightly identified a need to focus on investment in, uptake and deployment of trustworthy AI to ensure its benefits for society, sustainability and competitiveness continue to grow. ;5 - Very important;3 - Neutral;5 - Very important;4 - Important;5 - Very important;4 - Important;Europe’s standardisation framework has a crucial role to play in fostering excellence in AI. Market-relevant technical standards can support interoperability, technology transfer and enable competitive levers to lead in AI applications. An international approach is favorable. The  framework for the governance of common European data spaces must build on an agile approach that favors experimentation for development (such as regulatory sandboxes) and support industrial self-regulation.;4 - Important;5 - Very important;5 - Very important;Permitting confidential testing and piloting of AI in the development stage, free of market access requirements, should be permitted in any future framework to support Europe’s AI research community. This could be done by experimentation clauses and regulatory sandboxes at EU-level.;4 - Important;4 - Important;4 - Important;4 - Important;3 - Neutral;;4 - Important;4 - Important;4 - Important;4 - Important;2 - Not important;3 - Neutral;These concerns shouldn't be underestimated and therefore it is highly important to communicate how those areas of concerns are already regulated. Any AI outcome must be legally compliant. ;Other;There may be gaps in legislation that need to be addressed, but since tech is emerging fast any new legislation need to be principal-based and tech-neutral. ;Yes;;Other;The definition of high risk must be narrow in order not to hamper innovation and the use of AI. A clear approach to risk assessment should identify all high-risk cases without the need to list high-risk sectors or areas. The Confederation of Swedish Enterprise is very concerned with the proposal that certain purposes (e.g. employment area) would always be considered as high-risk thus excluded from many of the potential benefits of AI.;One suggestion for identifying high-risk AI would be to focus on learning rational AI systems (self-learning systems), with possible disproportionate impact on humans and/or the environment. High risk AI should be limited to learning rational AI systems since those systems fall outside the scope of existing regulations like the product safety or product liability directives. Rational AI systems should fall out of the high-risk definition and the scope of new compulsory requirements. ;3 - Neutral;3 - Neutral;5 - Very important;3 - Neutral;4 - Important;3 - Neutral;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);Biometric identification systems may be needed in autonomous vehicle for safety reasons by determining the intention of pedestrians and other drivers.;Rather not;"A labeling system risks placing a significant burden on SMEs. This would favor large players who can afford to meet the requirements whilst delivering minimal benefit to consumers. Self-regulation and self-assessment are preferable to show adherence with Ethical guidelines for Trustworthy AI and we therefore do welcome a more advanced assessment list from the High level expert group on AI to be released.   
";A combination of ex-ante compliance and ex-post enforcement mechanisms;;"We believe the best approach is a combination of ex-ante self assessment and ex-post enforcement mechanisms for high risk AI applications. Care should be
taken to ensure the self-certification process is feasible, especially in terms of documentation requirements, so as to avoid discouraging innovation or putting an undue burden on SMEs.";Mental health risks;Any review of the GPSD should focus exclusively on areas where the unique properties of new technologies create a risk to the health and safety of consumers. To the largest extent possible this should be done at the level of special safety regulation (e.g. Regulation (EU) 2019/2144 on type-approval requirements for motor vehicles). ;Yes;Carrying out a new risk assessment should only be required when there has been a significant change to the functionality of the product. Generic over-the-air updates (OTAs) such as security fixes or bug fixes after placing a product on the market should not trigger a renewed risk assessment.;No;The current PLD remains fit for purpose, being both effective and technology neutral. The PLD provides both legal certainty and compensation to consumers. ;No;;A third party that upgrade or make important changes of a device or service after it has been placed on the market need to have strict liability of the product or service they change. ;AI_Position_Paper_SN_final2.docx
F530307;14-06-2020 13:10;German;Academic/Research Institution;Carsten;Orwat;;Karlsruhe Institute of Technology (KIT), Institute of Technology Assessment and Systems Analysis (ITAS);;Medium (< 250 employees);Germany;The feedback can be published with your personal information;4 - Important;3 - Neutral;4 - Important;4 - Important;2 - Not important;2 - Not important;;3 - Neutral;3 - Neutral;2 - Not important;2 - Not important;4 - Important;No opinion;;1 - Not important at all;3 - Neutral;No opinion;;3 - Neutral;5 - Very important;4 - Important;4 - Important;No opinion;;4 - Important;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;;Other;Current legislation my have some gaps and there is a need for a new legislation.;No;;;;Please see contribution/statement for further details. ;4 - Important;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Not at all;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;No opinion;;Yes;;No opinion;;;KIT-ITAS_-_Stellungnahme_zum_Weissbuch_zu_KI.pdf
F530306;14-06-2020 12:52;English;Academic/Research Institution;Jiahong;Chen;;Horizon Digital Economy Research Institute, University of Nottingham;;Small (< 50 employees);United Kingdom;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;Strengthening ethics reviews for EU-funded AI research projects. (See para 4 attachment);5 - Very important;5 - Very important;4 - Important;;4 - Important;4 - Important;5 - Very important;4 - Important;4 - Important;To provide training on managing AI-related risks in selection and deployment of AI systems. (See para 5 attachment);5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;AI may reinforce existing social injustice or economic inequality.;There is a need for a new legislation;;Yes;;No;;;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;Such technologies should be limited to specific contexts and allowed only when special safeguards have been provided in sector-specific legislation. (See para 6 attachment);Much;"An effective labelling system would not just involve recognition of ""safe"" products but also support consumers to make comparisons and identify potential risks. (See para 7 attachment)";A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;"The concepts such as ""producer"" and ""supplier"" need to be renewed to reflect the nature of AI systems and the structure of the ecosystem. (See para 8 attachment)";Yes, for specific AI applications;"Strict liability or no-fault compensation schemes should be considered for such products as self-driving vehicles and robots (or ""cobots"") in the workplace.";;Response_to_Consultation_on_the_AI_White_Paper.docx
F530305;14-06-2020 12:40;English;Academic/Research Institution;Mattia;Ceracchi;;PromethEUs;;Micro (< 10 employees);Belgium;The feedback can be published with your personal information;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;A clear understanding of different company needs should be acknowledged according to their position in the value chain. While the EU should strive to achieve better results in research and innovation, most companies, especially SMEs, would be either only or mainly AI users. Therefore, for a competitive economy, the EU regulatory framework should lead the vast majority of companies to adopt AI easily and at a cost to be competitive.   ;5 - Very important;3 - Neutral;5 - Very important;4 - Important;5 - Very important;4 - Important;The afore-mentioned objectives should not jeopardize the possibility for EU citizens and companies to have access to the best available AI technologies at a competitive price. A balanced approach should be used taking into full account the interests of all the concerned parties, including the vast majority of citizens and companies that  would be adopters rather than R&D and/or commercial producers in the AI ecosystem.   ;5 - Very important;4 - Important;5 - Very important;While AI requires vast amounts of computing power, data and expertise, a wide gulf exists between the few companies that can afford these resources and everyone else. We need a close partnership between academia, government and industry providing affordable access to those high-end inputs. This means democratizing access to the essential tools and ensuring that they remain open and potentially shared by all.  ;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;4 - Important;Both training and advice to SMEs should be key activities for AI specialized digital innovation hubs (DIHs). For this reason, foreseeing only one DIH per Member State may involve a sizeable geographical barrier for SMEs, especially in larger countries. A more distributed network of DIHs providing expertise to SMEs in different regions should be pursued, possibly involving trade associations and larger AI technology players.;4 - Important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;Although many afore-mentioned concerns deserve a high level of scrutiny and sometimes need to be addressed by ad hoc regulation, it would be fairer to compare AI applications with a human-based benchmark. It would not be realistic to expect AI achieving an error-free perfection where, in the same field, the same standard is not currently applied. This requirement could significantly stifle innovation, especially from SMEs and new entrants.    ;Other;Although some new legislation is certainly required and we fully agree that a EU-wide regulatory framework is preferable to national, current legislation should apply whenever it is possible in order to avoid excessive market fragmentation and uncertainty and increase compliance costs for companies, especially SMEs. ;Yes;;Other;If the two proposed cumulative criteria to determine “high-risk” AI applications seem quite logical and could help provide legal certainty, exceptional additional instances should be better defined and limited to specific cases in order to avoid any ambiguity where the aim of the risk-assessment approach is exactly the opposite.  ;;4 - Important;3 - Neutral;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);A right balance should be struck between security issues (e.g. terrorism), that under certain circumstances could justify the massive use of biometric identification systems in public spaces, and privacy concerns, that should rightly limit any abuses of these tools. We totally agree that a EU-regulatory framework is by far preferable to leaving the question solely to Member States.  ;Much;Compliance procedures and costs related to a voluntary labelling system should be minimized to also allow SMEs and startups to take advantage of this instrument and so maintain a level playing field in the internal market;Other enforcement system;While ex-ante self-assessment, instead of an external procedure, is key to speeding up the innovation process and ensuring a thriving European AI ecosystem, ex-post enforcement should play an important role in guaranteeing compliance by AI developers and deployers.;In our opinion, some requirements for high-risk applications should be relaxed, for instance data and record-keeping. Given that data keeping requirements may easily conflict with GDPR prescriptions, accurate records and data may not be available to firms developing AI based on open source models. We also believe that it is important to enhance the resilience of AI against possible attacks, given, precisely, the high risk character of the sectors affected. ;;;Yes;Notwithstanding the need for new risk assessment procedures should be considered in case of significant changes, it should be limited in time and scope reflecting a right balance with other important public policy objectives such as ensuring low compliance costs for companies and other entities, especially SMEs, and a competitive innovation ecosystem.   ;Yes;Reforming the current EU legislative framework to better cover the risks entailed by AI applications may generate a serious fragmentation in the internal market between AI and non AI-based products, discriminating against innovation. For this reason, while some amendaments to the current legislation could be justified by better attributing responsibility in certain situations and providing legal certainty, we believe that such revisions should be limited and targeted to a well-defined scope.;No;;For the same reason as previously stated, we believe that an EU-wide approach would be preferable in order to avoid major risks of internal market fragmentation. Therefore, Member States should refrain from unilateral moves and look for agreements and alliances at EU level.;
F530304;14-06-2020 12:15;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;;1 - Not important at all;1 - Not important at all;The EC should include actions such as democratic oversight of AI in the public sector, consultations with civil society, the general public and affected communities, as well as stringent human rights safeguards.;4 - Important;3 - Neutral;1 - Not important at all;2 - Not important;3 - Neutral;3 - Neutral;"The coordinated plan on AI should be updated to include criteria (scientific and policy) about how the EU will allocate its resources of AI. The plan should include a section on human rights, societal impacts of AI and automation, and how to ensure democratic oversight for the
application of AI systems.";3 - Neutral;4 - Important;1 - Not important at all;"Funding for EU projects on AI should be conditional on meeting the EU’s own ethical
standards for AI & fundamental rights laws. EU funds, e.g. Horizon2020 fund, should comply & immediately stop funding for projects which pose a risk to fundamental rights, e.g. iBorderCtrl - which aims to use facial and emotion recognition technology to supposedly detect lies in the course of visa applications, but is not substantiated by scientific evidence and significantly infringes upon human dignity.";1 - Not important at all;No opinion;No opinion;No opinion;No opinion;Small businesses should not enjoy any exemptions to protecting human rights. Some of the biggest AI-related scandals have involved small companies, and so the EU should ensure that when small businesses to take up AI, whatever the circumstances, they should respect data protection, privacy and other fundamental rights. There should be no blanket exemptions to fundamental rights standards.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;AI's negative and discriminatory impact on marginalised communities (PoCs, migrants, women, people with disabilities), e.g. in already racialised policing/surveillance, healthcare, recruitment;Other;AI regulation should not provide loop-holes to data protection legislation, or other frameworks, like discrimination law. Current law does not address use of non-personal data for AI, types of data which do not fall under the GDPR. AI can have huge collective impacts, such as furthering overpolicing, surveillance, inequalities – all not addressed in existing legal frameworks but are still major issues.;Other;Uses of AI which breach fundamental rights - like biometrics/ facial recognition for mass surveillance - should be banned outright.;;;"Determining ‘risk’ should be rights and outcomes focused. These uses of AI are incompatible with human rights and should be banned:
• The use of AI to determine delivery of essential public services,
• predictive policing
• autonomous lethal weapons
• identification/ analysis of emotion and identity traits,
• and indiscriminate biometric surveillance.";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;"1. Their use in public spaces will lead to mass surveillance;
2. This will irreversibly limit our fundamental rights to privacy, freedom of assembly, expression, non-discrimination, data protection, dignity and the right to a fair trial, creating societies of suspicion; and,
3. Even uses which do not contribute directly or indirectly to mass surveillance in public spaces still pose significant threats to privacy, data protection, non-discrimination, and dignity.";Rather not;"1. Self-labelling systems can be confusing for people and may give a false sense of security since it is the same company that develops a product the one saying that it is safe.
2. We believe that the high/low risk distinction is overly simplistic and could very well
allow for loop-holes for systems with potentially very significant impacts on peoples’
safety and rights.";Other enforcement system;We think all systems should undergo a mandatory ex-ante human rights impact assessment from an external body. To specifically address the need to make human rights a priority in the AI regulation, and ensure that there are no loop-holes just because a system falls into a low-risk category.;"Compliance should be external to guarantee fundamental rights are protected; we cannot rely on self-regulation for this. ";Mental health risks;Potential risks of discrimination posed by AI systems. E.g. the use of AI in online products and services requires collection and use of data lending toward discrimination in many fields related to targeted advertising. This poses risks of differentiated pricing, discrimination and financial detriments, the risk of creating filter bubbles, interfences in the political process, all based on sensitive inferences or associations.;Yes;Internal supervisors, such as Data Protection Officers under GDPR should be included and and asked for advice.;Yes;"We think that AI developers and deployers should be accountable for harm generated by their
products, and that products developed using AI should not enjoy exceptions to any EU laws,
whether it be discrimination, data protection, or product liability.";Yes, for all AI applications;;The EU needs to address copyright and database protections which prevent proper oversight of AI applications. Liability rules should provide incentives for openness.;
F530303;14-06-2020 12:12;Spanish;EU Citizen;Arturo;Fernández de Velasco;;;;;Spain;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;5 - Very important;;4 - Important;4 - Important;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;;4 - Important;5 - Very important;3 - Neutral;;4 - Important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;;There is a need for a new legislation;;Yes;;No opinion;;;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Not at all;;No opinion;;;Mental health risks;;No opinion;;Yes;;No opinion;;;Comments_White_Paper_14Jun20.pdf
F530302;14-06-2020 12:09;English;NGO (Non-governmental organisation);Joël;De Decker;;AEPL - Association Européenne de la Pensée Libre;188133714462-27;Large (250 or more);Belgium;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;;4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Human should be in the loop;Current legislation may have some gaps;;No;;;;Military & Law enforcement applications;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;"""Bad guys"" identification should be allowed on Court order.";Rather not;Hard Law should prevail;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F530301;14-06-2020 12:07;English;Academic/Research Institution;Christof;Schoech;;Digital Humanities im deutschsprachigen Raum (DHd-Verband);;Micro (< 10 employees);Germany;The feedback can be published with your personal information;5 - Very important;4 - Important;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;;5 - Very important;4 - Important;3 - Neutral;4 - Important;5 - Very important;5 - Very important;;3 - Neutral;5 - Very important;4 - Important;;4 - Important;4 - Important;5 - Very important;4 - Important;2 - Not important;;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;Broad integration of information on AI into school and university education to foster better understanding of possibilities, limitations and risks of AI. ;There is a need for a new legislation;;Yes;;No opinion;;;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Rather not;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;;;Yes;;No opinion;;No opinion;;;
F530300;14-06-2020 12:02;Spanish;Academic/Research Institution;Beatriz;Villarejo;;University of Deusto - Community of CREA;;Large (250 or more);Spain;The feedback can be published with your personal information;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;- Las personas, desde menores y adultas, debe formarse con programas educativos que les permitan conocer la AI y sus derechos como ciudadania europea, especialmente a los colectivos más vulnerables como pueden ser colectivos de diferentes etnias o persona;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;- Se debería centrar los esfuerzos de la comunidad de investigación e innovación de diferentes áreas, no solo de los centrados en AI.  Por ejemplo, aquellos centros o grupos de investigación que quieran  colaborar en el campo de la AI, estos pueden ser de;No opinion;No opinion;No opinion;5 - Very important;No opinion;;4 - Important;5 - Very important;5 - Very important;No opinion;5 - Very important;4 - Important;Hay que revisar en relación a los derechos de las personas personas  sobre sus datos con la finalidad de que sean ellas mismas las propietarias y manejen sus propios datos (eliminarlos, venderlas a empresas o dar voluntariamente al gobierno). Esto otorgar mayor libertad de decisión de qué poder hacer con ellas. Estos casos quedan ejemplos aquellos casos que ponen en riesgo la democracia y la vida de otras personas.;Current legislation may have some gaps;;Yes;;No opinion;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Cyber risks;;Yes;;No opinion;;Yes, for all AI applications;;;
F530299;14-06-2020 11:49;English;NGO (Non-governmental organisation);Matthias;SPIELKAMP;;AlgorithmWatch;268308931088-81;Small (< 50 employees);Germany;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;1 - Not important at all;Trust requires accountability. Accountability mechanisms must ensure that when ADM / AI-bases systems are used, fundamental rights are respected, public interest criteria are adhered to and meaningful transparency is offered. Rigorous transparency mechanisms (see our attached paper for details) have to be considered from the very outset of and throughout any strategy, as a basis of accountability.;5 - Very important;5 - Very important;3 - Neutral;;5 - Very important;5 - Very important;The coordinated plan and member state strategies should make a clear commitment to develop answers to the societal impacts of ADM / AI-based systems in a multi-disciplinary and multi-stakeholder fashion, including civil society organisations. This must entail research into how public scrutiny and democratic control can be guaranteed, as well as into the externalities of optimisation technologies. “Promoting AI uptake” has to be conditional on binding commitments guaranteeing good practice.;4 - Important;4 - Important;3 - Neutral;Public interest should define the priorities of research centers and research partnerships. Research must be conducted multi-disciplinary, including the humanities, and in a multi-stakeholder fashion, including civil society organisations. Research priorities must include the societal implications of the development and use of ADM / AI-based systems. Robust, legally-binding data access frameworks to support and enable public interest research need to be developed.;4 - Important;5 - Very important;5 - Very important;;No opinion;The proposed “skills and training“ activities of DIHs have to include mandatory modules on risks related to the use and implementation of ADM / AI-based systems, e.g. discrimination, that need to be completed in order to receive DIH support.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;The use of ADM/ AI-based systems may limit people’s access to life chances and to public goods and services, exacerbating inequality. Use of ADM / AI-based systems in societal relevant areas, without holding it accountable to public scrutiny and democratic control.;There is a need for a new legislation;;No;;;;ADM / AI-based systems that are based on biometric technologies, including facial recognition, pose a particular, serious threat to the public interest and fundamental rights as they clear the path to indiscriminate mass surveillance and other misuses.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;Indiscriminate biometric identification should never be conducted in publicly accessible spaces. ADM systems that are based on biometric technologies, including facial recognition, pose a particular serious threat to the public interest and fundamental rights as they clear the path to indiscriminate mass surveillance. Biometric identification systems in public spaces that could amount to mass surveillance therefore have to be banned uncompromisingly.;Not at all;ADM / AI-based systems are socio-technological arrangements that are part of a larger context. It is implausible that possible externalities of such systems can be fully taken into account in a label. E.g.: Even if a system matching homestay offers with tourists were entirely free of discrimination or “bias”, it can still lead to gentrification and lack of affordable housing for certain communities. So a labeling system might lead to a false sense of security.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Research into the externalities of optimisation / classification technologies must be the basis for regulatory mechanisms ensuring accountability. Accountability has to be established as a combination of regulation with efficient oversight of and liability for the use of ADM / AI-based systems.;Personal security risks;Risks of discrimination due to optimisation and classification technologies.;Yes;We are aware that this is a major challenge that has already existed for products controlled (in part) by software but it needs to be addressed. If products’ properties are determined by algorithmic processes that entail highly complex probabilistic models that change over time, especially in cases without external impuls but based on pre-determined methods, then we need to come up with new ideas to govern these products’ changes.;Yes;We recommend to follow the findings of the report “liability for artificial intelligence and other emerging digital technologies” by the Expert Group on Liability and New Technologies New Technologies Formation.;Yes, for specific AI applications;We recommend to follow the findings of the report “liability for artificial intelligence and other emerging digital technologies” by the Expert Group on Liability and New Technologies New Technologies Formation.;;AlgorithmWatch_Submission_EC-White-Paper-AI_20200612.pdf
F530298;14-06-2020 11:25;English;Company/Business organisation;Hui;Cao;;Huawei Technologies;114467111412-38;Large (250 or more);Belgium;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"a) a multilevel ecosystem of talents, ranging from top researchers to skilled workforce;b) establishment of international standards, which will promote multi-lateral collaboration & large-scale AI adoption; c)affordable access to infrastructures, especially for SMEs and startups; d)development of common data sets, including public & industrial data sets; e) investments in basic research, such as areas of XAI & HPC; f)adoption of AI technologies focusing on the most competitive sectors of Europe.";5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;"A clear dialogue should be created between existing AI agencies within MS. For example the ‘AI Made in Germany’ strategy & the French ‘National Strategy on AI’ need to connect & reinforce each other’s goals and take a proactive search for common standards. 
The role of the UK post Brexit is also important to consider. The UK’s agencies on AI e.g. the Office for AI & Centre for Data Ethics & Innovation need to be kept in orbit of the EU strategy. 
";4 - Important;5 - Very important;5 - Very important;A network of existing institutions is better than establishing a new centre. A lighthouse research centre would be useful for attracting investments & talents, while a diversified & coordinated approach would better benefit different institutions. A market-driven & demand-oriented approach would foster innovation, where a closer public-private partnership is essential. A co-investment scheme should be established so that the public sector invests in the long-term research in addition to private.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"Involvement of platform vendors would support the DIHs in various ways including knowledge transfer, testing capabilities &access to cloud infrastructures which effectively support SMEs to develop & operate AI-related applications.
Excellent testing & reference facilities based on international standards are helpful for SMEs to reduce compliance burden & concerns Since clear standards are lacking, DIHs will help practitioners to better assess outcomes and meet requirements.
";5 - Very important;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;3 - Neutral;Before regulating AI technologies and services, any concerns must avoid generalization and be considered within specific use cases to help practitioners to develop appropriate solutions. Risks for safety should be separated from concerns of ethics. Engineering & technological best practice for existing technologies as regards safety can be leveraged in the domain of AI. ;Other;Extensive specificities of AI technologies, mean existing legislation cannot address increasing concerns. New, sector-specific rules with legal certainty and clarity should aim to reduce the burden of compliance and promote user trust. Regulation should not be a barrier to innovation and effective regulation needs to be future-proof to carefully balance realistic enforcement that is possible based on the technology.;Yes;;Other;Agree. The list of high-risk sectors would have strong impacts for the selected industries, which should be identified carefully based on thorough understanding of vertical sectors and defined at a more granular level. A dynamic adjustment mechanism of the high-risk list based on the maturity of applications would deal with the fast changes of technologies effectively, and in turn accelerate the maturity of industries.;We support a tech-neutral approach, where high-risk applications are identified within scenarios, not generic technologies. For example, facial recognition may be considered high-risk when applied in surveillance scenarios as opposed to security on consumer devices, or facial detection in photo retouching apps. In addition, AI technologies in healthcare scenarios should be considered as regards decision-making, such as diagnosis assistance with image detection AI algorithms.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;;Much;A voluntary labelling system would help build trust because it reflects practitioners’ commitment towards higher safety and transparency standards. Meanwhile, design of such a labelling system should be well-thought-out, making sure that the labels are practical to evaluate for practitioners and easy to understand for end-users. We would welcome more clarity around the new legal instrument that sets out the voluntary labelling framework.;Other enforcement system;Some self-regulatory compliance mechanisms may be based on subjective judgments such as individual interpretation or understanding on explainability, whereas external, third-party, conformity assessment procedures are more reliable. For high-risk applications, some ex ante requirements may be necessary according to the application scenario. However, for most products, we expect ex post market surveillance to be sufficient and innovation-friendly.;The assessments should consider shared responsibility from activities of different roles in a multi-actor framework. The AI supply chain is complex with multiple market participants providing different components, e.g. chips, sensors,etc, where one-size-fits-all cannot meet the requirements for all participants. To make the assessments feasible, an industry-led approach may be more effective through industry associations finding and establishing clear assessing criteria and guidance.;;Risks should be considered according to legal certainty and estimated vertically within industry sectors. The Commission should consider risks from legal certainty perspectives in more specific scenarios or systems rather than listing general risk types.;Yes;"The highly dynamic nature of AI systems makes it important to focus on both development and operation of AI system. A change to the contextual integrity of AI systems may trigger a critical re-evaluation of the performance or other properties of AI systems and trigger the re-training and continuous update of AI systems. Risk assessment procedures need to be done regularly throughout AI system lifecycle. We should carefully identify the stakeholders and contextualised risks.
";Yes;A multilevel liability system is more desirable than just applying existed legal mechanism to cover emerging technologies. Moreover, the applicability of negligence or gross negligence, the evidential loss and the causality should be all discussed in particular systems of AI using before discussing the applicability or amend of existed legal framework. ;Yes, for specific AI applications;Considering the harmonization of MS law and EU law, the respective competences of EU and MS in regard of the applicable liability rules of AI should be reviewed differently. It may not be appropriated to determine directly whether the national liability rules, which had been applied in the front of national courts, should be adapted for AI that will have significant impacts at EU level for internal market, the digital transformation and the goodness of EU citizens.;A multilevel system, which includes the perspective liability rules for different AI applications and services at MS and/or EU level, will be more preferred than a simplified determination of applicability of existed legal framework for all AI service and applications.;Huawei_s_Response_to_the_AI_White_Paper_Consultation_14_06_2020.pdf
F530297;14-06-2020 11:20;Spanish;Company/Business organisation;Pablo;LÓPEZ;;Creativigo Consultores de Innovación S.L.U.;069158838558-01;Micro (< 10 employees);Spain;The feedback can be published with your personal information;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;Digital Gobernance;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Decentralize Data as much as possible;5 - Very important;5 - Very important;4 - Important;Access to resources to develop AI tools , Sandbox to test them;5 - Very important;5 - Very important;5 - Very important;1 - Not important at all;1 - Not important at all;I don´t believe on Digital Innovation Hubs , just a middleman to distribute public funds. Developing a P2P plattform it will make it clear and faster;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;First test and the regulate , within the sandbox you can manage the impact and regulate of AI policy , this sandbox it´s in 2 ways , new develops and solutions in market that shows bad uses.;There is a need for a new legislation;;Yes;;Yes;;Data Privacy, always always data belongs to people and they decide when and how it can be used, ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);general interest conditions ( pandemia, war, criminal or terrorist alert ) importante to trust in the meaning of general interest and who decide the general interest, maybe the population through digital vote can decide if a general interest condition must me activated;Very much;voluntary, you decide if you need it or not.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;we can´t stop innovation, so a sandbox can be the tool to turn On/off solutions to market. AI Observatory is Fundamental ( Like a Crybersecurity Center );Risks related to the loss of connectivity;acces to data and modification of this data will make AI take actions influenced by humans;No;this is a non stop TRL .. there always exists new situations as well deeo tedhnologies are evolutioning;Yes;same point .. there is no single use;;;Cyber Attack;
F530296;14-06-2020 11:18;French;Company/Business organisation;Romaric;Redon;;AIRBUS;2732167674-76;Large (250 or more);France;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;"Achieving excellence in AI first requires excellence to be achieved on the enablers for AI.  Key enablers are; availability of data, computing capabilities and cybersecurity.  We insist on having EU support to (i) develop sovereign hardware for AI for Cloud/HPC and for Edge deployment including embedded hardware to be used within flying (aircraft, satellite) platforms, (ii) operational provision of competitive sovereign CLOUD providing Compute, Storage and Network services.
";5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;;4 - Important;5 - Very important;5 - Very important;"New mechanisms and funding tools should be developped to promote cooperation (and avoid overlap) between different AI-related partnerships in Horizon Europe (eg AI, data and robotics; Key Digital Technologies ; Made in Europe), national and regional Digital Innovation Hubs on AI, potential future IPCEI on AI.
A coordinated network among the many existing AI research excellence centres should be prioritised over creating a new one.";4 - Important;4 - Important;4 - Important;5 - Very important;3 - Neutral;Digital Innovation Hubs specialised on AI should facilitate efficient cooperation between local expert teams from different organisations, including SMEs, start-ups but also large companies, research organisations and public partners. Clear and simple terms and conditions should be defined and implemented to accelerate awareness, exchange and testing and enable a rapid take-up.;5 - Very important;5 - Very important;4 - Important;4 - Important;No opinion;3 - Neutral;Many of the concerns are not specific to AI, dealing with inaccuracies in aviation is a common concern.  If AI is misused as a ‘big black box’ then it could endanger safety, however if the engineering of safety critical systems is done correctly by deploying good architectures with relevant monitoring and redundancies, then AI will not endanger safety.  To the contrary, the appropriate use of AI could improve safety by solving problems in critical situations which are beyond human capabilities.;Current legislation may have some gaps;;Yes;;Other;We have some concerns with the cumulative criteria approach: even in aviation it is expected that most of AI applications will not be for safety critical systems. Classifying transport/aviation as a high risk sector will slow down AI adoption and reduce our overall competitiveness in the worldwide race. We would therefore suggest to rely only upon the second criteria. In aviation there are well established airworthiness standards to assess risks and we should continue to rely on them.;In the aviation industry risk levels are well defined by airworthiness regulations (minor, major, hazardous, catastrophic).  Using an appropriate approach to engineering of safety critical systems architectures, AI can be used to improve safety of flying, especially in the context of the autonomous flying scenario.  In scenarios such as urban air mobility, autonomous flying taxi’s of the future could use AI to minimise risks rather than creating them.;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;No further guidelines or regulations are needed;;Much;If the majority of AI developers are adhering to a voluntary labelling system then it could be beneficial to facilitate reuse and the transfer of technology across different sectors and organisations by building a larger and more efficient AI developer community, used to using common methodologies and tools.  Labelling could also be extended to confirm that AI developments reach a required level of cyber protection.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;For safety critical systems in aviation an approach with a combination of ex-ante compliance and ex-post enforcement mechanisms is already applied. We don't expect this to be changed;Risks related to the loss of connectivity;;No opinion;"There are two aspects: self-changing functionality of AI systems through 1) on-line learning and 2) off-line learning to update of predictive models with new data collected during lifecycle and controlled upload of these updated predictive models.
In aviation only the second approach is investigated for safety critical systems and specific regulatory framework for safe introduction of AI are being developed for Aviation regulation with EUROCAE and SAE 
";No opinion;;No opinion;;;Airbus_position_on_the_EU_White_paper_on_Artificial_Intelligence.pdf
F530295;14-06-2020 11:14;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;No opinion;"Partnership with the private sector is vital. To ensure the widest possible expert input, we would encourage industry participation to facilitate involvement of all relevant companies.
Funding mechanisms for applied research should be kept simple and lead times should be kept as short as possible. Bringing together industry, SMEs and universities through joint research on applied topics is also key. Research should prioritise the engineering and end-use of AI-based decision systems.";5 - Very important;3 - Neutral;5 - Very important;4 - Important;5 - Very important;3 - Neutral;For certain applications of AI, large-scale experiments will be required in order to facilitate deployment. ;3 - Neutral;5 - Very important;5 - Very important;;3 - Neutral;3 - Neutral;4 - Important;5 - Very important;No opinion;We believe there is significant awareness of AI amongst SMEs. What is important is wider educatation on the realities of what AI can and cannot do.;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;3 - Neutral;Any A.I. framework must be risk-based and proportionate, as well as maintaining flexibility to ensure it keeps pace with technological developments. Some of the statements used in the table above require further clarity in order to be answered fully, as some could be misinterpreted due to the lack of context. AI is a field of research that has produced and produces key components for decision systems, and thus needs to be considered in the context of its application or end-use.;Other;It is important to consider the appropriateness/feasibility of evolving the existing regulatory framework as A.I. applications increase and diversify. Where there is a demonstrated need for new legislation/regulation, this should be backed by comprehensive impact assessments, stakeholder consultation and underpinned by international coordination wherever possible. Legal certainty is a key for industry seeking to increase innovation in their products and services. ;Other;"Each AI based application should be categorized by its impact risk. It is not known today how fast AI progress will be, yet it is possible to assess the impact of AI decisions. We would advocate that focus should be on the specific use case, not a broader application class or category, and based on risk (which is a product of probability and severity/utility) of the possible harm. 

";;;;3 - Neutral;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;No opinion;;Not at all;More detail would be required to properly assess the feasibility and practical impact of such a concept.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;No;Refer to previous comments regarding impact effect based risk assessments ;No opinion;;No opinion;;;
F530294;14-06-2020 11:10;English;NGO (Non-governmental organisation);Juliana;Wahlgren;;ENAR - European Network Against Racism;09854512780-89;Small (< 50 employees);Belgium;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;1 - Not important at all;1 - Not important at all;We openly opose the promotion of AI by the public sector. It can bring serious breaches of fundamental rights (such is on the digitalisation of migration systems, the discriminatory policing practices and on law enforcement tools);5 - Very important;5 - Very important;1 - Not important at all;5 - Very important;4 - Important;5 - Very important;There is no section on Human Rights impact and protection in the White Paper. It should be revised including fundamental rights and racial justice. Technology is not neutral and it is disproportionally afffecting racialised groups. ;5 - Very important;5 - Very important;1 - Not important at all;There should be clear Fundamental Rights eligible requirements and assessment before funding EU projects on AI system, especially when they could potentially breach rights of racialised minorities, ie Horizon 2020 - Border Control. Funding could be allocated to targeted projects to better understand how AI impacts people of colour. Our report covering the issue is attached.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;There are today harmful policing practices across EU as seen in the NL, BE,UK, driven by the availability and misuse of technology - under the political guise of crime reduction, risk-identification and risk-management. What is self-evident is that the potential for racialised policing to become hardwired, codified and concealed within police and law enforcement technology tools is alarmingly high,  increasing further the reality of racialised criminalisation&potential for wrongful convictions.;Other;"Current law does not address use of non-personal data, and collective impact of AI, such
as furthering overpolicing, surveillance, inequality.";Other;ENAR is concerned with the use of high-risk applications as they put the fundamental rights of vulnerable groups even at a greater risk.There is evidently a need to develop rigorous monitoring processes to build European wide understandings of the utility and impact of police technology on minority groups and communities and to hold law enforcement agencies and technology companies to account for the consequences and effects of technology-driven policing.;;;"Some technologies are incompatible with human rights and should be banned. EU criminal justice systems are policing minority groups according to myths and stereotypes about the level of ‘risk’ they pose, rather than their behaviour. Facial recognition tools disproportionately mis-identify people from minority ethics groups. At the borders, police use mobile phone extraction technologies as part of their
work.  Even ifvnothing is found, the data can be held and transfered into databases.";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;Biometrics are likely to exharcebate over-policing and to increase the target on racialised communities. Studies looking at biometric identifications such as speech recognition tools found significant gender and racial biases. It violates the right to privacy, the right to private life and anonymity. Further, it is not proportionate with the objective of decreasing the feeling of insecurity felt by a proportion of the population, in particular when police statistics underline that European societies, generally, have never been so “secure” factually (homicides, and other petty crimes rates decreasing steadily). Finally, no regulations will be strong enough to prevent misuse of such identification by State authorities or other parties hacking systems in place for their own profit.;Not at all;It is important that EU does not allow major compagny to regulate themselves. There is already an imbalance of power within the spectrum with tech companies. There is no evidence that “self regulation” has improved any sector in a capitalistic economy driven towards the maximalisation of profit by private actors. The EC and MS authorities have a duty to regulate this sector, in particular when it comes to ensure that Fundamental Rights are in place and implemented to safeguard the rights of all.;No opinion;We strongly recommend mandatory human rights assessment for all AI applications. Public sector mechanisms should consider a mandatory demographic oversight. Consultations with representatives of the affected racialised groups is needed for an inclusive and balanced review.;We recommend the establishment of independent ethical committees to support all compliance mechanisms in this area.;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;data-driven-profiling-web-final-1-8.pdf
F530293;14-06-2020 10:57;English;Company/Business organisation;Nikolaus;TACKE;;Prosus N.V.;891417815208-71;Large (250 or more);Netherlands;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;See our attached, more detailed position.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;See our attached, more detailed position.;5 - Very important;5 - Very important;No opinion;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;See our attached, more detailed position.;;;5 - Very important;;;;See our attached, more detailed position.;Other;"Current legislation can be supplemented by co-regulatory measures; see our attached, more detailed position.";Other;"We agree that a risk-based approach is the correct model; see our attached, more detailed position.";;;;;;;;;;No opinion;;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;;;No opinion;;No opinion;;Yes, for specific AI applications;Focus on high-risk applications. Adaptation needs depend on level of existing regulation.;;Prosus_views_on_Artificial_Intelligence_-_A_European_approach_to_excellence_and_trust__Commission_consultation_response_14.06.2020_.pdf
F530292;14-06-2020 10:43;English;Company/Business organisation;George;Ivanov;;Waymo LLC;;Large (250 or more);United States;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;We believe the Commission is right in directing its efforts towards boosting R&D funding via its 'Ecosystem of Excellence' actions on AI, which will help develop and propagate the necessary know-how for the development of the EU's automated vehicle (“AV”) industry and supply chain ecosystem. As a technology leader in AV, and as we explore growing our team and our partnerships in Europe, we look forward to playing an active role in the EU's ecosystem of excellence on AI.;4 - Important;No opinion;5 - Very important;4 - Important;5 - Very important;No opinion;;5 - Very important;4 - Important;3 - Neutral;;5 - Very important;3 - Neutral;5 - Very important;4 - Important;No opinion;;;;;;;;The AI white paper’s regulatory proposals in relation to fostering an ecosystem of trust for AI potentially conflicts with the approach developed by the Commission for implementing the updated type approval framework for AVs, and may create new and unanticipated barriers to the development of the technology in Europe. Waymo’s supplemental comments provide additional feedback in this area.;Other;Current legislation applicable to automated vehicles is fully sufficient. Both the GPSD and PLD consider a product's safety at the moment when it is placed on the market. Relevant developments after a product was placed on the market are already properly covered by the existing principles such as product monitoring obligations and related risk assessment guidelines, etc.  EU and UNECE rules being developed will specifically address issues related to automotive post-production system updates.;Other;Waymo’s supplemental comments provide specific feedback in this area for AVs.;;;;;;;;;;;;Rather not;;Other enforcement system;As with existing vehicle type approval in the EU, the updated type approval framework for AVs, in Regulation (EU) 2019/2144), will address all important safety risks presented by AVs, which by necessity will include ensuring that the AI element of such vehicles does not present unreasonable risks. There is no need for a separate EU compliance mechanism for AVs. Waymo’s supplemental comments provide additional feedback in this area.;;;;;The GPSD and the PLD consider an automotive product's safety at the moment when it is placed on the market, including self-learning capabilities, and relevant developments after a product is placed on the market are covered by the existing principles such as product monitoring obligations and related risk assessment guidelines, etc.  EU and UNECE rules being developed for AV safety will specifically address issues related to post-production system updates.;No;The PLD provides sufficient protection and strikes the right balance between claimants and defendants. Changing individual aspects (e.g. burden of proof) would inevitably jeopardize this successfully achieved and proven balance. Therefore it must be possible to avoid liability by showing that AI/AV met the reasonable safety expectation. Waymo’s supplemental comments provide additional feedback in this area.;No;;The PLD is fully harmonized in all EU Member States and thereby sufficiently protecting anybody from product defects including proper compensation. There is no basis to further limit national sovereignty by forcing additional national liability rules on EU Member States. Moreover, EU Member States have continued to demonstrate that they are able to independently adapt and shape national liability rules such as national tort law, car driver's/owner's liability, insurance, etc.;Waymo_Submission_to_the_EC_AI_Consultation.pdf
F530291;14-06-2020 10:37;English;Academic/Research Institution;Zbyšek;Stod?lka;;National Archives of the Czech Republic and State Regional Archives in Prague (research team of project Analysis of personal data processing in the archives 2019-2022);;Medium (< 250 employees);Czech Republic;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;3 - Neutral;;3 - Neutral;4 - Important;5 - Very important;EU should promote research and sustainable implementation of open-source AI applications for public sector and public accessible records and data (e.g. preserving databases, data mining, anonymization of large datasets etc.).  ;4 - Important;4 - Important;5 - Very important;5 - Very important;3 - Neutral;;4 - Important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;It will be necessary to re-evaluate the rules and length of access periods of public records and archives for specific public administration agendas to minimize the risks of personal data misuse (census, registries, court files, medical records etc.). The application of AI tools to similar data files combined with other sources brings new challenges in the protection of personal data as preserving these records is essential for future historical and other research.;There is a need for a new legislation;;Yes;;Yes;;The assessment should include the setting up of special access rules for public records and archives for research or statistical purposes (e.g. individual and general derogation access rules applied in the French archival system - dérogations générales, dérogations individuelles), especially for the web environment and possible downloading of large datasets of anonymised data which can be if linked with other public or private sources to retrace and de-anonymise the personal data back. ;3 - Neutral;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;;Much;;Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;In addition to the role of authorities for the personal data protection and in the field of cyber-security, we recommend due to unknown effects of AI use to asses a special regulation of archiving in the public interest of selected source data (databases) and source codes (e.g. algorithms) of AI applications for future scientific or historical research and statistical purposes and for possible reconstruction of its decision making (Chapter D-b).;Personal security risks;An example of a possible risk is the enabling access of data on victims of the euthanasia program in Nazi Germany, where, after extensive public debate, the German Federal Archives published names including other data (e.g. place of birth). Experts pointed to the risks that this data may be misused e.g. by insurance companies for profiling of possible family health risks. An example of positive and negative use of personal data in archives can be found in all EU Member States.;Yes;;Yes;;Yes, for all AI applications;;;
F530290;14-06-2020 10:19;English;Academic/Research Institution;Joanna;?wi?tkowska;;AGH Cybersecurity Center - AGH University of Science and Technology;;Small (< 50 employees);Poland;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Cooperation with NGOs and civil society should be included. ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"Opening data is an essential step to create an environment for an AI to thrive. High-quality data is a precondition of success and the EU must create a system of governance rules and incentives to ensure access to it. 
All above actions given above must be supported by significant funding. ";3 - Neutral;5 - Very important;5 - Very important;It would be very beneficial to coordinate efforts with like-minded partners. ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Help to develop R&D capabilities, provide advanced infrastructure ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Accountability and responsibility for the AI functioning must be at the center of the discussion. ;There is a need for a new legislation;;Yes;;Yes;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);Biometric Identification Systems use should be restricted either to places that generally already perform identification of everyone (e.g. airports). It should not be used in general events or public gathering (i.e. no face detection for prevention), especially without public announcements that such identification takes place. ;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;The RA procedures ought to be open and transparent. ;Yes;It should be clear that the legislative framework applies also to services (based on AI technology) and it should cover whole life cycle of the products and services. ;Yes, for all AI applications;;All rules adopted at the national level should be harmonized across the whole Union to avoid fragmentation. ;AGH_-_WP.pdf
F530289;14-06-2020 10:03;English;NGO (Non-governmental organisation);Fredrik;HEINTZ;;Swedish AI Society;653642831085-59;Medium (< 250 employees);Sweden;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;Yes, please see our detailed response.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Yes, please see our detailed response.;5 - Very important;5 - Very important;4 - Important;Yes, please see our detailed response.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;Yes, please see our detailed response.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;;Current legislation may have some gaps;;Other;The requirements should be proportional to the risk.;;;;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Please see our detailed response.;;;Yes;;No opinion;;No opinion;;;SAIS_EC_AI_Whitepaper_Response_-_Final.pdf
F530288;14-06-2020 10:03;English;Company/Business organisation;Aliki;FOINIKOPOULOU;;Salesforce;991195317279-06;Large (250 or more);United States;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Promoting RESPONSIBLE adoption of AI is key. We encourage the Commission to include metrics on AI adoption by the private and public sectors in its next DESI report. Increased awareness of the benefits of responsible AI can have a significant impact on uptake. Salesforce’s latest Technology Trends report shows that AI strategies are among the most poorly defined within organisations, and skill sets are notably low. This can be a significant barrier for businesses to deploy AI in an ethical way. ;No opinion;No opinion;5 - Very important;No opinion;5 - Very important;4 - Important;The International Aspect is of great importance. We encourage the Commission to participate in the work of the World Economic Forum's (WEF) Centre for the 4th Industrial Revolution, which conducts important work with governments and industry representatives in areas such as use of AI in the public sector. Regarding the European Data Space, this could be of great importance as long as it remains open to global actors that respect European values and rules. ;No opinion;No opinion;No opinion;On ethics, we recommend that the Research/AI community begin documenting best practice for things like how to measure bias in different types of AI systems, how to remove it, creating and open-sourcing representative/unbiased training data sets, defining and measuring fairness in different applications of AI, and creating responsible data visualisation for model explainability. ;5 - Very important;No opinion;5 - Very important;5 - Very important;No opinion;SMEs often don’t have large enough or high quality/representative training data sets. SMEs are also less likely to be able to afford to hire data scientists or ethicists, leaving them dependent on third parties to help them. Therefore, it will be very important to ensure that SMEs are educated about responsible development and uptake of AI, and become familiar with the available AI products and services that will help them in that context. ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;These concerns are not unique to AI. Context is crucial to assess the severity of the impact. Regulation should focus on understanding the source of error, especially if it's causing harm, so that it can be corrected (e.g., remove any incorrect data or inferences that were made about the individual and then correct the underlying model or training data that could harm other people). ;Other;Current legislation is to a large extent sufficient. Any identified gaps should be addressed primarily through guidance (e.g., on surveillance, including facial recognition), and only where necessary through targeted changes to existing rules. ;Yes;;Other;The two-level approach is appropriate to address the most pressing concerns resulting from the use of AI. Additionally, we suggest that AI applications that fully replace human activity be more closely scrutinised, whereas applications that simply augment human activity (e.g., helping humans be more productive) should be considered of lower risk. ;It is not easy to single out one application or use of AI, because of the many different contexts in which AI is used. The technology is not good or bad per se – it is what we do with it that matters. Applications with the potential to violate the UN’s Universal Declaration of Human Rights could be considered the most “high-risk”. ;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;4 - Important;No opinion;Per Salesforce’s Acceptable Use Policy, (https://www.salesforce.com/content/dam/web/en_us/www/documents/legal/Agreements/policies/ExternalFacing_Services_Policy.pdf) our customers are not allowed to submit images of individuals for the purposes of creating or analysing biometric identifiers, such as face prints or fingerprints or scans of eyes, hands or facial geometry, to our services.;Rather not;We would welcome further concrete proposals by the Commission, including on who will be the monitoring body/responsible authority to build the scheme and perform compliance checks. Self-regulatory models in the area of data protection based on international standards and certifications have been developed successfully, and the process could be followed here as well. The HLEG’s checklist could be used for the same purpose instead of designing a new scheme from scratch.;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;We welcome more detail from the Commission on the proposed ex ante conformity assessment framework, in particular on who would be the responsible authority. This authority should have the appropriate resources, including personnel that has received the necessary training and has a good understanding of the specificities of the sector in which the app is deployed. ;;We believe that the current product safety legislation adequately covers the great majority of the risks stemming from the use of AI. Some specific areas where further regulation could be considered are deepfakes and private use of surveillance technology. ;No;;No;Before the Commission decides to review the Product Liability Directive, it should conduct a thorough analysis of existing legislation and identify any gaps, clearly specifying the risks that are not covered by the current rules. In any case, we suggest that any amendments on the current rules should be applicable to high-risk applications, per Section 2 in the questionnaire.;No opinion;;If national liability rules were to be adapted to the operation of AI, ensuring coherence and a uniform approach across Europe would be of the utmost importance.;Salesforce_response_European_Commission_public_consultation_AI_White_Paper.pdf
F530287;14-06-2020 09:52;German;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;4 - Important;No opinion;3 - Neutral;3 - Neutral;;4 - Important;4 - Important;2 - Not important;2 - Not important;4 - Important;2 - Not important;;4 - Important;5 - Very important;2 - Not important;;2 - Not important;4 - Important;No opinion;2 - Not important;2 - Not important;;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;;There is a need for a new legislation;;No;;;;Jede Anwendung, die aufgrund von gesammelten persönlichen Daten die Vergabe von Arbeit,  Leistungen des Gesundheitssystems, Strafmaßnahmen u.Ä. reguliert;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;No opinion;;Other enforcement system;"Nicht nur ""high-risk applications"" müssen geprüft werden, auch andere Anwendungen von KI, die Grundrechte gefährden können, und das jedenfalls vor Marktfreigabe durch eine unabhängige, kompetente Institution";;Personal security risks;;Yes;;Yes;;Yes, for all AI applications;;;
F530286;14-06-2020 09:52;English;Company/Business organisation;Raquel;RESENDES;;Bayer AG;3523776801-85;Large (250 or more);Germany;The feedback can be published with your personal information;5 - Very important;4 - Important;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;Bayer believes on the value of multi-stakeholder collaboration to advance health research and improve standards of care as well as health outcomes. Bayer asks the European Commission to continue to foster patient-centric projects under investments programmes such as Horizon Europe and Digital Europe to unlock the potential of the Digital Transformation in healthcare in Europe.;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;Bayer considers very important to further strengthen links between networks of leading educational institutes and the private sector to secure mutual learning and knowledge transfer. Furthermore, European data spaces will help to enable focus. It is then important to design a system able to allow exchange and integration of data beyond the EU. An exclusively European data space will limit its impact and foreclose opportunities to collaborate with data intensive advanced economies. ;5 - Very important;5 - Very important;5 - Very important;Bayer believes that priority should be also given to current societal challenges such as the consequences of the COVID-19 pandemic in healthcare systems and in society at large. The use of the existing structures of excellence should also be integrated to continue to make use of existing expertise (e.g. CLAIRE and ELLIS).;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;Bayer believes that multi-stakeholder collaboration is key to unlock the Digital Transformation in Europe. The Digital Innovation Hubs are an opportunity to connect networks of SMEs, larger private organisations, academia and the public sector. These Hubs may focus on developing and sharing machine readable data sets which are critical to all aspects of AI and machine learning.;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;For the healthcare sector and for medicines, it is important that AI systems are built with consideration to reduce bias and under-representation of marginalized groups in large data sets. Additionally, for AI involving clinical diagnosis, it is important that physicians can rely on AI with reproducible results and in different clinical contexts. In the United States, the Standards for Reporting of Diagnostic Accuracy initiative may provide a framework which can be introduced in the EU.  ;Other;"Bayer believes that the European Commission should enhance sector specific requirements in existing legislation rather than designing new additional legislation.For the healthcare sector, it will be important to establish frameworks to enable access to publicly available healthcare data (e.g. including the potential for pandemic tracing; and rare diseases); and it may be important to at a minimum monitor commercial data sets and how these are priced in the market (e.g. price gouging potential). ";Yes;;Other;Bayer agrees with the general approach by the European Commission, however, the determination of what is a high-risk AI application should be clear and easily understandable and applicable for all parties concerned. A risk-based approach is the right option, as long as there is a clear scope, and the number of high-risk applications are not out of proportion.  ;Discrimination and the use of opaque AI methods to implement non-ethical procedures are of concern. Also, sector specificity should always be considered to render the definition of high-risk relevant in practise. Many applications with no pose risk (uncritical as per the White Paper), thus resist putting individual sectors in a high risk category. ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);Bayer believes that the use of biometric identification should only be allowed when there’s a clear benefit to society such as in the identification of criminals, to prevent the spread of diseases and in humanitarian aid.  ;No opinion;A voluntary labelling system could support the increase of trust by society. However, the proposed concept should be further analysed.  ;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Bayer believes that the European Commission should make use of the existing control obligations currently in regulatory regimes including data privacy, information security, ethical use and avoid adding another layer of control. A holistic AI review would add value instead of being just an administrative task. Also, AI applications stemming from outside the EU should be governed by the same principles to generate a level playing field.;Mental health risks;"Regarding cyber security, it would be helpful to consider what enforcement or retaliatory measures would be in-play, if for example a nation-state were to use AI to hack a private company. Also, in general, an AI specific risk regime is likely to create extra levels of review without proportionate benefit. Bayer believes it is more efficient to have a technology neutral risk regime applicable to any IT application, and only if needed, AI specific elements should be added.
";Yes;Only in cases of substantial changes. To note that the term is so vague that it needs further analysis to be rendered practical and if possible addressed in existing frameworks such as IT security or privacy.;No opinion;;No opinion;;Bayer believes that supplement sector specific rules and regulations to a specific risk in a practical context is more efficient than creating an AI specific regulatory framework. ;
F530285;14-06-2020 08:29;English;NGO (Non-governmental organisation);Nicolas;Moës;;The Future Society;473310732515-30;Small (< 50 employees);United States;The feedback can be published with your personal information;3 - Neutral;5 - Very important;4 - Important;4 - Important;3 - Neutral;3 - Neutral;On the “international aspects” of the ecosystem of excellence: the EU’s world-class testing facilities, high standards, and purchasing power gives it leverage over US and Chinese developers. Alongside the European Commission’s International Alliance for Human-Centric AI, these EU testing facilities could explicitly aim to bring foreign tech giants and startups within the EU’s regulatory sphere of influence.;4 - Important;5 - Very important;3 - Neutral;3 - Neutral;4 - Important;5 - Very important;National testing authorities’ governance structure must be carefully designed. The internal and external incentives the authorities generate will be a major determinant of the European Approach’s success. Governance design is therefore crucial, perhaps with peer reviews of each facility’s activities.;4 - Important;5 - Very important;3 - Neutral;Experimentation, testing & auditing authorities developed for the European Approach to AI should developm integrate and support relevant research & innovation programmes. Relevant programmes are those that help advance or develop the authorities’ experimentation, testing & auditing activities or make them more cost-effective.;3 - Neutral;5 - Very important;4 - Important;4 - Important;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;"An additional concern is the possibility that AI algorithms be compliant with the European Approach but not fully “loyal” to their consumers and instead subtly orient their outcome to marginally benefit third parties (e.g. algorithms lightly rerouting an itinerary to pass by an advertiser’s shopping centres; B2B IT diagnostic tools recommending an advertiser’s fixes without disclosing it; etc.) It is similar to the concept of platform loyalty already discussed in the French policy community.";There is a need for a new legislation;;No;;;;AI uses that are Critical Systems should be considered high-risk. Critical Systems are engineering systems that are critical to safety (e.g. chemical manufacturing plant control system), mission (e.g. power grid management systems), business continuity (e.g. ERP systems) or security (e.g. cybersecurity tools). For an overview of Evolving Critical Systems like those enabled by AI, see [Hinchey, M. & Coyle, L (2010) “Evolving Critical Systems: A Research Agenda for Computer-Based Systems”, IEEE];4 - Important;3 - Neutral;4 - Important;5 - Very important;4 - Important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Experimentation, testing & audit capabilities would enable the European Commission to assess compliance and to make informed decisions when it comes to the potential harms that directives (or absence thereof) on AI could have on society. ;Mental health risks;;Yes;Experimentation, testing & audit capabilities would enable the European Commission to assess risks and to adapt its risk assessment framework as the technology, market applications and citizens’ preferences evolve. ;Yes;In addition to factoring in the risks to mental health, the amended framework should also consider more agile and cost-effective enforcement mechanisms that enable SMEs, local governments & authorities, start-ups, self-employed individuals, NGOs, researchers, etc. to develop and deploy AI without lowering the EU’s quality standards.;No opinion;;;The_Future_Society_Contribution_to_Consultation_on_the_White_Paper_on_AI_June_2020.pdf
F530284;14-06-2020 08:16;English;Academic/Research Institution;Wayne;Holmes;;Nesta;384071617717-17;Large (250 or more);United Kingdom;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;No opinion;No opinion;3 - Neutral;It is necessary to establish domain specific guidance and regulations as well as general guidance and regulations for the ethical use of AI. In any case, the adoption of AI should not be 'promoted', not until AI is by design under the full control of humans and properly addresses genuine human values (e.g., is fair, transparent and accountable).;5 - Very important;5 - Very important;2 - Not important;4 - Important;5 - Very important;4 - Important;It is necessary to establish domain specific guidance and regulations as well as general guidance and regulations for the ethical use of AI.;5 - Very important;5 - Very important;No opinion;It is necessary to establish domain specific guidance and regulations as well as general guidance and regulations for the ethical use of AI in research and innovation.;No opinion;No opinion;No opinion;No opinion;No opinion;It is necessary to establish domain specific guidance and regulations as well as general guidance and regulations for the ethical use of AI for use by SMEs.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;5 - Very important;The concerns listed above are general concerns. However, it is also essential to account for domain specific concerns, which differ according to the domain in which the AI is being applied (i.e., there is a 'collision' between the ethics of AI and the ethics of the domain). For example, when AI is applied in education, the developers, researchers, policymakers etc. must account for the ethics of education (e.g., choice of pedagogy) as well as the more general ethics of AI (e.g., privacy).;There is a need for a new legislation;;Other;Yes, but we need to be clear about what is meant by 'high risk'. For example, in education, the use of AI is unlikely to lead directly to death. However, it could seriously impact the child's life outcomes - and hence is high-risk and must be addressed.;;;There are many. My particular concern is the use of AI in education without proper regulations and oversight.;No opinion;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Rather not;We do not believe that a labelling system that is only voluntary would be effective.;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;Effective, easily accessed, and easily applied robust guidance.;Mental health risks;It depends on the domain in which the AI is applied. It is not possible to answer these questions fully without reference to a domain. In education, there are multiple risks including (but not limited to) that children might be discriminated against, that their consent might not be properly secured, and that their learning might be seriously negatively impacted.;Yes;;No opinion;;Yes, for all AI applications;;;White_Paper_on_AI_-_A_European_Approach_-_Nesta_response.pdf
F530283;14-06-2020 07:58;English;NGO (Non-governmental organisation);Gry;Hasselbalch;;DataEthics.eu;;Micro (< 10 employees);Denmark;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;"- Support market innovation and research in trustworthy AI components (legal, ethical and robust). 
- Integrate HLEG ethics guidelines and assessment list in ERCEA and Horizon2020 standards for ethics checks/reviews of EU funded research and innovation. I";4 - Important;3 - Neutral;3 - Neutral;5 - Very important;5 - Very important;5 - Very important;Integrate guidelines, support and innovation of trustworthy AI components in all of the above mentioned areas. We do not have thriving trustworthy AI innovation yet, that is something member states need to develop and that should be invested in. ;5 - Very important;5 - Very important;3 - Neutral;Support the establishment of an independent lighthouse research centre and/or a network of AI research centres and thinktanks specialising in trustworthy AI components (such as AI ethics, ethics and social impact assessments, technical implementation of the GDPR, privacy enhancing technologies, state of the art anonymization techniques etc.). Support with funding civil society organisations working with AI ethics, citizen rights, citizen AI awareness and skills development. ;4 - Important;4 - Important;4 - Important;3 - Neutral;4 - Important;Support and promote innovation and SMEs that focus on developing trustworthy AI components (ethics) in technical design, skills development, business model and organisational structure.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"1. When AI is considered a solution in and by itself, a ""complete system""  and a societal goal. 2. Areas where AI as the ""approach"" has severe ethical and rights implications (e.g. mass social control, childhood). 2. Private opaque commercial interests that overrule human interests in the public AI infrastructure. 3. Autonomous unaccountable decision-making regarding citizens' lives 4. AI used to reinforce power imbalances between stakeholders in democracy. ";Other;"A Directive on public procurement of Trustworthy AI-based services and solutions for the public sector.
GDPR and fundamental rights legal frameworks must be strengthened and supported. Competition law must be strengthened to curve data monopolisation and data asymmetry. ";No;;;;AI used in connection with the evaluation, monitoring and tracking of children. AI for critical public services to citizens. AI used in judicial system without human intervention. Predictive policing. Mass social scoring and social control. Autonomous lethal weapons. ;No opinion;No opinion;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Rather not;An AI system's potential risk cannot be decided by voluntary labeling at the beginning of its life cycle. A risk, ethical and social implications assessment of an AI system should be conducted throughout the life cycle of an AI system ensuring also potential independent review. ;A combination of ex-ante compliance and ex-post enforcement mechanisms;;A due diligence process that includes a data ethics and fundamental rights assessment and independent review should be part of all stages of an AI systems life cycle, design, application and deployment.;Mental health risks;"Addressing any adverse impact on human beings or groups of people, their rights
and freedoms, on democratic institutions and processes, and on society and the
environment.";Yes;Inclusion of relevant internal experts to perform risk assessments and consultations with stakeholders and rights holders in order to expose potential negative impact on or harm to people or groups of people due to their age, gender, ethnicity, sexual orientation, disability or other characteristics. Stakeholder dialogue is especially relevant where gaps in information exist.;No opinion;;Yes, for all AI applications;;;Dataethics_White_paper_Public_Procurement_AI_20042020.pdf
F530282;14-06-2020 07:26;Dutch;Company/Business organisation;karolien;de Bruine;;Oost NL namens partners Th!nk East Netherlands;;Medium (< 250 employees);Netherlands;The feedback can be published with your personal information;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;Overzie de gehele transformatie door digitalisering en inzet van AI in de toeleveringsketen. Voor toepassing in en adaptatie in KMOs gebruik maken van de  bestaande regionale infrastructuren. Verbinding met aanpalende key enabling technologies. Investeren in een groene data infrastructuur. ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;adresseer onderwerpen zoals data eigenaarschap in relatie tot verwaardingsmodellen van data. Bouw sterke Europese innovation partnerships op AI en ingebed in de verschillende apllicatiedomeinen. Focus naast hoog technologische ontwikkeling op de brede uitrol van KI toepassingen in het bestaande KMOs en maak hierbij gebruik van hun sterke regionale inbedding.  ;3 - Neutral;5 - Very important;5 - Very important;"Een publiek-privaat partnerschap voor industrieel onderzoek én implementatie, dat vraaggestuurde innovaties ontwikkeld. Verbind de bestaande regionale excellente onderzoeks- en toegepaste onderzoeksinstellingen en ondersteun deze met lange termijn investeringen en onderzoeksprogramma's.  Verbind talentontwikkeling aan Europa. 
Versterk het eigenaarschap van de industrie door deelname aan een Europese PPS, inclusief paricipatie vannuit KMOs. ";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"Benut de regionale inbedding van (E)DIHs en de ontwikkeling van kennis en kunde op een specifieke (niche) toepassingsgebied, zoals Industrie of health. 
De relaties leggen door de DIH tussen KI toepassingen en de beoogde bijdrage aan maatschappelijke uitdagingen in hun communicatie en voorlichting aan het KMO. 
Betrek ook bedrijven uit de ICT sector. Maak de DIH verantwoordelijk voor het voeden van de Europese Commissie bij de verdere ontwikkelingen van KI op basis van opgedane ervaringen. ";5 - Very important;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;5 - Very important;Hier gaat het om het volgen van een mensgerichte methodologie en kader als uitgangspunt voor de ontwikkeling en uitrol van KI toepassingen. Laat ruimte voor nieuwe ontwikkelingen door reguliere updates in het kader voor te stellen. Definieer kader voor data eigenaarschap i.r.t. businessmodellen. ;Current legislation may have some gaps;;Yes;;Yes;;;3 - Neutral;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;;Very much;Draag zorg voor een eenduidig Europees labelling system. ;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;;Mental health risks;;Yes;accommodeer nieuwe innovaties door tussentijdse updates van assessment procedures. ;No opinion;;No opinion;;;AI_paper_Think_East_Netherlands_S.pdf
F530281;14-06-2020 05:10;English;Academic/Research Institution;Amba;Kak;;AI Now Institute, New York University;;Small (< 50 employees);United States;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;3 - Neutral;3 - Neutral;4 - Important;3 - Neutral;3 - Neutral;See section on SMEs on page 3 of our lengthy submission;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;See our lengthy submission attached;Other;See our submission attached on strengthening existing EU legislation, while introducing additional safeguards and protections. ;No;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Not at all;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;"See section on ""Algorithmic Impact Assessments"" in our lengthy submission attached";;"See section on ""safety"" in our lengthy submission attached";Yes;;Yes;"See section on ""safety"" in our lengthy submission attached";No opinion;;"See section on ""safety"" in our lengthy submission attached";AI_Now_Submission_European_Commission.pdf
F530280;14-06-2020 04:07;English;Academic/Research Institution;Jessica;Cussins Newman;;The UC Berkeley Center for Long-Term Cybersecurity (AI Security Initiative);;Small (< 50 employees);United States;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;Building an ecosystem of excellence requires working with Member States, but also engaging with the broader international community. For example, the proposed lighthouse research centre would benefit from collaborations with the global AI community, leading to increased resources, prestige, and impact. The adoption of AI by the public sector is also key to excellence, but only to the extent that it is done responsibly and with accountability, given the stakes and potential impact of missteps.;5 - Very important;5 - Very important;3 - Neutral;4 - Important;5 - Very important;4 - Important;Coordination across all of these areas is important, but is especially important for testing facilities given the need for consistency in application across States. This will play a key role in the ability of the testing facilities to develop internationally respected references and standards. In addition, coordination will be critical for States to evaluate the effectiveness of the regulatory framework and adapt practices as needed to accommodate learnings as well as technological advances.;5 - Very important;4 - Important;4 - Important;The EU has a competitive advantage globally in its standards-setting abilities and its vision for accountable and trustworthy AI development. This values-driven approach is key to attracting and retaining world-class talent and to generally strengthening the research and innovation community. These commitments will help facilitate research partnerships with allied countries around the world, which will further the successes and beneficial impacts of EU AI development.;3 - Neutral;5 - Very important;5 - Very important;4 - Important;3 - Neutral;The specialised Digital Innovations Hubs can play a valuable role in focusing on issues too often neglected in industry including AI safety, security, and social impact. Moreover, Digital Innovations Hubs can encourage the incorporation of any voluntary labeling or other trustworthy AI certification mechanism from the design stage through implementation. They should also carefully record developments and share failures as well as successes to support overall growth of the industry.;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;AI technologies facilitate the creation of synthetic media and disinformation, filter bubbles, and pervasive surveillance systems, all of which can have corrosive impacts on core tenets of democracies including equality, accountability, free and fair elections, and control of the abuse of power. Additionally, the drive to control and lead the development of AI technologies globally can exacerbate geopolitical tensions in potentially dangerous and destabilizing ways.;There is a need for a new legislation;;Other;Compulsory requirements only for high-risk applications in a binary system may be insufficient. AI risks are broad and mutable, making determinations at the outset a challenge. This is particularly true for complex interactions between different AI systems, and for complex human-AI interaction, both of which can produce unforeseen and tragic outcomes such as the 2010 “flash crash,” or the 2019 Boeing 737 Max crash. The inclusion of “exceptional instances” is an important and necessary caveat.;;;"The AI applications that are most concerning include those that automate and exacerbate uses of force and repression including autonomous weapon systems and biometric surveillance; and those that are especially uncontrollable and unsafe due to vulnerabilities, advanced and uninterpretable capabilities, or mis-aligned goals. Importantly, AI technologies will continue to advance and it is critical that the European Commission considers likely capabilities within the next 5-10 years.";4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;Biometric identification systems in publicly accessible spaces pose immense threats to privacy and civil liberties. Moreover, persistent racial and gender disparities in the accuracy of these systems renders their use in public spaces problematic in societies that value fundamental and human rights. Use of these systems should be prohibited unless and until these and other challenges can be addressed by meaningful technological and regulatory developments. ;Very much;If only one tier of obviously high-risk AI applications are to have regulatory oversight in the EU (as opposed to several more nuanced tiers) then there will likely be remaining gaps in the governance of the AI landscape. A voluntary labeling system may not be sufficient to fill these gaps (and additional methods should be considered,) but it will be extremely useful and important compared to an alternative of no labeling or oversight system. Clear communication with users should be prioritized.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;The combination of ex-ante compliance and ex-post enforcement is critical for the governance of AI systems in particular, given the unique ability of self-learning AI systems to adapt and change once deployed. Ex-ante compliance is needed to ensure that risky products do not flood the market, while ex-post mechanisms are needed to incentivize responsible oversight. These mechanisms should include ongoing monitoring by producers and the requirement of periodic assessments by third parties.;Mental health risks;Complex AI interactions, for example between numerous AI systems, may need to be assessed (in addition to looking at them in isolation.) Additionally, convergences between AI technologies and other consequential technologies (for example biotechnologies, neuro-technologies, and nuclear technologies) can create novel risks that are not well-understood. Such instances would benefit from greater oversight and legal certainty.  ;Yes;Risk assessment procedures that only occur once will be an insufficient mechanism for AI products and services subject to important changes during their lifetime.These self-learning systems will need to be reevaluated at designated intervals. This will be harder for products that have learned from their unique context using personal information, but is no less important. ;Yes;Producers of AI systems that design the systems such that they purposefully continue to learn and evolve should remain liable for any damages caused in deployment, even if the damages result from unexpected or unanticipated changes to or malfunctions of the system. This liability structure will encourage monitoring of AI products and services by those most knowledgeable about them and best positioned to remedy any issues.;Yes, for all AI applications;;To ensure proper compensation for damage from AI accidents and risks, current national liability rules should be adapted to account for the possibilities of a relative lack of transparency, improper communications about updates and changes, and unique or disproportionate harm inflicted on particular communities. Improving communications about existing and potential risks to users of AI products and services should be prioritized. ;
F530279;14-06-2020 03:40;English;NGO (Non-governmental organisation);Jared;Brown;;Future of Life Institute;;Small (< 50 employees);United States;The feedback can be published with your personal information;4 - Important;5 - Very important;3 - Neutral;3 - Neutral;4 - Important;4 - Important;Instead of focusing on having “at least one digital innovation hub per Member State” for SME action item, the EU should be ensuring it has at least one specific hub for specialty areas of AI, e.g., on AI safety, on AI in robotics, etc. The public-private partnerships action item is especially needed to “ensure coordination of research and innovation in AI” such that the public sector is able to identify and address gaps in private research agendas.;3 - Neutral;5 - Very important;3 - Neutral;4 - Important;4 - Important;3 - Neutral;"Our answers reflect our views on the relative importance of ""strengthening coordination"" in key topics across the EU (harmonization), not the importance of the area. E.g., the EU approach to trustworthy AI depends on having a coordinated, world class testing capacity for evaluating the risk of AI applications, whereas different approaches to promoting uptake and improving data spaces can be experimented by Member States.
";4 - Important;4 - Important;5 - Very important;We believe the Commission should create a public-private partnership on AI safety research and innovation, including for testing AI applications prior to and after they have been placed on the market. This could include coverage of continual self-learning AI systems, as described in our additional comment. Such a partnership should be established transparently and subject to oversight to avoid becoming ‘regulatorily captured.’;3 - Neutral;5 - Very important;3 - Neutral;4 - Important;3 - Neutral;;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;3 - Neutral;As explained in greater detail in our additional comments, FLI also has particular concerns and recommendations for ensuring the safety and proper liability for continual self-learning AI systems. We also explain that the concept of AI ‘loyalty’ should be considered as a risk that can cause immaterial harm. ;There is a need for a new legislation;;Other;We believe that new compulsory requirements should be applied differently to systems based on their relative risk. However, as described in greater detail in our written comment, we believe the Commission should use a multi-tiered risk assessment with greater nuance. Such a framework will create a better system that is “clear and easily understandable and applicable for all parties,” as desired by the Commission.  ;;;In this White Paper, the Commission has excluded consideration of “the development and use of AI for military purposes.” However, to answer this question honestly, the application of AI for military purposes, including autonomous weapon systems, is the “most concerning.” Future advanced AI systems may also be concerning if they are especially uncontrollable and unsafe due to vulnerabilities, advanced and uninterpretable capabilities, or misaligned goals.  ;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;Please see our additional written response, but FLI is concerned that 1) companies may not “volunteer” to participate as much as anticipated, and 2) it may be difficult to label AI systems for trustworthiness in an easily recognized and understood way for consumers. Thus, we support additional methods being considered beyond only a voluntary labelling system. ;A combination of ex-ante compliance and ex-post enforcement mechanisms;;We offer recommendations in our written comment for improving both ex-ante compliance and ex-post enforcement. A combination is especially necessary for systems that continue to learn ex-post, labeled as continual self-learning AI systems in our recommendations. Recommendations for ex-post include: creating an obligation to monitor (akin to pharmacovigilance), restricting the development and later defect defences for liability protection, and requiring periodic reassessments of AI systems. ;Mental health risks;AI systems can interact with other AI systems in unpredictable ways, causing material and immaterial harm. AI systems will also be applied to other powerful technologies in ways that could be difficult to assess the safety of in isolation, such as with biotechnologies, neuro-technologies, and nuclear technologies. Thus, the Commission should consider expanding legal certainty on how interactions between AI systems with other technologies will be assessed for their safety.;Yes;"Please see our additional written comments. We recommend the Commission include consideration of societal harms in risk assessments, create a multi-tiered, as opposed to binary, risk assessment framework; and evaluate supplemental non-regulatory methods to accompany a voluntary labelling scheme.";Yes;As described in our additional written comment, we believe legal certainty needs to be provided on the treatment of software as a product or service. We also believe limitations should be placed on the use of the development risk defence and the later defect defence for liability protection on continual self-learning AI systems.;Yes, for all AI applications;;The Commission sought “views whether … it may be needed to mitigate the consequences of complexity [of AI] by adapting the burden of proof required by national liability rules.” At this time, we do not have a definitive opinion on this issue. However, we acknowledge it is critically important for the Commission to carefully consider this topic, possibly through additional funded expert research, akin to the work of the German Data Ethics Commission on liability for algorithmic systems. ;Future_of_Life_Institute___Additional_Comments_on_European_Commision_White_Paper_on_AI_.pdf
F530278;14-06-2020 02:23;English;EU Citizen;TRAVIS WAINE;VOWINKEL;;;;;United Kingdom;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;5 - Very important;1 - Not important at all;3 - Neutral;;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;4 - Important;;5 - Very important;5 - Very important;1 - Not important at all;If public funded all rights should be maintained in the public domain;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;AI can kill, AI can take over;Other;Legislation will not keep pace with AI development so needs foresight and permanent review. Laws need to be in place. Rules and regulations are too risky.;Other;Not rules and regulations but laws need to be in place. ;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;Against Human rights. Do not provide value to public. Lead to abuse by government agencies. Innocent until proven guilty not in reverse;;Labelling must be done for ALL AI;Other enforcement system;If high-risk why use?;Keep it switched off.;Mental health risks;There should be no risks if product has been tested thoroughly. However profit leads to risks as we all know. Strong legal accountability must be in place. We are not discussing an egg timer here.;Yes;;Yes;;Yes, for all AI applications;;There must not be damage or danger.;
F530277;14-06-2020 02:23;English;Non-EU Citizen;Ronald;Bodkin;;;;;Canada;The feedback can be published with your personal information;4 - Important;4 - Important;5 - Very important;3 - Neutral;4 - Important;4 - Important;;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;3 - Neutral;;4 - Important;5 - Very important;5 - Very important;Contribute to open source AI software to allow EU developers to build on a strong foundation and reduce the control of open source by large technology companies from outside the EU.;3 - Neutral;3 - Neutral;4 - Important;5 - Very important;4 - Important;;4 - Important;5 - Very important;4 - Important;5 - Very important;3 - Neutral;4 - Important;The use of AI by a small number of technology companies with control of the public conversion is having a negative impact on the democratic foundations of information and diverse opinions as well as damaging the long-term psychological health of many people notable the young, fostering digital addiction. The EDPS Opinion on online manipulation and personal data elaborates on some of these harms.;There is a need for a new legislation;;No;;;;I see media and technology as the most concerning AI application due to the scale of negative impacts of AI that has already occurred. AI algorithms are having a significant impact on the media, political, social and psychological environment and in shaping and manipulating consumer behavior. The damage caused by voter manipulation and digital addiction are specific examples of this risk.;4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;4 - Important;Other special requirements in addition to those mentioned in the question above should be imposed (please specify);There should be strong guarantees about limits on data retention, restrictions on access and accountability for degree of access and usage and assessment of impact even for GDPR-compliant uses of face recognition such as government uses like for public security or public health. Measuring impact is important to understand the benefit that is resulting from this compromise of privacy.;Rather not;There should be a certification requirement for regulated applications including third party verification that should applied both to voluntary labelled systems and high risk systems. I worry that voluntary labelling will see little compliance from entrenched companies with market power that would prefer to stay under the light touch self-regulated approach.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;I would recommend requiring private 3rd party auditors that both assess risk and for high-risk applications, compliance in both ex-ante and ex-post scenarios. This would allow recruiting the necessary skills to perform competently and allow for a variety of state of the art techniques for assessment, while allowing a market to develop so those delivering AI applications can choose among effective approaches.;Mental health risks;The risk of manipulation - if an AI algorithm has an objective that is different from what is claimed is also one to address, e.g., a search engine that favors low quality profitable items rather than the best matches in an organic search. It can be very difficult for customers to understand what AI algorithms are optimizing for, so minimally requiring clear labeling and adherence to claimed objectives are important.;Yes;Private audit firms could also be mandated to provide AI risk assessments - again to allow market mechanisms to bring the necessary skills and innovation to bear in a fast moving space.;Yes;;Yes, for specific AI applications;I think it's important to provide regulatory clarity for how to assign responsibility and liability among supply chain providers, e.g., developers of AI components, assemblers of AI systems, customizers of AI systems, and those offering final products that use these systems.;;Feedback_on_EU_AI_White_Paper.pdf
F530276;14-06-2020 01:52;Spanish;EU Citizen;David;Ruiz;;;;;Spain;The feedback can be published with your personal information;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;;;Current legislation may have some gaps;;Yes;;Yes;;;5 - Very important;4 - Important;5 - Very important;4 - Important;4 - Important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;Los que afectan a la protección de derechos fundamentales  y otros derivados de reconocimiento facial, uso de datos biométricos involuntarios, identificación automática, sistemas clasificatorios, armas autónomas para no dejar indefensos a los ciudadanos a la hora de proteger sus derechos y su seguridad. Hay que resolver el problema de las licencias de software con cláusulas “as is” por las que el usuario se adhiere y asume el riesgo de su mal funcionamiento y evitar datos sesgados;Yes;La implantación de planes de evaluación de los riesgos, instaurando controles estrictos para guiar el desarrollo y el uso de los sistemas de IA, asegurar una supervisión adecuada y crear procedimientos y planes de contingencia. Ajuste de las normas de seguridad comunitarias sobre seguridad de los productos y garantizar la información de los usuarios sobre cómo utilizar esos productos. Es conveniente el establecimiento controle ex ante y ex post, que se amolden a los constantes cambios;Yes;La actual Directiva se debe adaptarla al nuevo contexto. Si una persona sufre un daño causado por un sistema de IA defectuoso y se quiere solicitar una indemnización al productor esta Directiva es el cauce legal para lograrlo. Si el daño es causado por un tercero que interfiere, sería de aplicación el sistema de responsabilidad basado en la culpa de los diferentes EEMM. Buscar la armonización de los diferentes marcos jurídicos y definir  de nuevo los conceptos de productor, operador y defecto;Yes, for all AI applications;;La inversión de la carga de la prueba sobre las personas que hayan participado en el desarrollo, producción y comercialización y La presunción de culpabilidad como un criterio de imputación objetiva por el cual estos mismos agentes deberán demostrar que han actuado de manera diligente y que no se les puede responsabilizar por las consecuencias del evento dañoso. Implantar el seguro obligatorio de responsabilidad civil haciendo especial atención al riesgo inherente a la aplicación de la IA;A_vueltas_con_la_Inteligencia_Artificial_y_la_Responsabilidad_Civil.pdf
F530275;14-06-2020 01:09;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;2 - Not important;It will be essential that each of the proposed actions show clear implementation and consistent monitoring of gender mainstreaming mechanisms, including, mandatory gender impact assessment, gender budgeting and sex disagregated data and gender indicators, as well as the promotion of gender equality training and awareness raising across the sector to ensure institutional transformation and a more inclusive private/research sector. Member States must also proceed, abiding by these parameters. ;5 - Very important;5 - Very important;2 - Not important;5 - Very important;5 - Very important;4 - Important;Specific measures, within an extensive monitoring framework, as well as dedicated budget lines must be guaranteed to ensure the gender gap in skills, employment research are closed and support to women-led AI SME’s are guaranteed. With this, it is essential these measures are aligned with the EC Gender Equality Strategy 2019-2024, Digital Education Action Plan, Updated Skills Agenda and recommendations of the Women in Digital Scoreboard into each of the proposed actions;4 - Important;4 - Important;4 - Important;"Women in research and innovation structures for AI and STEM related fields is critically low-only 20% of AI professionals globally being women. This is due to the high levels of sexist discrimination & harassment they face within these sectors. Actions to strengthen this community must include the development of specific initiatives to support and promote the inclusion of women % girls, particularly in decision making and leadership roles; concretely in recognition & alignment with the Work Life";3 - Neutral;5 - Very important;4 - Important;5 - Very important;5 - Very important;Each proposal for the specialised Digital Innovation Hubs must include measureable gender mainstreaming objectives that can be regularly reviewed and monitored by specific experts. More broadly within the Digital Europe Programme, Horizon Europe and Structural & Investment funds, budget must be allocated to support women-led SME’s that statistically perform 63% better, however have received significantly less funding from existing EC programmes.;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;The application of AI has already proven to be discriminatory to women in Europe in software developed for recruitment and symptomatic detection in healthcare, thereby contravening EU Directives on equal treatment between women and men in employment and occupation. AI has also been used in the perpetration of violence against women and therefore must be expressly referenced as a potential breach of fundamental rights in the proposed risk assessment. See attached opinion.;There is a need for a new legislation;;No;;;;;5 - Very important;2 - Not important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;The use of any AI based technology which existing analysis suggests, significantly increases the risk of discrimination, particularly for persons from an ethnic minority, should not be used within the EU. Taking into consideration that data that informs such technologically has systematically ignored the inclusion of women and girls in testing phases, there is no indication that this technology would be safe for use or from being abused within increasingly democratically-challenged environments.;Very much;The labelling system should not be voluntary but mandatory. Though some systems clearly have higher risks than others (health, public sector, recruitment) all applications of AI have the potential to become high risk. This mandatory labelling system must ensure providers have conducted adequate testing in order to acquire the necessary labelling and use within the EU such as with consumer protection and to ensure products don’t contravene any aspect of the Charter of Fundamental Rights.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;AI technologies should be subject to rigorous assessment with specific tests to ensure non- discrimination and must continue to be subject to regular monitoring as the technology adapts to maintain compliance with these provisions. The human oversight committee and supporting entities at member state level must be given the mandate and support to establish such a mechanism without interference.;Mental health risks;Personal security and Mental Health risks should be elaborated to include risks of violence & discrimination against women and girls, expressly within the legislative framework. It should be clear that use of artificial intelligence and associated risks do not just apply to the end user but also the use of AI in the perpetration of acts that violates another’s personal safety and health. Therefore, these need to reference the Victim’s Rights Directive and existing relevant EU legislation.;Yes;"Risk assessment procedures must be at the heart of the safety legislative framework to
ensure that all AI related uses are consistently monitored. These procedures must be
operated transparently to ensure maintenance of European Fundamental Rights. This
assessment should include specifications on ensuring the safety of all women and girls
and be subject to regular review; see attached document";Yes;;Yes, for all AI applications;;;European_Womens_Lobby__AI_European_Commission_Consultation_Supporting_Paper.pdf
F530274;14-06-2020 00:55;English;NGO (Non-governmental organisation);Christiane;WENDEHORST;;European Law Instituteh;455458212010-50;Large (250 or more);Austria;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;ELI_Response_AI_White_Paper.pdf
F530273;14-06-2020 00:09;English;Company/Business organisation;Marjorie;Dickman;;BlackBerry Limited;251259032370-30;Large (250 or more);Canada;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;BlackBerry_-_Europe_AI_White_Paper_14_June_2020.pdf
F530272;14-06-2020 00:00;English;Other;Konrad;Siemaszko;;SIENNA project (Stakeholder-Informed Ethics for New technologies with high socio-ecoNomic and human rights impAct), European Horizon 2020-funded project. https://www.sienna-project.eu/;;Small (< 50 employees);Poland;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;2 - Not important;EU’s strength must be first and foremost pitched in terms of fundamental rights and EU value-respectful AI with no exceptions. ;4 - Important;4 - Important;2 - Not important;4 - Important;4 - Important;No opinion;;4 - Important;4 - Important;;Funding in AI R&D should be matched with funding to study ethics/social and human-rights aspects and challenges and carry out full-blown and strong impact assessments of AI (in the short, medium and long term).;4 - Important;4 - Important;4 - Important;4 - Important;;In raising awareness about potential benefits of AI (in Q1), it is also important to address ethics and human rights issues.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;There is a need for a new legislation;;Other;;;;;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Much;Self-regulation and use of voluntary measures might only be optimal for very low-risk areas and in cases with known low potential impacts (and even then these could dramatically differ per sector and technology).;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Prior conformity assessment for high-risk applications should be rigorous not symbolic in compliance and auditing. There should be appropriate procedures, governance, and legislation (hard and soft law, sectoral agreements) to support it (there is a need to be ‘motivated’ about certification). Other incentives include sanctions for non-compliance (greater than mere recall of certification). For further information, refer to the results of the EU-funded SATORI project that explored the potential ;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;SIENNA_white_paper_consultation_13.06.2020.pdf
F530271;13-06-2020 23:56;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;4 - Important;Focusing on the impact (positive and negative) of AI on vulnerable consumers is also important and should be considered.;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;"Addressing AI ethics is a necessary task, another area where it is important to align policies. Developing some high-level ethical framework for transparent and explainable AI could help to guide firms as they adapt their operations and apply AI responsibly.
More focus should also be put on data partnerships between public & private sectors in order to improve  prevention work in the insurance industry

";4 - Important;4 - Important;4 - Important;We believe that it is crucial to ensure coherence and co-operation of AI research efforts across the EU. In this respect, having a strong network of existing AI research excellence centres would be efficient. It would ensure for example that AI developed in Europe can cross borders without too many diverse obligations, which is particularly important for SMEs. ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;"Another task would be to ensure and provide access for SMEs to modern oversight tools, such as regulatory sandboxes. This would help them to be innovative.
These hubs need to be well supervised and regulated so that SME’s using or developing AI can have trust in them. These hubs should promote responsible AI standards and should ensure a level playing field with incumbent players to ensure a gradual transition without creating unfair competition. 
";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"AI introduces new classes of risk for the consumer and in many cases can trigger information asymmetry between the consumer and businesses. From a regulatory perspective, there are risks regarding fairness, discrimination and exclusion. 
More transparency is needed on the purposes for which the data is collected.
The challenge for regulators is to find a balance between allowing business to take advantage of the innovation offered by AI while at the same time protecting consumers' interests.";Other;AI systems do not operate in a lawless framework in the EU. Existing and binding EU texts apply to them and their development. The existing EU financial regulatory framework is mostly fit to deal with emerging technologies, to promote innovation and safeguard financial stability. Some  EU legislation in the insurance sector may need to be slightly revised to  adequately capture the growing digital trend and the provision of information in a paperless electronic and responsive form. ;Other;"Limiting those requirements for AI systems to “high-risk” applications may not allow to take into consideration adequately the implementation of an AI system. It would be more efficient to consider applications on a case-by-case basis, with a clear distribution of liability across the supply chain. In any case, any new AI requirements will need to drafted in a flexible way in order to adapt to the changing environment.
A set of high-level principles on responsible AI should be the basis. ";;;"
";5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;No opinion;;Much;Legible and recognised across the EU.  Proportional approach. ;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Risks related to the loss of connectivity;"Data is a crucial in relation to “tech” in the insurance sector, particularly the use and ownership of data. 
One of the relevant aspects is the GDPR and the principles outlined therein. Issues of “ethical” nature should be carefully examined in order to ensure fair treatment of consumers and business actors ( privacy and data protection and ownership issues for consumers). It is necessary to respect key principles such as proportionality, market integrity and technological neutrality.


";No opinion;;No;"BIPAR supports the balance of interests achieved in the PLD for consumers and producers. 
Any changes in the burden of proof for claims under the PLD may lead to an increasing number of unjustified claims and jeopardise the insurability of product liability. T Before discussing liability regime, it should be first identified who in the chain of the AI development is responsible for what operation. Strict liability regime should be considered only in the case of fully autonomous AI systems.";Yes, for all AI applications;;To clarify how obligations are to be distributed among the economic operators as many actors are involved in the lifecycle of an AI system. For ex, who would be held liable in the case where insurance intermediaries use the AI system of the the insurer they represent? It is necessary to introduce clear definitions of the terms “producer/developer”, “operator/deployer” and “user”. A GDPR-like solution could be that the parties involved have the option to agree on who is operator/deployer;Additional_comments.docx
F530270;13-06-2020 23:49;English;Academic/Research Institution;Cláudio;Flores;;Faculdade de Direito e Ciência Política da Universidade Lusófona do Porto (FDCP-ULP);;Medium (< 250 employees);Portugal;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;4 - Important;Focus on the education of citizens in general on AI Ethical issues;5 - Very important;5 - Very important;4 - Important;3 - Neutral;5 - Very important;4 - Important;;5 - Very important;5 - Very important;4 - Important;"The lighthouse research center should have a specific unit, dedicated to the validation of
algorithms before their usage by private or public sector entities. This validation unit
should test the algorithm and propose any necessary change in order to assure its
complete safety and compliance to the existing legal framework. A favorable report from
this unit should be a condition for the approval of any new AI based system.";5 - Very important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;Yes, these hubs should also raise SME's awareness about the potential risks of AI.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"Machine learning algorithms may self-adapt in order to circumvent fundamental rights or
at least make their breach very difficult to identify or classify. E.g., bias and discrimination
have already been identified as a possible problem, therefore, we should expect that a AI
system will already act in a way that makes it difficult (or even impossible) to detect if that
decision was based in any criteria susceptible of contradicting European principles and
fundamental rights.";There is a need for a new legislation;;Yes;;No;;"Considering machine learning systems in particular, it seems to us that the cumulative
criteria for assessing whether an AI application should be considered high-risk, is neither
adequate nor sufficient, taking into account the possibility of AI to self-adapt in order to
circumvent its classification in the predefined risk categories. Hence, Independently of
certification and risk activities classification, human agency and oversight is always
necessary for preventing any misuse of AI.";5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);"The current coronavirus pandemic showed how important it is the collection of data for
public health purposes. Also, for security reasons (e.g. terrorism prevention), all available
technology should be put to place.";Rather not;;Other enforcement system;;"A solution we understand could reduce the risks mentioned should involve taking
advantage of existing state entities (for example, to control the collection and use of data)
and articulate them with the central supervisory entity for the use of AI technologies. We
think that in view of the inherent risks, double-checking (at the European centralized level
and national) would be justified.";Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;"It is paramount to establish a legal presumption of fault against the AI developer in case of
liability for damages, therefore exempting the burden of proof from persons who have
suffered harm caused with the involvement of AI systems. Having into consideration the
national differences on the matters of liability for damages, we strongly recommend this
subject to be specifically regulated in a future European Regulation on AI.";Lusofona_University_of_Porto_Comments.pdf
F530269;13-06-2020 23:48;German;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;3 - Neutral;4 - Important;5 - Very important;4 - Important;5 - Very important;;3 - Neutral;3 - Neutral;4 - Important;5 - Very important;5 - Very important;4 - Important;Geschlechterrollen im Bereich Schule/(Aus-)Bildung: Es gibt nachweislich in der Arbeitswelt nach wie vor viele Männerdomänen. Bereits in der Schule wird der Grundstein für das grundsätzliche Interesse/Zutrauen im Umgang mit Robotik/AI, etc. gelegt.;2 - Not important;5 - Very important;2 - Not important;"eine Umgebung schaffen in der die ""research and innovation community"", beispielsweise auch Start-ups nicht anstreben von amerikanischen/ englischen Großunternehmen aufgekauft zu werden, sondern im Entdeckerland verbleiben möchten.";5 - Very important;4 - Important;5 - Very important;5 - Very important;4 - Important;Finanzierung / Projekte sollten die Nachfrage der Privatwirtschaft berücksichtigen, d.h. digitale Innovation Hubs sollten sich nicht nur durch Top-Down sondern auch durch Bottom-Up-Ansatz finanzieren. ;3 - Neutral;5 - Very important;5 - Very important;;3 - Neutral;3 - Neutral;geschlechtsspezifische Benachteiligung aufgrund der zuvor bereitgestellten Informationen;There is a need for a new legislation;;Yes;;No;;"Ergänzend sind Bereiche zu nennen, die unsere Versorgung (Wasser, Strom, etc.) sicherstellen sowie unsere Sicherheit beeinflussen ""Waffen- und Rüstungsindustrie"", ""Rechts- und Polizeiwesen""";2 - Not important;3 - Neutral;5 - Very important;4 - Important;4 - Important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Not at all;Ich befürchte, dann wird es eine Vielzahl nicht aussagekräftiger Etiketten geben (vgl. Bioprodukte);A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;"""Deepfakes""
Algorithmische Verzerrung durch schlechte Daten
Sozioökonomische Ungleichheit
Automatisierung von Waffen";Yes;;Yes;;Yes, for all AI applications;;;
F530268;13-06-2020 23:33;English;Academic/Research Institution;Jan;LUKES;;HEC Paris / CEMS;;Large (250 or more);France;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;HEC_CEMS_Response_to_WP_AI.pdf
F530267;13-06-2020 23:09;Spanish;Academic/Research Institution;Enrique;ONIEVA;;University of Deusto;388924736529-03;Large (250 or more);Spain;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;4 - Important;;5 - Very important;4 - Important;5 - Very important;5 - Very important;4 - Important;;3 - Neutral;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;;Current legislation may have some gaps;;No;;;;;5 - Very important;4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;;Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;;Personal security risks;;Yes;;No;;Yes, for specific AI applications;;;
F530266;13-06-2020 23:03;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;In general, the AI whitepaper points out in the first sentence that AI is developing fast. Dedicated measures to become quicker in acting/reacting would be welcomed. ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;AI requires special infrastructure beyond a common data space. The EC could also strengthen the efforts to build European AI infrastuctures / pipelines. ;5 - Very important;4 - Important;3 - Neutral;"We would encourage the EU to create ""grand vision"" like everyone is able to communicate with everyone with the help of AI and the creation of research and innovation activities to achieve the visions.";4 - Important;3 - Neutral;5 - Very important;3 - Neutral;2 - Not important;The Digital Innovation Hubs are a good idea but communication, financial power, and community building of the DIHs could be improved.;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;4 - Important;4 - Important;;Current legislation may have some gaps;;Yes;;No;;;4 - Important;2 - Not important;3 - Neutral;3 - Neutral;3 - Neutral;4 - Important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);Most critical are databases of faces combined with personal data. The use of biometric identification in combination with personal data and profiling should only be used for very concrete suspitions or for identification of criminals determined by trained bodies.;Much;The labelling system needs to impose no further cost to SMEs to avoid that AI application is slowed down.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;In many cases AI is part of a system and needs to be assessed on a system level. Many mechanisms exist already and might already be sufficient.;;;No;The framework should consider updates and changing functionalities throughout the lifetime of the product. This is already standard in software based products and AI based systems should be handeled similarly;No;;No;;;Response_AI_Whitepaper-Feedback_Document.pdf
F530265;13-06-2020 22:54;English;Business Association;Ena;Salihovic;;EuroCommerce;84973761187-60;Small (< 50 employees);Belgium;The feedback can be published with your personal information;4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;The data economy needs to be fostered and encouraged in the EU to be able to develop and use AI to its full potential. We see data protection as a valuable asset that needs to be preserved for consumers and businesses. However, data protection must be coherent and allow processing based on legitimate interest. Data is an essential ingredient for AI and business development, and it should not become a competitive disadvantage for EU companies. ;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;We welcome the opportunity to establish Data spaces that will foster AI development. Data sharing, for the purpose of AI development, should remain voluntary and have a clear purpose. A voluntary approach would support data sharing while safeguarding European businesses' competitiveness and adequate investment for data management. Future AI strategies should secure effective coordination at the national level to prevent the fragmentation of the European Single Market.;4 - Important;5 - Very important;4 - Important;Investing in skills, digital education, and research should be the priority of the European Union. We strongly encourage the European Commission to support investment in various digital skills and cooperation with education providers to secure digital literacy across the EU so that everyone can flourish in an AI-powered future.;5 - Very important;4 - Important;5 - Very important;4 - Important;4 - Important;The future EU framework for AI should support the digital development of SMEs. SMEs need to be supported in their digital transformation and provided with the right set of digital skills and training that will help them responsibly use the potentials offered by AI. In most of the case, SME's do not have the ability nor the digital skills to develop AI, but more of them can become users of AI.  We encourage the Commission to provide SME's with easier access to external AI service. ;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;3 - Neutral;The relevance of these concerns greatly depends on the context in which AI is used and developed. Retailers and wholesalers are already complying with existing EU laws that provide safeguards to the development and use of AI, as set out in the General Data Protection Regulation, Platform to Business Regulation, Product Liability Directive, General Product Safety Directive, to name some. Better enforcement of the already existing rules and regulations needed to be provided.;Current legislation is fully sufficient;;No opinion;;;;"Any future EU framework for AI should be technology-neutral, risk-based, and use-based driven, whereby it is essential to have a clear, specific, and future-proof definition of what is considered under a high-risk AI application. It needs to be taken into consideration that the high-risk AI applications depend on the sector where they are being used; we encourage the sector-specific approach. ";3 - Neutral;4 - Important;4 - Important;4 - Important;3 - Neutral;4 - Important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);"Certain biometric identification systems contribute to significant innovations in retail, such as paying by fingerprint. When properly managed, such innovations facilitate the customer journey and benefit internal efficiencies.
EU rules for remote biometric identification systems should balance privacy concerns with opportunities for consumer experience improvement. The General Data Protection Regulation (GDPR) has already created a clear framework for remote biometric identification systems in which biometric data processing should be a last resort option. Retailers and wholesalers would welcome the opportunity to explore innovative biometric-based services for our consumers, such as to cashier-less check-out processes. 
";No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;No;;No;;;EuroCommerce_-_Contributions_to_the_White_paper_on_Artificial_Intelligence_-FINAL.pdf
F530264;13-06-2020 22:34;English;Business Association;Jesse;Spector;;Software & Information Industry Association (SIIA);502425118410-86;Small (< 50 employees);United States;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;SIIA_Comments_-_EU_White_Paper_on_Artificial_Intelligence_-_June_2020.pdf
F530263;13-06-2020 22:31;English;NGO (Non-governmental organisation);Rania;Wazir;;Vienna Data Science Group;;Medium (< 250 employees);Austria;The feedback can be published with your personal information;5 - Very important;4 - Important;5 - Very important;4 - Important;2 - Not important;5 - Very important;"Focus on increasing diversity (gender, socio-economic, ethnic, ...) in AI research and innovation community.
Skills: attract new talent with university and professional development programs; create attractive jobs/research opportunities to avoid brain-drain; increase skills of managers, HR, and those who have to work with AI.
Promote adoption of AI by public sector _only_ after proper regulatory framework is in place. 
Work with NGOs and civic organizations.
Focus on SMEs - mainly start-ups.";5 - Very important;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;5 - Very important;"Testing centers: testing facilities should be not-for-profit, with EU-wide reach.   Examples: IEEE & TÜV.  Domain experts should be represented in the standards committees and certification bodies; care should be taken that these bodies are inclusive of all sectors of civil society.";3 - Neutral;5 - Very important;4 - Important;"Advocate diversity and inclusion in AI research.
Criteria for successful public-private partnerships: focus on startups, reduce bureaucracy.";3 - Neutral;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Increase funding, but this should be purpose-driven: to what extent do the AI startups support the mission of the EU White Paper?;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;3 - Neutral;"The use of AI can lead to loss of skills;
it can also codify existing stereotypes/prejudices, and create a false sense of objectivity.";There is a need for a new legislation;;Other;"Rules for high risk algorithms - absolutely. But safeguards and user/consumer protections for medium risk applications are necessary.  Use a risk model as defined in some other ethics guidelines - for example the Deutsche Ethik Kommission.
Also, environmental impact should be a part of the risk calculation.
_All_ data sets used for training/validation/testing need to be kept.";;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Compliance should be mandatory and independently assessed;Mental health risks;;Yes;;Yes;In situations where humans are accountable, and opacity makes AI systems less so, this actually creates an incentive for companies to replace their human decision makers with algorithmic systems -- creating job loss, and _less_ safety and accountability towards consumers.;Yes, for all AI applications;;Liability rules need to specify who is accountable when using, for example, open source materials, such as pre-trained neural networks;
F530262;13-06-2020 22:30;German;EU Citizen;Wolfgang;Nitsch;;;;;Austria;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;4 - Important;4 - Important;;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;;4 - Important;5 - Very important;4 - Important;;5 - Very important;3 - Neutral;4 - Important;3 - Neutral;4 - Important;;4 - Important;4 - Important;4 - Important;4 - Important;3 - Neutral;2 - Not important;;Current legislation may have some gaps;;Yes;;No opinion;;;5 - Very important;4 - Important;4 - Important;5 - Very important;3 - Neutral;5 - Very important;No further guidelines or regulations are needed;;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Risks related to the loss of connectivity;;Yes;;No;;No;;;
F530261;13-06-2020 22:18;English;NGO (Non-governmental organisation);Robert;Whitfield;;World Federalist Movement - Institute for Global Policy  Transnational Working Group on the Global Governance of AI and Disruptive Technologies;407363614347-45;Small (< 50 employees);United Kingdom;The feedback can be published with your personal information;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;Global regulation: There is little mention of a global regulatory framework.  This should however be the urgent goal as AI has tremendous power both to support and destroy humanity.  The sooner this power is under responsible and effective global regulation the better.  This should be an explicit goal for the European Union, CAHAI and the United Nations.  Such matters should start to be discussed in global fora as a prelude to the negotiation of an initial Global Treaty on AI.  ;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;3 - Neutral;5 - Very important;;5 - Very important;5 - Very important;3 - Neutral;"There is a major problem with whether it will be possible to ensure that the goals of a superintelligent AI will always be aligned with the interests of humanity
Expenditure on AI research needs to embrace the concept of Differential Technological Development, where a significantly higher proportion of AI related research funds is systematically allocated to resolving the AI goal alignment than hitherto.  Ref: Ord, T. (2020) The Precipice – Existential Risk and the Future of Humanity, London";No opinion;No opinion;No opinion;No opinion;No opinion;"
";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"Three other concerns
a.	The negative impact on labour, both economically and socially, undermining an individual’s sense of identity and purpose.  It is essential that the planning required to address these two issues takes place hand in hand with the encouragement of AI in this White Paper.

b.	The loss of control through a lack of goal alignment with human interest. No current solution to a significant existential threat. 

c.	The epistemological threat posed by deepfakes, using GANs.
";Current legislation may have some gaps;;Yes;;No;;"The current AI applications that are the most concerning are 
a.	Lethal Autonomous Weapon Systems 
b.	Generative Adversarial Networks (GANs) in the development of deepfakes.
";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;;No opinion;;Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;;Cyber risks;;Yes;;Yes;;No opinion;;;WFM_TWG_AI_EC_White_Paper_050620.pdf
F530260;13-06-2020 22:08;English;NGO (Non-governmental organisation);Alyna;SMITH;;Platform for International Cooperation on Undocumented Migrants (PICUM);19093363838-48;Small (< 50 employees);Belgium;The feedback can be published with your personal information;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;1 - Not important at all;1 - Not important at all;"AI should not be promoted for its own sake. If governments use AI for public functions, it is critical that there be clear, published reasons to justify its use; scientific evidence that the technology works; and that people have a say in whether or not AI can be acceptably used in a democratic society, especially where it will play a key role in determining access to vital services or affect the enjoyment of fundamental rights and freedoms.";;;1 - Not important at all;;;;The coordinated plan should be updated to include a section on human rights, societal impacts of AI and automated decision-making, and how to ensure democratic oversight for its application. ;3 - Neutral;4 - Important;1 - Not important at all;"Funding for EU projects involving AI should be conditioned on meeting the EU's ethical standards for AI and fundamental rights; and EU funds such as Horizon2020 should immediately stop funding for projects that pose a risk to fundamental rights, such as iBorderCtrl (which aims to use facial and emotion recognition technology to purportedly detect lies in the course of visa applications; as with other projects that facilitate mass surveillance.";1 - Not important at all;;;;;There should not be exemptions for small businesses' obligation to protect human rights, including the rights to data protection and privacy. ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;"We are particularly concerned about the use of AI breaching fundamental rights in the areas of policing and immigration control, and the creation of exemptions from the application of safeguards or from ethical rules for particular classes of individuals (e.g., migrants/non-EU nationals); as well the use of AI in sensitive areas, such as the use of public services without adequate democratic oversight, transparency or evidence to justify the need or purpose of its use.";Other;We wish to strongly underline that AI regulation must not create loop-holes to data protection legislation or other frameworks, such as anti-discrimination law. ;Other;The high/low risk binary is inadequate. New rules should be put in place that clearly outline criteria for determining which AI systems are legal, and which are not. This should include establishing that the system works and is needed, and require a mandatory fundamental rights impact assessment for all applications, as well as mechanisms for oversight involving all relevant (affected) stakeholders. Biometrics or facial recognition for mass surveillance should be banned.;;;The following uses of AI are incompatible with human rights and should be banned: (1) to determine access to essential public services, (2) predictive policing, (3) autonomous legal weapons, (4) identification or analysis of emotion and identity traits, and (5) indiscriminate biometric surveillance.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;"Their use in public spaces leads to mass surveillance, which will irreversibly restrict fundamental rights to privacy, freedom of assembly, non-discrimination; and even uses that do not contribute to mass surveillance pose unacceptable risks to privacy, data protection, non-discrimination and human dignity. ";Rather not;Self-labelling systems can be confusing and may give a false sense of security since it is the same company that develops a product that declares its safety. ;Other enforcement system;All systems should undergo a mandatory ex ante human rights impact assessment by an independent third party.;Compliance must be evaluated by a trusted external actor, and not on the basis of self-regulation. ;Mental health risks;;Yes;;Yes;"AI developers and deployers should be accountable for harm generated by their
products, and that products developed using AI should not enjoy exceptions to any EU laws, i.e. those related to discrimination, data protection, or product liability.";Yes, for all AI applications;;"The EU should address copyright and database protections that create barriers to proper oversight of the deployment of AI technologies. Liability rules should provide incentives
for greater transparency about the use of AI.";PICUM_submision__EU_consultation_on_AI_June_2020_FINAL.pdf
F530259;13-06-2020 22:04;English;Public authority;Gianluca;GRASSO;National;Scuola Superiore della Magistratura;;Small (< 50 employees);Italy;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;5 - Very important;;5 - Very important;5 - Very important;3 - Neutral;4 - Important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;3 - Neutral;;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;;4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;;There is a need for a new legislation;;Yes;;Yes;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Cyber risks;;Yes;;Yes;;Yes, for all AI applications;;;
F530258;13-06-2020 21:54;English;Business Association;Geraldine;PROUST;;FEDMA;39300567160-02;Micro (< 10 employees);Belgium;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;4 - Important;4 - Important;No opinion;No opinion;No opinion;No opinion;;Current legislation is fully sufficient;;Yes;;Other;The approach, with its exceptions, leaves quite some room for legal uncertainty. It would be wise to change the criteria to a system with no or barely any exceptions. The sector-specific approach could also be troublesome as it might be harder to distinguish sectors in practice. A better set of criteria might be: 1. If the AI use has possible risks and 2. if this will have a legal or similar impact on ‘large’ audience.;;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;No further guidelines or regulations are needed;;Rather not;A voluntary labelling scheme should be considered but the risks of such a system should not be overlooked, e.g. an advantage for undertakings that can comply easier to conditions for labelling, even though the advantage for customers might be minimal. ;Other enforcement system;Proper enforcement of current rules. ;;;;;;;;;;;20200504_FEDMA_draft_position_paper_on_AI_V2_FINAL.pdf
F530257;13-06-2020 21:44;English;Non-EU Citizen;Lucien Yen-Hao;Chen;;;;;Norway;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;4 - Important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;Monitoring function from both public and private sectors might be important, especially when it comes to partnership. For example research centre may follow all the rules and regulations to avoid racial discrimination, but not necessarily understand the social impact the system may lead to. Adjustment shall happen during the research period already, not after.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"It is unclear how ""the right to access"" can be applied in the framework addressed in this white paper. (As for GDPR art. 15.)
Shall voluntary licensing be compulsory for certain critical AI based system? Especially high risk one. ";There is a need for a new legislation;;Yes;;Yes;;In addition to the risks addressed in the white paper (such as human rights, fundamental rights), for example non reversible damage or decision shall also be considered as high risk.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;Yes;;Yes, for all AI applications;;;
F530256;13-06-2020 21:33;English;Business Association;Emir;DEMIRCAN;;SEMI Europe;402302029423-14;Small (< 50 employees);Belgium;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;SEMI_Europe_Comments_on_EU_AI_White_Paper_June_2020.pdf
F530255;13-06-2020 21:18;Portuguese;NGO (Non-governmental organisation);Claudia;Columbano;;BUSINESS as NATURE - Associação para a promoção da produção e consumo sustentável e economia circular;;Micro (< 10 employees);Portugal;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;4 - Important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;3 - Neutral;;There is a need for a new legislation;;No;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F530254;13-06-2020 20:55;English;NGO (Non-governmental organisation);Victim Support;EUROPE;;Victim Support Europe;83945428894-94;Small (< 50 employees);Belgium;The feedback can be published with your personal information;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;;4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;;4 - Important;4 - Important;5 - Very important;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;Current legislation may have some gaps;;No;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Rather not;;Other enforcement system;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;AI_consultation_VSE.pdf
F530253;13-06-2020 20:44;English;Business Association;Karina;Stan;;Developers Alliance;135037514504-30;Micro (< 10 employees);United States;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;The global dimension is equally important. Many AI systems, like the large majority of software solutions, are developed in a collaborative environment at a global level. The developer community relies on open source software from both inside and outside the EU.  Global considerations such as the free flow of data, privacy and trust, and security must be reflected in any recommendations.;4 - Important;3 - Neutral;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;"A harmonized approach across the EU is essential in achieving the proposed objectives, not only for the appropriate legal and governance frameworks, but also to tackle the digital divide within the EU.
European developers and entrepreneurs don’t need a new layer of red tape within the current fragmented landscape of the Single Market. ";3 - Neutral;3 - Neutral;4 - Important;The EU will benefit from continuing international cooperation in R&D&I. A scenario of working in isolation will deprive European researchers and innovators of fruitful exchanges in ideas and collaborative projects with their fellows in other parts of the world.;5 - Very important;4 - Important;5 - Very important;4 - Important;4 - Important;It is essential to create the right environment for innovative entrepreneurship across the EU. Innovation Deals and regulatory sandboxes will allow innovators to develop new AI solutions, but also go further and bring them to the market.;4 - Important;4 - Important;4 - Important;3 - Neutral;2 - Not important;2 - Not important;"
";Current legislation may have some gaps;;Other;"AI is not developed and deployed in the EU in a legal vacuum. We suggest a proportional regulatory approach that assesses the gaps in legislation that is already applicable. 
The “high-risk” formula should be applied case-by-case, according to the proposed cumulative criteria (selected sectors and use cases).";;;;4 - Important;3 - Neutral;4 - Important;3 - Neutral;3 - Neutral;3 - Neutral;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);Governments could set conditions and limits in their usage to ensure public security while respecting the fundamental rights of citizens and the democratic values. ;Not at all;The proposed labelling system will create pressure on startups and scaleups.;Other enforcement system;"We recommend an ex-ante risk self-assessment (similar to the data protection impact assessments under GDPR), complemented by ex-post enforcement.
Also, regulatory sandboxes and Innovation Deals could provide controlled environments in which AI systems can be tested and scaled-up.";It is important to consider that AI technologies can have dual use, or new forms of use can be discovered after they are placed on the market. ;Risks related to the loss of connectivity;;No;Please see the attached position paper.;No;"The basic principles of the current product liability regime have proved their worth, and there is no need for a substantive change.
The assessment of the appropriate liability regime for AI, as for software in general, should take into account the unique aspects of software development, especially in open source environments.";No;;The existing fault-based tort law of the Member States, offers, in most cases, a sufficient level of protection for persons that suffer harm caused by another.;Developers_Alliance_Submission_To_The_European_Commission_s_Consultation_On_The_White_Paper_On_AI.pdf
F530252;13-06-2020 20:27;French;Company/Business organisation;Amélie;Segard;;Quantmetry;;Medium (< 250 employees);France;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;2 - Not important;4 - Important;4 - Important;;5 - Very important;3 - Neutral;4 - Important;3 - Neutral;4 - Important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;;3 - Neutral;3 - Neutral;3 - Neutral;5 - Very important;2 - Not important;;4 - Important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;4 - Important;;There is a need for a new legislation;;Yes;;Yes;;;5 - Very important;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;No opinion;;No opinion;;;
F530251;13-06-2020 20:05;English;NGO (Non-governmental organisation);Sascha;MARSHANG;;European Public Health Alliance (EPHA);18941013532-08;Small (< 50 employees);Belgium;The feedback can be published with your personal information;5 - Very important;4 - Important;5 - Very important;3 - Neutral;3 - Neutral;4 - Important;Especially in health and care, data challenges need to be tackled first. A multi-stakeholder dialogue needs to be created between digital and other sectors, including civil society, to improve awareness, education, and skill. End users need to be involved in the design, implementation and evaluation of AI solutions. Next to building up specific digital skills, it is important to focus on digital literacy, key in the health sector. Transparency and knowledge are crucial for ordinary people.;4 - Important;4 - Important;3 - Neutral;3 - Neutral;5 - Very important;4 - Important;Coordination and collaboration must be strengthened not only between Member States but also between other stakeholders at national level. Extensive coordination and involvement of end users is needed to improve the understanding of the benefits of using e.g. personal health data for the public good. A “Health in all Policies” approach needs to be introduced in the technology sphere, too. Cross-sector bridges must be built to improve the understanding, meaning and deployment of AI solutions.;3 - Neutral;4 - Important;3 - Neutral;Public-private partnerships must involve civil society groups, such as patients, healthcare professionals and public health experts who are meant to be the beneficiaries of AI and data-driven healthcare solutions, including pandemic surveillance technologies. The further development of AI in healthcare must not be driven by technology firms, to ensure it will bring benefits for everybody and address real needs. Research should address how AI can tackle health inequalities and improve access.;3 - Neutral;3 - Neutral;3 - Neutral;4 - Important;3 - Neutral;While AI holds potential for SMEs, and Digital Innovation Hubs may be one way of encouraging more research and investments in it, the creation of a multi-stakeholder ecosystem as outlined above is just as important. The solutions of SMEs must correspond to real societal needs and take into account end users' ethical concerns.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Key ethical challenges need to be addressed by taking into account end users’ needs and concerns: In heath and care, the right balance must be struck between harnessing the positive effects AI could generate and continuous respect for and safeguarding of core values. Protecting personal data and privacy is particularly important for society’s most vulnerable. A thorough and inclusive reflection about ethics and governance is key in relation to the six concerns expressed above.;Current legislation may have some gaps;;No;;;;EPHA being a public health membership organisation, we are most concerned about any AI applications in healthcare and public health that could be operating on the basis of biased algorithms (not taking into account gender, ethnicity, etc.) and that do not take into account the broader determinants of health and histories of individual patients. We are also concerned that increased machine-generated decisions could potentially exacerbate existing health inequalities, discrimination and exclusion.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;There may be a use of biometric identification sytems in situations of crisis, which could also be a public health crisis. However, the use of such systems should be exceptional and subject to very strict rules to make sure that individuals do not suffer negative consequences or harm and misuse by state or private actors cannot occur. It really depends on the situation and purpose of deployment. There is always a risk of generalised surveillance beyond the declared purpose, which would impoverish democracy in Europe.;Much;It depends again on the purpose of the AI solution and perhaps on the sector. In health and care, there could be value in such labels for certain AI-solutions deemed to be not high risk but that could potentially generate ambiguous results or produce harm.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;N/a;Mental health risks;The impact of AI in workplaces needs to be further addressed, including in different healthcare settings. A more specific and legal approach to AI is a missing point given that its complexity of AI could produce unintended side effects for healthcare professionals, patients and citizens, in addition to causing mental health issues arising from, e.g. machine-generated decisions impossible to comprehend for lay persons.  ;Yes;N/a;Yes;Yes it should be amended to better cover the risks engendered by certain AI applications due to its complexity. The EU product liability law has to keep pace with new technology and needs to reconsider certain definitions that could now lead to different outcomes. The interplay between hardware, software and services is becoming more intricate, most importantly as they can evolve without human input. There is increasingly a fine line between a physical product and the services it delivers.;Yes, for specific AI applications;As mentioned above, certain health and care solutions might be relevant here, especially if they are for use by patients in their own home. Adapting national liability rules is the only way to ensure a common and transparent approach in all Member States. A standardized approach for AI applications is desirable in healthcare, given that many are built through complex supply chains. The fault-based liability regimes in many Member States that apply in other scenarios may not be sufficient to protect those harmed by AI applications in all scenarios. ;To ensure a proper compensation and a fair allocation of liability, rules should be adapted at EU and national level. The specific characteristics of many AI technologies, including opacity (‘black box-effect’), complexity or partially autonomous behaviour, make it hard to verify compliance with, and may hamper the effective enforcement of, rules of existing EU law meant to protect fundamental rights.;EPHA_Moving_beyond_the-Hype_2019.pdf
F530250;13-06-2020 19:59;English;Company/Business organisation;Gabriel;Cosgrave;;Xperi Holding Corporation;Xperi4018061850;Large (250 or more);United States;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;The Commission should recognise the importance of the role of large companies and the importance of non-European companies to Europe. IP regulation, in particular the patenting of software inventions, should be brought into line with international best-practice. The Whitepaper on AI needs to carefully consider its definition of AI. It is important to acknowledge that AI as an engineering tool to develop components of systems. Training programmes should be also supported in a CPD manner.;5 - Very important;2 - Not important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;It is important to bolster and support those companies that already advanced in relation to AI. We would encourage the European Commission and Member States to consider initiatives to ensure that Europe builds a strong “Single Market for AI”. It is critically important that AI is defined properly. We note the action related to reference testing facilities for AI, but we believe this is neither a practical nor appropriate priority.;2 - Not important;5 - Very important;5 - Very important;We regard a lighthouse research centre as unimportant. Europe should support diversity of excellence research in AI. This is reflected in our scoring of networks of excellence centres as very important. Public policy should not attempt to create AI researchers in universities in the likeness of industry. It is important that sufficient resources are made available in Europe for blue-sky thinking in relation to AI. A long-term view is necessary within the publicly-funded research system.;5 - Very important;2 - Not important;4 - Important;4 - Important;4 - Important;DIH’s could support companies undertake AI self-assessments as well as support the development of consensus-based voluntary standards for AI. DIHs could help certify that organisations have undertaken specific training that ensures familiarity with EU guidance in relation to Trustworthy AI. We recommend consensus-based voluntary standards for AI. These can be developed in dialogue with industry, be based on best practices, and would avoid the risks associated with ill-informed regulation.;2 - Not important;3 - Neutral;3 - Neutral;3 - Neutral;2 - Not important;4 - Important;The positioning of AI in the whitepaper is extremely negative. This is compounded by a vague definition of AI. The concerns highlighted in the “Ecosystem of Trust” section of the Whitepaper are not unique to AI. Often AI is used in a modest and specific manner within an overall system. It would appear that the GDPR covers all the concerns raised in the Whitepaper. Introducing further “one size fits all” regulation on a technology is not justified at this time.;Current legislation is fully sufficient;;Yes;;No;;The risk-based approach in the Whitepaper is, essentially, binary: high or low risk. It is not nuanced enough. The approach is also not proportionate. Labelling sectors and applications, regardless of market, as high risk makes little technical sense. Focus should be given to the purpose of an AI application, and the role that this has in the overall system. Regulation in Europe should not be driven by exceptional cases;2 - Not important;2 - Not important;2 - Not important;2 - Not important;2 - Not important;2 - Not important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);The GDPR is very relevant to this setting. The critical point is the issue of consent which should be obtained in an informed manner. However, we need to protect freedom-of-choice for consumers, especially when this relates to the protection of their fundamental rights. We do not support any form of mass surveillance where informed consent cannot be granted.;Not at all;"The approach should focus on self-assessment and consensus-based voluntary standards. We are concerned at the burdens that would arise due to a poorly defined labelling systems, possibly of a lower standard than those already in use in industry. The term ""AI system"" is imprecise. AI might be used in many ways in a complex engineered system, but it doesn't make the system an ""AI system"". It is unclear what the threshold for the use of AI would need to be in order for a system to be so labelled.";Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;We strongly advocate for an ex-post market approach.;Cyber risks;These risks are risks associated with a variety of advanced technologies. None are uniquely related to AI. When considering the notion of risk stemming from the use of a technology it is extremely important that we define how the technology is used and the degree to which the risk is uniquely associated with the technology.;No;No additional risk assessment procedures that are specific to AI are necessary.;No;There is currently no evidence that liability rules need any adaptation for AI. Where damage arises, the case for why additional regulation is necessary has not been made. In fact, poorly justified and unfocused amendments could be extremely harmful in themselves.;No;;There is currently no evidence that liability rules need any adaptation for AI. Where damage arises, the case for why additional regulation is necessary has not been made. ;xperi-whitepaper-response_-FINAL.pdf
F530249;13-06-2020 19:47;English;Trade Union;Maxime;LEGRAND;;CEC European Managers;10426402966-04;Micro (< 10 employees);Belgium;The feedback can be published with your personal information;5 - Very important;4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;Considering the EU’s decentralised digital landscape, considering the challenges with digital skills and considering the lacking European autonomy in digital infrastructure, CEC European Managers insist on building a common digital infrastructure, helping managers to become digital facilitators, as well as developing European tools to help education provider with providing digital skills, at all levels. As highlighted in our Guidelines on Digitalisation, we need above all a mindshift;4 - Important;3 - Neutral;4 - Important;5 - Very important;5 - Very important;4 - Important;;4 - Important;4 - Important;3 - Neutral;"European Commission Highlight the leading role of social partners in the process of adopting AI in working environment.To ensure such an involvement, each AI system should be discussed by employee representatives through information or consultation procedures.
This will allow critical concerns to be assessed and identified, as proposed by the so-called Trustworthy AI Assessment List.

";5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"Impact on managers’ labour rights
Impact on accountability and legal responsibility
Environmental and social impact of AI 
";Current legislation may have some gaps;;No opinion;;;;;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);"consent-based collection of biometric data
right to be forgotten
";No opinion;The question is how the criteria of such labels will be set and by whom. Social partners, civil society organisations and democratic institutions need to be involved.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;"Risk in accuracy of (resulting human) decision making (bias)
Risk for minorities
Risk for managers‘ and workers‘ rights
";Yes;;No opinion;;No opinion;;;AI_letter.pdf
F530248;13-06-2020 19:47;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;4 - Important;4 - Important;5 - Very important;3 - Neutral;3 - Neutral;;5 - Very important;2 - Not important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;;4 - Important;5 - Very important;3 - Neutral;;5 - Very important;3 - Neutral;3 - Neutral;5 - Very important;5 - Very important;;3 - Neutral;4 - Important;3 - Neutral;1 - Not important at all;3 - Neutral;1 - Not important at all;;Current legislation may have some gaps;;Yes;;Yes;;;4 - Important;3 - Neutral;4 - Important;3 - Neutral;4 - Important;5 - Very important;No opinion;;Much;;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;;;;No opinion;;No opinion;;No opinion;;;
F530247;13-06-2020 19:43;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;In the context of the European Health Data Space, the Commission should take into account the responsible use of data for the development of AI solutions by medtech companies. ;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;;Current legislation is fully sufficient;;Other;.;;;These risks are relevant for all products and services on the EU market & are addressed by legislation on fundamental rights, consumer protection, product safety & liability rules. This goes a fortiori for regulated products, such as medical devices. Therefore, with medical devices (including software as a device), we challenge the assumption that a non-compliance issue in AI has a much larger effect, escalating the risk to a critical level for which the existing regulation is not sufficient.;4 - Important;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;No opinion;;No opinion;Please see our attached letter that outlines concerns regarding the risks of arbitrage and “regulator shopping” that could apply here. ;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Existing legislative provisions for products regulated under MDR should be factored into considerations. ;Cyber risks;;Yes;;No;;No;;;MDT_Submission_to_European_Commission_AI_Consultation_-_Final_.pdf
F530246;13-06-2020 19:37;English;EU Citizen;Bogdana;Rakova;;;;;Bulgaria;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;Current legislation may have some gaps;;Yes;;Yes;;;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;Opportunities_for_European_Leadership_in_AI_Governance.pdf
F530245;13-06-2020 18:54;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;It is essential that the ecosystem of excellence fosters competitiveness of EU companies vis-a-vis firms from other geographies. Authorities should promote and support AI adoption by companies of any size and create incentives that allow EU firms to create and retain talent, avoiding brain drain.Also access to data will play a fundamental role.;3 - Neutral;3 - Neutral;5 - Very important;4 - Important;5 - Very important;4 - Important;Regulators and supervisors’ expectations and practices should be harmonized to ensure a common approach to this technology by each EU Member State and by sectoral and cross-sectoral authorities.;3 - Neutral;5 - Very important;5 - Very important;"Two actions:
- Make available training datasets based on anonymised data from the real digital footprints generated by millions of citizens or IoT devices to avoid barriers of fragmented/low scale datasets compared to those inother geographies (mainly USA and China)
- Create standards, applications and quality methods to generate such cross-country datasets throughout key sectors (health, transport, environment, finance, etc)";5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;Although supporting SMEs is convenient for their AI adoption, access to such facilities should also be possible for larger firms.;;;;;;;"AI entails risks and opportunities as any technology. Identifying AI with higher discrimination or safety risks is undermining trust on AI adoption. This could divert vital resources from deployment to compliance.
Citizens and customers do not trust AI or any other technology, but they trust the firms, organisations or services using it. Risk assessment criteria should be defined linked to applications and use cases, not to a specific underlying technology such as AI.";Other;"While the current regulation may be sufficient to cover the risks generated by the application of AI, some adjustments are likely to be needed to address the barriers to its use. 
 
In this regard, consideration could be given to the possibility of EU authorities working with industry to develop guidance on how to comply with existing rules in order to avoid some of the unintended consequences of the use of IA and to achieve appropriate levels of explicability and accountability.
";Other;Although we don’t think new rules are needed, we support the risk-based approach to focus on apps that could cause serious harm to citizens. But it must be considered that the approach to determine high risk apps may leave outside of the scope cases where regulatory arbitrage is possible, and an activity can be carried out by firms that do not belong to a designated sector. In these cases, the Com should ensure that the exception foreseen (application regardless the sector) should be applied;;;;;;;;;;No opinion;;Rather not;"From the point of view of its usefulness, it raises many questions.
If a voluntary labelling scheme is finally developed, we would recommend that the framework be simpler and based on a risk-based approach and be oriented to the certification of activities and not firms. A labelling scheme that adopts the same requirements as those in the regulation for high-risk applications could discourage companies of all sizes from applying for the label to the detriment of consumers. 
";Other enforcement system;;"A strict ex-ante oversight could delay the launch of products leveraging AI to the market.
Enforcement and oversight tasks should be undertaken by current supervisors to avoid overlapping or contradictory practices. Adequate ex-post enforcement mechanisms should be required for firms providing high risk apps not submitted to specific supervision. Citizens do not trust AI, but the firms using it. Therefore, it is essential that clear and technology-agnostic governance exists.
";Mental health risks;"Among cyber risks, the following could be highlighted: Expansion of existing threats (increase of the complexity and reduction of the costs of cyber-attacks), introduction of new threats, increase of the speed and success rate of cyber-attacks, hinder the detection of malware or hackers.

A security baseline of technologies or products that are developed should be established, detailing the minimum-security measures to be implemented to avoid misuse or tampering of these technologies
";No opinion;;No opinion;;No opinion;;;AEB.20.06.12__Final_AEB_annex_proposal_to_the_European_Commission_AI_White_Paper.docx
F530244;13-06-2020 18:50;German;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;4 - Important;5 - Very important;No opinion;1 - Not important at all;1 - Not important at all;#NAME?;5 - Very important;3 - Neutral;1 - Not important at all;4 - Important;No opinion;4 - Important;DSGVO;No opinion;5 - Very important;1 - Not important at all;;No opinion;No opinion;No opinion;No opinion;No opinion;müssen KMUs für sich selbst entscheiden;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"- Überzogene Erwartungen an Entscheidungen durch KI; es wird alles blindlings geglaubt (siehe J. Weizenbaum, in den 60er Jahren am MIT mit dem Programm ELIZA)
- Predictive Policing hat sich bisher nicht bewährt
- es werden nur Korrelationen gefunden
- KI ";Other;Ich kenne nicht alle Rechtsvorschriften. Es gibt aber bestimmt genügend Schlupflöcher.;Other;"Was ist ein großer Schaden – für die Allgemeinheit oder für einzelne Personen? Wenn KI über Personen entscheiden darf, wie sollen die sich wehren, wenn wahrscheinlich die Datengrundlage und die Algorithmen Geschäftsgeheimnisse sind; KI sollte nicht über Menschenschicksale entscheiden. Für Geschäfts-, Maschinen-, Prozessdaten etc. ist es in Ordnung.";;;KI ist immer dann hoch riskant, wenn sie alleine (ohne Kontrolle) Entscheidungen treffen darf, die über die Sicherheit oder Leben von Personen direkt oder indirekt entscheiden: seien es Kraftwerke, Waffensysteme oder Bewertungen von z. B. der Kreditwürdigkeit von Personen (die Algorithmen dazu werden nicht offengelegt – siehe Schufa). Wie sollen sich Personen da wehren? Deshalb müssen Trainingsdaten und Algorithmen zugänglich sein.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;"Das käme einer Rasterfahndung gleich; jede anlasslose weitere Überwachung ist ein weiterer Schritt in den totalen Überwachungsstaat hinein (amerikanische Städte wie z. B. San Francisco verbieten biometrische Gesichtserkennung im öffentlichen Raum; IBM stellt seine Aktivitäten diesbezüglich ein, andere Firmen wie Amazon, Microsoft überdenken ihre Tätigkeiten diesbezüglich). Deshalb sollte auch die EU biometrische Gesichtserkennung im öffentlichen Raum verbieten.";Very much;;Other enforcement system;"Es sollte eine unbedingte Produzentenhaftung geben; wer das Produkt in Umlauf bringt, ist gegenüber dem Kunden haftbar. Der Produzent kann dann seinerseits Zulieferer, die für den fehlerhaften Teil der KI zu seinem Produkt verantwortlich sind, haftbar machen.
Algorithmen, Datenbasis etc. müssen offen gelegt werden, bzw. einsehbar sein!";;Mental health risks;"Es muss immer die höchste Sicherheitsstufe in Bezug auf Verschlüsselung, der Integrität und Sicherheit der Produkte Verwendung finden; nicht wie zur Zeit darf die Sicherheit absichtlich von Behörden geschwächt werden (ETS (eTLS) durch die ETSI, hier wird bewusst die Verschlüsselung geschwächt; oder der Einsatz von Trojaner, der durch die Ausnutzung von Schwachstellen in der Sicherheit der Produkte aufgespielt wird); das stellt kein Vertrauen her und gefährdet die öffentliche Sicherheit!";Yes;;No opinion;Ich kenne den EU-Rechtsrahmen nicht.;Yes, for all AI applications;;;
F530243;13-06-2020 18:37;English;Public authority;Konstantinos;SFIKAS;National;Hellenic Ministry of Digital Governance;;Large (250 or more);Greece;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;"The interoperability between the aforementioned axes should be facilitated as well as
the mobility of knowledge and people involved in AI in terms that both methods and data can be easily available regardless of their origin and/or target site.";5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;In scope of recent AI growth, traditional digital skills should not be left behind in terms of access to technology and data sources. Handcrafted methods (deterministic methods) and other machine learning algorithms (statistical approaches that do not utilize training) should also be able to test against real life environments (access to testing facilities).;3 - Neutral;5 - Very important;5 - Very important;Local economies could (and should) benefit both from the AI growth and the attract of scientists and engineers focused on AI. The existing AI research centers should be given equal opportunity in excellence to achieve distribution of knowledge and cultural, economic benefits. In this scope the lighthouse should be considered as a coordinating/unifying body than an actual research center.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;Provide access to freely available datasets and opportunities to freelancing developers and researchers for resources (both in terms of data and computer power) so that access to advanced AI research tools is available equally. Also, they should also be able to organize hackathons and competitions on national/regional level for the public and educational institutes.;4 - Important;3 - Neutral;3 - Neutral;4 - Important;4 - Important;5 - Very important;"Since AI systems incorporate mathematical models, they should be considered ""fair"" in terms of discrimination and fundamental rights (therefore the concerns are characterized as neutral), however AI systems could ""hide"" discrimination rules in their trained states and/or future training stages (via biased inputs and/or modelling). Therefore, a continuous auditing process should be considered.";Other;There is a need for a new legislation, AI decisions should be considered both human and machine based. Therefore, legislation that takes into account both of these aspects should be considered. Furthermore, procedures that rule out biases existing into the AI systems should be developed. No rules should be considered for research systems.;Other;YES, Continuous auditing should be considered for high-risk AI systems. The rules should be strict for both the system as well as the publisher who needs to be considered responsible for any harm done by the system.;;;Medical, military, self-driven vehicles, power distribution, water distribution, traffic control.;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);Biometric ID systems should be considered in areas where many people are gathered and access is required. Furthermore, due to recent events, Biometric ID could aid in the prevention of COVID-19 and other diseases spreading (contactless identification and access).;Very much;Specialized services from existing providers (e.g. Amazon Mechanical Turk) could be used and/or a dedicated service in similar terms could be established (opening new jobs and opportunities for (semi)specialized workers).;Other enforcement system;"Again, AI systems could ""hide"" discrimination rules in their trained states and/or future training stages. Therefore, a continuous auditing process should be considered by a group of experts in terms similar to those considered in patent offices.";;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;Comments_on_AI_and_Data_WP_EU.docx
F530242;13-06-2020 18:26;English;Company/Business organisation;Arnaud;David;;Amazon Europe Core Sarl;366117914426-10;Large (250 or more);Luxembourg;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;We believe that the Commission has identified the right course of actions in order to ensure the uptake of AI in the EU. Supporting general education initiatives on AI for citizens, professionals and academics is crucial for AI adoption and safe use. Any investment plan and future regulatory framework for AI should keep as a guiding principle the objective of accelerating and facilitating the development and use of AI. ;5 - Very important;3 - Neutral;5 - Very important;4 - Important;5 - Very important;3 - Neutral;We believe that the uptake of AI will mostly depend on the use of AI supporting technologies that cloud computing capabilities can offer. Public and private organisations need to be able to have easy access to a huge range of ready-to-use AI services helping them to develop, train and deploy their AI systems in order to meet their needs in an efficient way. The revision of the Coordinated Plan should promote procurement policies facilitating access to such cloud computing capabilities.   ;4 - Important;4 - Important;5 - Very important;Practical research and development on specific use cases should be supported by EU funding and centers of excellence should promote innovation and foster highest quality/most effective AI. We would welcome EU support and investment to maximize AI accuracy and effectiveness.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;Moving to AI cloud services would accelerate and broaden the access, development and use of AI by SMEs. SMEs should have access to the widest range possible of AI technologies which cloud-based offerings are well-suited to provide. ;5 - Very important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;AI is not unique in the potential to endanger safety or breach fundamental rights, especially if misused.A distinction should be made between actual risks that are inherent to use of an AI system versus risks that apply to all technology or risks related to intentional misuse of any technology.To assess potential regulatory gaps, the performance of AI systems should be compared against the performance of non-AI systems and processes, and the benefits and disadvantages of each carefully balanced.;Other;To best preserve the benefits of innovation, the EU should rely on existing legislations to the extent possible and create AI-specific regulation only to address specific, articulable harms that are not already addressed. The sensitive areas identified in the White Paper as having greatest potential harms (e.g., surveillance, medical diagnosis and treatment, autonomous vehicles) should be reviewed individually and accompanied with policy responses appropriate to the particular application of AI.;Yes;;Other;We agree with the general approach that legislation should only apply to high-risk applications. It is critical that any regulation should be specific to the individual use case, and build on existing industry-specific legislation where applicable, to account for the specific-risks associated with the use case/industry. High risk applications should be specifically enumerated so there is no ambiguity over whether regulations may apply. ;The sensitive areas identified in the White Paper as having greatest potential harms (e.g., surveillance by law enforcement agencies, medical diagnosis and treatment, autonomous vehicles) should be reviewed individually and accompanied with policy responses appropriate to the particular application of AI. ;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;Other special requirements in addition to those mentioned in the question above should be imposed (please specify);We have proposed guidelines for a legislative framework that protects individual civil liberties and helps ensure that governments are transparent in their application of facial recognition technology.;Rather not;;Other enforcement system;We support a combination of (i) ex-ante self-assessment through testing and other methods that accurately measure performance and help mitigate any bias in AI systems and (ii) ex-post enforcement. Accurate tests are important not only to advance the acceptance of AI systems, but also to guide responsible uses of the technology.;Mandatory record-keeping and record-sharing raises significant intellectual property right concerns and presents large potential regulatory burdens. We support consideration of specific, measured requirements for enumerated high-risk AI applications.;;Any proposed regulation should be very clear on safety risks that are inherent to specific uses of AI, versus general risks that may or may not exist depending on how AI is used.Any regulation should be appropriately tailored to specific uses of AI that present clear safety risks meriting extra precautions.Moreover, the performance and safety of AI should be compared against the performance and safety of non-AI systems and processes, and the benefits and disadvantages of each carefully balanced.;No;;No;The current liability framework is technology neutral. Therefore, it covers potential risks raised by the use of AI systems. Expanding the scope of the Product Liability Directive to software and AI would mean that the AI producer would be held liable for damages related to uses for which the producer has no control (including malicious use), knowledge, or ability to foresee. Such expanded scope would likely have a profound chilling effect on development and innovation of AI in the EU. ;No;;Determining who is primarily responsible for negligence or other potential harms associated with use of AI technology is critical so that (i) harmed parties have appropriate avenues for recourse and (ii) the components on the supply chain understand their levels of responsibilityand liability. Deployers should be primarily responsible as they are in the better position to make sensible decisions about whether a particular AI system is fit for a given use and implement risk-mitigation processes.;Amazon_Coew_Sarl_Consultation_on_the_White_Paper_on_Artificial_Intelligence.pdf
F530241;13-06-2020 18:17;English;Academic/Research Institution;Karen;Yeung;;University of Birmingham;16470 The University of Birmingham;Large (250 or more);United Kingdom;The feedback can be published with your personal information;5 - Very important;3 - Neutral;4 - Important;3 - Neutral;2 - Not important;1 - Not important at all;;5 - Very important;5 - Very important;1 - Not important at all;1 - Not important at all;4 - Important;3 - Neutral;;3 - Neutral;4 - Important;3 - Neutral;;2 - Not important;4 - Important;3 - Neutral;4 - Important;2 - Not important;;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;See separate submission;There is a need for a new legislation;;No;;;;See attached submission;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;Other special requirements in addition to those mentioned in the question above should be imposed (please specify);Much more rigourous and careful analysis needed AND open public debate and discussion;Rather not;See attached submission;Other enforcement system;See attached submission;See attached submission;Mental health risks;See attached submission;Yes;See attached submission;Yes;;No opinion;;I do not have expertise in all national liability rules across EU;Karen_Yeung_Response_to_EU_White_Paper_on_AI_June_2020.pdf
F530240;13-06-2020 18:16;English;Company/Business organisation;Irina;Michalowitz;;Twilio Inc.;067223231522-58;Large (250 or more);United States;The feedback can be published with your personal information;5 - Very important;3 - Neutral;4 - Important;4 - Important;5 - Very important;5 - Very important;;3 - Neutral;3 - Neutral;4 - Important;3 - Neutral;4 - Important;3 - Neutral;;No opinion;No opinion;No opinion;;4 - Important;2 - Not important;2 - Not important;4 - Important;No opinion;;4 - Important;4 - Important;4 - Important;2 - Not important;3 - Neutral;3 - Neutral;While several important topics are raised, these challenges are context-specific to certain uses of AI rather than the deployment of AI overall. Addressing these context-specific challenges requires addressing careful nuances and addressing harms in ways that do not inhibit the development of AI overall.;Current legislation is fully sufficient;;No opinion;;;;We welcome a regulatory distinction between high risk and low risk applications of AI and support a nuanced approach that enables this, including differentiation not just by sector or type of AI application but by context and use.;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;No opinion;;Rather not;Voluntary labelling systems can be confusing and misleading for businesses and consumers and may become de-facto mandatory through misperception or subsequent procurement requirements. Further development of the technology and approaches to transparency is advised prior to implementing requirements.;Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;As technolgoies and their uses evolve, ex-post market surveillance is the most effective way to assess compliance. Strict ex-ante requirements risk disincentivizing smaller developers and may not take into account real-life deployment scenarios.;;;No opinion;The safety framework should consider the intended use of products versus how companies may choose to deploy them. The entity deploying an AI system should bear the safety risk as it can best foresee risks and is closest to consumers who may suffer harm, making it easier for them to seek redress.;No;At this stage, the focus should be on identifying clear liability gaps. We have outlined in our general comments our views on how existing liability rules can be applied to AI situations.;Yes, for all AI applications;;The entity deploying AI should bear liability in most cases as this enables the most direct avenues of redress for consumers and insulates AI developers for risks arising for deployment scenarios over which they have no control. Developers should not be liable for the misuse of their AI products by their enterprise customers.;Twilio_Submission_EU_White_Paper_on_AI.pdf
F530239;13-06-2020 18:15;English;Other;Anastasiya;Kiseleva;;Vrije Universiteit Brussel;;Large (250 or more);Belgium;The feedback can be published with your personal information;No opinion;5 - Very important;5 - Very important;4 - Important;No opinion;No opinion;;5 - Very important;No opinion;No opinion;5 - Very important;5 - Very important;No opinion;;No opinion;5 - Very important;No opinion;;No opinion;5 - Very important;5 - Very important;5 - Very important;4 - Important;;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;Instead of AI's explainability, the regulatory framework shall be focused on AI's transparency. Also, accuracy of AI is directly correlated to its safety and non-discrimination. Instead of discriminatory affects, the concerns shall be focused on more general consequence such as AI's unexpected correlations. ;Current legislation may have some gaps;;Yes;;Yes;;Healthcare AI applications used for medical purposes ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;High-risk AI applications shall be certified by external authorities ex-ante and then monitored ex-post (see the attached comments);;;Yes;To avoid legal uncertainty and too much burden on AI manufacturers, the scope of the most important changes that should lead to new risk assessment shall be defined in advance. The changes outside this scope should not lead to a new risk assessment. ;No opinion;;No opinion;;;Kiseleva_COMMENTS_ON_AI_WHITE_PAPER_13062020.pdf
F530238;13-06-2020 18:14;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;Quality digitization of public sector;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;3 - Neutral;;3 - Neutral;5 - Very important;4 - Important;;No opinion;No opinion;No opinion;No opinion;No opinion;;3 - Neutral;4 - Important;3 - Neutral;2 - Not important;4 - Important;2 - Not important;;Current legislation may have some gaps;;Other;Instead of top down regulation separate Industries may have quality/speed or any other metric thresholds defined for AI. Voluntary verification may provide AI system metrics to validate the compliance.;;;military, medical, transportation;3 - Neutral;4 - Important;3 - Neutral;4 - Important;1 - Not important at all;3 - Neutral;No further guidelines or regulations are needed;;Rather not;public NIST biometric technology evaluation reports, medical complacency certification;No opinion;;;;;Yes;public NIST biometric technology evaluation reports, medical complacency certification;No opinion;;No opinion;;;The_position_of_ecosystem_of_Lithuania_s_business_and_startups_who_work_in_ai_field.pdf
F530237;13-06-2020 18:10;English;Other;Rob;Heyman;;Knowledge Centre Data & Society.;;Small (< 50 employees);Belgium;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;KCDS_Feedback_on_EC_Whitepaper_20200613.pdf
F530236;13-06-2020 17:59;English;Other;Vania;Putatti;;EuroHealthNet;48562122691-12;Small (< 50 employees);Belgium;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;No opinion;No opinion;4 - Important;"The introduction of AI tools in the public sector will be crucial to improve health and social equity and wellbeing, given that: accountably is guaranteed; trust is built by raising awareness on the advantages (eg. improved personalised care); social needs are met to counter job losses. The introduction of AI in health settings should be done ensuring equal access (see Pharos, eHealth4ALL model). AI also means better sustainability, but wellbeing should be always priorised over economic issues";4 - Important;4 - Important;5 - Very important;No opinion;5 - Very important;4 - Important;Specific actions should be taken to ensure control over the outcomes generated by the uptake of AI technologies. This is crucial to protect social determinants of health and wellbeing while boosting sustainability and equity. Health Equity Impact Assessments should be a fundamental precursor to the introduction and expansion of AI systems (across sectors and levels).;3 - Neutral;No opinion;No opinion;;No opinion;No opinion;No opinion;No opinion;No opinion;Digital Innovation Hubs should be used to address public needs as part of their priorities. The public sector should be able to benefit more from these settings.;4 - Important;4 - Important;5 - Very important;3 - Neutral;3 - Neutral;4 - Important;AI algorithms are mostly based on questionnaires in which low socioeconomic groups, people with less education and people with a migrant background are underrepresented. More inclusive research is needed to overcome discriminationatory outcomes. There should also be more concern for client/patient participation and co-creation. ;There is a need for a new legislation;;No;;;;In general, inappropriate design of AI applications can lead to discriminatory outcomes, thus the increase of inequalities across the society. In most cases, outcomes of AI systems cannot be fully predicted. As such, any AI application should be considered high-risk and well regulated. In health, performance of AI health devices should be guaranteed by regulation with focus on robustness/vulnerability and explainability.;5 - Very important;4 - Important;5 - Very important;5 - Very important;No opinion;5 - Very important;Other special requirements in addition to those mentioned in the question above should be imposed (please specify);Ethics - including health ethics, must be a key factor and considered at all levels;Rather not;In health, it has been shown that voluntary and self-regulating systems result less effective than legislative frameworks based on precautionary principles to ensure fair and safe outcomes. Thus, we suggest that exclusively well regulated frameworks should be applied to AI systems, for both high and non-high risks applications .;Other enforcement system;A quality requirements conformity assessment should be incorporated. Potentially, a labelling system too ;;Mental health risks;Studies have shown that AI systems present risks of discrimination, resulting in the widening inequalities across the society. This can be caused by different factors, such as the use of datasets that represent society unevenly (eg.: gender, origins, socioeconomic status, etc.), or potential bias in the actual design of AI systems. It is therefore crucial that these risks are addressed through strong regulation to ensure undiscriminatory behaviors, in particular toward more exposed groups.;Yes;Risk assessment procedures should be updated taking into consideration the development of existing societal and enviromental challenges or the emerging of new ones. Each AI application has also the potential to transform the sector they are implemented in. Thus, continue monitoring for the appearence of new challenges or opportunities for improvement should feed and update risk assessment procedures.;No;;No opinion;;;
F530235;13-06-2020 17:40;English;Company/Business organisation;David;Ohrenstein;;Autodesk, Inc.;684824020045-19;Large (250 or more);United States;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;;4 - Important;4 - Important;5 - Very important;4 - Important;5 - Very important;4 - Important;;4 - Important;4 - Important;4 - Important;;5 - Very important;4 - Important;4 - Important;4 - Important;3 - Neutral;;4 - Important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;Deployment of AI technologies will have a significant impact on the workforce, eliminating some jobs and tasks and creating new ones. Governments, educators and industry must collaborate to provide students and workers the skills they need to use and benefit from AI, and to aid workers displaced by AI. Autodesk is working to better prepare our customers, communities and employees for the future of work. ;Current legislation may have some gaps;;Yes;;No opinion;;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;No opinion;;No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;;There should be a careful assessment of where current laws are sufficient to address risks posed by use of artificial intelligence before creating new laws that could lead to less legal certainty for developers of this technology.;No opinion;;No opinion;;No opinion;;;Autodesk_Artificial_Intelligence_Policy_Paper_2020.pdf
F530234;13-06-2020 17:38;English;Company/Business organisation;Roisin;Watson;;Aviva;86270761494-62;Large (250 or more);United Kingdom;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;To build an ecosystem of excellence that can support the development and uptake of AI we need to build trust and understanding which in turn, we believe, will drive adoption. Therefore in addition to the proposed six actions, we would recommend a targeted education agenda focused on demystify AI technologies and current misconceptions, how different technologies and techniques work together and building understanding of key benefits and risks. ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;In developing skills and adapting existing training programmes it will be key to recognise the range and combination of skills needed to ensure individuals, businesses and the public sector can support and benefit from the uptake of different AI technologies. For example, to collaborate well and with other teams and to help build consumer understanding and trust, communication skills are crucial alongside technical expertise.;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Any concerns about AI technologies are extremely important and should be addressed. Though we believe concerns highlighted above are not specific to AI – these are concerns that have the potential to be present in any system or process. For example, there is potential for bias in systems and processes old and new and we therefore need to challenge existing governance to adapt to new technologies. We do see potential new risks emerging from AI technologies which we have detailed in our attachment;Current legislation is fully sufficient;;Other;AI technologies are part of a process that leads to an outcome whether that be a product recommendation, a price or decision on whether to accept a claim. We believe legislation should be specific to the outcome that different techniques and technologies are trying to lead to and from an FS perspective current legislation is fully sufficient and safeguards against concerns highlighted. For ex. regulatory principles on treating customers fairly are applicable regardless of whether AI is used;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;;Much;To drive trust in the system, any accreditation would need to ensure it is meaningful and transparent to avoid a tick box exercise or accusations of “ethics washing” which would have the opposite effect. A principles based approach would be helpful in doing this and we would recommend working closely with a range of stakeholders to develop the approach. We do not think using requirements developed for high-risk AI would be the right approach to achieve the desired effect.;Other enforcement system;We fully agree that high-risk AI should face greater scrutiny and would welcome a more nuanced approach. As noted in the White Paper, not all requirements may be suitable to be verified through a prior conformity assessment - though clearly certain applications would - and an approach that is designed around the requirements (for example, that certain AI systems evolve and learn from experience), impact and outcomes of different AI systems is welcome.;Compliance should be consistent with an outcomes based approach and be embedded within existing regulatory frameworks where they exist rather than building stand alone frameworks or new bodies. ;;;;;;;;;;EU_AIWhitePaper_AvivaAdditionalResponse_13Jun2020.docx
F530233;13-06-2020 17:38;English;NGO (Non-governmental organisation);Anna;Tahovská;;Confederation of Laboratories for Artificial Intelligence Research in Europe (CLAIRE, AISBL);205170133342-09;Large (250 or more);Belgium;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;Yes, please see our detailed response.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Yes, please see our detailed response.;5 - Very important;5 - Very important;4 - Important;Yes, please see our detailed response.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;Yes, please see our detailed response.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;Yes, please see our detailed response.;Current legislation may have some gaps;;Other;Please see our detailed response.;;;;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;;;Yes;;Yes;;No opinion;;;CLAIRE_Response_to_the_EC_White_Paper_on_AI.pdf
F530232;13-06-2020 17:22;English;Business Association;Stephan;MIETKE;;Bundesverband deutscher Banken e.V. (Association of German Banks);0764199368-97;Large (250 or more);Germany;The feedback can be published with your personal information;4 - Important;3 - Neutral;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Improved overall conditions for using and exchanging data, such as establishing a sandbox and pooling data, would facilitate experimentation with AI and encourage collaboration across corporate/sectoral boundaries. This could enable providers to respond more quickly to market trends and customer needs, gain new insights using AI methods based on a broader pool of data, and hence enhance their innovative capability and their competitiveness, including in the international competitive environment.;3 - Neutral;2 - Not important;4 - Important;3 - Neutral;2 - Not important;5 - Very important;AI-specific expertise may not be strengthened at the expense of training in basic disciplines (MINT). The practical experience of our member companies indicates that there is significantly higher demand for AI applications and a shortage of specialists with a sound mathematical/statistical background, rather than demand for highly specialised AI experts.;3 - Neutral;4 - Important;5 - Very important;By themselves, well-equipped research institutions are not enough to ensure the necessary knowledge transfer to the user industries. Greater support for application-centric AI research and collaborative AI projects with broad corporate involvement should be encouraged, and could stimulate upskilling and talent development by increasing the exchange of specialists between industry and research.;5 - Very important;5 - Very important;5 - Very important;2 - Not important;2 - Not important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;The risks outlined above must be adequately addressed by means of appropriate internal processes at the companies developing and deploying AI. Banks are subject to a strict supervisory regime for all their business activities, including regulatory requirements applicable to the use of AI and their monitoring. This meets high standards with regard to the concerns referred to above.;Current legislation may have some gaps;;Other;We advocate introducing minimum requirements for “critical decision-making processes” if decision-making is fully automated and the decision has a significant impact on those affected. This could be done by establishing a framework of objective criteria that defines and validates the requirements for decision-making. It would allow the confidence of affected persons in fully automated decisions that are important for them to be strengthened in a technology-neutral approach.;;;;5 - Very important;5 - Very important;4 - Important;5 - Very important;3 - Neutral;5 - Very important;No opinion;Future developments must be monitored and, if necessary, legislation must be adapted if concrete problems become evident. ;Not at all;In our opinion, voluntary labelling systems offer no benefit, as effective application and monitoring in practice hardly seems achievable.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;;;No opinion;;No opinion;;No opinion;;;
F530231;13-06-2020 17:22;English;Business Association;Kirsty;REID;;European Federation of Pharmaceutical Industries and Associations (EFPIA);38526121292-88;Medium (< 250 employees);Belgium;The feedback can be published with your personal information;5 - Very important;4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;"Initiatives supporting adoption: 
- investment in cutting edge innovation to develop a positive ecosystem; leadership & coordination to differentiate activities of EC and Member States; instil urgency to ensure competitiveness; education & communication to embrace acceptance of AI by public sector; skills strategy should emphasise early education and schooling; responsibility for public sector to promote adoption; investment in computing infrastructure and access to high quality datasets";4 - Important;3 - Neutral;4 - Important;4 - Important;5 - Very important;5 - Very important;"Continuum of education and literacy, empowerment, skills including:
- Setting the ambition & urgency of AI; Strengthen excellence in application of AI; Clear guidelines for interpretability, validity & trust of AI; Educate and empower Citizens, young and old, in AI literacy and skills; Develop AI training programmes in medical schools and amongst HCPs; Coordinate integration of AI into public healthcare systems; Create a framework for uptake of AI with EU health data space as key enabler";4 - Important;3 - Neutral;5 - Very important;"- PPPs are key to developing skills & careers, attracting talent & minimising skills export.
- AI in the Education sector to develop skills, empower citizens and continuous advancement.
- Incentivise funding of AI literacy, especially to the under-privile";3 - Neutral;3 - Neutral;4 - Important;5 - Very important;4 - Important;"- Ease the bureaucratic burden required for an SME to exist (access to fund, tax declarations, employment rules).
- Proactively identify and mitigate challenges to realising innovation.
- Providing core expertise to accelerate AI innovations – regulatory,";5 - Very important;4 - Important;4 - Important;5 - Very important;3 - Neutral;4 - Important;"-	Balancing concerns with gains of AI 
-	Clarity on AI decision-making criteria and collateral effects 
-	Decision-making is traceable and reproducible
-	Evolve knowledge and legislative framework at the same pace of AI discovery & use
-	Managing AI syste";Other;"The current privacy framework is sufficient. New rules for AI systems should not duplicate existing regulations (e.g. GDPR, MDR). Further clarifications of existing rules may apply to: 
- specify how GDPR enables innovation in AI 
- reconcile the GDPR principle of ""data minimisation"" with the need for extensive datasets in AI
- clarify the notions of ""anonymized"" data under GDPR 
- foster a practice of transparency and accountability by organizations ";Other;"Not new rules but clarification of existing rules can be limited to high-risk. 
Adopting a risk-based approach would: 
- ensure greater attention is paid to data uses with greater risks; drive the adoption of adequate precautionary measures; justify higher resource & time on mitigation plans & actions in order to address any resulting harm or prejudice; Deliver transparency to ensure meaningful choice & human intervention when there is a real risk of individual exposure or risk of misuse.";;;"Self-learning systems; where there is least intervention between steps; level of autonomy";4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;"Specific guidelines should specify the need for increased accountability ensuring:
- accuracy of personal data to avoid mistaken identity
- safeguards and security around the processing of other sensitive data
- transparency
- disciplining linkage between biometric data and other available databases
- safeguards around tracking, profiling and automated decision-making
- revocation/rectification of the biometric data by data subjects
- limited data retention
- right of individual to exercise autonomy";No opinion;"It is a good idea if there is a purpose to it. It is important not to make this a disadvantage to SMEs i.e. it has to add value that is clearly communicated.

It is possible that such a system could help the adoption of AI in healthcare practice, while fostering better understanding among relevant actors.";A combination of ex-ante compliance and ex-post enforcement mechanisms;;"Compliance provisions should be structured to refer to existing requirements, contributing to enhanced legislative clarity & avoiding inconsistencies, through the development of sector specific guidance documents issued by regulatory authorities
Enforcement system should be built on HLEG recommendations including: Explaining AI system algorithms to empower citizen base; Transparency rules to increase trust; Sharing positive & negative examples; Connecting to other parts of the ecosystem";Risks related to the loss of connectivity;"- Risk of ""ignorance"" and subsequent uninformed reliance.
- Risk of use by non-informed public.
- General EU safety legislation currently in force applies to products and not to services, and therefore in principle not to services based on AI technology;
";Yes;"A multi-party private-public partnership (like the one overseeing cybersecurity good practices in Germany) could be conformed to issue good practice certificates. Furthermore, there is already a good basis under MDR guidance dealing with significant changes in software as a medical device.
Companies could benefit from compliance programs guiding the development and use of AI systems, ensuring proper oversight, staff training and contingency plans.";No;There is already a good basis under MDR guidance dealing with significant changes in software as a medical device.;No opinion;;;EFPIA_EuropeanArtificialIntelligence_briefing.pdf
F530230;13-06-2020 17:17;English;NGO (Non-governmental organisation);Orsolya;Reich;;Civil Liberties Union for Europe;544892227334-39;Small (< 50 employees);Germany;The feedback can be published with your personal information;No opinion;No opinion;No opinion;No opinion;1 - Not important at all;1 - Not important at all;The Civil Liberties Union for Europe (henceforth “Liberties”) would like to express its deepest concern about the European Commission’s basic assumption in the White Paper, namely that it is imperative that uptake of AI is promoted, especially in the public sector. Liberties believes that this assumption needs to be carefully scrutinized.;No opinion;No opinion;1 - Not important at all;No opinion;No opinion;No opinion;Liberties would like to stress that promoting AI ought not to be regarded as a self-evident aim. Liberties believes that the approach the European Commission seemingly takes, the approach sometimes coined as “technosolutionism”, is in fact dangerous. In certain cases technology may help us in solving societal problems, but in others it may exacerbate them and/or undermine our fundamental rights.  ;No opinion;No opinion;No opinion;While the White Paper discusses strengthening European AI research at length, the need of research into the human rights and social impacts of new technologies based on AI are clearly not given appropriate attention. The EU must generously fund such research. In addition, EU research funding on technology (AI included) should always be conditional on meeting strict ethical standards. European AI ought not only look trustworthy. It needs to be trustworthy.;1 - Not important at all;No opinion;No opinion;No opinion;No opinion;While catering to the needs of small and medium enterprises may well be in the public interest, it is imperative to emphasize that no business should be exempt from respecting human rights. Businesses of any size are not to be granted exemptions from ensuring that the technology is safe, fair and rights-respecting.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;Liberties believes that the use of AI (especially in the public sector) should be based on, or informed by, rigorously established scientific evidence, should meet certain transparency requirements, and should be subject to democratic oversight. Liberties believes that these concerns are not adequately addressed in the White Paper.  ;Other;"Some yes, some not. However, even the best piece of legislation becomes paltry without proper enforcement. Enforcement authorities need to be sufficiently funded and upskilled to face the challenges posed by emerging new technologies. The EU must urgently address the issue of lack of appropriate GDPR enforcement.
In addition, the EU must ensure that no new regulatory framework for AI allows loopholes to the GDPR (or any other legislation protecting our fundamental rights). ";Other;"Mandatory fundamental rights impact assessments ought to be conducted for all AI applications, and some types of applications should be banned outright.
Categorising all AI into ""high-risk"" and ""low-risk"" is fundamentally flawed. Some uses of technology are not “high-risk”, but au fond incompatible with the values the European Union is founded on and stands for, and, as such, unacceptable. The sector-based approach should be replaced by an outcome-based human-rights approach. ";;;The White Paper does not suggest a ban on remote biometric identification. This is a grave mistake. Without a ban, law enforcement agencies will use the technology in ways they see fit. Police surveillance coupled with remote biometric identification technology endangers our democracies. It hinders our right to speak our minds, to meet others, and publicly express our disagreement with people in power. It is especially dangerous in times when authoritarianism is on the rise.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;Liberties strongly believes that such systems pose significant threats to our fundamental rights and ought to be banned. ;Rather not;Liberties does not find the high-risk/low-risk categorization acceptable. The voluntary labelling system ought not to be adopted.;Other enforcement system;Liberties is of the standpoint that all AI systems should undergo a mandatory ex ante human rights impact assessment from an external body. ;In order to guarantee that fundamental rights are respected, we need external bodies to assess compliance. In addition, a strong opportunity for democratic oversight needs to be established. AI systems need to be transparent in a meaningful way. Grants for capacity-building and for assessing system compliance need to be available to watchdogs and to the independent media. ;Mental health risks;Microtargeted political advertising and its detrimental consequences to the democratic political process needs to be expanded on.;Yes;;Yes;;Yes, for all AI applications;;The EU needs to review copyright and database protections to allow users to seek redress.  ;AI_WPcon_Lib_Annex.pdf
F530229;13-06-2020 17:04;English;Academic/Research Institution;Cecilio;ANGULO;;Universitat Politècnica de Catalunya - UPC BarcelonaTECH;;Large (250 or more);Spain;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;An empowered legislation at industrial and business level to promote digital skills from childhood. It is compulsory to protect data inside the EU. Focus on collaboration between academia and indsutry.;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;"Promote regional development centres to improve results. Besides, promote world-class networking infrastructures not depending on local funding. Build on public datasets and the ""data donation"" concept. Avoid elitist education on only 'leading' universities. Spread training programmes everywhere, for all the levels, including K-12 education.";3 - Neutral;5 - Very important;5 - Very important;Work on a strong network of specialised AI research excellence centres, avoiding duplications that are unnecessary. ;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;To exploit and disseminate the concept of DIH working on vertical levels as well as horizontal ones, following a regional policy. ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Generate awareness outside the academic world and promote a political dialogue. This is not a drawback, it is the European opportunity to demonstrate our values worldwide. ;There is a need for a new legislation;;Yes;;No;;Legislation ought to protect the EU space. It should allow/promote innovation but also empower the respect of the law. A system should be established to avoid the possible misuse of AI-based tools and systems. Current definition is vey unclear. It leaves the door open to interested interpretations and allow the misuse of the technology. For instance, biometric recognition, especially in the case of pervasive one, autonomous weapons, advanced fintech.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);"Biometric identification, only when tutored by a judge. In general, we need a transversal discussion of these issues. There should be also an effort not only to ensure the security of the data and its correct (and useful) re-usability, but to ensure that all new applications, systems are built following the concepts of ""Privacy by design"" and impose this approach to all new EU products.";Not at all;The labelling system ought to be compulsory;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Independent informed audit should be installed. ;Personal security risks;Mental health risks should be better defined. Moreover, as technology progresses, new risks will be added. Law should be flexible enough.;Yes;;No;It should be stricter and it should be disseminated.;Yes, for all AI applications;;Autonomous weapons, fintech, electronic currencies, fake information, biometric identification.;
F530228;13-06-2020 16:59;English;EU Citizen;Maximilian;Laurenz;;;;;Germany;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;3 - Neutral;2 - Not important;3 - Neutral;Actions G and H are not listed in the question. Why? I consider section H / last paragraph as very important.;5 - Very important;5 - Very important;4 - Important;3 - Neutral;4 - Important;3 - Neutral;;5 - Very important;4 - Important;3 - Neutral;;3 - Neutral;4 - Important;5 - Very important;3 - Neutral;No opinion;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;There is a need for a new legislation;;No;;;;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Not at all;;Other enforcement system;There should be a strict and equal regulation for all AI systems (meaning systems that make own decisions on provided data).;;Personal security risks;;;;Yes;;Yes, for all AI applications;;;
F530227;13-06-2020 16:46;English;NGO (Non-governmental organisation);Aurélie;BARANGER;;Autism-Europe;816993514049-42;Micro (< 10 employees);Belgium;The feedback can be published with your personal information;4 - Important;4 - Important;5 - Very important;3 - Neutral;4 - Important;4 - Important;"Coordinated Plan should ensure societal wellbeing, broadest possible range of users, all members of society equally benefit from AI
Autistic people & Disabled People Orgs (DPOs) must be involved in Research & Innovation (R&I), AI development & public oversight together with private sector
Skill-building & awareness must be accessible for people with disabilities
Public procurement of AI must comply with EU public procurement, anti-discrimination, accessibility laws and standards";4 - Important;5 - Very important;2 - Not important;4 - Important;4 - Important;4 - Important;"• R&I, update of AI, skill development must ensure accessibility and cooperate with organisations of persons with disabilities. Co-creation is important to address the unmet needs of the autistic population through AI solutions.
• Support open source assistive technologies to foster full inclusion of people with disabilities (PWD) in society by transparent and collaborative solutions. 
• AI human rights and societal impact assessment, democratic oversight must be ensured.
";4 - Important;5 - Very important;5 - Very important;Ensure participation of DPOs in R&I, by funding too. Inclusive, participatory principle is crucial to ensure AI doesn’t increase discrimination and to use PwD’s valuable contribution to reach substantive equality. Universal design and new assistive technologies should both be prioritised. Ex-ante rules for funding with reference to accessibility, fundamental rights assurance, participation of PwD in R&I are crucial. Solutions obtained by public funds should be released by a Free Software license;3 - Neutral;5 - Very important;3 - Neutral;4 - Important;No opinion;"DIHs must ensure that fundamental rights, accessibility and inclusive participation principles and laws are upheld by SMEs and partners.
DPOs to participate in partnerships at equal level in AI projects.
Ex ante rules for funding of AI projects by SME should apply in reference to accessibility, fundamental rights and involvement of DPOs. 
Public funding on AI applications should benefit the public, outcomes should be freely available and applications licensed under Free Software licenses";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;AI can increase existing inequalities for PwD and lead to their discrimination and harm. There have already been several worrying practices. So, there is great need for a strong regulatory framework (safeguards, oversight, redress mechanisms) against discrimination and increase of inequalities, and criteria for accessibility based on design for all approach. Closed source AI solutions are a risk due to trust issue, lack of possible verification, control as well as possible software weaknesses.;Other;"Important to consider that current EU equal treatment & accessibility-related legislation is not comprehensive. So, specific rules guaranteeing accessibility and protection of fundamental rights for AI systems are needed
Publicly financed software should be made publicly available under a Free and Open Source Software license: if it is public money, it should be public code as well, so you can always verify if an AI solution does what the developers say and you do not need to trust them";Other;Lack of possible verification due to Non-transparency of algorithms with closed source softwares  and ‘mutation’ capabilities of AI-based products may result in unforeseen risks and expansion of our understanding of safety. So, limiting requirements to a list of ‘high-risk’ AI use is not future-proof. Risks may evolve over time and vary considering human diversity. At first, the focus should be on setting boundaries to the domains in which AI can be deployed. Even if there is small risk of harm.;;;AI datasets referring to people risk overseeing minorities (e.g. PwD) even in low-risk qualifying use. Domain by domain assessment must be done, seeing all potential implications for the widest range of people. EU must set the limits as well as ex ante and regular ex post evaluations of allowed AI systems. Applications affecting citizens must be considered with the presumption of high-risk, especially if closed source. Long-term consequences of PwDs' personal data use for AI must be assessed.;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;Biometric identification systems present high risk of being highly discriminatory, often inaccessible for PwD, deployed without the consent of affected people. Sensitive data about an individual can be gathered without their consent and later be used to discriminate against the person. However, such systems may be used for assistive technology for understanding social interactions by PwD, for personal use only, a necessary safeguard could be an open source software audited by a third party.;Rather not;It’s difficult to establish risk levels, as AI use poses considerable transparency concerns and can change functionality during their lifespan. Strong regulatory frameworks with safeguarding mechanisms, including clear governance structures and robust enforcement are vital. This needs to be applied to all AI usage for now, with the possibility to review and soften compliance, if proven unduly demanding. However, a label for free software solutions that secure transparency should be encouraged.;Other enforcement system;"Ensure ex ante by means of external conformity assessment procedures, and an enforcement mechanism, to include a redress mechanism for users.
Public authorities must safeguard citizens against potential risks associated with AI, as many people who currently use AI-powered ‘solutions’ do not understand how their data is used by the technology industry. 
For transparency, free software solutions should be promoted and information technology audits should be performed by an independent party.
";It should be clearly stated that external conformity assessment procedures should be carried out by an independent public organisation to avoid private entities from unduly certifying clients for commercial reasons.;Mental health risks;Automated discrimination is more difficult to detect as people do not have access to the algorithms that underpin them. It creates significant legal uncertainty and makes it difficult to determine liability for AI-powered ‘solutions’. There are other risks, including risks impacting personal security, mental health, discrimination, inequality, and lack of accessibility and data privacy. Loss of freedom linked to AI solutions that are closed source, again it is important to promote free software.;Yes;Any assessment of risk to fundamental rights, non-discrimination, equality, accessibility, privacy, personal security and mental health should be based on an intersectional approach, taking into account the full diversity of affected persons/groups in society. DPOs should be involved in such assessment procedures.;Yes;New AI solutions have liability issues not covered by current framework that might disadvantage users or block new AI solutions (e.g. assistive technologies for PwD). It should be strengthened to take new risks into account. Any revision of current EU legislative framework on liability should ensure mechanisms for the users to claim their rights. ;Yes, for all AI applications;;Persons affected by AI applications must be able to redress issues that have been observed. Copyright, database rights protection, or other forms of business confidentiality principles should not be used to prevent affected persons from seeking redress for harm caused, especially (but not limited to), cases of discrimination and breach of fundamental rights.  There should be an accessible non-judicial mechanism prior to Court action that should be able to remedy the situation.;
F530226;13-06-2020 16:42;English;Academic/Research Institution;Rossano;Schifanella;;University of Turin;459368424625-42;Large (250 or more);Italy;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;Promoting a human-centric view that complements the technological/economic-centric focus that emerges from the White Paper. ;5 - Very important;4 - Important;5 - Very important;To build the vision of a trustworthy AI is necessary the critical engagement from parties that are not mentioned in this consultation, like civil society, public interest groups, and communities who would be affected by the innovations.  ;4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;;There is a need for a new legislation;;No;;;;;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;;Yes, for specific AI applications;;;
F530225;13-06-2020 16:06;English;Business Association;Antonio;CIMORRA;;AMETIC (ASOCIACIÓN MULTISECTORIAL DE EMPRESAS DE LA ELECTRÓNICA, LAS TECNOLOGÍAS DE LA INFORMACIÓN Y LA COMUNICACIÓN, DE LAS TELECOMUNICACIONES Y DE LOS CONTENIDOS DIGITALES);013076816891-46;Small (< 50 employees);Spain;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"• Supporting the R&I community.
• Ensuring the right skills can prosper from the AI benefits. 
• Focus on SMEs item is indicated as very important since the business fabric of large companies must function as a catalyst to reach all companies, prioritizing the mechanism as well as the objective.
• EU must promote digital platforms where ecosystems of private and public stakeholders share data in controlled ways.
• Building sector-wise partnerships over interregional networks.
";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"• Intellectual Property 
• The use of sandboxes in tests, differentiating this type of sandbox from the regulatory sandbox
• Democratize the use of AI from regular citizens to SMEs and got’ bodies through the use of AI as a service.
• Standardize algorithm transparency and foster its use.";5 - Very important;5 - Very important;5 - Very important;"• Promote open source initiatives to accelerate research and development. At the same time, review the intellectual property laws to ensure IP protection to developers and R&D.

• It is important to highlight the first action: Support the establishment of a lighthouse research centre that is world class and able to attract the best minds, but always in an inclusive and networked way.

• Strengthen the networks for interdisciplinary research so to promote AI based on EU values.
";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"
• It is necessary to know the functionality of a single Center
• Apply more flexible financing models focus con R&D and absorption of the market.
• Showcase business and collaboration models for both tech- and non-tech SMEs.
• Facilitate access to broad spectrum of access to talent for SMEs beyond simply hiring.
• Design funding mechanisms for SMEs that favour the growth in size and relevance. 
";5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;"• Evaluation the benefits of the adoption of Artificial Intelligence in decision making
• Assurance models must be ideated to reasonably cover potential peculiarities of AI.
• AI may lead to business concentration and decrease competence.
• AI may cause blurring accountability.
";Other;"In order to learn by doing, to avoid delays and gaps Vs US and China, we suggest enabling regulatory sandboxes for AI 
Regulation on facial recognition systems
Multidisciplinary research may help anticipate negative reactions & deliver recommendations for policies & production
High Risk AI apps should be regulated to preserve users' privacy and fundamental rights
Legislation must keep the pace with technology progress but avoiding legislative desserts where old non-adapted norms are adopted";Other;"It is relevant to provide rules for critical systems in a different level that the ones for lesser critical business. 
Internationally agreed best practices should be used or to consider for AI software and the data cycle-AI (Government, Management and Data Quality). For example, to follow are the international ISO standards.
Requirements must be built in level according to risk. What is a high-risk or medium-risk application must be defined according to a standardized set of concerns.";;;There is wide range of them, from applications operating in Critical Infrastructures to health-related applications, criminal records, inter-business applications, dual-use applications, etc.;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;Other special requirements in addition to those mentioned in the question above should be imposed (please specify);"• For each use it is necessary to specify which regulation is appropriate and how it is used
• Biometric identification systems in publicly accessible spaces may bring few advantages and numerous important disadvantages (exposure of personal data, limitations to individual freedom, etc.). Their use must be very limited, very well described, subject to total transparency and it must be a last-resort option.
• The latency of the personal data from biometric identification should be regulated. 
";Very much;"• Nevertheless, as standardization bodies start delivering norms on the AI domain, certifications will provide enough labelling.
• Labelling must be built on top of legal requirements and can use different techniques for validation, so it looks like can generate additional business for specialized agents.

";Other enforcement system;Compliance for a basic set of requirements must be required for all the applications. Third parties can assure that the procedure is simple and affordable. Ex-post market surveillance must be applied only to a limited number of high-risk applications and under very clear rules for not harming investments.;;Cyber risks;• Risks related to discrimination and misuse of personal/corporate image.;Yes;"Only changes that result in the functionality of a product altering in such a way as to impact safety testing or disclosures should be classed as an 'important change' requiring a new risk assessment

The responsibility for the safety of a product should be with the entity that puts it on the market. This gives clarity on who people need to seek redress from if there is a problem.
 
Iterative risk assessment procedures seem cumbersome and prone to originate continuous legal litigation";Yes;"• Strict liability should not be introduced for AI systems, because it would mean that anyone involved in making an AI system could be liable for problems. Such a regime would have a chilling effect on innovation and undermine the uptake of AI by businesses in Europe. 

• Software vulnerabilities should not be treated the same as product defects from a legal perspective.  Once launched, physical products are set in stone, however patches to protect software can be rapidly released. 
";Yes, for specific AI applications;"• Adapt the liability in a specific way for the High-Risk applications and for certain sectors such as health
• To analyse the applications classified as high Risk and apply, the national liability rules must be continuously reviewed
";"• Also, product liability
• Is important to find mechanisms with enough safety and protection but, at the same time, with less bureaucracy involved
";
F530224;13-06-2020 15:31;English;Other;Shane;Collins;;The Orthogonal Methods Group (OMG) - Research Platform;;Micro (< 10 employees);Ireland;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;1 - Not important at all;Working with stakeholders to increase EU citizens’ AI awareness and knowledge is key. We are less concerned with a focus on the private sector for reasons identified in our position statement. Building AI skills and education should take prominence over investment in the private sector. We are against the use of public procurement at Member State level to induce systematic change toward unregulated AI adoption e.g for reasons of bias.;5 - Very important;5 - Very important;1 - Not important at all;3 - Neutral;4 - Important;5 - Very important;AI technologies present increasing & insurmountable asymmetry between decision-makers & users. Creating & developing tools & literacy for individuals & civil society groups to address asymmetries is vital to mitigate/prevent AI harm. Establishing world-reference testing facilities & engaging with institutions (GOs/NGOs) & representative groups (Unions) is key. We must fund AI literacy initiatives (e.g. AI4EU) & rebalance the European data space to ensure people have more control of their data.;5 - Very important;5 - Very important;3 - Neutral;Include societal forums as part of robust impact assessment mechanisms (RIAM) in regulatory framework to involve citizens & leading experts who can detail applicability of AI to certain domains & its societal impact. Forums should be chief anchor of the Lighthouse Research Centre serving to reduce distance between domain experts, stated policy & implementation process. Provide significant resources to AI4EU to ensure it is positioned to collaborate or lead the type of societal forums envisaged.;4 - Important;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;DIH could play a bigger role than currently envisaged as a venue to encourage cross-disciplinary work/fora. There is a need to highlight the potential drawbacks of AI for SMEs. DIH fora should elicit tacit knowledge about pros/cons of deployment in situational contexts & involve technical & non-technical stakeholders. This is crucial for sandboxing testing.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;See OMG AI White Paper Position Statement;There is a need for a new legislation;;No;;;;The use of AI in policing, provision of services, determination of risk pricing, hiring amongst other areas are by definition subject to historical bias and therefore reproduce multiple forms of existing inequality. We believe these should be banned by default within the EU and not simply monitored as High Risk AI as suggested in the White Paper.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;See OMG AI White Paper Position Statement;Very much;We support a labelling system (LS). Data subjects must be informed about automated decision-making processes currently (GDPR). Due to this & our view it promotes public trust, a LS should be compulsory. Anything more specific than ‘AI is used here’ for mandatory LS will require technical understanding by system operators & subjects. Whilst desirable, some may suggest it’s unworkable. DIH should therefore be involved in related capacity building across EU-States, which could make this possible. ;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Any new regulatory requirements should be applicable to AI in any form that pose a risk & may adversely affect parties or society. We also believe a robust impact assessment mechanism (RIAM) that reviews direct & unintended consequences of an AI application is required (see OMG AI WP Position Statement). In line with our view that ex-ante compliance and ex-post enforcement mechanisms are needed, RIAM must be reviewed periodically. The introduction of RIAM could be required by law under 5.D.c. ;Mental health risks;;Yes;;Yes;Our response is based on our reading of the White Paper and related documents.;Yes, for all AI applications;;Our response is based on our reading of the White Paper and related documents.;OMG_AI_WP_Position_Statement.pdf
F530223;13-06-2020 15:23;German;Business Association;Aimée;Klutke;;Verband der Chemischen Industrie e.V. (VCI);15423437054-40;Medium (< 250 employees);Germany;The feedback can be published with your personal information;4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;;4 - Important;4 - Important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;2 - Not important;;Current legislation may have some gaps;;Yes;;No;;;5 - Very important;1 - Not important at all;3 - Neutral;4 - Important;2 - Not important;3 - Neutral;No opinion;;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;;;No;;No;;No;;;VCI-Positionspapier_AI-Whitepaper_final_pdf.pdf
F530222;13-06-2020 15:12;Danish;Trade Union;Grit;Munk;;IDA Danish Society of Engineers;;Medium (< 250 employees);Denmark;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;;Prepare citizens on how to act to secure personal rights in a world with Ai;4 - Important;4 - Important;3 - Neutral;4 - Important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;;There is a need for a new legislation;;No;;;;;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;Important that legislation is as clear and simple as possible to make it understandable and useful to SMV's.;Very much;There are different labelling projects going on in different EU countries. It is important to support these to get the experience and to make labelling a tool as soon as possible. The EU can then collect experiences to make one common label.;Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;;Personal security risks;;Yes;;Yes;;Yes, for all AI applications;;;
F530221;13-06-2020 14:55;English;;;;;;;;;The feedback can be published in an anonymous way;No opinion;No opinion;No opinion;No opinion;1 - Not important at all;1 - Not important at all;People should have a say in whether or not AI can be acceptably used in a democratic society.;No opinion;No opinion;1 - Not important at all;No opinion;No opinion;No opinion;The plan must be updated to include criteria (scientific and policy) about how the EU will allocate its resources of AI. The plan should include a section on human rights, societal impacts of AI and automation, and how to ensure democratic oversight for the application of AI systems.;3 - Neutral;4 - Important;1 - Not important at all;;1 - Not important at all;No opinion;No opinion;No opinion;No opinion;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;Deployment of AI in sensitive areas w/o democratic oversight, transparency or sufficient evidence to justify the need/purpose. AI posing collective or societal-level harms not having remedies in anti-discrimination of data protection frameworks. Machine learning leading to unauthorised use/purpose and function creep. Companies and governments using excuse of ‘innovation’ to justify trials without safe-guards.;Other;AI regulation shouldn't provide loop-holes to data protection legislation, or other frameworks, like discrimination law. Current law doesn't address use of non-personal data for AI, types of data which don't fall under the GDPR. AI can have huge collective impacts: furthering overpolicing, surveillance, in-equalities – all not addressed in existing legal frameworks but are still major issues. Discrimination on financial status and other grounds, usually not protected in discrimination law.;Other;New rules should clearly outline criteria to determine which AI systems are legal and which are not. Such criteria should be based on proving that they work and are needed, conducting mandatory fundamental rights impact assessment for all applications, and ensuring democratic oversight. Uses of AI which breach fundamental rights  - like biometrics/facial recognition for mass surveillance - should be banned outright.;;;The use of AI to determine delivery of essential public services, predictive policing, autonomous lethal weapons, identification/analysis of emotion and identity traits, and indiscriminate biometric surveillance.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;"Their use in public spaces will lead to mass surveillance; This will irreversibly limit our fundamental rights to privacy, freedom of assembly, expression, non-discrimination, data protection, dignity and the right to a fair trial, creating societies of suspicion; even uses which do not contribute directly or indirectly to mass surveillance in public spaces still pose significant threats to privacy, data protection, non-discrimination, and dignity.";Rather not;Self-labelling systems can be confusing for people, may give false sense of security since it's the same company that develops a product the one saying that it's safe. The high/low risk distinction is overly simplistic, could very well allow for loop-holes for systems with potentially very significant impacts on peoples’ safety and rights. Especially so if ‘low risk’ systems are only voluntarily controlled, essentially asking us to let big tech companies regulate themselves.;Other enforcement system;All systems should undergo a mandatory ex ante human rights impact assessment from an external body.;Compliance should be external. To guarantee fundamental rights are protected. We can't rely on self-regulation for this.;Mental health risks;Potential risks of discrimination posed by AI systems. Use of AI in online products and services requires collection and use of data lending toward discrimination in many fields related to targeted advertising. This poses risks of differentiated pricing, discrimination and financial detriments, the risk of creating filter bubbles, interfences in the political process, all based on sensitive inferences or associations. AI may impact on accessibility and other rights of persons with disabilities.;Yes;Internal  supervisors,  such  as  Data  Protection  Officers  under  GDPR  should  be included and asked for advice.;Yes;AI developers and deployers should be accountable for harm generated by their products, and that products developed using AI should not enjoy exceptions to any EU laws, whether it be discrimination, data protection, or product liability.;Yes, for all AI applications;;EU must address copyright and database protections which prevent proper oversight of AI applications. Liability rules should provide incentives for openness.;
F530220;13-06-2020 14:30;English;Other;Jenny;Arlington;;GLIA Foundation;;Micro (< 10 employees);United States;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;GLIA_Foundation_Submission-_European_Commission_s_White_Paper_on_AI-_13_June_2020.pdf
F530219;13-06-2020 14:17;English;NGO (Non-governmental organisation);Eva Deborah;KOHNER;;European Network of National Human Rights Institutions;684831617188-52;Small (< 50 employees);Belgium;The feedback can be published with your personal information;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;;No opinion;No opinion;No opinion;;No opinion;No opinion;No opinion;No opinion;No opinion;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;No;;;;ENNHRI considers face recognition/biometric identification systems as among the most concerning AI applications with regard to their human rights impact and strongly recommends banning the use of such technology in the EU.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Very much;The labelling system should be mandatory and include more than two levels of labelling (risk/non-risk).;A combination of ex-ante compliance and ex-post enforcement mechanisms;;"Ex-ante compliance and ex-post enforcement mechanisms should be combined and in both cases verified by relevant (external) competent authorities.

Audit procedures should not be voluntary or based on ethical principles, but should be mandatory and implement international human rights obligations of states and also of companies";Mental health risks;;Yes;;No opinion;;No opinion;;;ENNHRI_letter_White_Paper_AI_final__1_.pdf
F530218;13-06-2020 13:55;German;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;No opinion;No opinion;4 - Important;4 - Important;4 - Important;Diversity (Ethnicity, Gender, Race, Age, Culture/Religion, Sozio-Economic Status/Class);3 - Neutral;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;;3 - Neutral;5 - Very important;4 - Important;;3 - Neutral;4 - Important;4 - Important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;There is a need for a new legislation;;No;;;;;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Very much;All AI systems must be unambiguous and appropriately labelled so that people can make informed decisions about AI use and acceptance of the AI result.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;No opinion;;No opinion;;No opinion;;;
F530217;13-06-2020 13:00;English;Academic/Research Institution;Mattia;Ceracchi;;I-Com - Institute for Competitiveness;859942318143-66;Small (< 50 employees);Italy;The feedback can be published with your personal information;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;A clear understanding of different company needs should be acknowledged according to their position in the value chain. While the EU should strive to achieve better results in research and innovation, most companies, especially SMEs, would be either only or mainly AI users. Therefore, for a competitive economy, the EU regulatory framework should lead the vast majority of companies to adopt AI easily and at a competitive cost.;5 - Very important;3 - Neutral;5 - Very important;4 - Important;5 - Very important;4 - Important;The afore-mentioned objectives should not jeopardize the possibility for EU citizens and companies to have access to the best available AI technologies at a competitive price. A balanced approach should be used taking into full account the interests of all the concerned parties, including the vast majority of citizens and companies that  would be adopters rather than R&D and/or commercial producers in the AI ecosystem. ;5 - Very important;4 - Important;5 - Very important;The proposed lighthouse research centre should coordinate a network of existing or newly setup AI research excellence centers (at least one per Member State) and be accompanied by a technology transfer organization (with a flexible and market oriented management) to achieve the critical mass needed to succeed in the global race for AI excellence. SMEs should play a key role in the proposed public-private partnership for industrial research and in other forms of cross-border collaboration involvi;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;4 - Important;Both training and advice to SMEs should be key activities for AI specialized digital innovation hubs (DIHs). For this reason, foreseeing only one DIH per Member State may imply a sizeable geographical barrier for SMEs, especially in larger countries. A more distributed network of DIHs, providing expertise to SMEs in different regions, should be developed, possibly involving trade associations and larger AI technology players.;4 - Important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;Although many afore-mentioned concerns deserve a high level of scrutiny and sometimes need to be addressed by ad hoc regulation, it would be fairer to compare AI applications with a human-based benchmark. It would not be realistic to expect AI achieving an error-free perfection where, in the same field, the same standard is not currently applied. This requirement could significantly stifle innovation, especially from SMEs and new entrants.    ;Other;Although some new legislation is certainly required and we fully agree that a EU-wide instead of a national regulatory framework should be pursued, current legislation should apply whenever possible in order to avoid excessive market fragmentation and uncertainty, and a significant increase in compliance costs for companies, especially SMEs. ;Yes;;Other;If the two proposed cumulative criteria to determine “high-risk” AI applications seem quite logical and could help provide legal certainty, exceptional additional instances should be better defined and limited to specific cases in order to avoid any ambiguity where the aim of the risk-assessment approach is exactly the opposite.;;4 - Important;3 - Neutral;5 - Very important;3 - Neutral;4 - Important;4 - Important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);A right balance should be struck between security issues (e.g. terrorism), that under certain circumstances could justify the massive use of biometric identification systems in public spaces, and privacy concerns, that should rightly limit any abuses of these tools. We totally agree that a EU-regulatory framework is by far preferable to leaving the question solely to Member States.  ;Much;Compliance procedures and costs related to a voluntary labelling system should be minimized to also allow SMEs and startups to take advantage of this instrument and so maintain a level playing field in the internal market.  ;Other enforcement system;While ex-ante self-assessment, instead of an external procedure, is key to speeding up the innovation process and ensuring a thriving and trustworthy European AI ecosystem, ex-post enforcement by public authorities should play an important role in guaranteeing compliance by AI developers and deployers.;In our opinion, some requirements for high-risk applications should be relaxed, e.g. b) (data and record-keeping) and d) (robustness and accuracy). For the former, given that data keeping requirements may easily conflict with GDPR prescriptions, accurate records and data may not be available to firms developing AI based on open source models. For the latter, reproducibility may not be feasible or be conflicting with how some AI systems work. ;Mental health risks;In general, we would be against new legislation to be applied to AI products in order to minimize economic risks such as internal market fragmentation and/or discrimination towards innovation. We would prefer instead a clearer interpretation of current legislation where needed for afore-mentioned instances or elsewhere to be applied to all products, including those embedding AI. ;Yes;Notwithstanding the need for new risk assessment procedures should be considered in case of significant changes, it should be limited in time and scope reflecting a right balance with other important public policy objectives such as ensuring low compliance costs for companies and other entities, especially SMEs, and a competitive innovation ecosystem.   ;Yes;Reforming the current EU legislative framework to better cover the risks entailed by AI applications may generate a serious fragmentation in the internal market between AI and non AI-based products, discriminating against the former. Therefore, while some amendaments to the current legislation could be justified by better attributing responsibility in certain situations and providing legal certainty, we believe that such revisions should be limited and targeted to a well-defined scope.;No;;For the same reason as previously stated, we believe that an EU-wide approach would be preferable in order to avoid major risks of internal market fragmentation. Therefore, Member States should refrain from unilateral moves and look for agreements and alliances at EU level.;
F530216;13-06-2020 12:51;German;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;4 - Important;5 - Very important;1 - Not important at all;1 - Not important at all;4 - Important;;4 - Important;5 - Very important;2 - Not important;4 - Important;5 - Very important;5 - Very important;;2 - Not important;5 - Very important;3 - Neutral;Establish fcilities which allow to align AI research with public interests of the citizens.;2 - Not important;4 - Important;4 - Important;4 - Important;4 - Important;;4 - Important;5 - Very important;5 - Very important;4 - Important;;5 - Very important;;There is a need for a new legislation;;Yes;;No;;Crime persecution and prediction, Survalience, application processes (job, credit,...);5 - Very important;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Very much;Should be mandatory for all AI systems;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;Yes;;Yes, for all AI applications;;;
F530215;13-06-2020 12:41;English;Academic/Research Institution;Ulises;CORTÉS;;Barcelona Supercomputing Center - Centro Nacional de Supercomputación (BSC-CNS);735762730933-66;Large (250 or more);Spain;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;AI technology can modernize government information storage and distribution streams very efficiently, but we have to make this process trustworthy and fair for citizens. It is necessary to promote a program to assure European technological independence.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;Linking High-Performance Computing facilities with the AI excellence facilities and with the European data space. This will pave the way for a new generation of computing resources that might better integrate newer academic and industrial’s needs.;5 - Very important;5 - Very important;5 - Very important;Create a specific line of the ERC programmes devoted to AI. Promote the definition of societal, research and industrial challenges concerning the usage of European data using AI and HPC. ;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;The EU has to secure the access to data – in particular for research-, Ai resources, and computing infrastructures, including HPC facilities, to SME’s through the DIHs. Promote the link of DIHs with the AI4EU platform.;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;We support a European approach for a human-centered trustworthy AI where ethics and fairness by design are the norm. As well as to promote explainability as sine qua non requisite for AI systems interacting with humans or giving support to decisions that may affect individuals or the environment;There is a need for a new legislation;;Yes;;Yes;;The most concerning AI applications are  (a) autonomous weapons,  (b) indiscriminate use of private data, and  (c) AI-based applications taking the man out-of-the-decision-loop.;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;Active biometric systems should be allowed in emergency situations, always regulated by law. Data processing, storage and deletion ought to be fully specified.;Very much;Regardless fo the risk level, users must always have the right to know when they are dealing with an AI system. This is purely an ethical issue, concerning the right of knowledge. Furthermore, risk assessment is a subjective measure, which is hard to measure and to anticipate (e.g., something that may seem harmless today, may be dangerous in a very short time).  Labelling ought to be compulsory;A combination of ex-ante compliance and ex-post enforcement mechanisms;;One of the possible alternatives to promote trust in AI-based applications is to make them transparent-by-design when they refer to citizens or when they used in situations that can affect their rights.;Personal security risks;EU has to produce legislation to avoid any possible attack to the human rights;Yes;;Yes;;Yes, for all AI applications;;;
F530214;13-06-2020 12:24;English;NGO (Non-governmental organisation);Eline;Chivot;;Center for Data Innovation;;Micro (< 10 employees);United States;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;;5 - Very important;3 - Neutral;5 - Very important;4 - Important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;4 - Important;;5 - Very important;3 - Neutral;4 - Important;4 - Important;5 - Very important;;1 - Not important at all;1 - Not important at all;4 - Important;1 - Not important at all;2 - Not important;4 - Important;"The risks of AI may be overinflated.
AI may provide many benefits not fully recognized by the public.";Current legislation may have some gaps;;Other;;;;;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);"The Commission should consider new regulations in high-risk scenarios where there is clear evidence of consumer harm. Further guidelines would help in clarifying the possibility for a company or a public organization to use and deploy this technology: Legal uncertainty currently stands in the way of businesses being able to collaborate with municipalities, schools, or airports, where uses of facial recognition can be beneficial. 
Most common facial-recognition applications are benign, and there are countless examples of how they help reduce financial fraud or prevent medical errors. The biggest area of controversy is law enforcement, but current protections under EU laws put limits on the misuse of these technologies by governments. And any operator of such systems must comply with privacy laws. 
What is required is more oversight and transparency to ensure accountability and a safe, responsible use by public and private organizations, not fewer resources or more regulations to limit the use and effectiveness of these technologies.";Rather not;The Commission should clarify the implications for stakeholders who may choose not to endorse voluntary labelling, such as the risk that it may penalize companies who do not adhere to it.;Other enforcement system;;The Commission should encourage the continued development and testing of voluntary industry best practices. Adding a new layer of rules on the current fragmented landscape of the Single Market will further impede the ability of EU and non-EU businesses to innovate and commercialize their systems, and deter them from investing in AI in the EU. Regulatory sandboxes is a better way to experiment with AI while developing systems that are compliant.;Risks related to the loss of connectivity;;Yes;;No;Any update to existing EU legislation that apply to AI should include clarification to avoid legal uncertainty and diverging interpretation across member states, but should not add more to the existing bureaucratic and compliance burden that impacts companies. Given the evolving nature and diversity of AI systems, rules should remain flexible. The review of the existing body of EU laws should involve an assessment of the impacts these laws have on AI development in the EU – and as some of these ;No;;;2020-eu-ai-whitepaper-response.pdf
F530213;13-06-2020 12:01;English;Company/Business organisation;Johan;Zeltner;;EDF (Électricité de France);39966101835-69;Large (250 or more);France;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;;3 - Neutral;4 - Important;3 - Neutral;3 - Neutral;5 - Very important;5 - Very important;European data spaces should be recognized as service of common interest. ;4 - Important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;4 - Important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;The certification of AI systems is a major concern for AI use cases in industry, especially in highly controlled environment such as power plants. This concern also highlights the need for further standardisation in AI. In addition, in the context of climate change and digital sobriety, the use of AI may raise concerns relating to its environmental footprint, and even more in the energy industry.;Current legislation may have some gaps;;Yes;;Yes;;"From EDF SA’s perspective, AI systems in electric systems are the most concerning use cases, especially AI applications in power plants.
New risks may also appear when two AI systems are connected (for instance, one system selects data, which are used by one other system). Such “network-related” risks could lead to damages irrespective of any system defect and are hardly predictable: though the renewed need for AI standardization.";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;;Very much;"The label may include an environmental dimension (""green-AI""), along with digital sobriety requirement.";A combination of ex-ante compliance and ex-post enforcement mechanisms;;"EC should be careful not to create a disproportionate administrative burden for the assessment of compliance, by restricting mandatory requirements to ""high risk"" and by promoting self-assessment rather than external procedure (which should also be carefully designed to protect trade secret).";Mental health risks;Stakeholders need clarification on who is responsible for the further development of a self-learning system, after it is put on the market. Such clarification can be made by precizing the scope of the development risk defence to AI systems, mentioned in the Product Liability Directive.;Yes;;Yes;;Yes, for all AI applications;;"French liability regimes appears to be the most adaptable regimes in the EU.
However, some changes may be needed to guarantee legal certainty for suppliers and to ensure proper compensation for damage and fair allocation of liability (in the current state of the legislation, if several actors are involved in a damage, the liability cannot be shared, except contractually).";
F530212;13-06-2020 11:59;English;Company/Business organisation;Cornelia;Kutterer;;Microsoft Cooperation;0801162959-21;Large (250 or more);United States;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;We recommend broad engagement with the public to ensure that the full range of human diversity is part of this debate, both to ensure inclusive design and for setting the right public policy goals and priorities. The ongoing calls for justice and equity and for the end to disparities demand that we examine how we use AI and work together to ensure AI advances shared social goals. We encourage the Commission to foster international cooperation in the areas above given the global nature of AI.;5 - Very important;No opinion;5 - Very important;5 - Very important;5 - Very important;5 - Very important;We welcome many of the proposals above. These measures are important for establishing an ecosystem of AI excellence in Europe. Whether reference facilities are needed turns on what AI regulatory framework the EU ultimately adopts. If created, any such facilities should operate based on international norms that govern conformity assessments, and should avoid duplicative testing requirements, which would be disproportionately harmful to SMEs and less efficient from a risk management point of view.;4 - Important;4 - Important;4 - Important;We agree that research and innovation is a key driver for delivering cutting-edge AI. Any investments in this area should also support the goal of ensuring that AI is trustworthy. Research efforts should include a focus on responsible AI practices, techniques, and technologies that can help developers and deployers achieve e greater transparency and accountability. Areas of research in this regard might cover, e.g., fairness, intelligibility and transparency, privacy, reliability, safety, etc. ;5 - Very important;No opinion;5 - Very important;5 - Very important;5 - Very important;Digital Innovation Hubs can play an important role in ensuring that SMEs, while expanding and scaling their AI capabilities, implement policies to develop and deploy their systems responsibly. Please see our comments on question 1.4 in relation to testing and reference facilities. ;5 - Very important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;5 - Very important;"Efforts to regulate AI must begin with a carefully considered assessment of risk. Yet, the list above is not comprehensive. Some AI systems will pose some of the risks listed, while others will not; there are also other risks (e.g., physical harm) not listed above. In addition, any risk assessment must also consider both the severity of potential harm and the likelihood that this harm will occur, as well as weighing the AI system’s benefits against the “risks” of not adopting it. ";Other;The EU has an extensive framework covering e.g. safety, security, consumer protection, fundamental rights. These laws apply equally to AI-powered products and services. Although new rules might be necessary to address specific gaps in the EU acquis, policymakers should rely on existing laws to the extent possible and augment them with guidelines where appropriate. We also acknowledge that the application of these laws may require new interpretations due to the specific attributes of AI.;Other;The definition of high risk should be refined. Most of the high-risk sectors identified are already subject to extensive safety regulation. It would be more efficient to extend existing rules to cover new types of similar risks. The Commission’s approach also means that certain uses of AI that pose a significant risk of harm may be excluded from regulation.  It is unclear how a compliance regime will operate in such situations, or consider decoupling the sector and high-risk use requirements.;;;The use of facial recognition by the public sector is an exemplary high-risk use scenario that requires specific rules. Government use of such systems to specifically identify individuals raises significant risks to privacy and to other important fundamental rights such as freedom of expression and assembly. Bias in such systems can also meaningfully increase the risk of decisions, outcomes, and experiences that are discriminatory, exacerbating existing cultural and societal challenges.;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);The existing EU fundamental and human rights frameworks impose important constraints on whether the use of biometric identification systems in public spaces is permissible. If not, the EU’s existing laws require to adopt appropriate rules and safeguards, which must comply with the test of necessity and proportionality. In those cases, rules should extend to developers. Please also see our response to question 2.5.;Much;A voluntary labelling scheme should be designed to encourage the greatest participation. Labels should be used to demonstrate adherence to good governance processes rather than outcomes (e.g., “this AI system is accurate”). Given the diverse range of AI solutions, and the need to keep labelling efficient and cost-effective, it should not be one-size-fits-all. Instead, a common set of high-level criteria is needed to ensure that labels are recognizable and understood by customers and end-users.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;"Policymakers should incentivize actors to adopt AI governance, such as procedures to: envision the range of harms that an AI system might impose and take mitigation steps; train employees on these processes and assign individuals or groups with responsibility for overseeing implementation and compliance; be transparent with customers and other stakeholders about relevant risks; adopt an escalation process for employees and others to raise concerns and seek guidance on the company’s compliance.";Mental health risks;We agree that the concept of AI safety goes beyond risks of material harm, and that risk assessment procedures should be expanded to ensure that they are capturing risk in a broad sense. We do not think new laws around all of these risks are necessary—many are covered by existing rules—but acknowledge that certain AI systems, when used in certain scenarios, may raise unique safety risks in these areas. ;Yes;We believe that risk assessment procedures should be accompanied by appropriate technologies, systems, and tools to help regulated actors identify and mitigate relevant risks. We encourage the Commission to actively support the development of these tool, and to make this a priority in any AI regulatory framework as well as in the Commission’s “ecosystem of excellence” efforts. ;No;The EU product liability regime, including the PLD and the national liability rules, has worked well in a wide variety of contexts. There is little evidence that consumers are unable to recover for harms caused by defects in PCs, laptops, smartphones, and other ubiquitous digital devices. Before considering changes to the PLD, empirical evidence is needed to identify those cases where parties are unable to obtain redress under the existing regime and to understand the reasons for that inability.;No;;Rules on product safety and product liability serve complementary purposes—one seeks to protect consumers from harm ex ante, and the other applies ex post, ensuring compensation where harm occurs. Given this relationship, considering the regimes holistically makes sense. But we recommend waiting to undertake a review of the EU product liability regime until the parameters of the AI safety regime are settled, so that the liability rules reflect the safety framework.;Microsoft_Response_-_EC_White_Paper_on_AI_June_2020__small_file_.pdf
F530211;13-06-2020 11:45;English;Company/Business organisation;Jennifer;Pougnet;;F. Hoffmann-La Roche Ltd;18940431725-51;Large (250 or more);Switzerland;The feedback can be published with your personal information;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;4 - Important;Innovation should not be limited to specific centers, supported by funding and there should be differentiation of the activities of the EC and member states. Acceptance of AI by the public sector needs to be supported by education, communication, skills both in general literacy and technical. Trust in the technology and application of existing laws to help mitigate existing concerns. And can be further supported by accountability and transparency of the use and execution of AI;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;Data privacy & protection principles with data standards for quality and interoperability in alignment with the FAIR principles serve as a foundation for AI. Data governance maintains these standards/principles. Data literacy is required by policymakers, health system leaders and technical experts. And promoting awareness of AI use by the public sector through specific use cases to demonstrate societal benefit will build trust in AI.;4 - Important;5 - Very important;5 - Very important;Any framework for AI should be based on agile processes to accommodate the speed with which AI evolves. It is also important for it to work well together with existing laws, security and ethical frameworks, such as data privacy. It is very important member states are incentivized or rewarded for creating AI literacy and coordinating AI centers are working together to support standards and adoption at an international level lending to equity and fairness. ;4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;Digital Innovation Hubs could contribute to the development and promote the adoption of AI standards and the establishment of best practices regarding AI's role in healthcare. Incentivising or rewarding ideal behaviours and best practices could enable success of Digital Innovation Hubs. ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;4 - Important;Acknowledge different forms of AI each have their own risks and potential benefits.  Eg. there are locked models and continuous learning models, and there are black box AI and fully transparent AI.  When discussing risks of AI we need to evolve to proposing pragmatic solutions to balancing the risks with the benefits. Decision making needs to be traceable/reproducible. Confidence levels should be assigned to tools. Both will help mitigate bias and potential reidentification.;Other;In general, the current privacy framework is sufficient. Specific new rules for AI systems may duplicate already existing regulation, in particular GPDR. However further clarity could be provided through policy recommendations put forward in the Strategy for Data including articulation of anonymization processes in AI. Additionally frameworks need to consider risks balanced with access to data to enable manageable risk. Transparency and public input are key to achieving this.;Other;Introduction of new rules should only be done based on a risk/benefit balance, in alignment with sectoral rules. In medtech, MDR and IVDR address the need to safeguard patient safety in response to technological progress. Both regulations address the risk classes (and respective regulatory requirements) and, as such, an AI “medtech software” may in principle fall in any of the risk categories (low to high). ;;;;5 - Very important;4 - Important;5 - Very important;4 - Important;4 - Important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;Further guidelines and potentially legislation are required for use of this technology in a way that respects privacy and protects data. It holds the potential to address care of some vulnerable populations ie. dementia however needs to be implemented in a thoughtful way that risks or potential harms do not outweigh the benefits.;No opinion;;No opinion;;Compliance assessment of an AI framework should factor in existing provisions in other pieces of legislation which require certain classifications and risk assessment. Therefore, such compliance provisions should be structured to refer to existing requirements in order to contribute to enhanced legislative clarity and to avoid any inconsistencies, through the development of sector specific guidance documents issued by regulatory authorities. Codes of Conduct can also help build trust.;;;Yes;Risk mitigation should include risk-specific controls to guide the development and use of AI, ensure oversight, establish policies, training, and contingency plans, as part of the overall AI life cycle. Leveraging existing frameworks, such as data protection impact assessments. Many healthcare products are already obliged to perform a continuous assessment of risk.  Yet, different risk assessment methodologies may be necessary for continuous learning AI systems.  ;No opinion;;Yes, for all AI applications;;A pan-European framework would be preferable to different national liability rules, potentially overlapping. An EU-wide framework would also prevent ‘local AI safe havens’.   For similar reasons, it is also important to find common ground internationally and not only within Europe, given that many of the AI challenges are global in nature.;
F530210;13-06-2020 11:17;English;Academic/Research Institution;Veronica;Barassi;;Institute of Media and Communication Management, University of St. Gallen;;Small (< 50 employees);Switzerland;The feedback can be published with your personal information;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;3 - Neutral;;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;3 - Neutral;;;3 - Neutral;5 - Very important;5 - Very important;Create Research Centres and Independent Bodies to Monitor Human Rights breaches in AI innovation;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Particular regulations need to be applied to AI systems that profile humans for data-driven decision making;Current legislation may have some gaps;;Yes;;Yes;;All AI systems that profile human beings for data driven decision making should be considered as High Risk;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;No opinion;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Personal security risks;For AI Systems profiling individuals for data-driven decision making Inaccurate profiling Risks should be describe and an appeal procedure should be guaranteed by the companies using them.;Yes;see above;Yes;see above;Yes, for all AI applications;;;
F530209;13-06-2020 11:09;German;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;1 - Not important at all;1 - Not important at all;"Vertrauen der Bevölkerung auf Basis von Transparenz ist Grundlage für erfolgreichen, europäischen Weg.
Jede Maßnahme auf EU-Grundrechte-Charta UND EU-KI-Ethik-Leitlinien überprüfen.
Gewährleistung demokratischer Aufsicht, Einbeziehung Zivilgesellschaft und Betroffener.";4 - Important;3 - Neutral;1 - Not important at all;2 - Not important;5 - Very important;4 - Important;"Gemeinsame Strategien sollten Regelungen beinhalten zu:
Transparenz, 
auch Offenlegung aller Finanzierer der jeweiligen Forschung,
demokratische Kontrolle, 
Passung mit EU-Grundrechte-Charta UND EU-KI-Ethik-Leitlinien, 
Verantwortung, ethische Leitlinien, Grundrechte, Sensibilität für gesellschaftliche Auswirkungen und Auswirkungen auf die Demokratie als einer der Ausbildungsschwerpunkte und als Voraussetzung jeglicher Förderung/Geldmittel.";2 - Not important;5 - Very important;1 - Not important at all;"Das Gemeinwohl sollte Prioritäten bestimmen.
Transparenz, auch Offenlegung aller Finanzierer der jeweiligen Forschung, Passung mit EU-Grundrechte-Charta UND EU-KI-Ethik-Leitlinien, Aufmerksamkeit für gesellschaftliche Auswirkungen und Auswirkungen auf die Demokratie sollte allen Maßnahmen und Förderungen zugrunde liegen.";1 - Not important at all;4 - Important;4 - Important;1 - Not important at all;4 - Important;Förderungsschwerpunkt für Freie-Software-Lizenzen: KMU, Forschungseinrichtungen und fachkundige Bürger*innen können in Open-Source-Projekten kooperieren, dadurch enormer Schub an Innovationen und Stärkung europäischer Unternehmen.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;"KI in sensiblen Bereichen ohne Transparenz (hinsichtlich Finanzierer, Funktionsweise, Zweck, Auswirkungen für Gesellschaft, Demokratie und Einzelne), ohne demokratische Aufsicht, ohne ausreichende Regulierung und Rechts-Durchsetzung.
„Innovation“ als Rechtfertigung für Risiken, mangelnde Regulierung und Rechts-Durchsetzung.
Mangelnde Hinterfragung von KI (Einzel-Ergebnisse im Kleinen und Entwicklungen im Großen) aufgrund Technikgläubigkeit. 
Bewusste Verdunkelung von Verantwortung.";Other;"KI-Rechtsvorschriften dürfen die Europäische Datenschutz-Grundverordnung nicht ersetzen, sondern sie müssen sie ergänzen und stärken.
KI-Risiken müssen konkret adressiert werden (durch viele Beispiele, keine Allgemeinplätze, keine „Gummi-Paragraphen“).
Das geltende Recht befasst sich nicht mit der Verwendung nicht-persönlicher Daten und den kollektiven Auswirkungen der KI. Es verbietet nicht Diskriminierung aus nicht geschützten Gründen.";Other;Eine differenzierte Risikobewertung ist erforderlich. Das ist mit Hoch - Mittel - und Niedrig nicht erledigt!;;;"    • Zugang / Nutzung personenbezogener Gesundheitsdaten
    • Zugang / Nutzung individueller Bewegungsgewohnheiten
    • Speicherung / Nutzung von Gesichtszügen und anderer persönlicher Eigenschaften oder Verhaltensweisen zur  Identifizierung";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;"Wenn biometrische Daten zur Identifizierung von Personen im öffentlichen Raum benutzt werden sollen, kann dies nur gelingen, wenn solche Daten sehr umfassend erfasst, gespeichert und zum Vergleich herangezogen und einer breiten Nutzung zugeführt werden. Dies kommt einer Rasterfahndung gleich und muss verhindert werden.

Biometrische Fernidentifikationssysteme im öffentlichen Raum sind ein typisches Merkmal von Diktaturen. Ihr Einsatz ist in Demokratien entschieden abzulehnen. Sie verwandeln öffentliche Räume in Orte ständiger Überwachung und beschädigen irreversibel wesentliche Grundrechte sowie Elemente und Funktionen der Demokratie.";Not at all;"Von Industrien, die risikoreiche Produkte und Dienstleistungen anbieten, lernen.
Von Kontroll- und Aufsichtsbehörden lernen, die risikoreiche Produkte und Dienstleistungen anbieten.
Generell sollte Abstand davon genommen werden, KI-Systeme als risikoarm zu betrachten und daraus zu schließen, sie bräuchten keine Aufsicht zur Gewährleistung der EU-Grundrechte-Charta und der EU-KI-Ethik-Leitlinien.";Other enforcement system;Bewertung der Passung zur EU-Grundrechte-Charta und zu den EU-KI-Ethik-Leitlinien, Folgeabschätzung für gesellschaftliche Auswirkungen und Auswirkungen auf die Demokratie in den Phasen des Entwurfs, der Entwicklung, der Erprobung und des Einsatzes. Schnell durchsetzbare Konsequenzen für den Fall, dass Anwendungen diese Standards nicht erfüllen, einschließlich der Möglichkeit, KI-Einsätze zu stoppen.;Die Einhaltung von Regulierungsmaßnahmen sollte durch externe und unabhängige Stellen erfolgen. Selbstregulierung sollte vermieden werden. Es sollte sichergestellt sein, dass es keine Schlupflöcher beim Schutz der Grundrechte und Ethik-Leitlinien gibt.;Mental health risks;"Unklare Verantwortungs- & Haftungsstrukturen innerhalb von Behörden, Unternehmen und sonstige Strukturen, die KI anwenden. 
Fehlende Strafverfolgung von Unternehmen in ein paar Ländern in Europa.
Uneinheitliche Regulierung & Haftung & Strafverfolgung in den Mitgliedsstaaten.
Profiling und Scoring – mit den Gefahren der Manipulation und Diskriminierung.";Yes;Mögliche Änderungen sind in Risikobewertungsverfahren einzubeziehen, insbesondere in Datenschutz-Folgeneinschätzungen.;Yes;;Yes, for all AI applications;;Die Regeln der Transparenz, Verständlichkeit und Vollständigkeit von Datenschutzerklärungen und weiteren Risikodarstellungen von KI-Systemen für Privatnutzer*innen müssen den Anforderungen an Packungsbeilagen für Medikamente entsprechen.;
F530208;13-06-2020 10:55;English;Other;Friederike;Ladenburger;;COMECE ( Commission of the Bishop´s Conferences in the European Union);47350036909-69;Small (< 50 employees);Belgium;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;"COMECE welcomes the objective of building up an ecosystem of excellence by developing the necessary skills of all stakeholders involved.
But it is important to see that we still have a lack of a broad social - ethical discussion. Europe needs an overall assessment how Artificial Intelligence should be promoted and regulated serving the common good and putting the human person into the center.
Parallel to each EU activity concerning AI we need an accompanying ethical discourse.
 ";5 - Very important;3 - Neutral;4 - Important;4 - Important;5 - Very important;5 - Very important;"Implement into a revised Coordinated Plan on AI also the possibility of a broad ethical discussion.
Strengthening the excellence in research should include a broad scientific approach. 
The main goals of the Coordinated Plan on AI to encourage synergies and cooperation across the EU, should include an accompanying  discourse on ethics of AI and could give visibility to different national social ethical specificities - respecting national competences for regulating ethical aspects. ";5 - Very important;5 - Very important;5 - Very important;COMECE suggests a close link between the above proposed actions and the existing EU programs like Horizon Europe. The third pillar of the Horizon Europe program will establish a strong role of the European Innovation Council and the European Institute of Innovation and Technology.It would be helpful to establish synergistic effects between AI lighthouse research centre and AI research excellence centers and these elements of the new  Horizon Europe framework.  ;3 - Neutral;5 - Very important;4 - Important;5 - Very important;5 - Very important;The Digital Innovation Hubs should be connected closely with the concept of European partnerships of  the framework of Horizon Europe. The objective to link the private sector, foundations and other stakeholders of the European partnerships could give a deep possibility to integrate the Digital Innovation Hubs into the existing research landscape. With such a broader interdisciplinary connection it could be easier to establish more excellence of the ethical reviewability of AI systems. ;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;"AI may lead to environmental damage.High energy consumption of ""server farms"" as well as of connection and transmission technology are problematic.  At the same time, the computer capacity is growing due to a steady expansion of the application areas of artificial intelligence, which in turn leads to an increased energy demand. AI in the context of sustainability is an important concern and has to be tackled by the EU. 
";There is a need for a new legislation;;Other;"COMECE welcomes the approach based on new compulsory requirements limited to high-risk applications of AI. But we are not convinced by the suggested risk-based approach consisting of two cumulative criteria.
To ensure full legal certainty we would suggest a system based on the examination of whether a certain AI application is used in a manner causing significant risks. In other words, for every AI application a single case analysis should be taking place.";;;AI applications influencing democratic processes, AI applications in the administration of justice , border patrol by unmanned mobile robots, care robots in the health care sector;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);"On facial recognition technologies we would support a focus on the strict application of GDPR standards to the issue, while welcoming the possibility of exchanges and discussions on the topic. The principles of Article 5 GDPR are  particularly relevant for the AI sector, especially the ones of lawfulness, fairness and transparency; data minimisation; integrity and confidentiality; and accountability. Further EU guidance on biometric identification systems would be useful underlining the principles of proportionality, necessity and limitation based on purpose.";Much;A voluntary labelling system would be only useful when certification bodies would realize an external conformity assessment. These certification bodies have to fulfill all relevant requirements to carry out conformity assessment.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;"Establishing a risk-adapted regulatory system for the use of AI applications  could be foreseen for: > applications with some potential for harm > applications with regular or significant potential for harm > applications with serious potential for harm > applications with an untenable potential for harm. 
Different consequences  for each application could refer to the different level of the risk-adapted regulatory system: from an ex-ante approval procedure to a partial ban of the application.";Mental health risks;AI could cause a risk for the legal self - determination .The personal data of vulnerable and care-dependent persons can be in danger and consideration should be given to clarifying in the relevant legal provisions on living wills that these may also include dispositions with regard to the future processing of personal data as far as such processing will require the care-dependent person’s consent (e. g. for dementia patients who will not be in a position to provide legally valid consent). ;Yes;"The current product safety legislation does not fit for "" services"".It is important to tackle the issue of coverage of services. The extension of General EU safety legislation, at least to high-risk services as a first step, should be assessed, so as to overcome some of the difficulties and uncertainties. For these AI applications it is important to establish an external assessment which combines an ex- ante and an ex- post assessment of the application.";Yes;;Yes, for specific AI applications;Humanized robots in the care sector to support nursing staff taking care of elderly people;The use of care robots in nursing homes have to be regulated under a strict focus of transparency. The respective use has to be visible, predictable and terminable for each person who is supervised by an AI system of a care robot. Human oversight is most important for these AI applications and the danger of deception has to be reduced. ;Annex_paper_for_the_public_consultation_on_the_White_Paper_on_AI_1_.pdf
F530207;13-06-2020 10:40;French;Trade Union;Nicolas;Blanc;;CFE-CGE;;Large (250 or more);France;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;4 - Important;5 - Very important;Compte tenu du paysage numérique décentralisé de l'UE, des défis posés par les compétences numériques et du manque d'autonomie européenne en matière d'infrastructure numérique, la CEC European Managers et la CFE-CGC insistent sur la mise en place d'une infrastructure numérique commune, sur l'aide à apporter aux managers pour qu'ils deviennent des facilitateurs numériques, ainsi que sur le développement d'outils européens pour aider les prestataires de services éducatifs à fournir des compétences;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;;4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"- Impact sur les droits du travail des cadres 
- Impact sur l'obligation de rendre compte et la responsabilité juridique
 - Impact environnemental et social de l'IA ";Current legislation may have some gaps;;No;;;;Pour nous organisations syndicales, ce sont les applications de ressources humaines pour les salariés (module Human Resource Management dans les ERP de type SAP, outil pour aide à la mobilité, outil pour gérer les compétences et les carrières....) mais aussi pour les candidats potentiels (entretiens effectués via les mobiles avec systèmes d'AI appliqués en temps réels sur les candidats : mesure du stress, détection du mensonge, analyse sémantique....);5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);"Les systèmes d'identification biométriques ne devraient être autorisés dans les espaces accessibles au public que dans certains cas ou si certaines conditions sont remplies (Aéroport) : 
- collecte de données biométriques sur la base du consentement 
 - droit à l'oubli";Much;La question est de savoir comment les critères de ces labels seront fixés et par qui. Les partenaires sociaux, les organisations de la société civile et les institutions démocratiques doivent être impliqués.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;"ex-ante: liste d'évaluation de l'IA vérifiée avec les partis prenantes concernés (salariés, représentants, consommateur, citoyens, comité d'éthique....)
post-ante: suivi par comité d'éthique, comité de représentants des salariés, comité de représentants des consommateurs.....";Mental health risks;"Risque pour l'exactitude de la prise de décision (humaine) 
(biais) Risque pour les minorités 
(biais) Risque pour les droits des dirigeants et des travailleurs";Yes;;No opinion;;Yes, for all AI applications;;;livre_blanc_final.docx
F530206;13-06-2020 10:36;English;Other;Massimo;Pellegrino;;The Institute of Electrical and Electronics Engineers, Incorporated;79856747620-58;Large (250 or more);United States;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;2 - Not important;5 - Very important;5 - Very important;IEEE EPPC & SA believe that supporting and expanding the network of existing AI research excellence centres in conjunction with an industrial research PPP would serve best the objective of a united and strengthened research and innovation community. This contributes to maintaining diversity and inclusivity, and ensures that a broad church of stakeholders including SME’s and startups are involved. The establishment of a “Lighthouse” research centre is more a siloed approach and counter intuitive.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;The European Commission needs to ensure that any new Public Private Partnership includes the broadest consortium of stakeholders possible (Start-Ups, SMEs, Government Agencies). Consultation with European Standards Development Organizations would be encouraged.;;;;;;;IEEE EPPC and IEEE-SA do not believe that AI applications per-se endanger health or breach fundamental human rights. However, IEEE EPPC and IEEE-SA believe that reassuring users about AI products and services from the perspective of technical, physical, ethical, personal, public, cybersecurity and privacy standards should be encouraged.;Current legislation may have some gaps;;Yes;;Other;"AI applications which might trigger irreversible actions require specific consideration. 
IEEE EPPC&SA recommend having more granularity in levels.
The EC should address sector specific properties of “high-risk” for instance, use of biometric information in surveillance vs. medical context.
IEEE EPPC&SA recommend that the European Commission considers compliance audits and  simple reporting procedures. These will help to motivate AI-based solution providers to perform proper self-assessment.";;;;;;;;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);"IEEE EPPC and IEEE-SA contend that European Commission policy including human-in-the-loop should not be used as a mechanism to avoid compliance with other regulatory requirements such as explainability, e.g. human supervisors being held accountable for
failures of the AI applications.
IEEE EPPC and IEEE-SA would ask the European Commission to expand the debate on the circumstances of use for biometric identification to applications beyond public spaces. IEEE EPPC and IEEE-SA believe that European citizens and industry would benefit from expanding the debate to include use in private spaces, such as workplaces and smart homes.
IEEE EPPC and IEEE-SA believe that the clarity about public acceptability that should be gained from the debate will help to provide confidence for deploying biometric systems in those areas, public and private, where these yield benefits without infringing on European values.
";;"IEEE EPPC and IEEE-SA would like to raise the EC's attention to work on developing voluntary certification schemes that are already underway at various organizations (e.g. IEEE’s Ethics Conformity Assessment Program for Autonomous and Intelligent Systems initiative). 
IEEE EPPC and IEEE-SA suggest that it is important to provide clarity about which certification schemes are recognized/accredited as being compliant with the intended labelling framework.

";Other enforcement system;;;Mental health risks;;Yes;;Yes;;;;;
F530205;13-06-2020 10:33;Spanish;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;Información inteligible y realista a la opinión pública sobre ventajas e inconvenientes de la IA;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Promover el conocimiento de la IA en las escuelas e institutos de formación secundaria;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;No;;;;"Producir una división de la sociedad en ciudadanos de primera y segunda clase. Puede también promover la creación de una mueva moralidad de acuerdo a los intereses de los sectores más poderosos de la sociedad.
";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;La identificación biométrica, si se hace mal, puede llevar consigo a discriminaciones en la sociedad y a una deshumanización de ésta;Not at all;Si se hace, tendría que hacerse de forma que no produzca una división de la sociedad en  ciudadanos de primera (los que se etiquetan) y segunda clase (los que no se etiquetan);A combination of ex-ante compliance and ex-post enforcement mechanisms;;Todas las actividades de evaluación ex -ante y ex-post deben ser lo más transparentes posibles a la sociedad. De ahí la importancia de formar a las sociedad bien antes de llevar a cabo este tipo de actuaciones;Mental health risks;Riesgos relacionados con la pérdida de libertad de acción y pensamiento;No opinion;No sé qué sistemas de evaluación de riesgos existen actualmente;No opinion;No conozco en profundidad el marco legislativo actual.;No opinion;;No conozco las normas actuales de responsabilidad nacional;
F530204;13-06-2020 10:29;Italian;Public authority;Daria Provvidenza;Petralia;National;Presidenza del Consiglio dei Ministri - Dipartimento per le Politiche Europee, sulla base dei contributi del Ministero dello Sviluppo Economico e del Dipartimento per la Trasformazione digitale;;Medium (< 250 employees);Italy;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Necessità di accelerare l’attuazione della Strategia europea dei dati, quale prerequisito dello sviluppo dell’AI, tenendo conto delle specifiche necessità delle micro, delle PMI e delle start up;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Lo sviluppo dell’intelligenza artificiale dovrebbe essere strettamente interdipendente e interconnesso con la strategia europea in materia di dati.   ;5 - Very important;5 - Very important;5 - Very important;La promozione di un approccio open science tra centri di ricerca di eccellenza che si occupano di intelligenza artificiale. ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;La promozione della ricerca scientifica e tecnologica e la tutela della proprietà industriale. Si sottolinea che affinché si possa beneficiare del potenziale della data economy è necessario che sia efficacemente implementata una strategia europea dei dati.     ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Si rinvia ai commenti in allegato ;Current legislation may have some gaps;;Yes;;Other;"Si esprime apprezzamento per l’approccio delineato nella sezione 5B (nel senso di uniformare la legislazione europea); tuttavia, per quanto riguarda la determinazione dell’alto rischio di un’applicazione di AI si propende per un approccio basato fondamentalmente sul tipo di utilizzo e non anche sul settore.  ";Un’applicazione AI va classificata “ad alto rischio” se ha un forte impatto su sicurezza, salute e ambiente.  ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;"Occorrerà ad ogni modo tenere conto delle “Guidelines 3/2019 on processing of personal data through video devices” adottate nel gennaio 2020 dalla EDPB dove si riferisce che “The use of biometric data and in particular facial recognition entail heightened risks for data subject’s rights. It is crucial that the recourse to such technologies takes place with due respect to the principles of lawfulness, necessity, proportionality and data minimisation as set forth in the GDPR”  


";Much;"SI segnala che soprattutto per le imprese di minori dimensioni i sistemi di etichettatura volontaria per applicazioni AI non considerate ad alto rischio non dovrebbero tradursi in meccanismi eccessivamente complessi e ed onerosi, che tra l’altro rischierebbero di generare discriminazioni sul posizionamento nel mercato.    

";A combination of ex-ante compliance and ex-post enforcement mechanisms;;Si rinvia ai commenti in allegato ;Risks related to the loss of connectivity;;Yes;Ciò che rende l’AI diversa dalle innovazioni precedenti è la sua discrezionalità nell’assumere determinate decisioni. Un ambito normativo importante da approfondire riguarda il contesto B2B che deve assicurare sempre l’applicazione dei requisiti di trasparenza e responsabilità, chiarendo chi è responsabile e per cosa lungo la catena del valore, comprendendo anche i servizi.    ;Yes;Potrebbe risultare utile lavorare a stretto contatto con il Network OCSE di esperti in maetria di AI (ONE AI). Inoltre, si ribadisce il ruolo chiave della trasparenza per le applicazioni AI.  ;Yes, for all AI applications;;Si ritiene importante evitare la frammentazione del mercato interno. ;ulteriori_commenti_AI.pdf
F530203;13-06-2020 10:17;English;NGO (Non-governmental organisation);Hisaki;Suzuki;;Japan Electronics and Information Technology Industries Association;;Medium (< 250 employees);Japan;The feedback can be published with your personal information;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;"It is important to: 
-Ensure business continuity to maintain effective data, computer infrastructure, applications and other tools which are required for social benefits.
-Consider balance between saving people’s lives, securing safety and health of individuals while securing privacy as the COVID -19 crisis highlighted.
-Consider international cooperation and coordination to promote common rules on the ethical development and use of AI, which preserve an enabling environment for innovation.
";5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;"-Build a reliable framework between companies inside and outside Europe through free and secure cross-border data flows.
-Establish a project development methodology by utilizing AI to address issues with data-driven solutions.
-Provide support to improve";4 - Important;4 - Important;5 - Very important;#NAME?;5 - Very important;4 - Important;5 - Very important;5 - Very important;3 - Neutral;None;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;"-Although the above 4 concerns JEITA checked “Neutral” need adequate and reasonable consideration, these concerns should not be over-emphasized. They are not specific to AI, and they have already been alleviated to some extent. 
-Risks AI imposes should b";Other;"-Guidelines on “High Risk AI applications” is desirable at this stage as more time needed to identify and regulate them.
-In healthcare and railway sectors, physical and operational safety measures are regulated by the existing sectorial laws to eliminate";Other;"-For technology progress, use of “High Risk” AI systems showing extremely high performance may be allowed under certain procedures (with regulatory supervision).

-The definition of ""High Risk"" should be aligned with the discussions at international SDOs ";;;None;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);"-The benefits of ICT and AI for protection of life and livelihoods (public interest) should be balanced with privacy protection as the COVID-19 crisis highlights.
-Although we acknowledge the concerns on using data collected in public place without indivi";Rather not;"-Proliferation of voluntary self-labelling systems that are similar or incompatible would confuse markets and users. Such confusion should be avoided.
-International AI standards are being developed by international standardization bodies such as ISO/IEC ";A combination of ex-ante compliance and ex-post enforcement mechanisms;;"-Conformity assessment by third parties should not require disclosure of competitive or proprietary information (e.g. algorithms, data set details, etc.) 
-Method and physical location of retraining of AI should not be unreasonably restrictive (e.g., Retr";;"-Above risks are neither specific to AI nor inherently related to AI. Discussion on these risks should be based on concrete applications of a particular AI system.
-Providing legal certainty should focus on risks inherently related to characteristics of A";;"New risk assessment procedures should be limited when critical functional changes may significantly alter the performance disclosed in tests or safety reports.
-Applicable requirements should be eased when AI assessment confirms sufficient low-risks; human oversights should be exempted for mitigated risks in designs or more reliable mechanical control
-Balanced allocation of liabilities and responsibilities is required among AI stakeholders such as developers, service providers, and end users";;"-Since the existing framework for liability is robust, technologically neutral, and flexible enough to cover challenges stemming from emerging technology, changes to the current framework should not be made without due consideration.
-Changes to the curre";;;"-Use of AI should be promoted as adequate safety measures eliminating or mitigating risks AI imposes are being implemented as the entire system incorporating AI  
-To ensure proper compensation and to fairly allocate liability on harms caused by AI, adequ";JEITA_Position_Paper_on_EU_White_Paper_On_AI_200613.pdf
F530202;13-06-2020 09:35;English;Business Association;Liga;Semane;;European Banking Federation;4722660838-23;Small (< 50 employees);Belgium;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;"We recommend to:
-	 Promote fundamental research to guarantee the long-lasting impact of AI techniques, and incentivise medium/long term research at a disadvantage in this fast moving field. 
-	Natural language processing in promoting the creation of shared resources and datasets in local languages other than English. 
-	Talent retention through the creation of incentives.";5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;In order to leverage the wealth of industrial data available in Europe, there should be a greater focus on industrial applications. This is often difficult due to the the lack of standard datasets, reference problems and a lack of communication between the industry at large and AI researchers. The EU should encourage the creation of reference datasets and well-defined benchmark industrial problems which the research community can address. ;5 - Very important;5 - Very important;5 - Very important;Support the identification of benchmark industrial problems, strengthen the focus on research and implementation of practical solutions. Access to the facilities mentioned above should also be open to larger firms. ;4 - Important;4 - Important;5 - Very important;4 - Important;4 - Important;;;;;;;;Concerns flagged are valid, yet not all are relevant to financial sector. As FS sector is  heavily regulated, risk management frameworks in place can help to ensure effective mitigation of relevant concerns. Risk assessment criteria should be defined & linked to applications & use cases, not to a specific underlying technology. Need to bridge AI awareness, information & education gap between industry & the public. Communication about the aims and limitations of AI critical for its adoption.;Other;"We do not think new, AI specific legislation is needed. Recommend relevant EU authorities work with industry & civil society to develop guidance on how to apply existing requirements to  AI use cases and provide clarity on issues such as validation criteria.
Increase resources and training for regulators to ensure that they are capable of providing oversight and supervision of AI, particularly for EU experimentation frameworks. See annex for further comments. ";Other;"- On approach in paper to determine high-risk applications,  we express caution that there might be cases where regulatory arbitrage is possible, where an activity can be performed by companies that fall outside a designated high-risk sector. 
- To avoid ";;;;;;;;;;No opinion;;Not at all;Caution against creating a labelling scheme. They can be difficult to set up and are likely to complicate the development and implementation of AI systems, which are often scalable or self-learning, or encapsulated into larger systems in the form of internal or external components that are difficult to isolate, etc. If a scheme is created, the framework should be more specific, simple, and follow a risk-based approach. Please see Annex for further comments.;Other enforcement system;A strict ex-ante oversight could delay or hamper the launch of products/services leveraging AI to the market. ;;Risks related to the loss of connectivity;The following risks could be highlighted: expansion of existing threats (increase in the complexity of cyber attacks, and a reduction in costs to launch them), introduction of new threats, and an increase of the speed and success rate of cyber attacks. ;;;No opinion;;No opinion;;;EBF_041600_-_EBF_Annex_to_EC_AI_White_Paper_consultation.pdf
F530201;13-06-2020 09:07;English;;;;Regional;;;;;The feedback can be published in an anonymous way;4 - Important;4 - Important;5 - Very important;4 - Important;4 - Important;5 - Very important;It is crucial to take into account all languages of the European Union —not only those which have European official language status— in order to all citizens of the UE are represented by the development of the AI.;4 - Important;4 - Important;5 - Very important;4 - Important;5 - Very important;4 - Important;All European languages, especially official languages in some of the states of the EU, should be included in the Coordinated Plan on AI.;4 - Important;4 - Important;4 - Important;To foster the development of AI in non-official European languages, especially in official languages in some of the states of the EU.;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;It is of utmost importance to bear in mind the development of AI plans in non-official EU languages since it would represent an important improvement for European regions that speak that languages. ;5 - Very important;4 - Important;4 - Important;No opinion;No opinion;No opinion;A concern for us is that AI is not implemented in non-EU (but official in some states) languages, so state languages may be in a privileged position.;There is a need for a new legislation;;No;;;;The loss of privacy and the possibility that non-European official languages stay behind even more.;4 - Important;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;No opinion;;No opinion;;Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F530200;13-06-2020 09:01;English;Academic/Research Institution;Jaana;Kokkonen;;Svenska Handelshögskolan / Hanken School of Economics (Hanken);524965237868-38;Large (250 or more);Finland;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;No opinion;No opinion;No opinion;European AI approach needs to take into account the new skill demands for management and leadership of AI dependent organisations;5 - Very important;No opinion;No opinion;No opinion;5 - Very important;No opinion;In line with the Communication on Building Trust in Human-Centric AI (COM(2019) 168), we proclaim that the ‘ecosystem of trust’ does not limit to the development and use of AI technologies, but extends to the human and behavioral organizing around and in relation to them. Therefore, there is an urgent need for generating research-based in-depth knowledge of the everyday practices of managing organizations that heavily depend on algorithmic decision-making.;5 - Very important;No opinion;5 - Very important;We suggest that the EU (AI strategy) acknowledges the importance of social science research – including research in management and organization--  in relation to the research, development and implementation of AI technologies across fields. ;No opinion;No opinion;5 - Very important;5 - Very important;No opinion;We want to raise awareness for the need to understand the design, management, and performances/behaviors of work in organizations that heavily depend on AI technology and how algorithms transform human relations, both within organizations and outsides them, with customers, partners, and other stakeholders. ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;It is crucial that European AI/Digital strategies, legislative actions and European funding programmes take fully into account the importance of effective as well as responsible and sustainable management and leadership of AI decision-making heavy organisations in order to secure ethical and European value based efficient development of European economics and society. ;Current legislation may have some gaps;;Other;Depending on the case, this should be researched and discussed further, thus it is a high important topic.;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;No opinion;;No opinion;;This requires new skills from the management, executive and strategic leadership level of all organizations using algorithmic decision-making, as well as attention to the ‘behavioral implications’ of AI-supported decision-making to the expected and actual behaviors of organizations’ employees, customers, and other stakeholders;;;No opinion;;No opinion;;No opinion;;;
F530199;13-06-2020 08:22;English;Business Association;Gilda;Amorosi;;Eurelectric;4271427696-87;Small (< 50 employees);Belgium;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Collaborations between both public and private universities as well as support from start-up incubators and info-providers should be incentivized. Moreover, the public sector should be an early adopter of AI to improve public services to citizens. ;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;An enabling policy environment is key for AI to be an enabler of solutions itself. Common data sources andcommon ethic framework should be considered to identify best practices, evaluate applicable proper solutions, promote digital transformation in every state andfoster environmental protection through AI.;5 - Very important;5 - Very important;5 - Very important;"More academic courses with fundamentals IT and AI studies must be provided across Europe, with clearly defined standards of competencies and specialisation. Both private and public companies could be involved in the training. Courses on Data Governance and AI in High school could foster the data culture. 
Strategic investments under the EU budget should focus on AI enablers (such as R&I clusters and businesses) and areas of strong European expertise, allowing to face international competition. ";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Digital Innovation Hubs should be managed by a central European direction with common and approved research objectives. In each State, start-up, universities and companies should be aggregated in innovation hubs with specific research programs to progress. Fostering a broad and speedy AI adoption and innovative solutions by SMEs should also be a priority.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;A human-centric approach is paramount to build awareness and acceptance. AI should be designed with specific attention to cyber security, data protection and confidentiality. It should always be managed through experience and supervised by data and domain experts. ;Current legislation may have some gaps;;No;;;;AI applications on human beings, i.e. safety, healthcare, consumptions, instruction, consumers habits, in addition to environmental applications.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);In general biometric identification should be used only if applied for public benefit, i.e..identifications, criminal identifications, healthcare.;Very much;It is also useful to promote existing systems and quality parameters.;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;Policy-makers should be in constant dialogue with all stakeholders on AI benefits and ethical concerns.;Personal security risks;Product reliability on specific data domain.;Yes;All major changes should satisfy risk assessment.;Yes;;Yes, for all AI applications;;;
F530198;13-06-2020 02:36;English;Company/Business organisation;Lev;Sugarman;;Workday, Inc.;021253717373-58;Large (250 or more);United States;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;;5 - Very important;3 - Neutral;5 - Very important;4 - Important;5 - Very important;4 - Important;;3 - Neutral;3 - Neutral;4 - Important;;No opinion;No opinion;No opinion;No opinion;No opinion;;3 - Neutral;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;4 - Important;Workday agrees that the public should “expect the same level of safety and respect of their rights whether or not a product or system relies on AI.” The concerns raised by the Commission are not unique to AI. Existing EU law offers strong, technologically neutral safeguards. Workday recommends that the Commission consider this body of legislation in a targeted way, identify possible gaps and then decide on whether to propose new legislation, AI-related or not.;Current legislation may have some gaps;;Yes;;Other;"Workday agrees with the cumulative two-pronged framework to determine “high-risk” AI applications, but does not agree with the notion that certain AI applications are inherently “high-risk as such” and therefore subject to heightened requirements by default. Instead, the proposed two-pronged risk analysis should account for all AI applications.
";;4 - Important;3 - Neutral;4 - Important;3 - Neutral;5 - Very important;4 - Important;No opinion;;Rather not;;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;;;"The Commission Report on liability advises that the existing safety legislation framework already covers the potential risks from the use of AI or other emerging technologies. Workday urges the Commission to further assess legislative gaps before introducing additional regulatory burdens. Without a demonstrated gap in protection, new regulations may cause unnecessary uncertainty. 
";No opinion;;No;The PLD sets out clearly understood and time-tested rules that apply across a variety of products, including those with software embedded. Under the current regime, consumers have the possibility to obtain compensation for possible harms due to AI, or other products or services. Changing liability rules without additional assessment could chill socially beneficial innovation, with little benefit for consumers.;No opinion;;;Workday_AI_White_Paper_Consultation_Response.pdf
F530197;13-06-2020 02:03;English;NGO (Non-governmental organisation);Lisa;Dyer;;The Partnership on AI;;Small (< 50 employees);United States;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;The reducing inequality and the future of work.;5 - Very important;3 - Neutral;3 - Neutral;4 - Important;5 - Very important;3 - Neutral;;4 - Important;4 - Important;3 - Neutral;Budgetary actions;3 - Neutral;;3 - Neutral;3 - Neutral;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;Please see attachment.;Other;"In some cases legislation is relevant; in other cases it may be premature.  Please see attachment.";Other;It is often difficult to discern or agree on what constitutes high risk.  While some applications are obviously high risk, many fall into a grey category, and the answer often depends on the person evaluating the use and the organization they represent.;;;Any technology used to target ethnic minorities.;5 - Very important;5 - Very important;3 - Neutral;4 - Important;5 - Very important;3 - Neutral;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;No opinion;;Other enforcement system;"Some systems that could be considered high risk are already on the market; others are under development. It will take time, and may be difficult, to retroactively assess those that are already in operation.";;Mental health risks;;No opinion;;No opinion;;No opinion;;;PAI_response_to_EC_White_Paper_on_AI_June_13_2020.pdf
F530196;13-06-2020 00:46;English;Company/Business organisation;Paul;Reinitz;;Getty Images (UK) Ltd;994509831613-80;Large (250 or more);United Kingdom;The feedback can be published with your personal information;5 - Very important;5 - Very important;4 - Important;3 - Neutral;5 - Very important;3 - Neutral;Please see response to Section 1, Q1 in the additional documentation submitted.;4 - Important;2 - Not important;4 - Important;2 - Not important;5 - Very important;4 - Important;Please see response to Section 1, Q2 in the additional documentation submitted.;4 - Important;3 - Neutral;5 - Very important;Please see response to Section 1, Q3 in the additional documentation submitted.;3 - Neutral;3 - Neutral;4 - Important;5 - Very important;2 - Not important;Please see response to Section 1, Q4 in the additional documentation submitted.;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;Please see response to Section 2, Q1 in the additional documentation submitted.;Other;Please see response to Section 2, Q2 in the additional documentation submitted.;Other;Please see response to Section 2, Q3 in the additional documentation submitted.;;;Please see response to Section 2, Q4 and Q5 in the additional documentation submitted.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Other special requirements in addition to those mentioned in the question above should be imposed (please specify);Please see response to Section 2, Q6 in the additional documentation submitted.;Very much;Please see response to Section 2, Q7 in the additional documentation submitted.;Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;Please see response to Section 2, Q8 in the additional documentation submitted.;;Please see response to Section 3, Q1 in the additional documentation submitted.;Yes;Please see response to Section 3, Q2 in the additional documentation submitted.;No opinion;Please see response to Section 3, Q3 in the additional documentation submitted.;Yes, for all AI applications;;Please see response to Section 3, Q4 in the additional documentation submitted.;Getty_Images_-_Response_to_EU_Consulation_on_AI_White_Paper.pdf
F530195;13-06-2020 00:32;English;Business Association;Digital;Future for Europe;;Digital Future for Europe;;Micro (< 10 employees);United Kingdom;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;Digital_Future_for_Europe_-_response_to_the_EU_AI_white_paper.pdf
F530194;13-06-2020 00:15;English;Company/Business organisation;Ann;Becker;;Interactive Software Federation of Europe;20586492362-11;Micro (< 10 employees);Belgium;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;ISFE_response_AI_White_Paper_consultation_12_06_2020.pdf
F530193;12-06-2020 23:45;English;Trade Union;Richard;POND;;European Federation of Public Service Unions (EPSU);04902121531-04;Large (250 or more);Belgium;The feedback can be published with your personal information;5 - Very important;4 - Important;5 - Very important;No opinion;3 - Neutral;5 - Very important;The introduction of AI has major implications for employment and working conditions and it is essential that trade unions play their role through the appropriate processes of information and consultation, social dialogue and collective bargaining. Public services need to be able to control the introduction of AI and make the necessary investment without being led or constrained by the private sector and particularly the major corporations that dominate AI.;4 - Important;4 - Important;4 - Important;No opinion;5 - Very important;5 - Very important;Promoting the uptake of AI in the public sector should be on the basis that public authorities have the resources to ensure its proper introduction and that their AI policies are driven by the public interest and users' real needs and not private profit. Developing skills, in consultation with trade unions, will be very important to ensure that public service workers are fully equipped to deal with AI and that the public sector doesn't have to rely on private providers.;3 - Neutral;3 - Neutral;3 - Neutral;Any initiatives on research should ensure that the public interest is taken into account and that priorities are not simply set by the private sector but by broader social and environmental policy objectives. Above all research should include assessments of the impact on workers and citizens of AI, taking into account questions of equality, human-controlled decisions, meaningful job content, health and safety and environmental impact. ;No opinion;No opinion;No opinion;No opinion;No opinion;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;All these issues are of major concern in relation to both workers and users of public services. The challenge is to ensure a transparent and human-centred and -controlled approach. Trade unions need the right and capacity to monitor and intervene at the appropriate level and public authorities need the resources, including relevant expertise and sufficient staffing to regulate the use of AI.;There is a need for a new legislation;;No;;;;There are several areas of public service, not least health, where AI applications would fall into the high-risk category. EPSU's response to the Commission's communication on e-health in 2018, underlined the importance of ensuring that priorities are driven by public health needs, clearly assessing the potential impacts on patients' mental, emotional and physical well-being and health professionals rather than dominated by priorities of private corporations. ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;EPSU endorses the ETUC's position that facial recognition should remain exceptional and reduced to clearly specific circumstances fixed in law. Any aspect of AI collection and processing of personal data should be based on sound, public and democratic rules, taken in cooperation with legitimate social partners.;Rather not;Any labeling system is best run as a publicly regulated and monitored system rather than a self-regulated, voluntary system and should take account of the fact that there may be changes to the way that AI systems are categorized and not assumed that low/medium-risk applications remain so.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;In line with the ETUC, we would argue that a mandatory AI framework is needed to improve the level of compliance of business. The EU should have the ambition to leave up to the fundamental rights values anchored in the Treaty and to set up a legal system for AI. If GDPR is open for revision to further regulate personal data for AI applications, European and national trade unions need to be on the table of negotiations.;Mental health risks;AI-related risks are many and varied and subject to change and it is essential to keep them under public review. Various factors should be considered to attribute liability fairly between the organization that uses AI technology and the organization responsible for its development. ;Yes;Existing processes for AI-related risk identification, assessment, control and monitoring should be reviewed and adapted in consultation with trade unions.;Yes;In line with the ETUC we would call for more discussion, involving trade unions, on whether an AI application is product or service. Organisations using AI should make sure that the AI application works safely before it is applied and using AI should not be an excuse to breach the duty of care. In amending the EU liability framework, trade unions need to be properly consulted and involved.;Yes, for all AI applications;;In line with the ETUC we would argue that national legal systems need to clarify the scope of potential liability of designers, hardware manufacturers, operators, network service providers, etc., including in relation to the potential of safety and psychosocial issues at the workplace.;
F530192;12-06-2020 23:45;English;Business Association;Douglas;Johnson;;Consumer Technology Association (CTA);;Medium (< 250 employees);United States;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;CTA_Comments_on_European_Commission_White_Paper_on_Artificial_Intelligence__Filed_12_JUNE_2020_.pdf
F530191;12-06-2020 23:22;Polish;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;no;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;;There is a need for a new legislation;;Yes;;Yes;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F530190;12-06-2020 23:15;English;Business Association;Wolfgang;GESSNER;;EPoSS e.V.;583500530587-70;Micro (< 10 employees);Germany;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;;5 - Very important;5 - Very important;5 - Very important;AI is the new opportunity as well as the new challenge in Europe. Technological leadership in AI requires mastering of new digital technologies, in particular new computing architectures, software algorithms, semiconductors and HW systems. This calls for a deep, sustained partnership on Key Digital Technologies (KDT) beyond the current levels of cooperation up and down the value chain. The KDT partnership proposal will be available on the EC website on “European Partnerships in Horizon Europe”.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;Keep up with developments in the future KDT partnership;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;4 - Important;;Current legislation may have some gaps;;Yes;;Yes;;;;;;;;;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);;No opinion;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;;;Yes;;No opinion;;Yes, for specific AI applications;;;
F530189;12-06-2020 22:58;English;NGO (Non-governmental organisation);Richard;Wingfield;;Global Partners Digital;;Small (< 50 employees);United Kingdom;The feedback can be published with your personal information;4 - Important;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;"Developing an appropriate regulatory and governance framework for AI which ensures the protection of human rights
Engagement at international and regional forums to promote a human rights-respecting approach to AI.";No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;;No opinion;No opinion;No opinion;"Require any AI research grant applications to set out how they are human rights-respecting and for this to be a key consideration in funding decisions.
Require prospective research projects to undergo human rights impact assessments.
Place as a specific condition for funding evidence that the research will not undermine human rights.
Promote collaboration between different research disciples on AI-related funding.";No opinion;5 - Very important;5 - Very important;5 - Very important;No opinion;"Ensure that digital innovation hubs in member states offer opportunities to build the capacity of SMEs to understand the human rights implications of AI, and support in helping mitigate risks to human rights as SMEs develop, use and offer AI technologies.
Tie prospective funding for innovative developments in AI to a requirement for human rights impact assessments to be undertaken. 
Place as a specific condition for funding evidence that the funding will not undermine human rights.";5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;;There is a need for a new legislation;;Other;We support the approach of the most onerous requirements being limited to AI systems which are “high-risk”, provided that any AI system which has the potential to significantly interfere with an individual’s human rights should always be considered as “high-risk”. We believe that those developing or applying AI systems should be encouraged, or required, to undertake some form of human rights impact assessment in order to determine whether there are any significant risks to AI. ;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;No opinion;We repeat our suggestion that any regulatory framework make clear that any use of AI which has the potential to significantly interfere with an individual’s human rights should always be considered as “high-risk”. For non-high risk applications of AI, we support transparency to ensure that individuals are always aware of when AI is being used in ways which affect or might affect them. We believe that any labelling scheme should be mandatory, rather than voluntary.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;;Risks to human rights.;Yes;;No opinion;;;;;European_Commission_White_Paper_on_AI_-_Global_Partners_Digital_response.docx
F530188;12-06-2020 22:49;English;Company/Business organisation;Dominique;Grelet;;Atos SE;249876817241-03;Large (250 or more);France;The feedback can be published with your personal information;4 - Important;5 - Very important;4 - Important;4 - Important;5 - Very important;3 - Neutral;European funding of Research and Innovation should emphasize and prioritize projects which embed potential for industrialization by European industrial companies. Europe should establish an agenda to invest and reinforce European AI technology sovereignty (e.g., EPI, neuromorphic computing, HPC centres, quantum computing). Adoption in strategic industrial verticals should be fostered through lighthouse projects and large-scale pilots. Publication of open standards, guidelines, and good practices;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;"i) Fostering transnational & cross-organisational collaboration & programs
ii) Data for Industrial AI. Europe can still claim leadership in industrial AI. For this to happen, there is a need to foster data sharing capabilities within & across industries.
iii) Creating European structures to support data collection & sharing
iv) General Public & SME awareness & training
v) Members states support the local adoption of the AI-on-demand platform & Gaia-X.
Nat'l instances should be created";5 - Very important;4 - Important;5 - Very important;"i) To set-up an innovation accelerator Fund that would complement and collaborate with (not replace) the existing programmes, with a potential lever effect (on equity and loans).
The Fund would include diversified TRL levels, industry sectors, and technology domains.
ii) To reinforce the role of the future PPP as the mechanism to join together the different
European groups working in AI.
iii) To create specific AI Networks of Excellence for AI at the edge and robotics.";3 - Neutral;5 - Very important;4 - Important;5 - Very important;5 - Very important;"i) Role and legal activity of DIHs must be defined to bring certainty to members, potential
partners, and users. Sustainability mechanisms should be also specified.
ii) Current DIHs landscape starts to be fragmented. Requirements must be elaborated to
maximize their excellence and impact.
iii) The relation between DIHs and industry must be encouraged, since connections
between SMEs and big companies lead to fruitful collaboration ecosystems.";3 - Neutral;5 - Very important;4 - Important;3 - Neutral;4 - Important;5 - Very important;"i)Sustainability of AI with regards to its impact on natural resources and energy
consumption. Europe should encourage the least voracious of usage AI and promote only
the use cases which have a positive impact on the society at large.
ii)Collaboration and co-existence (also physical) between humans and AI systems.
iii)Lack of resources and know-how to operate AI systems.
iv)Importance of cybersecurity to prevent attacks causing security and safety risks.";Current legislation may have some gaps;;Yes;;Yes;;"AI applied in use-cases involving critical infrastructures (e.g. nuclear plants, electricity
generation plants, manufacturing facilities) or where there is a potential risk for personal
health and lives. AI for automated decision-making control of Cyber-Physical Systems. AI
embedded in medical devices, especially in the case of implantable devices.";5 - Very important;5 - Very important;3 - Neutral;4 - Important;4 - Important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;Alongside systems based on biometric identification, there exist today alternatives where the individual does not need to be identified, and information of personal nature need not be shared or stored. These solutions should have priority in EU-sponsored deployment projects and in Research and Innovation.;Much;A labelling system should be implemented for AI systems. Critical AI applications would be awarded a Label valid for a set duration (e.g. 2 years) before it must be renewed. This label would qualify the compliance of the systems with a set of guidelines and requirements that can be derived from existing frameworks like HLEG Ethics Guidelines for Trustworthy AI. For SMEs to easily obtain the Label, a self-assessment framework should be provided and DIHs could support them in the process.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Our main concern is the availability of the tools necessary to assess an AI system as trustworthy, secure, and respectful of rules. These tools mostly do not exist today and are not in place. Thus, a strong investment in research and innovation is required, and the EC can concretely support this investment. Compliance assessment should be enforced through a referent in each Member State, in a model similar to that of the LNE (Laboratoire National de Métrologie et d’Essais) in France.;Mental health risks;In our view, the current EU legal framework on product safety covers all risks that arise or can arise from covered products and related technology, including AI. There is no need to expand the legal framework to address specific AI risks. Clarification of aspects such as relating to risk coverage or safety concepts can be done through other means e.g. guidelines, norms, standards, … where necessary.;No;All industries in Europe carry out iterative risk assessment and risk reduction measures according to processes that were introduced according to the requirements of safety legislation. All products benefit from such an iterative approach and the same processes can be applied to AI (and software).;No;No.;Yes, for specific AI applications;highly automated / autonomous systems where AI-enabled systems can cause great damage to people or goods;Under the existing EU legal framework, member states can introduce strict liability regimes in their national legislation for highly automated / autonomous systems to address situations where such AI-enabled systems can cause great damage to people or goods (e.g. cobots on the shop floor). Strict liability shall then trigger the insurance industry to come up with viable insurance solutions.;Atos_AI_position_paper_20200612.pdf
F530187;12-06-2020 22:48;German;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;2 - Not important;1 - Not important at all;5 - Very important;1 - Not important at all;5 - Very important;;3 - Neutral;3 - Neutral;2 - Not important;5 - Very important;3 - Neutral;1 - Not important at all;;2 - Not important;4 - Important;1 - Not important at all;;4 - Important;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;Yes;;No;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Cyber risks;;Yes;;Yes;;Yes, for all AI applications;;;
F530186;12-06-2020 22:47;English;Company/Business organisation;Lise;BITSCH;;Danish Board of Technology Foundation;;Small (< 50 employees);Denmark;The feedback can be published with your personal information;5 - Very important;3 - Neutral;4 - Important;3 - Neutral;3 - Neutral;2 - Not important;;3 - Neutral;2 - Not important;2 - Not important;2 - Not important;3 - Neutral;4 - Important;;3 - Neutral;4 - Important;3 - Neutral;;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;No;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;;Not at all;;Other enforcement system;;;Cyber risks;;Yes;;Yes;;Yes, for all AI applications;;;Feedback_EC_white_paper_AI_DBT.pdf
F530185;12-06-2020 22:44;English;Business Association;Evangelos;Razis;;United States Chamber of Commerce;483024821178-51;Large (250 or more);United States;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;U.S._Chamber_of_Commerce_Comments_EU_AI_White_Paper__final_.pdf
F530184;12-06-2020 22:29;English;Business Association;Micha?;ZAKRZEWSKI;;APPLiA - Home Appliance Europe;04201463642-88;Small (< 50 employees);Belgium;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"The Parliament and Commission should work to provide a clear definition of AI, as well as to promote a global or at least international  approach to AI in terms of regulation and standardization to secure level playing field and avoid unfair competition.
";5 - Very important;3 - Neutral;5 - Very important;5 - Very important;4 - Important;3 - Neutral;The EU’s AI goals will only be achieved with sufficient EU funding mechanisms behind them. The funding is to speed up/foster innovation in this area and not to impose additional constraints.;3 - Neutral;3 - Neutral;3 - Neutral;We would like to understand better what aspects of AI will be subject to research, and we suggest that at least some of the research is done on applications of AI rather than AI fundamentals;3 - Neutral;5 - Very important;4 - Important;4 - Important;4 - Important;The data economy is far broader than just AI, and the EU should focus on this broader data economy rather than just AI;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;5 - Very important;5 - Very important;"The answers we have given are from the perspective of the home appliance industry and what is relevant for our products; not all questions are relevant for our industry";Current legislation may have some gaps;;Yes;;Other;the proposal on risk lacks the clarity and legal certainty required to legislate. Nevertheless, we believe the approach isn’t necessarily wrong, but needs some improvement or refinement;;4 - Important;4 - Important;2 - Not important;4 - Important;2 - Not important;5 - Very important;No opinion;we are not using biometric identification system for home appliances ;Not at all;no;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;we do not regard household appliances as high-risk products ;;we don’t believe all the risks mentioned above are specific AI-related risks;No;We don't believe that risk assessment procedure should be changed, nor do we believe the idea of 'placing on the market' is outdated. Products should be expected to remain safe even if software evolves after placing on market. ;No;We believe safety and liability are 2 sides of the same coin in Europe - you are liable if your product doesn't have the level of safety a consumer can reasonably expect;No opinion;;APPLiA does not have an opinion on the effectiveness of national liability regimes. However, we believe a harmonized EU approach is preferable to avoid market fragmentation;
F530183;12-06-2020 22:28;English;Business Association;Jade;Nester;;GSMA;30988577529-37;Large (250 or more);United Kingdom;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;At a high-level, we would welcome more specificity about the goals of the ecosystem of excellence, e.g., whether certain types of AI will be pursued, and across which sectors, and possible international coordination. Regarding funding, the EC could further consider the crucial lever of public procurement of AI systems, taking into account that half of the EU's GDP is public money. We also note that AI research and funding should be facilitated for all players, not just SMEs. ;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;The development of pilot projects could be encouraged, similar to the existing pilots for connected cars. ;5 - Very important;5 - Very important;5 - Very important;An EU-funded national and transnational lighthouse project could facilitate private development and implementation of AI. ;3 - Neutral;5 - Very important;3 - Neutral;4 - Important;3 - Neutral;Whilst we agree that sharing expertise across sectors can help drive AI innovation, we also recognise that AI research requires investment in a range of areas, including data analytics. If knowledge transfer requires data sharing, any personal data must be protected according to relevant data protection rules, and trade secrets and intellectual property must also be protected. ;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;3 - Neutral;While AI does raise some concerns, many concerns can be mitigated through existing data protection, product safety, and liability. MNOs advocate for protecting privacy and human rights, and this remains true for AI applications. AI liability may be complicated, but this is often due to the complexity of the ecosystems where AI is deployed, e.g., a Smart City involving multiple systems and services. Also, while accuracy is important, other issues may prompt more concerns. ;Other;Existing privacy rules address many of the issues identified in the White Paper. While in some cases it is unclear if the White Paper refers to personal or non-personal data, in the context of personal data, the GDPR appears sufficient to close potential gaps. We believe that any new regulation should be limited to clearly identified problems. Guidelines may also be sufficient to clarify potential regulatory gaps. ;Other;"There may be some gaps in legislation but these gaps could be clarified through guidance. If new rules are promulgated, then we agree that they should be limited to high-risk applications. The two criteria to identify high-risk AI systems need further specification to avoid legal uncertainty, e.g., a clear definition of ""significant"" impact or the ""exceptional instances"" that classify an AI system as high-risk. Risk-grading, e.g., the German Data Ethics Commission scale, could help assess risk. ";;;Comprehensive surveillance systems involving the use of remote biometric identification and other tools to determine the identity of individuals in public places may require high levels of scrutiny. ;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;Provisions linked to such labels for low-risk AI must, on the one hand, support trust, while on the hand, not be overly burdensome. Otherwise such labels will not be used on a voluntary basis. Applicable provisions falling under the label could relate to transparency, robustness, and human oversight. Rules around enforcement and legal remedies for users should not apply in the same manner as under high-risk applications, given the different levels of risk. ;A combination of ex-ante compliance and ex-post enforcement mechanisms;;The proposed flexible and open EU-level governance structure proposed in the White Paper could present an efficient way forward. However, such structures need to be sufficiently strong to ensure effectiveness and cross-border consistency with regard to monitoring and law enforcement. Also, the system requires that regulatory roles are clear, and cooperation mechanisms are effective. ;Cyber risks;"Reforms should rather focus on ""ex ante"" mechanisms based on the Product Safety Directive rather than ex-post legislation as the Product Liability Directive. Connectivity is the basis of the digital market, therefore connectivity providers should not be held responsible for ""falsely attributed"" liabilities of increasingly complex connected devices that stem from potential defects in a supplier's products and services. ";Yes;For the sake of legal certainty, clear rules on who decides about whether a product imposes a high or low risk are required based on clear legal provisions. Considering the evolving nature of AI products, minimum safety safeguards (and related assessments) should be guaranteed throughout the entire life-cycle of a product embedding AI. To achieve this, it is essential that the producer is obliged to provide and install the necessary updates required for the correct function of the AI product. ;No;We believe that the current Product Liability Directive and recent legislation are sufficient to address legal concerns and the needs of consumers and businesses. If new rules are considered, it is essential to establish a balanced distribution of liability across the different actors in the value chain, with a stronger focus on producers/manufacturers/programmers that are often the only actors in the position to ensure the functioning of the AI product e.g., instalment of relevant updates. ;No;;Potential issues or gaps on the Product Liability Directive are not AI-specific. To address potential issues around liability, possibly required legal adjustments should be technology neutral and targeted without imposing excessive burdens on industry. ;
F530182;12-06-2020 22:21;English;Business Association;Sara;GHAZANFARI;;ETNO (European Telecommunications Network Operators' Association);08957111909-85;Micro (< 10 employees);Belgium;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;"•	Creation of EU-funded national and transnational lighthouse-projects to facilitate private development and implementation of AI. 
•	Use of public procurement.
•	Clear roadmap on funding instruments and priority sectors (AI application domains), including specific commitments by the Member States.
•	Promotion of intersectoral dialogue and collaboration.";4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;"•	Beyond these important areas, the Commission should consider implementation, i.e., prioritise those areas where it is most realistic to have an impact, and achieve the greatest value for money. 
•	A clearer division of roles and responsibilities between the European Commission and Member States should be explained.
•	Better funding and policy instruments to enable start-up ecosystems. 
•	More focus on AI scale-ups, and instruments to enable their growth.";4 - Important;4 - Important;5 - Very important;The EU should consider to pro-actively support international AI research. Although the EU needs to establish a stronger competitive position, it is most likely to succeed by leading on trustworthy responsible AI that benefits individuals and society. Many international researchers share the EU’s principles applicable to AI (e.g. GDPR). The EU can benefit from talents outside the EU to nurture the proliferation of European values and principles.;3 - Neutral;5 - Very important;4 - Important;4 - Important;4 - Important;"•	Digital innovation Hubs should be controlled by an independent third-party organisation, in accordance with national and/or European rules and obligations, in order to protect SMEs.  
•	Improve the digital maturity of each company and collaboration amongst the different players, to accelerate digital transformation at large.";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Competition issues and dominance positions of a few global big player on the data market should be taken into consideration, since they can hinder innovation and a flourishing European market on Artificial Intelligence.  ;Current legislation may have some gaps;;Yes;;Other;"ETNO supports a risk-based approach to an AI regulatory framework. Further clarifications are urgently needed.
For example, the intention to periodically review the list of those sectors considered as high risk can lead to legal uncertainty and might negatively impact the investment plans of sectors outside the “high risk” category, including telecommunications. Accordingly, the period for review needs to be adequate and based on clear principles that define what goes under “high-risk”.";AI systems involving high risks for physical integrity and fundamental rights should be addressed.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;"ETNO is of the opinion that the suggested possibility for AI solutions that are not ""high-risk"" to benefit from voluntary labelling can be helpful. It is important to ensure that the voluntary nature of the labelling system does not become a de facto legal requirement for market access. 
Certain proof of “trustworthiness” should be required and continuously re-assessed to ensure the validity of the label. ";A combination of ex-ante compliance and ex-post enforcement mechanisms;;To minimise risks, particularly high-risk products should fall under “ex ante” products safety rules that address new challenges. A balanced combination of ex ante and ex post safeguards is needed as an impact analysis for the potential risk resulting from a specific AI-application is not always fully predictable upfront. For the sake of legal certainty, clear rules on “who” decides about whether a product imposes a high or low risk are required, based on clear legal provisions.;Risks related to the loss of connectivity;Questioning whether the loss of connectivity is an AI-related risk appears misleading, as connectivity underpins a broad range of services and products. Sector-specific regulation already considers the crucial role of digital infrastructure and lays down provisions that address connectivity-related issues, such as in quality of service obligations and redress mechanisms for users. ;Yes;;No opinion;"In the review of the Product Liability Directive, it should be assessed whether new challenges that are not AI-specific but that relate to digital services and products more broadly can be better addressed in the scope of that horizontal framework. 
Risks that are not AI-specific cannot reasonably be solved by AI-specific rules. ";Yes, for specific AI applications;;"The creation of an AI-specific regime for compensation and allocation of liability is neither practical nor future-proof. This should remain covered by horizontal rules to ensure consistency.
Players belonging to different sectors may be exposed to different levels of risk, for which more clarity is needed. ";ETNO_position_on_the_EC_AI_White_Paper_FINAL.pdf
F530181;12-06-2020 22:18;English;Business Association;Jari-Pekka;KALEVA;;European Games Developer Federation (EGDF);57235487137-80;Micro (< 10 employees);Sweden;The feedback can be published with your personal information;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;"1. Access to talent: Europe needs the most inspiring and advanced companies to attract the best global talent and it is therefore important that immigration bureaucracy does not hinder European companies from recruiting the best AI talent from abroad. 
2. Firm action against intelligence services trying to steal European data: the Union has to defend its digital industries against European and foreign intelligence services trying to steal their data.";3 - Neutral;2 - Not important;4 - Important;5 - Very important;4 - Important;3 - Neutral;"1. Access to talent: Europe needs the most inspiring and advanced companies to attract the best global talent and it is therefore important that immigration bureaucracy does not hinder European companies from recruiting the best AI talent from abroad.
2. Going beyond technological innovation: Europe needs to support the use of AI in cultural and creative sectors by investing in technological, business as well as in content innovation. ";4 - Important;2 - Not important;2 - Not important;"Focus on the innovative SMEs instead of the most innovative project applications: Public-private partnerships, testing facilities and innovation hubs are usually able to reach the SMEs with the most innovative project applications instead of the SMEs with most innovative projects. In addition, these initiatives are often too focused on technological innovation and fail to address innovative artistic content and business models enabled by AI.
";4 - Important;2 - Not important;5 - Very important;4 - Important;5 - Very important;The Commission should secure that the equity funding pilot scheme has the capacity to reach also highly innovative AI-driven start-ups from cultural and creative industries with significant market potential that are struggling to access seed funding after the COVID19-outbreak. In practice, this means that the pilot scheme has to be incentivised to make also small high-risk investments from 25 000€ to 100 000€. ;3 - Neutral;3 - Neutral;5 - Very important;3 - Neutral;3 - Neutral;4 - Important;"1. The trustworthy use of AI is based on B2B transparency: The AI regulation can easily repeat the same mistakes as GDPR: European SMEs get new legal liabilities without sufficient rights and tools to secure that their B2B service providers are following the European legal requirements.
2.Freedom of arts, to conduct business and establish and provide services: AI-based ranking algorithms can discriminate certain business models, artistic content, or services from a certain region in the EU.";Current legislation may have some gaps;;Yes;;Yes;;;4 - Important;2 - Not important;5 - Very important;4 - Important;4 - Important;4 - Important;No opinion;;Rather not;Channelling all low risk AI applications in a voluntary legally binding scheme could create significant administrative burdens for SMEs. Guidelines or best practices that are no part of such a voluntary scheme could be as efficient in creating a responsible and trustworthy AI. ;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;;;;No;;No;;No;;;
F530180;12-06-2020 21:57;English;Business Association;Jussi;Mäkinen;;Technology Industries of Finland;39705603497-38;Medium (< 250 employees);Finland;The feedback can be published with your personal information;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;"EU acquis should provide a predictable and supportive basis for taking up AI solutions. 
EU should promote: 
•	a clear definition of AI.
•	framework for the definition and governance of regulatory sandboxes
•	strong cooperation between AI developers and cybersecurity experts (set up projects funded by Horizon Europe & Digital Europe Program):
•	international & open market and standards.
•	clear statistical indicators ";5 - Very important;3 - Neutral;5 - Very important;5 - Very important;5 - Very important;4 - Important;"•	Adopt an ambitious EU Financial Plan to support uptake of AI 
•	Ensure that European and national public research focus on AI applications i and address industry-specific challenges
•	Invest in very high-speed infrastructures and cybersecurity
•	Strategically use public procurement to create new markets for AI applications
•	Develop awareness and basic education on AI 
•	Build clarity for processing of personal data, especially anonymisation and pseudonymisation.";3 - Neutral;3 - Neutral;4 - Important;"•	A strong investment plan is required at EU level, that should be focused on the EU leading domains (machine learning, semantics, NLP, etc.) and on AI applications. 
•	However, setting up a lighthouse research centre could take very long time to install, be very expensive to set up, and increase rigidity whereas AI development requires agility.
•	Leveraging existing structures, including PPPs
•	Regulatory sandboxes to facilitate uptake of AI solutions.";5 - Very important;4 - Important;5 - Very important;5 - Very important;4 - Important;"•	In general, SMEs do not have a strong experience of working with Digital Innovation Hubs (DIHs)
•	DIHs could be instrumental if focusing on technology transfer, DIHs could specifically help SMEs to develop and test their use cases
•	They should focus on EU leading domains like edge computing and data analytics; 
•	Business models of data economy are still in development
•	In general, there is a huge potential for SMEs to improve their business by using AI.";3 - Neutral;3 - Neutral;4 - Important;2 - Not important;2 - Not important;1 - Not important at all;"•	The question underlines negative aspects of AI. These concerns are not AI-specific and apply to many other technology applications & are already covered in EU’s horizontal and sector-specific, and liability regulation. 
•	Risks are linked to the purpose and scope of AI usage. These are set by humans.  
•	The use of AI as a tool to perform specific tasks in industrial processes does inot interfere with human rights issues. 
•	It is important to build criteria for testing AI.
";Other;"•	Does the use of AI truly create concerns that are beyond existing acquis? 
•	Before choosing any option, existing sector-specific regulation needs to be carefully analysed, and the right tool adequately proposed, based on a realistic definition of AI. 
•	AI is  a technology embedded in products. Existing product safety law is still valid.
•	Regulatory sandboxes are key to understanding true implications of new technologies.";Yes;;Other;"Whereas it is good to limit regulation to high-risk cases, TIF is not convinced that they can be identified by the proposed model. More emphasis should be put on the purpose, process, and data used.  
•	A very clear process is needed for identifying risks. Process should be general and take into account graveness and probability of risk and consequencies of using other technologies.
•	System must be predictable for developers and deployers of AI systems.";;3 - Neutral;3 - Neutral;4 - Important;3 - Neutral;3 - Neutral;4 - Important;No further guidelines or regulations are needed;;Rather not;"•	There are no standards to support such a scheme. Standardisation and labelling activities should be industry-driven and not imposed upon by authorities. 
•	Risk of transformation of the voluntary scheme into a mandatory scheme (notably via public procurement requirements)
•	In industrial sphere, the benefits may be limited
•	An AI-specific labelling scheme might create confusion and could only be envisaged at a broader level, e.g.in conjunction with labels meant in the GDPR ";Other enforcement system;"A combination of ex ante self assessment and relevant ex post measures has worked before and should work here as well.  AI is going to be embedded in products and services. Therefore too much weight should not be put on AI per se, but concentrate on the bigger picture on guaranteeing safety and trustworthiness. Ex ante criteria should not be too rigorous to allow market access also to SME AI companies and allow innovation. 

";"•	The priority is to establish standards; only then can methods to check compliance be developed and deployed
•	Many EU companies operate globally, we do want to steer clear of risk that overtly stringent requirements would isolate EU AI market from the global one and impose EU-specific standards and conditions for entering the market. 
";;"The notion of “personal security risk” is unclear. In any case, it should not be associated with the notion of risk: AI is one technology among others and as such not riskier than other technologies. Cybersecurity is a risk outside the scope of the product safety legislation, and currently addressed in B2B context by virtue of a contract. A new horizontal regulation addressing this specific risk may help bringing legal certainty and homogeneity. 

";No;"•	According to the Machinery Directive, the manufacturer must determine the intended use, taking full account of the functions, operators, persons present and the environment of the machine 
•	When the scope of the AI application changes, the risk assessment has to be re-initiated. But this does not require new risk assessment procedures, just new methodologies (e.g. considering the lifecycle of AI) 
•	Finally, such terms as “important” or “change” are quite too vague to use in legislation
";No;Before any revision, the Product Liability Directive should be thoroughly analysed with sufficient empirical data to check whether the specific use of AI may lead to a situation where risks are not covered. Any analysis should be based on empirical data. Possible new legislation should only be applied to cases where completely autonomous AI makes existing liability law impracticable. ;No;;"•	There is no evidence that existing frameworks, both at EU and national level are not working and cannot cope with the specificities of AI applications 
•	We call for pragmatic, evidence-based approach to liability, limiting the EU interventions to those cases where the current liability regimes would not be suitable. Possible role of sector-specific regulation should also be carefully studied.";AI_WP_Open_Comment_TIF_200612.pdf
F530179;12-06-2020 21:52;English;;;;;;;;;The feedback can be published in an anonymous way;3 - Neutral;5 - Very important;4 - Important;5 - Very important;5 - Very important;4 - Important;;;4 - Important;5 - Very important;3 - Neutral;3 - Neutral;4 - Important;;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;;There is a need for a new legislation;;No opinion;;;;;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;4 - Important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;Yes;;Yes, for all AI applications;;;
F530178;12-06-2020 21:47;German;EU Citizen;KirstenA;Fi;;;;;Germany;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;1 - Not important at all;1 - Not important at all;It is difficult to respond to this question, especially regarding the private sector partnership and the adoption by public sector, as it depends on the area and context of deployment and the type of AI or ADM system used. In the past years, there were many cases where using private sector tools in public administration caused harm and violations of individuals' rights. See the report on digital welfare states by UN special rapporteur for extreme poverty. We need limits and strong safeguards.;4 - Important;4 - Important;3 - Neutral;3 - Neutral;4 - Important;2 - Not important;"1. there is a danger to give in to the trend of blindly promoting AI as ""snake oil"" - a solution to all kinds of problems in all kinds of sectors even though alternatives might be more efficient 2. it depends how the promotion of AI in Europe and testing is done - if EU rights & freedoms are enshrined by design, then this would be hugely contribute to Europe become a standard-setter for ""trustworthy AI"" 3. citizens need to be involved in all discussions & decisions on AI for public services.";4 - Important;5 - Very important;2 - Not important;"For the first two points (lighthouse and excellence centers), see the comment above - they should take European rights as enshrined in the Charter and Treaties into account by design. Regarding the public-private partnership, see my comment to the first question.
Financing by the EU of AI projects that have an undoubtedly negative impact on rights & freedoms, such as biometrics in public spaces, should stop immediately.";3 - Neutral;4 - Important;3 - Neutral;3 - Neutral;4 - Important;;5 - Very important;5 - Very important;5 - Very important;2 - Not important;5 - Very important;5 - Very important;"The choice between ""important"" and ""unimportant"" is strange when it would be more accurate to choose ""true"", ""false"" etc. Certain AI systems have already been proven to violate rights and to discriminate. However, a ""black box"" often is a myth and a deliberate design choice, for ex to keep the system opaque or to protect trade secrets. For higher risk systems, a black box should never be the model to follow, but human intervention and explainability should be obligatory.";There is a need for a new legislation;;No;;;;"1. biometrics in publicly available spaces
2. ADM and AI used by enforcement authorities
3. ADM and AI used at European borders
4. autonomous lethal weapons
5. identification/ analysis of emotion and identity traits
6. the delivery of essential public services";5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;Biometric identification systems should never be allowed in publicly accessible spaces;The use of biometrics in public spaces should be in the highest risk category and banned - it mainly consist of untargeted mass processing of personal data which has already been judged illegal by the CJEU. Such practices would facilitate mass surveillance in violation of fundamental rights, including privacy, data protection, equality, freedom of expression and information, freedom of assembly and association.;Rather not;"Voluntary mechanisms and self-regulation risk to be used by industry to embellish practices, and might not be accurate if strong oversight is lacking. Also, there are many cases that are not clearly cut into low vs high risk and that might need to be checked whenever authorities deem this necessary, a voluntary self-labeling system is probably not enough when a system that has ""improved"" itself starts breaching rights. A risk based approach needs to be much more nuanced.";A combination of ex-ante compliance and ex-post enforcement mechanisms;;"1. As AI is not static, there need to be regular assessments for middle to higher risk systems
2. Assessments need to include the impact on rights; the quality & integrity of the collected and processed data; the intensity of the possible violation of rights; the impact on society & the environment.
Moreover the German data ethics commission recommended to include the role of the algorithm in taking the decision; the dependence of people impacted; the reversibility of the decision.";Mental health risks;the risk of discrimination, the risk of being subject to micro-targeted advertising;Yes;There should be an obligatory ex ante assessment of the risks to fundamental rights, as well as regarding all points I mentioned above in further suggestions for compliance.;Yes;Liability framework updates need to consider the entire development and deployment chain. Furthermore, the burden of proof should be carefully considered as it is not always possible for victims to proof harm or damage. Collective actions should be made possible.;Yes, for all AI applications;;;
F530177;12-06-2020 21:36;English;Company/Business organisation;Torsten;Kuepper;;Qualcomm;;Large (250 or more);United States;The feedback can be published with your personal information;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;Access to quality data is key for the development of AI. Privacy and security are also key requirements and could be enabled by on device intelligence and edge-cloud. The combination of AI, 5G and cloud will be key for digital transformation of industries from autonomous driving to smart factories to user identity management, etc. Europe should consider investing heavily in this area as part of the EU economic recovery program. ;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;European data spaces in various vertical sectors including manufacturing, health and mobility will enable access to quality data, understanding not all data and related spaces are created equal. It will be important for Member States to support such initiatives. Furthermore, we are of the opinion that at least as important as supporting start-ups is the funding of scale-ups and enabling them to grow.;3 - Neutral;5 - Very important;5 - Very important;We think that it is more important to invest in the existing AI ecosystem and creating a coordinated network among the many existing AI R&D excellence centers (like e.g. Qualcomm’s AI Lab in Amsterdam) than creating a new (lighthouse) center. At least three factors will be key for the success of a network of AI research enters: a) creating a leadership structure to ensure coordination and coherent operation b) agreeing on a vision re. focus and priorities and c) continuous financial investments.;4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;A Digital innovation Hub should help SMEs to understand the policy framework and legal restrictions and obligations. In this context we consider that an easy to adopt and easy to understand policy framework is needed. ;5 - Very important;5 - Very important;4 - Important;4 - Important;3 - Neutral;3 - Neutral;AI should be human centric and its innovation should be fostered in Europe. Avoid risks that innovation is limited if AI must always meet a “gold standard”. Requirements should not exceed what is established for non-AI apps. A good balance between the benefits of using complex AI systems against the practical constraints that standards would impose is needed. AI's greatest value is seeing patterns in complex situations, beyond human comprehension. Not all in AI is explainable directly as such.;Other;It is important to ensure that any AI specific legislation is coherent with sector specific legislation and policies. In addition, legislation needs to be agile and flexible to be able to keep pace with future innovation. ;Other;Please see attachment to this consultation;;;Large scale people surveillance. Bio-metric identification systems.;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;No opinion;;No opinion;A voluntary labelling system for AI systems that are not considered high-risk may be useful in the future. The development of such a system would need to overcome important challenges in order to be widely-accepted across Europe and beyond. It must be consumer-friendly, be adequate for the enormous breadth of AI systems and their use cases and serve the right purposes.;Other enforcement system;Extensive exchanges with players across the AI value chain will be necessary to develop fit for purpose compliance and enforcement mechanisms. This shall include ex ante self-assessments. A global exchange and value driven global dialog is needed and it needs to be ensured that European AI and data will not be set at risk- and that the EU value driven approach will apply when AI applications will (partly) run outside the EU or if hardware which is physically not based in the EU is used. ;If too much upfront testing is needed there is a risk that this will lead to a backlog given that usually 3rd party testing capacities are limited and time consuming. This could lead to launch delays in the EU and would make Europe potentially as well less attractive for the development of new services. To allow self-checking where appropriate and the ex-post imposition of requirements could be advisable.;Risks related to the loss of connectivity;The risks pointed out above are risks which might be relevant for the usage of ICT products in general. The existing laws are often sufficient, but it could be useful to provide greater legal clarity as to their interpretation, as well in relation to AI applications. It might be advisable to access special risks and special safety legislation for special (critical) use cases.;No opinion;Carrying out a new risk assessment should only be required when there has been a substantial change to the product which is likely to have an impact on the security of its intended use. Regular updates such as security fixes, bug fixes, or simple non-critical improvements after product is on the market should not lead to a renewed risk assessment.;No opinion;As previously mentioned, the question needs to be more narrowly examined focusing on specific types or levels or risk as well as specific AI applications or uses or systems as appropriate. A one-size-fits-all approach in relation to AI and liability will not ensure the desired consumer protection or the necessary legal clarity. ;No opinion;;We believe that harmonizing liability rules across Europe will contribute considerably to AI development by start-ups, to consumer trust and protection and to legal clarity, thus to the overall uptake of AI in Europe. ;Qualcomm_Annex_to_the_EU_AI_WP_Consultation.pdf
F530176;12-06-2020 21:06;English;NGO (Non-governmental organisation);Christine;Meissler;;Brot für die Welt;46530831412-96;Large (250 or more);Germany;The feedback can be published with your personal information;;;;;1 - Not important at all;1 - Not important at all;"AI should not be promoted just for the sake of it. If AI is used for public functions, there must be: 
1. clear reasons to justify AI; 
2. scientific evidence that the technology works; 
3. where the technology plays a role in determining people’s access to services or to enjoy fundamental rights, people should have a say if AI can be used. 
The EC needs to include democratic oversight, consultations with civil society, the public and groups concerned, and stringent human rights safeguards. ";;;1 - Not important at all;;;;We propose that the coordinated plan on AI is updated to include criteria (scientific and policy) about how the EU will allocate its resources of AI. The plan should include a section on human rights, societal impacts of AI and automation, and how to ensure democratic oversight for the application of AI systems.;3 - Neutral;4 - Important;1 - Not important at all;There is little mention of research into human rights (HR) and the societal impact of AI. We are concerned about potential discrimination, disinformation, a lack of transparency. These issues require further research. The EU needs to consider which uses of AI are impermissible. Funding for EU projects on AI should be conditional on the EU’s own ethical standards for AI and fundamental rights. EU funds (as Horizon2020) should comply and stop funds such like iBorderCtrl, which pose a risk to HR. ;1 - Not important at all;;;;;Small businesses should not enjoy any exemptions to protecting human rights. Some of the biggest AI-related scandals have involved small companies. The EU should ensure that when small businesses take up AI, they are obliged to respect data protection, privacy and other fundamental rights. One of those examples is Clearview AI – a small  ‘startup’ which famously mined data to build a vast facial recognition database to sell to law enforcement agencies without peoples’ consent. ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;"Impact on migrants/people of color, people with disabilities and health problems, women, workers; 
The fact that AI could take decisions in public services without human and democratic oversight, transparency or sufficient evidence to justify its need. 
AI use in policing, recruitment, healthcare; conscious avoidance of liability for harms produced by AI technology; 
Increasing use of opaque, privately-developed technology in the public sphere, which do not meet transparency requirements.

";Other;"AI regulation should not provide loop-holes to data protection legislation, or other frameworks, like discrimination law; 
Current law does not address use of non-personal data for AI, types of data which do not fall under the GDPR; 
AI has huge collective impacts, e.g. furthering overpolicing, surveillance, in-equalities (all not addressed in existing legal frameworks); 
AI leads to discrimination on issues like financial status which are not protected in discrimination law.";Other;"New rules should clearly outline criteria to determine which AI systems are legal and which are not. Such criteria should be based on proving that they work and are needed, conducting mandatory fundamental rights impact assessment for all applications, and ensuring democratic oversight.
Uses of AI which breach fundamental rights - like biometrics/facial recognition for mass surveillance - should be banned outright.";;;"AI use to determine delivery of essential public services; 
Predictive policing; 
Autonomous lethal weapons; 
Identification/analysis of emotion and identity traits; 
Indiscriminate biometric surveillance. 
These uses are unacceptable because they are incompatible with fundamental rights. They are beyond risk and should be banned. We call on the EC to urgently rethink its risk-based approach and consider which uses of AI will never be acceptable, legal or complaint with human rights.";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;"Their use in public spaces will lead to mass surveillance; 
This will irreversibly limit our fundamental rights to privacy, freedom of assembly, expression, non-discrimination, data protection, dignity and the right to a fair trial, creating societies of suspicion; 
Even uses which do not contribute directly or indirectly to mass surveillance in public spaces still pose significant threats to privacy, data protection, non-discrimination, and dignity. 

We think that biometric identification is unlawful and incompatible with human rights law. Societally, biometric identification is likely to undermine peoples’ freedom and ability to engage in public life, and that there is a risk that such systems will disproportionately be used to target already over-policed and surveilled groups, including racialised groups, migrants, working class and poorer communities.";Rather not;Self-labelling systems can be confusing for people and may give a false sense of security, as it is the same company that develops a product, which says that it is safe. We believe that the high/low risk distinction is overly simplistic and could very well allow for loop-holes for systems with potentially very significant impacts on peoples’ safety and rights. This is especially so if ‘low risk’ systems are only voluntarily controlled. How could those harmed by low-risk systems seek redress?;A combination of ex-ante compliance and ex-post enforcement mechanisms;;All systems should undergo a mandatory ex-ante human rights impact assessment from an external body. Compliance should be external. We need this to guarantee fundamental rights are protected, we cannot rely on self-regulation for this. Also, it needs to be ensured that there are no loop-holes just because a system falls into a low-risk category.;Mental health risks;We highlight the potential risks of discrimination by AI systems.The use of AI in online products and services requires collection and use of data lending toward discrimination in many fields related to targeted advertising.This poses risks of differentiated pricing, discrimination and financial detriments, the risk of creating filter bubbles, interfences in the political process, all based on sensitive inferences or associations. AI can also impact on accessibility of persons with disabilities.;Yes;Internal supervisors, such as Data Protection Officers under GDPR should be included and asked for advice.;Yes;AI developers and deployers should be accountable for harm generated by their products. Products developed using AI should not enjoy exceptions to any EU laws, whether it be discrimination, data protection, or product liability.;Yes, for all AI applications;;EU needs to address copyright and database protections which prevent proper oversight of AI applications. Liability rules should provide incentives for openness.;
F530175;12-06-2020 20:26;English;Company/Business organisation;Angel;Martin;;Johnson & Johnson;75617941310-89;Large (250 or more);United States;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;"-Appropriate computing infrastructures and access to high quality and representative health data ensuring robust, accurate and safe AI solutions
-Investing in skill development and education of healthcare professionals to support an optimal use of AI
-Bui";4 - Important;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;"-Supporting Member States’ exchange on healthcare assessment models could foster best practice across Europe
-Address barriers to the development and uptake of apps and wearables supporting use of pharmaceuticals, promoting self-care and pharmacies and he";5 - Very important;5 - Very important;5 - Very important;"-EU framework for AI should be based on agile and fast processes, including mechanisms to shorten the application-to-granting duration for EU R&D funding
-AI research excellence centers should partner with healthcare actors to test AI solutions in real op";4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;-DIH can help the uptake of AI in healthcare by building models of integration in healthcare organizations, addressing their services, organizational arrangements and workflows as well as fostering best practices on testing and reference facilities, AI st;5 - Very important;5 - Very important;4 - Important;4 - Important;3 - Neutral;5 - Very important;-A robust and accurate AI needs adequate access to enough, accurate and representative  (preferably, pseudonymized or anonymized) data to train AI appropriately, particularly in certain application areas to deliver key health outcomes and high standards o;Other;-Legislation may be enough in sectors such as Medical Devices where safety of software as medical device is addressed by MDR/IVDR. However, guidance documents by MDCG will be needed to support authorities’ assessment and harmonizing requirements for manuf;Other;New AI rules should reference and respect existing legislation and avoid overlapping provisions or duplication of work as well as legislative uncertainty to which provisions apply. Risk is context-specific, and its classification should be defined and enforced by sectoral legislation (e.g. healthcare). MDR and its guidance documents provide a framework for risk classification of software classified as medical device, where the most stringent requirements are applied to higher risk applications.;;;"-Categorization of “risks” should be specified under sectorial legislation as sector rules and authorities are best positioned to judge the risk that the use of the AI system may entail for people and systems
-The representativeness of training and test d";5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;4 - Important;No opinion;;Very much;An ethical principle-based voluntary labelling system would need to be sector-relevant (i.e. healthcare) including specific scenarios and should benefit from customized assessment lists. It could address AI systems lifecycle considering all relevant aspects. It should have a value chain perspective where the role of each actor is adequately reflected, enabling better communication and trust between all relevant actors (including AI developers, hospitals, authorities, physicians and patients).;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Compliance assessment of an AI framework should factor in existing provisions in other pieces of legislation which require certain classifications and risk assessment. Therefore, such compliance provisions should be structured to refer to existing requirements in order to contribute to enhanced legislative clarity and to avoid any inconsistencies, through the development of sector specific guidance documents issues by regulatory authorities (e.g. Medical Devices Coordination Group).;Risks related to the loss of connectivity;-Rules on liability distribution between AI manufacturers, deployers and users are not always clear. Legal certainty is essential, whether it is through the interpretation of existing rules or a new strict liability regime, which will need to limit this s;No opinion;Compliance programs should include company-wide and risk-specific controls to guide the development and use of AI systems, ensure proper oversight, and put into place strong policies, procedures, worker training, and contingency plans.;No;"PLD contributes to a reasonable balance between protecting those who suffer injury and ensuring fair competition. Where it does not address AI challenges, guidance should be developed to clarify issues under the Directive (e.g. placing on the market). If guidance is not enough, a specific liability regime for AI should be considered, sitting next to the PLD rather than amending the PLD; such AI liability regime should come with clear definitions of AI and consider specific intended use of the AI";No;;A pan-European framework would be preferable to different national liability rules, potentially overlapping, given the global AI nature. An EU-wide framework would also prevent ‘local AI safe havens’. For similar reasons, it is also important to find common ground internationally and not only within Europe.;
F530174;12-06-2020 20:01;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;"Working with EU global champions in specific sectors. It will be essential to foster competition in the sector within the EU (allowing the start and development of efficient SMEs), but at the same time to allow the growth of EU big players which can compete among themselves and with other American and Asian global champions. The EU has experience in striking the right balance.
Points G and H in the paper are also important, but are not included in this question.
";4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;4 - Important;Definition of national agencies in charge of the implementation of the Coordinated Plan. It is unclear which national agencies (if existing or prospective) will be coordinating the plan with the EU Commission. Apart from organizational competences, they should have the necessary technical skills on AI-related matters. They may be Intellectual Property agencies (familiar with new algorithms), economic sections of competition or sector-specific regulators, ad hoc or newly created agencies.;5 - Very important;5 - Very important;5 - Very important;"1) Among the sectors of specialization, one of them should be sports, and in particular football, where Europe has competitive advantage over the US due to social support, the granularity of the analysis and the technologies used.
2) Gather best practices and learn from each other. It will be essential to share knowledge, to carry out peer-to-peer learning activities, in order to allocate resources efficiently and replicate success stories.
";3 - Neutral;4 - Important;5 - Very important;4 - Important;3 - Neutral;We suggest to create specialized sections in the hubs, one of which could be sports, and concretely football. In this regard, many European football clubs should be treated as and receive the benefits of SMEs described in the White Paper.;4 - Important;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;3 - Neutral;"Yes. Acknowledging the great importance of these risks, we raise 2 main concerns: (i) AI is not the one taking actions or decisions; it is ultimately humans, based on AI input; even robots are created, enabled and supervised by humans; (ii) the risks stated above should not hinder the development of AI algorithms and programs which can be extremely useful for economic and social development. Regulation should not be too restrictive, but always proportional (minimum needed to protect rights)";Current legislation may have some gaps;;Yes;;Other;"Partially agree. We think that, as a general rule, the sector of operation (healthcare, transport, energy, etc) should not be the first and one of the 2 cumulative deciding factors. Regulation should rather focus on the nature of the AI application and the specific risks and uses involved (see examples below). See also definitions of ""high-risk"" stated in the draft documents by the European Parliament on civil liability and ethics related to AI.";Algorithms for criminal detection or job selection processes are included in the paper and are valid examples. Along the same lines, the likelihood of non-payment used in the banking sector.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);"Biometric identification could be used in a ""proactive"" way in big events attendance, provided all the safeguards are met. It is already being used for entering football stadiums and other venues, for example. Some variations could be introduced in the context of the ongoing COVID-19 pandemic, such as temperature control. Another important use (in these venues and other public spaces) is to control criminal activity and identify ofenders. It is worth repeating that the respect to data protection and other human rights is essential.";Rather not;"Rather not, as defined in the paper. Most requirements in 5.D (quality of training data sets, keeping of records, robustness and accuracy, etc) should be met by all AI applications. Besides, there is not a radical Yes/No conformity, but different degrees of compliance. On the other hand, other requirements in 5.D (specific human oversight, clear liability and safety rules) should be certainly met by ""high-risk"" applications. In these cases, voluntary labelling for non-high-risk is a good option.";A combination of ex-ante compliance and ex-post enforcement mechanisms;;It will not be easy to put in place an appropriate combination of ex-ante and ex-post enforcement mechanisms. It will have to be sequential, and with an adequate trade-off. One of the main obstacles are the resources needed to implement ex-ante controls: for testing and certification, national centres should be equipped with infraestructure and skilled staff. ;Personal security risks;We think more certainty should be provided on the definition of risks related to discrimination and bias.;Yes;"We think it is essential to keep and explain the ""human oversight"" as the main criteria for evaluation. Products stemming from artificial intelligence applications should always be controlled by humans, in all relevant stages of the process. Humans should be responsible and made accountable. ";Yes;"The first efforts have been included in the draft report on Artificial Intelligence and civil responsibility (2020/2014(INL)) by the European Parliament. Some definitions need to be harmonized (such as ""harm"" or ""high-risk"") between that proposal, another proposal on AI and ethics, and this White Paper, in order to set up a coherent framework. ";No opinion;;We are not experts on national liability rules, apart from our own country´s. In that regard, harmonization efforts are welcome.;Additional_comments_White_Paper_on_AI.pdf
F530173;12-06-2020 19:39;English;Company/Business organisation;Pedro;Simoes;;Visa Europe;61954192201-58;Large (250 or more);United Kingdom;The feedback can be published with your personal information;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;3 - Neutral;The Commission should focus on policies which drive diversity and inclusion - as part of skills and literacy, and as part of research efforts to tackle bias in AI (please see Section 1 of Visa’s submission);4 - Important;4 - Important;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;With its world-leading privacy and security regimes, and its proposals for a Single Market for Data, Europe is a potentially rich environment for the development and use of AI. Visa is optimistic about the opportunity for AI in payments to power economic growth and positive societal transformation. A co-ordinated plan at European level will be essential.;5 - Very important;5 - Very important;5 - Very important;EU-level action (including funding programmes, R&D, skills, and development of diverse local talent) through a Coordinated Plan will leverage the potential of combined action to drive investment, innovation, and regional competitiveness. Public private partnerships and co-funding are critical to support synergies in the above areas. Facilitating a data sharing ecosystem will significantly enhance Europe’s potential to lead in AI. Research centres should be coordinated with, or include, regulator;4 - Important;4 - Important;4 - Important;5 - Very important;3 - Neutral;;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;The Commission has identified the key areas where potential harm might arise. We also agree that there may be harms from unduly limiting use of AI, where this could have positive benefits. Transparency is also important to build trust (to the extent that it demystifies rather than increases complexity. Particular focus should be placed on avoiding biased and discriminatory outcomes. (please see Section 2 of Visa’s submission);Other;Applying or adapting existing legislation is the most efficient route to effective outcomes in most cases, and likely to be sufficient to address many potential harms. A holistic review of sectoral regulation should be undertaken for this purpose, as a pre-requisite to introducing any new rules. Collaboration is critical (across member states, among sector regulators, and with industry) to ensure consistency and coherency of any new framework. (please see Section 5 of Visa’s submission);Yes;;Other;"Visa agrees risk should be a critical delineator and that high-risk applications merit greater oversight. We believe the criteria proposed are useful but do not go far enough in properly defining and assessing risk for the purposes of this categorisation. ‘Likelihood of harm’ should be added as a criteria; this, and all legal terms and definitions (such as ‘significant harm’) must be clearly defined. (please see Section 7 of Visa’s submission) ";;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;No opinion;;Much;Visa is optimistic about the potential of a voluntary certification and labelling scheme for low-risk AI applications. Applied appropriately, this could help drive best practice, enhance consumer trust in new products, and familiarize organisations with the principles and processes required to build trustworthy AI. However, voluntary schemes should not be subject to the same stringent legal requirements designed to mitigate the risks of high-risk AI. (please see Section 9 of Visa’s submission);Other enforcement system;Visa believes there are limitations to the suitability of process-based, ex-ante regulation. Moreover, we believe it is important to ensure that the proposed process of prior conformity assessment is designed in a way which is practical and effective for both regulators and industry participants. We would strongly recommend that any suggested framework be closely aligned with internationally recognised standards and avoid duplication of existing self-assessment frameworks. (please see Section 8);Please see Section 8 of Visa’s submission;Mental health risks;;No opinion;;No opinion;;No opinion;;;Visa_submission_to_AI_White_paper_Consultation_12062020.pdf
F530172;12-06-2020 19:25;English;Other;Alicia;Waterkeyn;;Avicenna Alliance (AISBL);551058626331-14;Micro (< 10 employees);Belgium;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;"Provide access to high-quality+representative datasets for health sector
Reduce costs health sector to acquire and build generalisable AI trainable datasets
Develop technology to reduce complexity for AI applications
Promote the sharing of latest technological developments with open source communities
Integrate AI in existing regulatory processes
Educate HCPs as key actors in the AI value chain
Enable and prioritize PPPs to test new AI technologies and their fitting in healthcare system";5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;"Increase efforts to study (and counteract) cyber-attacks aimed at AI systems.
Consider a harmonisation effort with other Agencies (e.g. FDA, NMPA (National Medical Product Administration, formerly known as China FDA). 
Use the best practises and regulations developed by other regulators and learn from what they have already done.  
Strengthen the excellence in research and increase competitive market.
Develop new AI knowledge and methods as part of research";5 - Very important;5 - Very important;5 - Very important;"Increase the synergies that exist between AI, IoT, and Virtual/Augmented Reality and Computing Simulations (in-silico clinical trials) as well as digital twins.
Besides basic work on ‘pure AI’, measures are needed to ensure embedding AI within the currently available toolbox and to avoid creating silos. 
Ensure that AI can be used to improve existing approaches where possible rather than starting from scratch. Interactions need to be considered when establishing networks and partnerships.";5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;"Help build local ecosystems and models of AI integration hospitals and their services
Set up competitions (ala Kaggle, Hackathons) to push the limits of knowledge
Ensure the adoption of AI technologies in healthcare systems and hospitals by building models of integration in healthcare organizations, their services, organizational arrangements and workflows.
Allow companies– like in the US STTR model project to use university research institutions to further develop their AI prototypes";3 - Neutral;4 - Important;5 - Very important;3 - Neutral;3 - Neutral;4 - Important;"Access to representative datasets to properly train AI in areas where key results and high standards of safety are essential
Risks of not using AI technologies in healthcare when it is proven to work better 
Big difference between the specific type of AI technology (closed vs self-learning). For the former, current regulations suffice.  For the latter, new regulations must be put in place
Differentiate the use as independent decision-making tool vs advisory tool (as in current regulation";Current legislation may have some gaps;;No;;;;Consider the context(s) of use, specificities of the application field and not only the risk associated with the AI system;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Very much;"The current AI HLEG guidelines are Generic and could become more relevant if they were specified by sector or even therapeutic areas
Many sections are redundant with existing horizontal or vertical legislation
Tension between areas (e.g. privacy and communication) need further guidance
Lack roles in AI value chain, incl. end-users
Should be dynamic (both questions and answers) as technology evolves
Ethical AI should also be achieved through adequate data access and benefit communications";Other enforcement system;In Europe the MDR requires a conformity assessment body prior going to the market. Therefore, the MDR/IDVR are the correct certifications schemes for health care industry.;Consider alignment with existing regulatory frameworks requiring classifications and risk assessments. Structure compliance provisions to contribute to greater legislative clarity and avoid inconsistencies, through the development of sector-specific guidance documents. If extensions are necessary, they should be sector-specific and take into account the context of use. Not get excessive regulation in this area, but rather sectoral guidance documents that are more flexible and easier to adjust;Mental health risks;If a new strict liability regime is developed, it will be essential to limit this strict liability to some extent, as traditional liability regimes usually do. For example, by limiting this to injuries caused by the specific risks of autonomy. This could be described as injuries due to bugs in the software, “an unpredictable self-learning behaviour, or defective data”.;Yes;"In the Medical Device field, change management is already part of the regulatory framework.
Risk Assessment procedures should depend on the context of use. Not all the medical AI applications are of highest risk. 
Self-learning AI algorithms are very different from existing regulated tools. Therefore, dealing with self-learning AI algorithms might require additional legislation/regulation. Risk systems or Risk classification need to be developed.";No;Any clarification of the rules of liability should be addressed through guidance documents. In any event, the burden of proof should not be affected. In terms of product liability, a concept generally linked to that of “placing on the market”. Defining “placing on the market” in the case of AI is tricky: a product already placed on the market may work differently after AI addition.Experience shows that guidance documents based on practical cases, i.e. Commission's Blue Guide are extremely useful;No;;"A possible solution to the complexity of allocating responsibility for damage caused by autonomous AI could be an obligatory insurance scheme
Insurance systems for AI products should consider all potential responsibilities in the chain, particularly where a healthcare professional or organization is owning/using the AI solution, deploying these systems for its benefit. These entities may be different than the original manufacturer. ";Final_Avicenna_Position_Paper_on_AI_and_Data__28.05.20.pdf
F530171;12-06-2020 19:24;English;Company/Business organisation;Mathias;van Malderghem Nagy;;Hangzhou Hikvision Digital Technology Co., Ltd.;181069237409-88;Large (250 or more);China;The feedback can be published with your personal information;4 - Important;4 - Important;5 - Very important;4 - Important;5 - Very important;4 - Important;Hikvision is the world’s leading provider of innovative security products, with its European headquarters in the Netherlands. We welcome the steps taken by the Commission to promote the uptake of AI in Europe. Hikvision, as a specialist in the market, believes that a further key action to consider is international cooperation in research on AI which is essential for the development of a trustworthy AI ecosystem. Our R&D teams operate globally. ;5 - Very important;3 - Neutral;5 - Very important;3 - Neutral;4 - Important;3 - Neutral;As a global company, Hikvision believes that a further key area to be considered is international cooperation in research could lead to unprecedented advancements in AI. The EU is already a leader in international scientific cooperation and could consider how such cooperation in relation to AI with institutions and companies beyond the EU borders could help foster excellence in Europe itself. In addition, diversity of views and approach regarding AI would foster innovation.;No opinion;No opinion;4 - Important;;No opinion;No opinion;No opinion;No opinion;No opinion;;4 - Important;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;3 - Neutral;We understand the legitimate concerns towards certain AI use-cases, but we are concerned that AI technologies are increasingly misportrayed as a risk to privacy rights. A further concern is that AI innovation could be restricted without a legal framework that appropriately categorises what constitutes “high risk”, sets out an appropriate distribution of obligations and liability among the economic operators, and provides a harmonized approach to AI regulation across the EU.;There is a need for a new legislation;;Other;Hikvision very much supports the risk-based approach for potential compulsory requirements. It should address many concerns regarding AI and takes into account that a “one-size-fits-all” approach would be impractical. Hikvision supports the Commission’s intention to establish a legal framework and create clear criteria to differentiate between AI applications across the EU. This would enable companies to have clarity and certainty as they develop and deploy AI technologies.;;;Hikvision agrees that AI applications may be considered ‘high-risk’ when both the sector and the intended use involve significant risks e.g., large-scale surveillance using biometrics. However, Hikvision urges the Commission to ensure there is a clear categorization as to what constitutes “high risk” in order to not stifle innovation. It is also essential to distribute obligations and liability appropriately among the economic operators best placed to address any potential risks. ;4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;Other special requirements in addition to those mentioned in the question above should be imposed (please specify);Technological innovation is our driving force and we continuously develop novel technologies using AI, facial recognition, biometrics, big data and deep learning. As a specialist in commercial video-surveillance we fully appreciate the privacy and fundamental rights sensitivities and believe that individuals should not have to choose between safety and privacy. However, due to a certain lack of understanding, such innovative technologies are increasingly misportrayed as a risk to fundamental rights.We believe such innovative technologies solutions can be extremely positive and help stakeholders better manage the complexities and risks of modern society and in certain cases enhance the privacy of individuals – especially if appropriate safeguards, technological, human or legal are put in place to ensure the protection of citizens’ rights. Biometric technology is also capable of ensuring fast and reliable protected access to information. Hikvision believes that only certain uses of biometric identification systems, such as large-scale surveillance should therefore be considered as ‘high risk’ and we would welcome clarification on common safeguards. Hikvision strongly supports the creation of a specific framework and guidance on the use of the technology in order to unlock the growth potential of the European Single Market in this sector. Hikvision considers that a moratorium or ban of these technologies would be detrimental to the European Single Market, notably relatively to other global markets. ;Very much;Hikvision strongly supports the creation of a voluntary labelling system (VLS) to ensure that other economic actors in the AI supply chain have a practical way of continuing to create innovative AI technologies, whilst adhering to a set of clear voluntary requirements that recognise compliance with EU-wide standards  Hikvision believes that this would increase the uptake of the technology. The VLS should be designed so that it is clear to consumers what the VLS represents.;No opinion;;;Personal security risks;;Yes;We suggest that cybersecurity risks should be considered as part of the risk assessment procedures in relation to all economic actors in the AI supply chain, including end-users. Cybersecurity obligations should be considered in relation to the design of AI but all by the end-user in terms of how they apply the product in practice.   ;Yes;"In relation to defects in AI-enabled products that result in injury to individuals, adjustments to the Directive could be made to consider: (i) whether the manufacturer could have foreseen changes to the use of the product by other economic actors in the AI supply chain which may lead to injury to individuals; (ii) the manufacturer’s knowledge at the time of the product circulation; and (iii) contributory negligence factors (e.g., where the end-user did not perform relevant safety updates). ";No opinion;;;
F530170;12-06-2020 19:14;English;Academic/Research Institution;Jenny;Brennan;;Ada Lovelace Institute;;Small (< 50 employees);United Kingdom;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;3 - Neutral;AI for the sake of AI is meaningless, we need to have clear, publicly debated, purposes for its use and robust governance around it. In particular in the public sector there needs to be oversight, means for effective redress and requirements of public procurement of AI. Elsewhere, moratoriums should be considered for uses that most threaten fundamental rights and societal values and a regularly reviewed list of prohibited uses of AI. Please see our attachment for more detail.;5 - Very important;5 - Very important;3 - Neutral;No opinion;4 - Important;No opinion;Coordination should aim for effective enforcement of existing legislation. Terms like ‘data spaces’ or ‘data pools’ must be clearly defined - do they mean shared access datasets, or more data portability between actors? Aim to incentivise research and systems that use less, higher quality data over undiscriminated mass data collection. Consider aligning policies on procurement processes for public sector AI to ensure positive use and prevent imbalances of power.;5 - Very important;5 - Very important;No opinion;There needs to be public deliberation and a public interest agenda for deciding the research and innovation priorities. Ethical and human rights assessments and social implications should be considered. Analysis and assessments should be public. Outcomes from public research funding should be openly licensed and available for re-use. Further research and development on privacy preserving AI should be explored to enable safe and responsible use and access to data.;No opinion;No opinion;No opinion;No opinion;No opinion;SMEs need to be subject to the same regulatory oversight as any other companies with respect to AI. With AI technologies, small teams can be responsible for systems that have a vast impact on society, individuals and their rights.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;AI lacks means to redress for decisions made or informed by it, or the impact it may have on people and society. In addition to risks to rights and individuals, there are risks of collective harms. Practice around independent and regulatory auditing (and publishing thereof) is still in early development. We are seeing increased reliance on private infrastructure for public services as they increasingly adopt AI, making indiscriminate calls to further adoption in the public sector premature.;Other;A combination of these approaches should be taken, including area/sectoral laws to strengthen the GDPR, developing an advanced legal framework to protect against blanket data-enabled (automated) decision making, especially in the public sector and new safeguards and legal guarantees against re-identification of individuals or collective harms from abusing aggregate datasets. There is also scope for more development around means for effective redress.;No;;;;Remote biometric identification poses such risk to fundamental rights that we need a moratorium on its use to enable further evidence building and deliberation. Other uses of biometric data, autonomous weapons, predictive policing, AI applications hampering access to justice or fair trials and systems which affect access to opportunities (for employment, education, public services or benefits) are also highly concerning. Moratoriums, with the potential for prohibition, must be a consideration.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Other special requirements in addition to those mentioned in the question above should be imposed (please specify);There should be a moratorium on biometric identification systems to enable evidence gathering and deliberation to establish if there may be any justifiable use and, if not, these must be prohibited. This moratorium should apply to all remote biometric identification, not only in public spaces, due to the increasing blur between public and private spaces, and the same risk of intrusion and rights infringement in private contexts. Please see our attachment for more detail.;Not at all;We highly discourage systems that avoid accountability, oversight and rules compliance since there is a high degree of uncertainty regarding the consequences (individual and societal) and we need tools for intervention regardless of the AI application. The meaning of risk can shift through time. We suggest to begin by holding all AI systems to at least the same requirements laid out for ‘high-risk’ systems. Please see our attachment for more detail.;Other enforcement system;Consider the role of more holistic impact assessments, such as human rights impact assessments, throughout the lifecycle of an AI system and ongoing. Develop approaches for independent and regulatory audit. Encourage the transparent publication of impact assessment and audit. Make sure system monitoring throughout the supply chain is in place and allow independent evaluation.;Support research and development of skills, approaches and methods to assessing compliance. Audit of AI, in the sense of regulatory inspection, is still very emergent. WE at the Ada Lovelace Institute are developing further work on this following our report with DataKind UK, Examining the Black Box: https://www.adalovelaceinstitute.org/examining-the-black-box-tools-for-assessing-algorithmic-systems/;Mental health risks;"Risks of collective harms that go beyond individual rights. 

There also needs to be clarity that risk is contextual, and questions about who and how risk is defined. The framing of risk as binary ‘high’ or ‘low’ risk is limiting, and fails to account for the potential for moratoriums or prohibitions on some AI applications. Please see our attachment for more detail.";Yes;Consider human rights impact assessments and other developing frameworks for impact risk assessment in AI. We surveyed literature on these approaches, and further research and development needed in Examining the Black Box: https://www.adalovelaceinstitute.org/examining-the-black-box-tools-for-assessing-algorithmic-systems/;Yes;;Yes, for all AI applications;;;European_Commission_AI_White_Paper_Response_-_Ada_Lovelace_Institute.pdf
F530169;12-06-2020 19:11;English;Company/Business organisation;Georgia;Cassarino;;GSK;7990322925-77;Large (250 or more);United Kingdom;The feedback can be published with your personal information;3 - Neutral;4 - Important;4 - Important;3 - Neutral;5 - Very important;5 - Very important;Any EU initiative in the AI space should ensure that the EU IP framework provides the right incentives for innovation. Current patent, copyright and other IP provisions in the EU do not permit an AI (machine) to be named as an inventor/author (see, for example, the DABUS case pending at the European Patent Office). We note that WIPO is carrying out a consultation into ‘AI and IP’ and would encourage the EU to consider the inputs and findings as it deliberates on an AI strategy. ;4 - Important;5 - Very important;3 - Neutral;4 - Important;3 - Neutral;4 - Important;;2 - Not important;5 - Very important;4 - Important;;3 - Neutral;5 - Very important;3 - Neutral;4 - Important;3 - Neutral;;4 - Important;4 - Important;5 - Very important;3 - Neutral;3 - Neutral;4 - Important;;Other;The current privacy framework is sufficient. There is a risk that new rules for AI systems may duplicate existing regulations (e.g. GDPR, MDR). There might be a need to conduct an analysis for gaps or further actions.;Yes;;Other;Specific regulations for AI in healthcare, possibly implemented through the Medical Devices Regulation, are needed instead of free-standing piece of legislation on AI. This would avoid difficulties arising where sector-specific principles are included in a regulation of general applicability. MDR framework covers AI settings with a well-established risk classification framework addressing the degree of autonomy accorded to the AI software. Unnecessary proposal for uses considered medical device.;Employment screening /aptitude testing;5 - Very important;4 - Important;4 - Important;5 - Very important;3 - Neutral;4 - Important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);In more details, we would encourage the evaluation of risk associated with downstream uses, which might need regulation.;No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Risks related to the loss of connectivity;;Yes;;No opinion; Proposals to amend the current liability framework and create a new AI-specific framework should be considered only if the current framework is proven inadequate to address demonstrable risks. Small changes may be appropriate, such as amending the EU Product Liability Directive to explicitly provide that it includes AI/Computer Software. Material changes such as departing from a strict liability or fault- based regime or reversing the burden of proof are not appropriate. ;Yes, for specific AI applications;;As above (on EU legislative framework);
F530168;12-06-2020 19:08;English;Academic/Research Institution;Nigel;Cory;;The Information Technology and Innovation Foundation (ITIF).;923915716105-08;Small (< 50 employees);United States;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;;5 - Very important;3 - Neutral;5 - Very important;4 - Important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;4 - Important;;5 - Very important;3 - Neutral;4 - Important;4 - Important;5 - Very important;;1 - Not important at all;1 - Not important at all;4 - Important;1 - Not important at all;2 - Not important;4 - Important;The risks of AI may well be overinflated, while AI may provide benefits not fully recognized by the public. ;Current legislation may have some gaps;;Other;;;;;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);"The Commission should consider new regulations in high-risk scenarios where there is clear evidence of consumer harm. Further guidelines would help in clarifying the possibility for a company or a public organization to use and deploy this technology. Legal uncertainty currently stands in the way of businesses being able to collaborate with municipalities, schools, or airports, where uses of facial recognition can be beneficial. Most common facial-recognition applications are benign, and there are countless examples of how they help reduce financial fraud or prevent medical errors. The biggest area of controversy is law enforcement, but current protections under EU laws put limits on the misuse of these technologies by governments. And any operator of such systems must comply with privacy laws. What is required is more oversight and transparency to ensure accountability and a safe, responsible use by public and private organizations, not fewer resources or more regulations to limit the use and effectiveness of these technologies.
";Rather not;The Commission should clarify the implications for stakeholders who may choose not to endorse voluntary labeling, such as the risk that it may penalize companies who do not adhere to it.;Other enforcement system;The Commission should encourage the continued development and testing of voluntary industry best practices. Adding a new layer of rules on the current fragmented landscape of the Single Market will further impede the ability of EU and non-EU businesses to innovate and commercialize their systems, and deter them from investing in AI in the EU. Regulatory sandboxes is a better way to experiment with AI while developing systems that are compliant.;;Risks related to the loss of connectivity;;Yes;;No;Any legislative update should include clarification to avoid legal uncertainty and diverging interpretation across member states, but should not add more to the existing bureaucratic and compliance burden that impacts companies. Given the evolving nature and diversity of AI systems, rules should remain flexible. The review of the existing body of EU laws should involve an assessment of the impacts these laws have on AI development in the EU.;No;;;20200612_ITIF_-_Nigel_Cory_-_EC_Submission_-_AI_and_Conformity_Assessments.pdf
F530167;12-06-2020 19:05;English;Public authority;Marta;Xirinachs;Regional;Directorate-General for Language Policy of the Government of Catalonia;;Medium (< 250 employees);Spain;The feedback can be published with your personal information;4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;5 - Very important;Any AI project developed by the públic sector must guarantee the language rights of citizens. Therefore, the languages that are official within Member States must also be taken into account. ;4 - Important;4 - Important;5 - Very important;4 - Important;4 - Important;5 - Very important;The role and responsibility of the public sector in developing models and projects is of utmost importance. These must guarantee the language rights of citizens and must also be made available to the private sector for further development or market implementation , which must take into account the languages and language rights of citizens. Languages such as Catalan, Galician or Basque must be taken into account, for exemple.;3 - Neutral;5 - Very important;4 - Important;;4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;There is a strong presence of SMEs within the business fabric of Catalonia. They are the ones that are more closely related to the territor, show a higher language awareness closer to the citizens and , therefore, have a higher number of multilingual tool &services respectful to the language rights of citizens.;4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;If AI does not take into account all the languages of citizens, it can be very harmful, not only in terms of language rights but also because AI may cause more harm due to language problems. ;;;;;;;Either with the need to omplement existint legislation or the need to develop new one, the language rights of citizens must be guaranteed, especially the rights that are fully operative within their own territories. ;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;;;;;;;;;;;;;;;;;
F530166;12-06-2020 18:51;French;Trade Union;Anissa;Kemiche;;Syntec Numérique;565078326484-60;Small (< 50 employees);France;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Un écosystème d’excellence en matière d’IA suppose une harmonisation des règles entre les Etats et une plus forte coopération entre public et privé, notamment entre les établissements de recherche et les entreprises de toutes tailles. La création d’une économie de la donnée efficiente est un prérequis, tout comme une attention particulière aux enjeux de cybersécurité et d’éthique permettant la confiance des utilisateurs. La formation est également un enjeu clé.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;La question de l’IA ne peut être traitée indépendamment de celle des données. Le besoin d’un environnement uniformisé en Europe est réel. Il est important pour l’Europe d’avoir une stratégie sur les données qui soit à la hauteur de l’enjeu de l’économie de la donnée. Pour cela, il faudrait supprimer les contraintes pour l’économie de la donnée. Enfin, il est également important d’associer un panel représentatif de l’écosystème aux travaux d'autorégulation.;5 - Very important;5 - Very important;5 - Very important;Les technologies de l’IA mobilisent des instituts de recherche, des universités, des entreprises de toutes tailles, qui travaillent en écosystème pour explorer les applications possibles pour la science et l’industrie. Le développement d’un réseau des centres d’excellence existants permettra une meilleure coordination, et pourra être complété par la création un centre de recherche phare. La formation est un enjeu central pour ces projets, comme l’investissement des Etats et de l'UE.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Les pôles d’innovation numérique spécialisés pourraient être orientés vers différents domaines prioritaires, comme la santé, l’environnement, etc. Les pôles d’innovation devraient par ailleurs avoir un rôle d’animateurs de l’écosystème, offrant davantage de visibilité aux startups et aux PME, et permettant d’accompagner les écosystèmes d’open innovation réunissant des universités, instituts de recherche et entreprises de toutes tailles.;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;Certaines de ces préoccupations ne sont pas propres à l’IA, et s’appliquent déjà à d’autres technologies. Pour garantir une IA de confiance, tous nos efforts visent à atténuer les nouveaux risques liés à l'IA, que cela soit en matière de sécurité, de responsabilité, d’éthique ou d’explicabilité des algorithmes. ;Current legislation may have some gaps;;Yes;;Other;L’évaluation du haut risque peut différer selon les secteurs ou les applications (transports, énergie, santé..). Des critères devraient être déterminés, éventuellement au cas par cas. Toute évaluation des risques doit tenir compte du contexte. Conformément à l’objectif du Conseil de concentrer les mandats réglementaires sur les scénarios à risque élevé, il est nécessaire de s’assurer que la définition de l’IA n’est pas large au point de balayer des milliers de de produits et services.;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No further guidelines or regulations are needed;;No opinion;Un tel label volontaire pourrait permettre de donner des orientations utiles aux développeurs et permettre d’identifier des acteurs moins visibles. Ce label devrait être adapté aux différents secteurs ou applications, et son application en serait d’autant plus complexe. Il serait par ailleurs nécessaire qu’un tel label volontaire ne puisse pas être exigé dans le cadre de la commande publique ou privée, ni retenir des critères excluant une partie des acteurs de l’IA. ;A combination of ex-ante compliance and ex-post enforcement mechanisms;;L’évaluation préalable de la conformité des applications à haut risque avec les exigences identifiées (avant de mettre le système sur le marché), et les procédures d’autoévaluation devraient être encouragées. Des échanges réguliers entre les industriels et les organismes d’évaluation de la conformité devraient être mis en place tout au long du processus de création de l’IA. ;;;No;Il convient d’encourager les utilisateurs d’IA à mettre en place des procédures d’autoévaluation régulières au cours de la vie des produits, éventuellement avec le concours des développeurs qui ont contribué à leur création. Il convient de tenir compte des textes réglementaires existants qui couvrent dans leur champ d'application les logiciels avec application d'IA et d’assurer leur cohérence. ;No;La législation européenne actuelle sur la sécurité des produits soutient déjà un concept élargi de sécurité protégeant contre tous les types de risques découlant du produit en fonction de son utilisation.Il conviendrait par conséquent de s’appuyer sur la législation existante pour élaborer un cadre qui incite les développeurs d’IA à établir une structure de gouvernance par des pratiques vérifiables fondées sur le risque sans imposer d’exigence spécifique (dans le respect des droits fondamentaux);No;;"Cette réponse pourrait différer selon les Etats ; il est nécessaire de penser les règles en matière de responsabilité à l’échelle européenne.";
F530165;12-06-2020 18:49;English;Company/Business organisation;Daniela;Braga;;DefinedCrowd;;Large (250 or more);Portugal;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;;5 - Very important;3 - Neutral;5 - Very important;4 - Important;5 - Very important;4 - Important;The relevance of high-quality, unbiased training data;5 - Very important;5 - Very important;5 - Very important;International knowledge exchange and collaboration;3 - Neutral;3 - Neutral;4 - Important;5 - Very important;3 - Neutral;;4 - Important;5 - Very important;4 - Important;3 - Neutral;2 - Not important;4 - Important;;Current legislation may have some gaps;;Yes;;Yes;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);Biometric identification systems should only be used in public spaces if the user has authorised it - this implies the user fully understands the application and usage of their data. Additionally, the data collected and used for a determined space and system should only be used for that particular setup - it's very important to ensure the data isn't used for other purposes, locations, or systems.;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;It should be assessed periodically.;No opinion;;No;;;
F530164;12-06-2020 18:46;English;Company/Business organisation;Jonathan;Kewley;;Clifford Chance LLP;;Large (250 or more);United Kingdom;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;Clifford_Chance_response_to_EU_AI_consultation_-_compiled.pdf
F530163;12-06-2020 18:46;English;Public authority;Mark;Dugdale;National;Government of Ireland;;Large (250 or more);Ireland;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Focus on SMEs is important as start-ups are often a useful means of bringing innovation developed in academia to the market.  However, it is not the only way as some large firms prefer to rely on their own R&D sections.  Given their established status it is suggested that they provide a more stable source of investment and are unlikely to seek take-over, possibly by a third country firm, to see a return on their own investment, which is accepted as a goal for many small developers.;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;In light of the development of an ecosystem of trust a well developed set of regulatory testbeds would seem advisable to understand the implications of the deployment of novel applications of technology.;5 - Very important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;"Providing access to legal advice concerning applicable and relevant existing regulation; the use and availability of regulatory testbeds; product safety and liability; and, other issues arising from the establishment of an ecosystem of trust together with associated technical advice such as that relating to the creation of trustworthy AI especially use of the HLEG’s assessment list.";5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;"The need to regulate AI tends to be based on the belief it is dangerous.  It is not; it is neutral.  People’s use and care in maintenance/curation of AI can possibly cause danger and this determines whether it falls into a high-risk category.  AI is an innovative field evolving issues will not be captured in a set of ex ante rules.  An oversight system should rely on more than regulation to deal with such issues.  The HLEG assessment list and standardisation can help provide this approach.";Other;The Commission has previously undertaken to collate the current acquis that applies to AI.  Until that is done it is not possible to answer this question definitively.  It is likely there may be some gaps especially re causation and liability in instances of unforeseeable outcomes brought about by Machine Learning and, maybe, unforeseen usage for unintended purposes.  It is felt that a pragmatic framework for a set of flexible, responsive interventions at appropriate levels should be developed.;Yes;;No;;;3 - Neutral;3 - Neutral;3 - Neutral;4 - Important;4 - Important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;No general monitoring of the population should be permitted except in very specifically classified situations of national emergency.  Specific monitoring in local areas for the apprehension of particular individuals suspected of predefined serious crime. ;Rather not;It is not clear what this offers over adopting an international standard. ;Other enforcement system;It is considered that a hybrid ex ante and ex post assessment system (to take into account evolving effects in use) together with an element of ex-ante compliance and ex-post enforcement for particular high-risk uses is optimal. ;;Mental health risks;Risks that effect loss of autonomy.;Yes;;No opinion;;No opinion;;;Irish_National_Submission_in_Response_to_AI_White_Paper_consultation.pdf
F530162;12-06-2020 18:38;English;Company/Business organisation;Jochen;MISTIAEN;;DIGITALEUROPE;64270747023-20;Small (< 50 employees);Belgium;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Global cooperation to ensure Europe benefits from trusted AI developed overseas but also that AI developed in Europe can cross borders without diverse obligations burdening SMEs. Lowering entry barriers to SMEs, through increased data availability and quality, as this is critical to training and design of AI. Training in cybersecurity as well as overall AI skills (incl. collaborative, soft & ethic skills) to improve broader societal understanding, embrace opportunities & address concerns.;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;All areas can be seen as equally important, but some should be prioritized in the short term.Beyond common infrastructures, the build-up of the European data space requires a number of actions around the availability and quality of data (incl. curating and evaluating against risk of biases). Genuine willingness by Member States to contribute to this initiative will be paramount to its success and consequently to the AI uptake across Europe.;4 - Important;5 - Very important;5 - Very important;A coordinated network among existing AI research excellence centres should be prioritised over creating a new one. This network needs to: create a leadership structure to ensure coordination and coherent operation, agree on a vision regarding the focus and priorities beyond national borders and provide continuous financial investment at the necessary level. AI funding be prominent in Horizon Europe, for core AI research plus AI components in research projects and relevant open source projects.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;SMEs & start-ups need to be supported in developing, accessing and using AI. Their variety and divergence in terms of digital literacy, sector of activity and size create different needs. Digital Innovation Hubs should provide point of contacts as well as services and tangible support in SMEs’ transformation, incl. helping assess what technologies to adopt & advising how to implement them. Legal certainty and simple rules are key, within a proportionate principles-based regulatory framework.;4 - Important;4 - Important;4 - Important;3 - Neutral;2 - Not important;2 - Not important;All concerns must be weighed against possibilities of AI for improvement, plus be linked with type of risk and context. Explainability and Accuracy’s importance vary greatly per sector and use case (e.g. healthcare), and can improve strongly with research. Data/model quality issues can be detected & addressed. A strong liability framework should be risk-based and clearly allocate responsibility among the AI chain operators. ;Current legislation may have some gaps;;Yes;;Other;We agree with the general approach but clear application rules are key. “Use” should weigh more heavily in the assessment and more granularity is needed within sectors, while consulting relevant stakeholders. Other criteria may also be considered (e.g. likelihood, level of oversight). Existing definitions of “harm” & “risk” that may differ in various sectors must be taken into account. Similarly, existing (sector) risk assessments may need to be reviewed and adapted for AI use cases.;;4 - Important;4 - Important;3 - Neutral;4 - Important;3 - Neutral;4 - Important;No opinion;;No opinion;A voluntary labelling system for AI systems that are not considered high-risk could have a complementary role in the future. But the development of such a system would need to have a multi-actor governance model and overcome important challenges including how a genuinely voluntary character will be ensured (e.g. in public procurement), how to apply it in a B2B context, what systems will be considered non high-risk, what the label would include and how to enforce it and prevent misuse. ;Other enforcement system;Extensive exchanges with operators across the AI chain will be necessary to develop compliance & enforcement mechanisms, including self-assessment, to achieve the stated purposes. A combination of ex-ante and ex-post may be purposeful as long as ex-ante mechanisms are limited to self-assessment. It will be important to consider how these mechanisms would apply to high-risk applications that are already regulated in this respect. ;Re-training algorithms in a specific location will not necessarily guarantee higher quality and a different output. Relying solely on European trained algorithms and European data sets could also cause challenges with regards to the diversity of datasets. Disclosure of confidential information (incl. algorithms & data sets) should be avoided. We need a global focus to ensure a diverse and fair user experience and avoid burdensome requirements for companies serving markets across the world. ;;All listed risks merit legal certainty through either guidance on existing rules or new rules if necessary. It is impossible to answer this question with such wide scope, specifically as regards the terms “use” of “artificial intelligence”. The listed risks are further not limited to or inherently related to AI. The European Commission should consider whether and how it may need to address these risks for specified uses of specific AI systems with existing EU product safety law.;No;No need for new risk assessment obligations for products ‘subject to important changes during their lifetime’. The used terms are vague and it’d be technically difficult to conduct risk assessments on products in users’ possession which may have evolved differently due to self-learning. Existing NLF procedures prior to placing products on the market could be adapted with new standards foreseeing changes over time. This should exclude products in immature stages (e.g. testing, research).;No opinion;The ultimate goal is to achieve responsible and safe AI & maintain equivalent liability protection as for other products. The envisaged broader regulatory changes (e.g. data quality, transparency, safety) diminish the need for new liability rules. Additional obligations reduce incentives to develop responsible and safe AI. Given the ongoing ethical/safety review, absence or scarcity of empirical data on actual AI liability risks, there is currently no need to change the tech-neutral PLD.     ;No;;Striking the right balance across Europe between the respective needs for responsible AI, safety requirements and liability protection will contribute considerably to AI development by start-ups, to consumer trust and protection and to legal clarity, thus to the overall uptake of AI in Europe. Further, no liability should accrue to any producer, manufacturer or component manufacturer (nor here, AI producers) when there is an intervening malicious actor.  ;DE_comments_on_AI_White_Paper_120620.pdf
F530161;12-06-2020 18:37;English;;;;;;;;;The feedback can be published in an anonymous way;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;bevh_Position_Paper_AI_White_Paper.pdf
F530160;12-06-2020 18:36;English;Business Association;Chiara;"DELL&apos;ORO";;European Association of Co-operative Banks (EACB);4172526951-19;Small (< 50 employees);Belgium;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;We agree on the importance of each of the actions. Europe does not lack AI expertise, but it is important to retain talent in Europe through remuneration and attractive projects. Concerning skills, it is important to capitalize on the excellent education that exist in Europe and, for the financial sector, to attract staff with technical and business expertise. The public sector should be encouraged to adopt AI to improve the efficiency of the administration but with respect for individual rights;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;;4 - Important;4 - Important;5 - Very important;;5 - Very important;4 - Important;5 - Very important;5 - Very important;4 - Important;;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;As recalled in the White Paper, AI can relate to a wide variety of possible risks. In our perception, these risks are more often directly related to the service as such than to AI itself. Overall, we would advocate an open multidisciplinary discussion the different topics.;Other;No additional regulations for AI is needed. This would cause overlap with (i) general regulations (e.g. GDPR), (ii) sector-specific regulations (e.g. for Operational Risk Management in financial services, safety regulations other industries) (iii) technology-specific regulations. Applicable EU legislation already addresses risks with especially GDPR and anti-discrimination in place. Additional regulation would cause silos, but no better management of technological risks.;Other;We do not recommend new rules (see Q 6). For “high-risk” industries, rules and regulations already exist: from oil-drilling/nuclear power (with general risks for the environment) to pharmaceutical products/automotive industry (with risks to individual citizens). For other industries such as financial services, regulations exist for ITC and/or Operational Risk Management (which cover the whole scope of ITC and processes) and for consumer protection (which protects consumer rights in principle);;;We do not recommend new rules, as existing regulation is sufficient (see Q 7). For real “high-risk” industries, rules and regulations exist. For any other industry, which is not “high risk” per se, we are in favor of a risk-based approach concerning products or services based on a level playing field and proportionality. As AI is a question of international competition especially with China and US, COM should remain vigilant to ensure that European players are not more regulated than others.;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;No further guidelines or regulations are needed;The issue calls for political choices and deserves a lucid and in-depth debate. We support the gradual approach proposed by Thierry Breton, who wishes to give himself a few months to study, anticipate and segment the issue properly. For the time being, no further guidelines or regulations are needed from our perspective. As an example for the complexity, we want to point out that biometrical identification (including face recognition!) is already tested for payment authorisation around the world! Therefore, it depends on the definition of a “public space”: Is a shop / a café on an open plaza or a pizza service at a door a public space?;Rather not;Based on the experience with existing labelling systems - from food with nutrition factors to PRIIPs in financial services, we want to point to the complexity and time consuming process to derive labels for PRODUCTS. For a technology, it would be even harder to agree on what AI means, as there is nor a common definition of AI, neither is AI the core of typical risks. If a product is “high-risk” (e.g. due a high probability of a financial loss), this is independent of the technology used.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;We believe that the - traditional - combination of ex-ante compliance and ex-post enforcement mechanisms as defined in existing regulations would be sufficient.;;On the banking industry perspective, our services based on AI should not be submitted to a new horizontal legislative framework.;No opinion;Please see our comments in the enclosed file.;No opinion;;No opinion;;Concerning “national liability rules”, the national communities are asked to evaluate the current status and potential concerns (European principle of subsidiarity).;EACB_DUD_PP_EC_Consultation_Annex_Key_messages_AI.docx
F530159;12-06-2020 18:36;English;Consumer Organisation;Javier;Morales;;Fédération Internationale de l'Automobile (FIA) Region I;84839535366-67;Medium (< 250 employees);France;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;;4 - Important;4 - Important;4 - Important;;4 - Important;4 - Important;4 - Important;4 - Important;No opinion;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;Current legislation may have some gaps;;No;;;;FIA Region I welcomes in-vehicle technologies however, the application of AI for autonomous cars presents great uncertainties for drivers.  To trust this development, it is of utmost importance that manufacturers bear the liability for any flaws in the system over the lifetime of the vehicle. This could be partially addressed by carefully designing liability schemes in case of an accident or infringement to the highway code and by harmonizing the EU/International type approval requirements. ;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Risks related to the loss of connectivity;Changes made to the car through software updates.;Yes;Certification according to Common Criteria as a possibility to do it.;Yes;"Liability schemes in case of an accident or infringement to the highway code need to be carefully designed for each level of automation and clearly communicated to the users to ensure a smooth transition between full driver liability to full manufacturer and road operator liability.  
From a legislative perspective, is key making sure that traffic accident victims are quickly compensated regardless of the liable party and that timely legislation allows for technology deployment.";Yes, for all AI applications;;;2015-06-09_FIA_Region_I_Policy_Brief_on_automationFINAL3_updated_layout.pdf
F530158;12-06-2020 18:33;English;Company/Business organisation;mario;ROMAO;;Intel Corporation;7459401905-60;Large (250 or more);United States;The feedback can be published with your personal information;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;4 - Important;Considering the global scope of the business opportunities driven by AI solutions, the EU could evaluate the feasibility of supporting and promoting European AI industry solutions in addressing international markets.;2 - Not important;5 - Very important;5 - Very important;Supporting financially the work and growth of existing research centres, aligning on a strategy and building upon and developing new synergies should be considered, instead of dispersing resources in the creation of a new structure. ;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;4 - Important;4 - Important;3 - Neutral;3 - Neutral;Explainability, safety, discrimination and fundamental rights are all important, but the associated risk and relevance is highly dependent on the specific AI usage and context. E.g. there are fundamental differences between business-to-business and business-to-consumer contexts. There is no evidence that the current legislation is not able to continue to ensure the same level of consumer protection.  An AI solution does not need to be 100% accurate to work as intended. ;Other;Determining if existing EU legislation is unable to address the concerns above is critical to avoid new redundant or conflicting requirements. If gaps are identified, voluntary, non-regulatory actions should be preferred. In any case, regulatory and non-regulatory actions should aim at reducing unnecessary barriers to the deployment of AI in the EU while ensuring a high level of trust.;Yes;;Other;Risk -based compulsory requirements already exist in many sectors (e.g. healthcare, automotive). New obligations  should be limited to the extent not covered by existing legislation and be created within those existing sectoral frameworks. If uses of AI in sectors without conformity assessment are determined as presenting risk, then compulsory requirements should only be placed on high-risk uses. What constitutes high-risk should be precisely defined and be appropriate to each sector and use.;;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;No opinion;;Rather not;A voluntary labeling system for non-high-risk AI would de facto create blanket conformance requirements for all AI systems, effectively creating market barriers for smaller companies by “mandating” high entry costs. Regardless, any voluntary setup should reference international standards, labeling through self-assessment, clear definition of risk and that no additional liability is attendant to the use of the label. We therefore recommend limiting focus to high risk AI applications only.;Other enforcement system;We recommend a two-step approach. The oversight of high-risk AI systems in sectors with existing conformity controls would continue sector specific. Ex-ante self-assessment against agreed standards with ex-post market surveillance would apply to high-risk AI systems for which there is no sector specific legislation. The EC would monitor and evaluate the application of this framework to determine the need for any potential modifications  in the light of technological or market developments.;It is important that potential requirements such as those listed in 2.6 and any related conformity assessment be flexible and made against recognised global standards. ISO/IEC JTC1 SC42 AI has a new proposal for an AI management systems standard to enable an organization to show that it has implemented e.g. processes identifying and treating bias of learning data, or more generally fairness, inclusiveness, safety, security, privacy, accountability, explainability and transparency. ;;We agree with the EC that the current product safety legislation already supports an extended concept of safety protecting against all kinds of risks arising from the product according to its use. We also note that the risks listed are not intrinsic to AI and that safety design requirements based on international standards continue to evolve to include AI .;No;We believe the existing procedures are sufficient (some are simplified after updates, like PPE or vehicles) and that current international standards for risk assessment are evolving properly to address risks in the entire product lifecycle, from production to commercialization. If a product is modified substantially during its lifetime, it must be re-evaluated and if a third party modifies a product and this creates new risks, the third party should be responsible for the product’s safety. ;No;If the legislator decides to amend the Directive anyway, we recommend a legal approach that determines product liability based on the “intended use” as defined by the AI producer, rather than “reasonably foreseeable use ”. We welcome the idea to link liability and safety compliance, i.e. a product not conforming to safety standards would be considered / presumed defective, while a product complying with regulatory safety standards would be considered safe unless  proven otherwise. ;No opinion;;We would welcome an effort towards the harmonisation of the current national liability rules within the EU.;
F530157;12-06-2020 18:32;French;Business Association;Stella;MORABITO;;AFNUM Alliance Fraçaise des Industries du Numérique;;Micro (< 10 employees);France;The feedback can be published with your personal information;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;S'assurer que l'Europe peut bénéficier d'une IA de confiance développée au niveau international mais aussi qu'elle pourra exporter l'IA qu'elle aura développée. Promouvoir une approche globale et internationale de l'IA de confiance, particulièrement dans le domaine des standards techniques. Favoriser l'accès des PME aux données, à la fois en termes de quantité et de qualité, afin de diminuer les barrières à l'entrée pour celles-ci. Promouvoir la coopération entre développeurs et cyberexperts;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;"- Investir dans les infrastructures à haut débit = accélérer le déploiement de la fibre, lancer le plus rapidement possible les enchères 5G et développer les usages pour les verticaux
- Utiliser stratégiquement la commande publique afin de promouvoir la c";4 - Important;5 - Very important;5 - Very important;La mise en place de réseaux de centres de recherche déjà existants nous parait l'action à privilégier car elle correspond à une approche agile et promeut la réactivité. La création ex novo d'un centre de recherche phare demanderait plus de temps et de ressources. Par ailleurs nous sommes persuadés de l'importance de la collaboration des entreprises avec les pôles de recherche et développement au sein de pôles de compétitivité ou d'autres clusters de promotion de l'innovation.;5 - Very important;4 - Important;5 - Very important;5 - Very important;4 - Important;"Il faut privilégier une approche pragmatique du travail des pôles de compétitivité numérique, non pas uniquement en tant que facteur de liaison entre les petites et grandes entreprises et le monde académique, mais également pour favoriser les transferts de technologies et les tests de différents cas d'usage.
Nous pensons que les DIH européens devraient se concentrer sur des applications de pointe dans lesquelles l'Europe pourrait faire la différence, telles que le edge computing  ";3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;Nous pensons que les risques potentiels de l'IA varient en fonction des secteurs et des cas d'usage et ne peuvent être appréhendés de manière globale. Il est important d'évaluer ces risques et d'en minimiser l'impact dès la conception d'un système d'IA (by design). Les travaux du HLEG soulignent d'ailleurs l'importance des données d'entrée choisies et de la formation des programmeurs et des utilisateurs.;Other;Le RGPD, le règlement sur le flux des données non personnelles et d'autres corpus législatifs couvrent déjà les principaux cas de figure. Il faut analyser précisément l'existant et ne réguler que dans des cas très spécifiques qui ne seraient pas déjà couverts. S'appuyer sur des solutions contractuelles flexibles dans le partage de données BtoB  pourrait favoriser l'adoption de l'IA par les entreprises européennes .;Other;Nous ne sommes pas forcément persuadés à ce stade de la nécessité de nouvelles réglementations. La définition de l'IA à haut risque ne semble pas assez précise. Plus de granularité sectorielle semble indispensable. Il conviendrait de créer un système de classification clair des risques afin que les entreprises puissent facilement évaluer si leur IA rentre ou pas dans la catégorie à haut risque.;;;Toutes les applications où l'intégrité physique, la vie ou des informations confidentielles et personnelles sont à risque doivent être évaluées au cas par cas selon la criticité de l'application elle-même. ;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;4 - Important;No opinion;Les technologies ne devraient pas être stigmatisées en soi, uniquement l'usage qui en fait. Cela dit, les actuelles dispositions du RGPD considèrent les données biométriques comme des données sensibles et leur collecte et traitement ne peut répondre qu'à des cas limités d'usage dont la finalité doit être dûment justifiée.;Rather not;Un système de label risque à ce stade d'être plus une contrainte bureaucratique qu'une solution efficace, sans compter le risque de fausses allégations par des acteurs peu scrupuleux. Le respect des normes de conformité existantes et l'auto-certification paraissent de meilleures approches.;Other enforcement system;"Une IA digne de confiance, sûre et respectueuse des valeurs européennes dépendra moins d'obligations règlementaires que du développement des compétences et de la sensibilisation des acteurs.
Dans le cas d'obligations règlementaires, il faudrait privilégier des processus d'auto-certification complétés quand nécessaire par des contrôles ex-post.";Afin de vérifier la conformité il faudra établir des standards, qui seuls permettent un contrôle efficace. Attention à ne pas mettre en place des processus trop contraignants qui isoleraient le marché EU de l'IA du marché mondial. S'en remettre uniquement à des algorithmes et des jeux de données EU, pourrait se révéler contre-productif pour la diversité des données. Il faudrait éviter néanmoins de divulguer de l'information confidentielle (inclus algorithmes, jeux de données).;;Les risques cités ne sont pas inhérents uniquement à l'IA et en clarifier la portée peut être bénéfique pour différentes technologies. L'IA est un outil technologique parmi d'autres et ne devrait pas être vu comme plus générateur de risques que d'autres technologies. La cybersécurité est un risque exclus du champ de la législation sur la sécurité générale des produits et est adressée contractuellement. Une régulation horizontale la concernant pourrait favoriser la sécurité juridique.;No;"Il conviendrait de définir plus précisément ce que l'on entend par ""changement important"", car la terminologie reste juridiquement imprécise. Il faudrait exclure des réflexions les application d'IA inhérentes aux tests et à la recherche. La législation sur la sécurité générale des produits ne doit pas définir de nouvelles procédures. Si le champ de l'application IA change, alors l'analyse des risques doit être ré-initiée.";No;L’objectif ultime est d’atteindre une IA responsable et sûre et d'obtenir une protection équivalente à celle des autres produits. Les modifications réglementaires déjà envisagés (p. ex., qualité des données, transparence, sécurité) réduisent la nécessité de nouvelles règles de responsabilité. Des obligations supplémentaires réduisent les incitations à développer une IA responsable et sûre. Il n'est donc pas nécessaire selon nous  de modifier la PLD qui est technologiquement neutre.;No;;À l’heure actuelle, rien n’indique que les cadres règlementaires existants, tant au niveau de l’UE qu’au niveau national, ne peuvent pas faire face aux spécificités des applications de l’IA. Il est essentiel d’identifier les lacunes avant de supposer que les régimes d’assurance et de responsabilité existants sont insuffisants. Les interventions EU devraient être limitées aux cas où les régimes actuels de responsabilité sont insuffisants et être abordées de manière sectorielle.;
F530156;12-06-2020 18:18;English;Business Association;Vivien;Zuzok;;Information Technology Industry Council (ITI);061601915428-87;Small (< 50 employees);Belgium;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;"Dear European Commission Team, 
Thank you for considering our contribution in form of a position paper uploaded below. 
We remain at your disposal should you have questions or wish to discuss our views in more detail.
We look forward to continue to engage with you on AI policy in the future. 
Best,
Vivien Zuzok on behalf of the Information Technology Industry Council (ITI)";ITI_response_to_EC_AI_White_Paper_12_06_2020_FINAL.pdf
F530155;12-06-2020 18:17;English;NGO (Non-governmental organisation);Nikoleta;Bitterová;;Philanthropy Advocacy/European Foundation Centre(EFC)/ Donors and Foundations Networks in Europe (DAFNE);;Small (< 50 employees);Belgium;The feedback can be published with your personal information;;;;;;;Consideration of other stakeholders such as civil society, including philanthropy, academia and their role / ability to engage. Consideration of effects on society and on fundamental rights instead of competitiveness of the market.;;;;;;;"Consideration of participatory and inclusive approach issues for all stakeholders, especially the civil society, including philanthropy and public at large; resources for empowering civil society to participate in policy making and the governance discussions. White Paper should highlight multi stakeholder participation - not just the industry.";;;;;;;;;;;;;;;;;There are reference points to fundamental rights issues. However, there is not a good balance. The paper seems to be more consumers/market oriented. There is a lack of balance between commercial and fundamental rights issues. When citizens are mentioned they are described as consumers. Some AI should not be deployed as it breaches human rights.first of all it should be taken a step back and ask whether it is “just” to use them in the first place.;There is a need for a new legislation;;No;;;;Human rights-based approach is essential to ensure that AI developed and deployed in the EU can be truly trustworthy and is not just an empty brand name. Where AI systems pose a threat to any of our fundamental rights, the EU must ensure that states uphold their obligation to protect and promote those rights and that companies conduct due diligence according to their responsibility.;;;;;5 - Very important;5 - Very important;;There is serious concern from a fundamental rights and freedoms perspective around mass-surveillance of any kind. The use of untargeted mass biometric processing systems - whether by law enforcement, public authorities (such as schools or local councils), or private actors - does not meet the required justifications or thresholds of necessity or proportionality to be considered lawful for the level of violation and intrusion they create. ;;When AI is a key component of a product/service making decisions with legal/social impact on individuals, individuals should know that this decision was taken by an algorithm. Any labelling system should not be voluntary. However, a labelling system is only one part of the safeguarding approach. Trust labels tend to give a false sense of security to consumers, especially when some of the threats (e.g. to security and privacy) can be difficult to anticipate or model.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;The compliance should be assessed ex-ante by means of an external conformity assessment procedure.  The process by which the AI is determined to be high or low risk must be reliable, verifiable, trustworthy, contestable and should be reassessed throughout the system’s life cycle. If other actors, in particular those affected by a given system, determine that a system does in fact carry risks despite having been determined to be low risk, mechanisms must be in place. ;;;;;;;;;;
F530154;12-06-2020 18:11;English;Business Association;Medtech Pharma Platform;Secretariat;;Medtech & Pharma Platform;234427831877-10;Micro (< 10 employees);Switzerland;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;MPP_comments_Whitepaper_Artificial_Intelligence_June_2020_signed.pdf
F530153;12-06-2020 18:09;English;Company/Business organisation;Louise;Touzé;;Manufacture Française des Pneumatiques Michelin;EC Register 1413466815 – 09;Large (250 or more);France;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;4 - Important;To be a global player in Artificial Intelligence and to ensure the deployment in the Union, the European Union shall guarantee the deployment of explainable and responsible artificial intelligence. ;5 - Very important;4 - Important;4 - Important;3 - Neutral;5 - Very important;5 - Very important;;4 - Important;5 - Very important;4 - Important;The cooperation between research and innovation stakeholders is necessary to make European Union a global player in Artificial Intelligence. All the measures taken in this area shall encourage this cooperation between stakeholders.;3 - Neutral;4 - Important;4 - Important;4 - Important;No opinion;;4 - Important;4 - Important;4 - Important;3 - Neutral;4 - Important;3 - Neutral;As far as AI is concerned, there are many concerns (safety, fundamental breach, job losses...). These concerns are not related to the results of AI, but are caused by a lack of trust. Therefore,  all stakeholders have to work together to improve systems and make them more understandable and acceptable to the public. Education, professional training could be relevant tools.  ;Current legislation may have some gaps;;Other;The concept of high risks application must be clearly defined and easily understandable. Requirements shall be put in place in the most dangerous cases while avoiding  the multiplication and specificity of these requirements , as this would represent a barrier to the deployment of AI. ;;;;3 - Neutral;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;No opinion;;Much;"Before setting up voluntary labelling, it seems important to set up thresholds that will make it possible to exclude the most problematic products from the market (e.g.explainability; transparency...).";A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Risks related to the loss of connectivity;;Yes;;Yes;At least, a revision of the PLD directive will be necessary to integrate the new definitions,  the roles and responsibilities of each actor in the framework of the deployment of Artificial Intelligence. In any case, it will be necessary to develop a common regulatory framework between all Member States on this subject in order to have a harmonised approach throughout the territory.;No opinion;;Depending on the regulatory approach selected and the resulting timeframe, there may be an interest in adapting national frameworks for certain AI applications. In any case, the European Union shall ensure consistency over time and establish a harmonised regulatory framework so as to ensure the deployment of Artificial Intelligence. ;
F530152;12-06-2020 18:08;English;Business Association;Angela;MILLS WADE;;European Publishers Council;4456380381-30;Micro (< 10 employees);Belgium;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;Guidelines to ensure respect for IP rights in professional content that is used as AI input. Exploitation can be facilitated by the underlying copyright infrastructure based on standard ways of expressing permissions through metadata and use of identifiers. See www.linkedcontentcoalition.org and http://www.copyrighthub.org/  This can be linked to the EC’s data strategy with a a designation of a Common Data Space for rights data. ;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;4 - Important;5 - Very important;important to align AI strategy with the creation of Common Data Spaces includng one for IP rights data (see comment above on importance of rights data to facilitate use of IP protected content). ;4 - Important;4 - Important;4 - Important;Important to facilitate IP clearance for AI input but also to incentivise IP protection for AI driven/assisted content creation, and of certain AI tools;4 - Important;5 - Very important;4 - Important;5 - Very important;4 - Important;Important to clear IP rights. Horizon and future funding around Common Data Spaces could address needs of media and CCIs to help them take advantage of AI;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;AI and ML provide enhanced ability to extract meaningful information from data, also media and journalistic content.this opens up an increasing demand for professional content as key inputs for AI and ML. unauthorised mass downloading and processing of publishers' content for AI training must be under contract.As stated above access can be facilitated by a Common Data Space of rights data;Current legislation may have some gaps;;Yes;;Yes;;;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;No opinion;We would recommend a risk-based approach  – rather along the lines of GDPR, with some pre-determined parameters, while requiring companies and research organisations to take responsibility for assessing risk.;No opinion;;No opinion;;;
F530151;12-06-2020 18:05;English;Business Association;Aisling;O Donoghue;;American Chamber of Commerce, Ireland;981023736582-85;Small (< 50 employees);Ireland;The feedback can be published with your personal information;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;We welcome the Commission's support of the development and uptake of AI across the EU and highlight that the promotion of the uptake of AI within the private sector should also be a top priority. To ensure the development of an eco-system of excellence, any public private partnerships should be open to enterprises regardless of the location of their HQ.;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;Skills are a crucial aspect of promoting the uptake of AI across Europe, within this providing upskilling and re-skilling of the current workforce would help meet this aim. ;4 - Important;5 - Very important;3 - Neutral;While the establishment of a lighthouse research centre is an excellent idea, it should be balanced with the needs of the existing centres of AI excellence across Europe which require further coordination. Any public -private partnership for industrial research should be open to all enterprises regardless of the location of their HQ. ;5 - Very important;4 - Important;4 - Important;5 - Very important;3 - Neutral;;5 - Very important;5 - Very important;4 - Important;4 - Important;3 - Neutral;3 - Neutral;To create an eco-system of trust, it is important that a risk based, proportionate and use case dependant approach is taken in relation to regulation of AI that balances the potential concerns as outlined above with the socio and economic benefits that AI can bring. ;Current legislation may have some gaps;;Yes;;Other;Risks assessments must take into account the context, as an AI application will pose different risks depending on the way it is integrated into business operations. As such, regulation must be context specific and use case dependant as well as proportionate and provide legal certainty. ;N/A;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);;Rather not;N/A;A combination of ex-ante compliance and ex-post enforcement mechanisms;;We strongly encourage further consultation take place with the private sector on the assessment of compliance and any governance framework. Existing conformity assessments in regulated sectors (e.g. automotive, medical devices) should remain sector specific. Potential new conformity assessments for high-risk AI should be self-assessed ex-ante. ;;;No;;No;;No;;;
F530150;12-06-2020 18:01;English;Company/Business organisation;Thibaud;LEBRETON;;SAFRAN (AeroSpace and Defence company);764184537594-67;Large (250 or more);France;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;No opinion;5 - Very important;5 - Very important;Implication of “non digital” Industrial partners is mandatory in order to insure that risks and opportunities of AI are taken in account for all applications (including B-to-B). For SAFRAN, AI main fields of applications are in engineering (materials,parts,systems), automated manufacturing, systems health monitoring and predictive maintenance, smart engines and equipments, augmented pilot operations, autonomous systems for new mobilities/logictics, enhanced services for passengers and crews.;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;No opinion;European Union must have sovereign capacities on AI: software, hardware, database, …. This is a critical issue if European Union is willing to challenge USA and China hegemonic position through their major digital companies (Google, Facebook, Amazon, Alibaba etc..) and the massive investments to come from these countries.. Specific AI challenges for aviation sector are described in EASA’s AI roadmap : (https://www.easa.europa.eu/sites/default/files/dfu/EASA-AI-Roadmap-v1.0.pdf).;4 - Important;5 - Very important;5 - Very important;Due to Covid19 crisis, private self-funding capacities for industrial research will severely decrease at least during 2020-2023 period. As AI is still seen as a disruptive technology with not a clear return on investment for B-to-B activities, AI dedicated budgets will be probably cut first, despite very promising potential opportunities for competitiveness and innovation. Specific actions must be launched quickly to support Industrial companies “in-house” competencies and projects.;No opinion;No opinion;No opinion;5 - Very important;No opinion;;4 - Important;No opinion;No opinion;4 - Important;No opinion;4 - Important;Main problems with AI solutions today come from the lack of precise functional requirements and the weakness of evaluation protocols. This is due to B-to-C dominant applications where no real performance and risks assessment is done before putting them on market. AI solutions in aeronautic systems will have to be compliant with international certification protocols. So the AI roadmap of EASA should be considered.;Current legislation may have some gaps;;Yes;;Yes;;;2 - Not important;3 - Neutral;4 - Important;5 - Very important;3 - Neutral;4 - Important;No opinion;;Much;"Efficient Ex-post audit by relevant European authorities will be mandatory if Voluntary labelling system is deloyed. This condition may be very hard to satisfy with the explosion of labelled B-to-C applications.
Such label may provoke confusion about level of qualification between AI solutions that satisfy “high risk AI” norms and current “nice-looking” B-to-C software. Two different terminologies and legal protection and consequences for business and consumers are required to avoid confusion.";Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;Assessment of compliance must focus on AI solutions performances and properties, not on the quality of the training dataset. For three main reasons. Training process may have more impact in the performances than the quality of training data itself. Some methods like Reinforcement learning do not use learning datasets but scenarios. Most of AI solutions for “high risk” applications will be hybrid and will mix “Data driven” IA and “Model driven” IA.;Mental health risks;;Yes;Existing certification processes (national, European) in the aeronautic sector should be adapted in order to take into consideration additional risks arising from the IA;No opinion;Current EU legislative framework is mainly applicable in a B to C relation;No opinion;;;
F530149;12-06-2020 17:59;English;Business Association;Kai;PETERS;;VDMA - German Engineering Association;976536291-45;Large (250 or more);Germany;The feedback can be published with your personal information;3 - Neutral;4 - Important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;"- monitoring and analysis of international developments
- international cooperation, particularly in the field of standardisation


";4 - Important;4 - Important;5 - Very important;4 - Important;4 - Important;5 - Very important;#NAME?;2 - Not important;4 - Important;3 - Neutral;"Priority must be given to cooperation between AI research and application-specific research, for example through concrete application projects in industrial manufacturing (such as the ""InPuls Project of VDM: http://www.i40-inpuls.de).

Due to the cross-cutting, enabling character of AI, industrial AI-research should be embedded in the relevant existing or planned industrial public-private partnerships (e.g. in the ""Made in Europe PPP"").
";4 - Important;3 - Neutral;5 - Very important;4 - Important;2 - Not important;advice and exchange of knowledge on non-technical issues and barriers, for example on legal and regulatory issues of data exchange and AI;1 - Not important at all;2 - Not important;3 - Neutral;3 - Neutral;2 - Not important;2 - Not important;Without reference to the application and the regulatory context, the questions suggest too high risks of AI. To assess the risks, it is necessary to analyse the degree of autonomy, the criticality of application and existing regulations. For example, we do not see an increased risk for safety because we assume that safety regulation already covers and minimizes risks of AI.;Other;AI is already covered by many laws. The examination of these laws must be based on the realistic possibilities and effects of AI. From the point of view of the VDMA, in industrial applications the need for adaptation is very low, as the relevant regulations are formulated in a technology-neutral way and AI does not lead to a completely new type of product or machine behaviour.;Other;"The approach of linking risk assessment to application is appropriate. However, the cumulative criteria of what is considered a high risk application of AI should be lead to a narrow definition. This will ensure that regulation remains focused.  It is also essential to develop a clear methodology to identify a ""high risk application"". Unclear criteria can lead to legal uncertainty and barriers to innovation.";;;;3 - Neutral;4 - Important;5 - Very important;2 - Not important;3 - Neutral;No opinion;No opinion;;Not at all;Voluntary labelling is not appropriate because it would not be a internal market regulation and would not be subject to market surveillance. However, the effort would be considerable and unjustified, especially in the case of third-party certification. In B2B-relations, requirements for AI are defined by contractual agreements and specifications. ;Other enforcement system;Due the wide range of possible AI applications, it is not possible to choose a general enforcement system. In principle, any system should be compatible with the internal market and innovation-friendly. The modular approach of the NLF is therefore preferable.;;;;No;"IT- and AI-induced changes can be considered as ""digital"" modifications and are already covered by the concept of ""substantial change"". Whether or not this change is based on AI is irrelevant. The conformity assessment procedures are valid. Any adaptations of the state of the art of testing and methodology are updated in testing technology standards and are not subject of legislation.";No;"The established liability rules should not be changed at this stage. Further analysis and identification of concrete gaps, for example in the context of ""regulatory sandboxes"", is necessary. A distinction should be made between products with embedded AI (which is  covered by safety legislation) and AI software. An insurance obligation is not necessary: Manufacturers insure themselves voluntarily, if they are liability addressees.";No;;;VDMA_Position_Paper_White_Paper_AI_June_2020.pdf
F530148;12-06-2020 17:59;English;Company/Business organisation;D;Wilkinson;;British Security Industry Association;;Small (< 50 employees);United Kingdom;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;AI skills strategies are required along the entire value chain (developers, Manufacturers, Service providers, end-users, testers) based on sector needs, taking account of STEM and non-STEM skills, to ensure a human-centric approach. For high-risk applications, curricula and a licensing framework based on formal qualifications should be considered, if necessary incl. vetting. Close cooperation with Social Partners is crucial. ;4 - Important;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;"- Adopt an EU multi-annual financial framework, notably to finance the Horizon Europe and the Digital Programmes
- Invest in ultra-high speed network infrastructures (5G, Harmonized spectrum's)";4 - Important;4 - Important;5 - Very important;Efforts should be focused on financing existing structures, including Public Private Partnerships that have proved their value, to bring skills and stakeholders together (in cluster structures), that would help guaranteeing progress in research and finally innovation on industrial AI.;4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;"- In general, BSIA companies, particularly SMEs, do not have a strong experience of working with Digital Innovation Hubs (DIHs)
- However, DIHs could be instrumental if focusing on technology transfer, rather than only on the ability of universities to co";4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;- The questions focus on negative connotations of AI. In many industrial applications, AI exceeds human ability in terms of accuracy: in such areas, complete accuracy is not necessary as long as it surpasses existing technologies and therefore increases s;Other;There is a need to build on current legislation such as GDPR. To build trust in AI, risk-based legislation is needed based on the 7 requirements identified in the Ethical Guidelines for Trustworthy AI. We believe that particular attention needs to be given to human review, governance mechanisms and trained staff. AI should not be used to make fully automated decisions that may result in civil rights violations. AI systems should support human decision-making.;Yes;;Yes;;"- Lack of human oversight / intervention enhances violation of civil rights, potential gender or racial profiling and risks of malicious use. 
- Human oversight must be required in varying degrees to support other security measures, depending on the AI sy";5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);"The BSIA supports the 7 key requirements identified in the Ethics Guidelines for Trustworthy AI by the European Commission. We also believe that new regulatory requirements as listed in the White Paper can be suitable to build trust in high-risk AI applications, including Facial Recognition. It is important to have on the one hand effective oversight, transparency, and accountability of all AI technologies, and on the other qualified staff among developers, Service providers, end-users, and testers, to effectively protect fundamental and consumer rights in a human-centric approach. Based on these requirements and following a thorough impact and risk assessment for each use-case, the BSIA agrees that the use of remote biometric identification systems, such as Facial Recognition, should be allowed in public spaces if they can bring considerable added value to public safety and security, and respond to risks to public security.

The BSIA objects to a moratorium on these tools based on concerns on their possible misuse only. A moratorium would negatively impact investment to improve accuracy, effectiveness, efficiency, and testing of remote biometric identification tools, and leave the market to other countries instead of establishing a European, human-centric leadership in these technologies.

Further, a moratorium would have negative consequences for the security of citizens, as it would preemptively deprive law enforcement of technologies that would bring considerable added value in fighting crime when used alongside human oversight and intelligence in specific cases. Facial recognition can be critical to enhance capabilities of solutions like video surveillance, access control, and identity management systems for the protection of people – especially at Critical Infrastructure. When deploying remote biometric identification technologies only in specific cases, it is not about nationwide surveillance of citizens, but a targeted search for criminals at particularly vulnerable locations. The automated comparison of video images with police databases, in which photos of criminals are stored, is not comparable with the use of systems for which millions of photos of uncontested citizens are stored. Also, the alternatives a ban would leave would be far slower and also prone to errors – for example when officers have to manually go through large quantities of videos and images of police databases. 

The BSIA would also like to stress that there are different use-cases of Facial Recognition, which do not all come with a higher threat to fundamental rights than other applications that the Commission may consider being “high-risk applications”. For example, when it is used for verification purposes (e.g. in banking, electronic device unlocking, access control), individuals have consented to, or are required to prove their identities without a negative impact on privacy, if the requirements are fulfilled as lined-out in the White Paper. It is therefore important to conduct impact and risk assessments before using remote biometric identification systems in public spaces:

First, remote biometric identification tools are not by default a necessity when there are other means to achieve the underlying purpose. I.e. proportionate and appropriate to the legitimate aim and necessary to meet an identified pressing need. When using AI technologies, their added value and expected impact must be clear. Depending on the mission and location, AI technologies, physical intervention, and the blending of the two through ”augmented security” must be considered in a risk and impact assessment. On-site law enforcement personnel or private security officers can often deliver better added value, e.g. by means of behavioural detection techniques or by being able to react and, if necessary, intervene directly on the spot. The deployment of surveillance technologies and/or physical guarding must always fulfil the objectives of a mission under careful consideration of data protection, privacy, and fundamental rights. Further, there must be legitimate interest to use remote biometric identification tools in public spaces based on a risk assessment process taking into account threats to public security, and an evaluation of physical and technological solutions that properly respond to the risk-level. 

In addition to this impact- and risk-based approach, the BSIA stresses the relevance of human oversight over remote biometric identification tools. Human review and intervention is crucial to ensure that any decision made by AI tools does not violate civil rights.

Technologies like facial recognition should not be used to make fully automated, final decisions. Human review of facial recognition results should be used to ensure rights are not violated in particular violating the rights based on gender or racial discrimination . To that end, those conducting human oversight need to have adequate training, skills and qualifications. 
";Much;To ensure uptake, any labelling system must respond to societal and industry needs. In the light of technological development, the standardization process must be efficient, flexible and cost effective. Participation of all relevant stakeholders is crucial. Investments must be made in the promotion of standards among industry and lawmakers - if of added value also for the uptake in laws, procurement, and contracts. Labelling should be complemented by accountability as well as review and redress.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;"The use of harmonized standards across countries is needed to not leave gaps. 
Support structures are needed for SMEs To ensure AI uptake.
Financial and administrative burdens to users need to be as low as possible. 
Predefined conformity assessments or standard scenarios for the use of AI should be considered. 
AI tools could include a self-assessment / reporting function (“ethics switch”) as addressed in the Ethics Guidelines on Trustworthy AI.";Risks related to the loss of connectivity;;Yes;When the scope of the AI application changes, like a new surveillance purpose etc, the risk assessment has to be re-initiated. But this does not require new risk assessment procedures, just new methodologies (e.g. considering the life-cycle of AI).;Yes;Legal certainty on victim rights and liability is key. Developers carry a responsibility, as they largely define AI behaviour and learning. Producers must ensure that all products put on the market are safe throughout their life-cycle. Service Providers and/or end-Users should only be held liable, if they are best suited to respond and entitled to autonomously intervene in AI decisions. human oversight, traceability, skills, and, if necessary, licensing, are key at all stages of the value chain.;No opinion;;If EU legislation regarding liability law and procedural law is deemed necessary, then due consideration must be paid to the national regimes and their specificities. Therefore, the starting point should be a more detailed study as to the applicable national regimes, to identify whether or not “liability gaps” do indeed exist.;BSIA_response_paper_on_Artificial_Intelligence_-_A_European_Approach_V2.pdf
F530147;12-06-2020 17:57;German;Other;Bundesrechtsanwaltskammer;Brüssel;;Bundesrechtsanwaltskammer;25412265365-88;Small (< 50 employees);Germany;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
F530146;12-06-2020 17:51;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Global cooperation to ensure Europe benefits from trusted AI developed overseas but also that AI developed in Europe can continue to cross borders and be sold internationally. To enable this exchange the EU AI Whitepaper must better acknowledge the need to align with global AI standards, such as the ongoing work by ISO and IEEE.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"Overall, increased adoption of AI by the private sector – especially SMEs – and the public sector should be a top priority for any policy initiatives, given that the EU is still lagging behind in AI adoption compared to other regions.

Further clarification of what is meant by ‘European data spaces’ would be welcome. A clear plan for fostering/ incentivising data sharing is key. This includes the opening up of public data sets, where appropriate, to enable innovation. ";2 - Not important;5 - Very important;5 - Very important;Rather than the proposed ‘lighthouse centre of research, innovation and expertise’ we would suggest prioritising a co-ordinated network of existing centres of excellence, to ensure coherence and co-operation of research efforts across the EU. This network of centres would require a shared vision regarding its focus and co-operation with non-European states. ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Under “raising awareness” Digital Innovation Hubs should also provide guidance on best practice and ethics for SMEs. ;5 - Very important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;;Current legislation is fully sufficient;;Other;We do not think new rules are necessary for AI systems. See supplementary paper. ;;;;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;No further guidelines or regulations are needed;See supplementary paper. ;Much;See supplementary paper. ;Other enforcement system;Our general preference is for ex-post checks rather than an ex-ante conformity assessment. Ex-ante assessment does not allow for agile development and can therefore inhibit innovation. A combination of ex-ante and ex-post may be purposeful as long as ex-ante mechanisms are limited to self-assessment only. It will be important to consider how these mechanisms would apply across different sectors that are already regulated. Regulatory sandboxes may also be a way of encouraging innovation. ;Re-training algorithms in a specific location will not necessarily guarantee higher quality and a different output. Relying solely on European trained algorithms and European data sets could cause challenges with regards to the diversity of datasets and create additional burden for companies selling to a global market.;;;No;The terms ‘change’ and ‘important’ here are vague and it would be technically difficult to conduct risk assessments on products in the end users’ possession which may have evolved differently due to self-learning capabilities.;No;No- a key benefit of this directive, that attributes to its longevity is that it remains tech-neutral. ;No;;See supplementary paper. ;
F530145;12-06-2020 17:50;English;Business Association;Matteo;Quattrocchi;;BSA | The Software Alliance;75039383277-48;Medium (< 250 employees);Belgium;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;;5 - Very important;3 - Neutral;5 - Very important;4 - Important;5 - Very important;4 - Important;;3 - Neutral;4 - Important;4 - Important;;No opinion;No opinion;No opinion;No opinion;No opinion;;4 - Important;4 - Important;4 - Important;3 - Neutral;3 - Neutral;3 - Neutral;BSA agrees with the White Paper that the public should “expect the same level of safety and respect of their rights whether or not a product or system relies on AI.” The concerns presented by the Commission are not unique to AI. The EU body of laws offers strong, technologically neutral safeguards. BSA strongly recommends that the Commission takes stock of this body of legislation in a targeted way, identify possible gaps and then decide on whether to propose new legislation, AI-specific or not.;Current legislation may have some gaps;;Yes;;No;;BSA agrees that future legislative proposals should focus on high-risk scenarios where the deployment of AI poses a threat to fundamental rights. The scope should be a function of the degree of risk and the potential scope and severity of harm. It will be important to carefully assess scenarios that should be deemed as high-risk and hence be subject to legal requirements. BSA recommends ensuring stakeholder involvement, especially as context and purpose will be essential elements to assess.;4 - Important;3 - Neutral;5 - Very important;4 - Important;4 - Important;4 - Important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);BSA acknowledges the Commission’s concern that the deployment of biometric identification systems can implicate heightened risks for fundamental rights. BSA welcomes the White Paper’s recommendation for the Commission to launch an inquiry to examine the appropriate regulatory framework for biometric systems.;Rather not;BSA agrees that public trust in AI is essential for the uptake of the technology. BSA urges the Commission not to pursue the creation of a blanket voluntary labeling system for all non-high risk systems. Given the diverse range of AI that will be considered non-high risk, a one-size-fits-all labeling scheme would be unworkable. The benchmarks for evaluating whether AI systems are trustworthy are likely to be highly variable, driven in large part by system functionality and deployment context.;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;BSA urges to the Commission to support self-attestation for ex-ante assessment, as legislative obligations may turn into market-access barriers. BSA recommends building upon the work done on the HLEG Assessment List, which may constitute a template for future assessment tools. These tools, including ex-post, allow for a more context-specific evaluation of the types of available risk mitigation measures, which is best suited for the particular deployment scenario.;;The Liability Report suggests that the existing safety legislation framework already covers the full range of risks that may be implicated by the use of AI and other emerging technologies. In the absence of a demonstrated gap in protection, BSA urges the Commission to further assess legislative gaps, before introducing additional regulatory burdens that may create unnecessary uncertainty.;Yes;Safety, like Security, is a process, not an end state. A key element for consideration should be whether the software being deployed was developed using best practices, such as the BSA Software Security Framework. A key goal of any reform to the EU product liability regime should be to ensure that liability for harm falls on the entity best positioned to foresee and mitigate risk – i.e. the entity that determines the purpose of the AI, similar to the concept of a “controller” under the GDPR. ;No;The PLD sets out clear, well understood and time-tested rules that apply across a wide range of products, including those with embedded software. It is important to underline that consumers have the possibility to obtain compensation for possible harms due to AI, or other products or services, under the current regime. Changing liability rules without further assessment risks chilling innovation into socially beneficial (and even safety-enhancing) AI, with little positive benefit for consumers.;Yes, for specific AI applications;Future legislative efforts should fully take into account the differences between B2C and B2B. Given the sophistication of B2B entities, and the tailoring of products/services to fit enterprise demand, it is important is to preserve contractual freedom coupled with contractual indemnification. Contracts provide the parties with added flexibility to negotiate terms that account for context-specific considerations related to the appropriate assumption of risk and allocation of liability.;;BSA_AI_White_Paper_submission.pdf
F530144;12-06-2020 17:35;English;Company/Business organisation;Matti;Aksela;;F-Secure Oyj;957279018810-11;Large (250 or more);Finland;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;;5 - Very important;2 - Not important;5 - Very important;4 - Important;5 - Very important;4 - Important;"Innovation on AI should be promoted also for larger companies, not just start-ups. 

While testing for AI is absolutely critical, we do not believe that separate testing facilities is the best way to approach this - rather for example looking at it as methods, tools and best practices on how to test AI that can be applied by developers of AI solutions";4 - Important;4 - Important;5 - Very important;;5 - Very important;3 - Neutral;4 - Important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;1 - Not important at all;2 - Not important;2 - Not important;"Explainability should not be the fundamental requirement - for example all medicine is not fully understood and that is why we have clinical trials; which would you prefer - a medicine that is completely understood and provides a 50% likelihood of saving your life or a medicine that is not understood but has been clinically proven to provide a 99.9999% chance of saving your life? 

Also humans are truly not always accurate and there are many situations where accuracy is poorly defined. ";Current legislation may have some gaps;;Yes;;Other;"Partially agree, and many of the proposed high-risk sectors do make sense, but further consideration should be put on how to define them clearly enough and with sufficient breadth, and also if there should be exceptions or not; for example having hiring in general as an exception does not make sense as for sure e.g. chatbots replying to questions should not be as high risk as applications taking filtering/hiring decisions.";Lethal Autonomous Weapons Systems;3 - Neutral;3 - Neutral;4 - Important;4 - Important;3 - Neutral;4 - Important;No further guidelines or regulations are needed;;Very much;A voluntary labelling system can be of great benefit as long as it is of sufficient level of complexity and depth - it must not be only achievable by large corporations but also be something that actually carries meaning and can be used by also smaller players, so the design of the system is of paramount importance;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Cyber risks;;Yes;;Yes;The Product Liability Directive should be updated, but the modifications should not apply only to AI applications, but also e.g. software, IoT and other services in general.;No;;;
F530143;12-06-2020 17:34;English;NGO (Non-governmental organisation);Rasha;Abdul Rahim;;Amnesty International;11063928073-34;Large (250 or more);United Kingdom;The feedback can be published with your personal information;5 - Very important;5 - Very important;4 - Important;4 - Important;2 - Not important;1 - Not important at all;A cross-EU approach could create access to new technologies for some member states, promote public interest AI research and open access to data for public bodies, but every exploration of AI at national and regional level must prioritise human rights protections over innovation. Promoting the adoption of AI by the public sector cannot be a goal in and of itself, but only for specific purposes where AI systems can benefit the public sector without harming human rights. ;4 - Important;No opinion;2 - Not important;3 - Neutral;;2 - Not important;"Investing in public research and common infrastructure could address market dominance by a small number of tech multinationals in the AI space. Again, human rights must take priority. Promoting AI ‘at any cost’ for the sake of innovation is likely to result in long-term damage to society if the impact on rights is not considered and meaningfully addressed. All training and education must include human rights and ethical impact of AI at the front and centre, including bias in design and data.
";3 - Neutral;3 - Neutral;2 - Not important;"The Commission must ensure research and innovation projects are governed and regulated, including through independence from member states and avoidance of corporate capture; mechanisms for oversight of innovation hubs and their outputs; and frameworks for accountability and redress. AI is widening the equality gap in our societies. The Commission must carefully consider which populations innovation serves, and who loses out. Public-private partnerships must be fully transparent and accountable.";3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;No opinion;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;The Commission must state outright that human rights will be prioritised ahead of any innovation initiatives. AI tech poses significant risk to human rights, including privacy and non-discrimination rights, as outlined in the Toronto Declaration (2018) from Amnesty and Access Now. The Commission should refer to the Toronto Declaration’s recommendations for states exploring, implementing and scrutinising state use of AI to create regulation that meaningfully protects rights. ;There is a need for a new legislation;;No;;;;Amnesty International has serious concerns that this proposal will create loopholes allowing high-risk AI applications to remain unregulated. Instead, the EU must legally require that all AI systems are subjected to an ongoing process of human rights due diligence, in line with international standards. In addition, all states should be mandated to declare publicly where automated or algorithmic decision-making technology is used in *any* public sector context. ;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;Amnesty calls for a ban on any facial recognition and other biometric technologies where they are used for identification (1:n) purposes by both state agencies and private sector actors. This does not include technology for authentication or verification (1:1). Biometric technologies including facial recognition technology have proliferated with no meaningful regulation, and pose a significant risk to privacy and non-discrimination rights, the right to peaceful assembly, freedom of expression and association rights. ;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;The onus is on States to make systems visible, allow outputs or impact to be queried, and create accessible and practical routes for remedy and redress when human rights are negatively impacted. Companies must be held legally accountable for human rights harms linked to such systems, including negative impacts resulting from the optimization decisions of algorithmic systems.;Mental health risks;;Yes;Human rights due diligence, including human rights impact assessments, should be conducted prior to the development and launch of any AI system. It may be difficult to predict and mitigate against all risk eventualities, so systems must be reviewed and interrogated, with independent oversight, regularly to ensure that there is no inadvertent negative impact on rights. Risk assessments for public sector applications must be made publicly available, even where a project has not launched.;Yes;Grounds for liability must be established on the basis of failure to carry out human rights due diligence in relation to AI systems and applications. ;Yes, for all AI applications;;;AI_white_paper_response_Amnesty_International.pdf
F530142;12-06-2020 17:30;Dutch;Public authority;Machteld;Vrieze;National;Agentschap Telecom Nederland / Radiocommunications Agency The Netherlands;;Large (250 or more);Netherlands;The feedback can be published with your personal information;5 - Very important;3 - Neutral;3 - Neutral;3 - Neutral;4 - Important;4 - Important;Een ecosystem of excellence vergt een duidelijk kader, standaarden en certificeringen. Hiervoor is Europese samenwerking essentieel, zodat goede standaarden en certificatietrajecten ontstaan. Het gebruik van duidelijke standaarden is met minimale meerkosten goed toepasbaar voor de sectoren en goed handhaafbaar. Een duidelijk stelsel creëert draagvlak en neemt binnen de sectoren veel onduidelijkheid weg, waardoor de sectoren gestimuleerd worden om AI toepassingen te ontwikkelen.;4 - Important;4 - Important;3 - Neutral;3 - Neutral;4 - Important;5 - Very important;Europese samenwerking (tussen  lidstaten en  overheden en private sector) is essentieel, zodat goede standaarden en certificatietrajecten ontstaan. De standaarden zijn enerzijds met minimale meerkosten goed toepasbaar voor de sectoren en anderzijds goed handhaafbaar. Een duidelijk stelsel creëert draagvlak onder de bevolking en gebruikers en neemt binnen de sectoren veel onduidelijkheid weg, waardoor de sectoren gestimuleerd worden om op een betrouwbare wijze AI toepassingen te ontwikkelen. ;4 - Important;4 - Important;3 - Neutral;Onderzoek, training van AI en aandacht voor betrouwbare data in de EU is aan te bevelen in de fase waarin AI zich nu bevindt. Europa wil een leidende positie innemen, dan moet dat ook terugkomen in de ambities. Deze ontwikkelingen helpen de lidstaten om de implementatie van AI en het toezicht daarop verder vorm te geven.;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;;5 - Very important;4 - Important;4 - Important;2 - Not important;3 - Neutral;4 - Important;"AI veroorzaakt in een deel van de maatschappij een groeiend wantrouwen in technologie in het algemeen. Dit wantrouwen kan de economische mogelijkheden van AI belemmeren. Als autoriteit in het digitale domein krijgen we hierover zorgwekkende signalen uit de markt.
AI veroorzaakt daarnaast mogelijk ook onduidelijkheid over (keten-)verantwoordelijkheid. Voor effectief toezicht is het daarom belangrijk het regelgevend kader van toepassing te verklaren voor álle partijen in de keten.
";Other;"Mogelijke witte vlekken in de actuele wetgeving zijn:  menselijk overzicht op de afwegingen/beslissingen die AI systemen maken; transparantie; traceerbaarheid; regels voor datasets en regels voor keuzes die gemaakt zijn in de software. 
Voor een effectief nalevingstoezicht is het nodig dat deze aspecten een grondslag krijgen in (generieke) wetgeving. Open normen moeten leiden tot standaarden die dit uiteindelijk nader invullen en concretiseren. ";Other;Het grote bezwaar van beperking tot high-risk is hoe je dat bepaalt: op economische, ethische of juridische gronden. De definitie van ‘high-risk AI’ versus ‘overig AI’ is niet heel scherp te formuleren. Het verschil tussen ‘high-risk’ AI en ‘overige AI’ is enorm groot wat betreft de opgelegde beperkingen en consequenties. Daardoor zullen partijen proberen onder de high-risk AI definitie uit te komen. ;;;Veiligheid van het publiek en van de vitale systemen is naar schade-potentieel duidelijk, maar bij high-risk AI moet daarnaast ook gekeken worden naar beschadiging van het vertrouwen: bijvoorbeeld aantasting van privacy, discriminatie en gebrek aan controle (angst). Ook AI toepassingen in de digitale infrastructuur die niet goed functioneren kunnen netneutraliteit ondermijnen of tot ontwrichting leiden. ;5 - Very important;5 - Very important;4 - Important;4 - Important;3 - Neutral;3 - Neutral;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);"Remote BIS zijn toelaatbaar indien de risicobeoordeling past bij het gebruik. Nadere regelgeving moet zich toespitsen op: 
1) formuleren van het risicobeoordelings niveau,  gebaseerd op de eisen;
2) formuleren van het toelaatbare risico voor vitale publieke dienstverlening zoals gezondheidszorg, rechtshandhaving, toegang tot overheidsdienstverlening, etc. 
";Much;"Voluntary labelling system’ kan bruikbaar zijn, mits concrete eigenschappen worden benoemd voor labelling, want anders is het niet informatief; ‘veilig’ of ‘ethisch’ zegt weinig.";A combination of ex-ante compliance and ex-post enforcement mechanisms;;Het regulerend kader moet ingericht zijn op een complex ecosystem.Wij adviseren aan te sluiten bij de methoden van de geldende regelgevende kaders in de telecom- en cybersecuritydomeinen, waaronder begrepen worden de standaarden voor technische en organisatorische compliance. Een standaard kan ook vrijwillig worden gebruikt voor toepassingen met een lager risico, om de transparantie en het vertrouwen in de markt te verhogen.Een toezichtssysteem moet zo eenvoudig mogelijk zijn. ;Risks related to the loss of connectivity;Risico van verlies van persoonlijke autonomie.;Yes;Het doel is dat producten tijdens hun levenscyclus conform de veiligheids- en beveiligingseisen functioneren en daarna geen risico vormen. Risicobeoordelingen (die voorafgaand de marktintroductie plaatsvinden), moeten ook de veranderende omgeving, en de reactie van het product op die verandering, meenemen. ;No;Onwenselijke uitkomsten van AI toepassingen zijn gerelateerd aan de aansprakelijkheid van de aanbieder, dan wel de fabrikant van het product. Daarbij wordt bij producten gekeken naar ‘voorzienbaar gebruik’. ;No;;;20200612_Appreciatie_Agentschap_Telecom_Witboek_over_Kunstmatige_intelligentie_definitief.pdf
F530141;12-06-2020 17:29;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;;5 - Very important;5 - Very important;3 - Neutral;;No opinion;No opinion;No opinion;No opinion;No opinion;;4 - Important;4 - Important;4 - Important;No opinion;No opinion;4 - Important;;No opinion;;No opinion;;;;;No opinion;5 - Very important;No opinion;5 - Very important;No opinion;No opinion;No opinion;;No opinion;;No opinion;;;Mental health risks;;No opinion;;No opinion;;No opinion;;;Feedback_from_the_European_Brain_Council_on_the_White_Paper_on_Artificial_Intelligence.pdf
F530140;12-06-2020 17:28;English;Other;Paula;Grzegorzewska;;OpenForum Europe;2702114689;Micro (< 10 employees);Belgium;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;Annex_to_AI_consultation_-_OpenForum_Europe.pdf
F530139;12-06-2020 17:28;English;Other;Nicole;Santiago;;SHERPA Project;;Small (< 50 employees);United Kingdom;The feedback can be published with your personal information;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;"Promote and fund AI that is ethical, protects human rights, and supports human flourishing; work with international organisations and civil society to identity and address issues and concerns related to the development and use of AI; set specific AI and innovation goals for the deployment of AI in the public sector ";5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;"Increase financing for start-ups innovating in AI; support innovation in established and larger companies that might have better access to relevant data.";5 - Very important;4 - Important;4 - Important;"Enhance interdisciplinary research and innovation; Ensure a lighthouse research centre and/or network and/or other institutional mechanism is inclusive for all potential new entrants.";4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;"Create certification around AI educational standards & training; help SMEs identify & address related ethical, legal & societal issues through funding, training activities, & connecting to other resources & best practices; promote public procurement of AI applications that are ethical, protect fundamental rights, & support human flourishing; monitor ISO & CEN activities with regard to AI & help formulate interventions where possible in the standardisation process

";5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;"Impacts economic & power distribution (e.g. few powerful corporations develop technologies & influence political processes; unequal distribution of resources & wealth); increased risks to vulnerable groups; affects crisis response (e.g. pandemic misinformation); increases surveillance & mass manipulation; undermines rule of law; affects human relations and decision-making; changes the delivery & accessibility of public services; environmental impacts";Other;'Current legislation may have some gaps' and ‘There is a need for a new legislation.’  A comprehensive gaps analysis is needed to identify & prioritise required regulatory reform. However, this is not an excuse for governments to delay actions, especially where AI is having negative effects. One such immediate action should/could be implementing a ban/moratorium on the use of lethal autonomous weapons systems.;No;;;;"AI use in warfare; AI use by law enforcement authorities; AI use in judicial decision-making systems; AI powered/AI lie detectors (e.g. Automated Virtual Agent for Truth Assessments in Real-Time); AI decision-making with no opportunity for human intervention; AI applications based on techno-solutionism (aka using AI to ‘solve’ the wrong problems – e.g. discouraging use of crumbling public transport systems instead of fixing them).";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;;Very much;"But, it must be robust enough that it is meaningful, but not too onerous for SMEs without the resources of large companies; specific goals must be clearly identified & communicated; the common criteria & application process should be carefully considered & articulated; labelling should be rigorous, not symbolic in compliance & auditing; attention should paid to the consistency & interoperability w/ other standardization/certification regimes to avoid a proliferation of inconsistent activities. ";Other enforcement system;Should include those of self-assessment ex-ante and assessment ex-ante by means of an external conformity assessment procedure. One potential ex-ante enforcement could take place at the time of patent registration (an individual or business filing a patent must affirm their innovation does not in any foreseeable way violate fundamental rights and freedoms). ;"At the int'l level, SHERPA research identified as most promising: Binding Framework Convention, CEPEJ European Ethical Charter, & Legislative Framework for independent & effective oversight. At the EU-level: general fund for smart robots & the Common Union registration of robots; algorithmic impact assessments under GDPR; & voluntary/mandatory certification of ADS. At the nat'l level: redress by design & proposed ‘specific’ legislation. For more, see SHERPA T3.3 on Regulatory Measures.";Mental health risks;"Discrimination and bias; lack of accessibility ";Yes;;Yes;Modifications should not apply only to AI applications narrowly, but also e.g. software, IoT and other services in general. ;Yes, for all AI applications;;;
F530138;12-06-2020 17:25;English;Business Association;Sarah;Callaghan;;Leaseurope and Eurofinas;Leaseurope: 430010622057-05, Eurofinas: 83211441580-56;Small (< 50 employees);Belgium;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;4 - Important;4 - Important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;;Current legislation is fully sufficient;;No opinion;;;;;4 - Important;4 - Important;4 - Important;3 - Neutral;3 - Neutral;3 - Neutral;;;Very much;;;;;;;;;;;;;;
F530137;12-06-2020 17:25;English;Academic/Research Institution;Maurizio;SPIRITO;;FONDAZIONE LINKS - LEADING INNOVATION & KNOWLEDGE FOR SOCIETY;;Medium (< 250 employees);Italy;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;"Communicate the potential of AI across multiple domains through examples
Inform non-technical people about benefits and implications of AI to improve public acceptance
Promote a decentralized approach to AI to avoid that the data ownership concentration turns into an oligopolistic AI leadership
Promote openness of datasets,standards,ontologies/linked data to unleash the benefits of innovative AI data-driven services, especially for smart cities and for achieving the EU Green Deal goal";5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;Investing in infrastructure and hardware development, such as through creating enhanced AI semiconductors to support continuous AI complex training and calculations.Foresee actions promoting a vision of technological convergence among exponential technologies (such as AI, DLT and IoT).Policy on data collection for member states to achieve an harmonized monitoring at EU level, considering all key indicators of our environment and society, including all productive activities.;3 - Neutral;5 - Very important;5 - Very important;"Promote open research,common methodologies and datasets(DS) to boost and improve AI reliability and precision also sharing best practices and approaches among stakeholders
Free access to all(incl. very high-res.)geospatial products
Improve existing DS spatial resolution,incl.a better EU Digital Elevation Model
Policy to enforce private companies (e.g.telcos) to release DS relevant for the public good, especially for emergency management (e.g. dynamic population maps),preserving privacy";5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;"The DIH allows SMEs to have directly access to the knowledge and to create innovation from competences already available within the DIH
Provide a clear map of the available dataset that could be used for innovative AI services. An updated and curated data catalog at the EU level, promoted and enriched by DIH at the local level. Competitions (hackathons) between DIH to stimulate cooperation and competition among DIHs on  cross-regional challenges, from the local up to the EU level.";4 - Important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;5 - Very important;"Invest more on the study of fundamental maths to gain a deeper insight into the convergence, stability and reliability of results

AI (especially machine learning) is data hungry and therefore energy hungry. Promote sustainable AI in line with the EU Green Deal considering the whole data chain and the availability of computational resources at all level of the Internet, from the Edge up to the Cloud.";Current legislation may have some gaps;;Yes;;Yes;;"The use of AI in the recruitment and broader employment context. 

Emergency management, public security including the protection of public spaces, border security, law enforcement including fight against crime and terror
";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;The use of AI in public spaces can be a deterrent for malicious behaviors and mitigate critical situations happening. Extra limitations to its use would reduce the benefits AI could have for the safety in public spaces;Much;"CE certifications for AI, as for other sectors, would be useful to recognize an AI solution as dependable.

Common taxonomy/ontology should be agreed before to proceed with labeling at scale";Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;"Research solutions should be subject of relaxed or special assessment of compliance in order to not slow down the innovation speed.

Depending on the application a certain accuracy should be met on a validation dataset. Such validation dataset should be regularly updated/extended to be always statistically representative. Hence, the compliance is not granted in perpetuity.
";Mental health risks;In cases where AI is used in industrial settings (e.g. in robots) or autonomous driving where a malfunction can endanger people safety.;Yes;"AI applications trained on data covering a sufficient # of std. dev’s from the avg. might provide outputs hampering the minority part safety;hence,even well-trained AI apps might cause risks for end-users outside the statistics
Risks assessment should be tailored to the specific AI applications
Risk assessment should consider the probabilistic nature of AI algorithms accuracy,treating such probability as a hazard,in function of the type of risk and considering vulnerability & exposure";Yes;"Research on novel approaches for accountability and dedicated computer forensic should be financed to support identification of digital fingerprints to precisely detect the reason why an autonomous component of an AI system failed.

The definition of product shall be updated to be sure to include all kind of AI services and derived solutions";Yes, for specific AI applications;For all the applications labeled as “high risk”.;"The liability rules shall be unique and defined at the EU level. A single market shall have the same rules.
";
F530136;12-06-2020 17:24;German;Business Association;Michael;Wunnerlich;;BDEW Bundesverband der Energie- und Wasserwirtschaft e.V.;20457441380-38;Medium (< 250 employees);Germany;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;"Energie- und Wasserwirtschaft zu einem Leitsektor für “KI made in Europe” machen: Eine zielgerichtete Forschung und Förderung erhöht in Fokusbranchen die internationale Wettbewerbsfähigkeit. Der Fokus sollte auf dem Transfer von der Wissenschaft in die Anwendung liegen.
Bei Fördermaßnahmen für KMU sind auch solche Unternehmen zu berücksichtigen, die mehr als 25% kommunale Beteiligung besitzen.";4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;;4 - Important;4 - Important;4 - Important;Um die Energiewirtschaft zu einem Leitsektor von „KI made in Europe“ weiterzuentwickeln, sollte die KI-Forschung mit Fokus auf Anwendungen in der Energiewirtschaft ausgebaut werden und insgesamt die Forschungslandschaft in Europa stärker koordiniert werden, z. B. im Rahmen eines europaweiten Exzellenzclusters. Darüber hinaus sollte ein entsprechendes zentrales Forschungsinstitut aufgebaut werden, das über geeignete Ressourcen und Ausstattung verfügt, um international Spitzenforschung anzuziehen.;4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;;3 - Neutral;3 - Neutral;4 - Important;4 - Important;4 - Important;4 - Important;Insbesondere in auditierten Bereichen (z. B. Stromhandel) erscheinen Leitlinien sinnvoller als Regulierung, da ein stetes Abwägen zwischen Vor- und Nachteilen (und damit den Bedenken) erforderlich ist. Risiken durch KI enstehen heutzutage insbesondere durch die Anwendung durch nicht ausgebildete Mitarbeiter. Deshalb sollte in die Ausbildung der entsprechenden Personen investiert werden anstatt in nachträgliches Prüfen der Anwendungen.;Current legislation may have some gaps;;Other;Anstelle starrer Regeln sind klare Richtlinien, Selbstverpflichtungen oder Verhaltenskodexe sinnvoll, die einen pragmatischen Rahmen für den alltäglichen Umgang mit Daten und KI-Modellen vorgeben. Einen besonderen Ansatz mit Vorbildcharakter über die Energiewirtschaft hinaus ist der von E.ON zusammen mit der Universität Oxford entwickelte „Oxford-Munich Code of Conduct for professional data scientists“ (Verfügbar unter: http://www.code-of-ethics.org/code-of-conduct/).;;;Die Energiewirtschaft bekennt sich zu Ihrer Verantwortung und wird auch zukünftig neue Technologien in besonders kritischen Bereichen (z. B. in der Netzsteuerung) nur mit weitreichender Risikoprüfung einsetzen. Eine pauschale Einstufung aller KI-Anwendungen im Energiebereich als kritisch bedeutet erhöhter bürokratischer Aufwand. Es besteht die Gefahr, dass viele sinnvolle Anwendungsfälle nicht umgesetzt werden würden und das große Potential im Energiebereich nicht ausgeschöpft wird.;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;No opinion;;No opinion;Die Frage ist, welchen Aufwand die freiwillige Kennzeichnung mit sich bringt. Kann nur ein Unternehmen mit hohem KI-Reifegrad den Aufwand betreiben, da Kapazitäten zur Verfügung stehen? Falls ja, wäre das eine Benachteiligung kleiner Unternehmen. Daher müsste der Aufwand minimal und kostenfrei sein.;Other enforcement system;Vorherige Selbstbewertung wie in der Energiewirtschaft (vgl. E.ON Code of Conduct).;Vorherige Selbstbewertung wie in der Energiewirtschaft (vgl. E.ON Code of Conduct). Von besonderer Bedeutung ist, dass eine möglicherweise verpflichtende Selbstbewertung und eine möglicherweise eingeführte ex-post Überprüfung die Entwicklungsgeschwindigkeit von KI nicht einschränkt. Damit hängt stark zusammen, welche Institution die Prüfung durchführt und wie pragmatisch dies erfolgt.;Cyber risks;;No;KI verändert sich immer während der Lebensdauer und lässt sich nicht auf bestimmte Produkte beschränken. Daher muss zu Beginn die Risikobewertung vorgenommen werden.;No opinion;"Auf Grundlage des Weißbuches ist es schwierig einzuschätzen, was genau reguliert werden sollen. Dazu bedarf es einer ausführlichen öffentlichen Diskussion, bevor eine gesetzliche Regelung möglich erscheint.

Auch bei KI-Ansätzen muss Rechtssicherheit bestehen, um die Markteintrittsbarrieren gering zu halten. Klärungsbedarf besteht u. a. bei Haftungsfragen, selbstständigen Entscheidungen, Urheberschaft, Fehlfunktionen etc. ";Yes, for specific AI applications;;Auf Grundlage des Weißbuches ist es schwierig einzuschätzen, welche Produkte/Anwendungen reguliert werden sollen. Dazu bedarf es einer ausführlichen öffentlichen Diskussion, bevor eine gesetzliche Regelung möglich erscheint.;
F530135;12-06-2020 17:18;French;Company/Business organisation;MARJORIE;VOLLAND;;TECH IN France;;Micro (< 10 employees);France;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;Créer des sets partagés de training data fiables pour les systèmes d’IA. Le coût de création de ces sets étant élevé pour les entreprises, un soutien de l’UE est essentiel.;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;#NAME?;4 - Important;4 - Important;4 - Important;;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;3 - Neutral;"- Compléter la sensibilisation des PME aux avantages potentiels de l’IA par des formations sur les obstacles à surmonter lorsque l’IA est implémentée au sein de ces entreprises.
- Développer les centres d’essai virtuels, très importants pour les PME, en c";4 - Important;4 - Important;4 - Important;4 - Important;3 - Neutral;3 - Neutral;;Current legislation may have some gaps;;Yes;;Other;L’approche cumulative fondée sur le risque, consistant à prendre en compte le secteur dans lequel l’IA est utilisée, mais aussi l’utilisation qui en est faite au sein de ce même secteur, apparaît raisonnable. Mais les critères peuvent être vus comme encore assez flous, ce qui pourrait laisser craindre que la notion de « risque » soit interprétée de manière extensive. Des éléments complémentaires pourraient ainsi être pris en compte lors de l’évaluation du risque. ;Fonctionnalités modifiées par des systèmes d’IA : l’intégration de logiciels, notamment d’IA, dans certains produits et systèmes peut modifier le fonctionnement de ces derniers au cours de leur cycle de vie. ;4 - Important;4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);Enregistrement des données à la CNIL ou à des organisations équivalentes et usage contrôlé de ces données (qui peut les utiliser). ;Much;;No opinion;;Toute évaluation préalable de la conformité devrait reposer sur une évaluation de la conformité des processus d’élaboration des technologies ou algorithmes, et non pas sur une évaluation des produits finis. ;Mental health risks;;Yes;;No opinion;Le développement des usages en matière d’IA à ce jour semble pouvoir se satisfaire des régimes existants. Des nouvelles réflexions pourraient avoir lieu selon les évolutions que pourraient connaitre ces usages ultérieurement. ;Yes, for specific AI applications;Applications à haut risque;;Contribution_consultation_Livre_blanc_IA-_TECH_IN_France-_12-06-2020.pdf
F530134;12-06-2020 17:17;French;NGO (Non-governmental organisation);Nicolas;Brien;;France Digitale;479234015862-06;Small (< 50 employees);France;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;La nouvelle stratégie PME de la Commission devrait faire l’objet d’un réexamen au regard de la crise actuelle. Ce réexamen sera en outre l’occasion de lancer un appel à la mise en oeuvre urgente de l’indice «Europe Startup Nations» qui contient les conditions d’un régime attractif de: stock options, visas Tech et de financement pour les startups. Une meilleure harmonisation en ce sens serait cruciale pour construire l'écosystème d'excellence européen.;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"- Soutenir l’investissement des Business Angels :  chaque État européen se dote d’un dispositif d’incentives similaires aux instruments britanniques EIS et SEIS 
- Offrir des perspectives de sortie en développant des fonds de tech buy-out
- Appliquer un c";4 - Important;4 - Important;4 - Important;"La Commission devrait concentrer une partie des efforts et investissements à la coopération entre les startups et les chercheurs en intelligence artificielle. 
- Inclure dans la stratégie européenne des dispositifs performants de transferts de technologies entre les acteurs académiques de l’Intelligence Artificielle et les startups.
- Encourager les programmes qui font la promotion de l’entrepreneuriat auprès des étudiants et des chercheurs dans des laboratoires et centres de recherche.
";5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;4 - Important;4 - Important;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;;Current legislation may have some gaps;;Yes;;Other;France Digitale est favorable à l’introduction de nouvelles exigences réglementaires pour les applications les plus risquées, à condition que ce régime réponde au principe de proportionnalité et de sécurité juridique qui permettent le développement d’un secteur de l’IA compétitif en Europe. Nous soutenons l’approche de la CE selon laquelle une application d'IA devrait généralement être considérée comme étant à haut risque, si des critères cumulatifs de secteur et d’utilisation sont réunis.;;4 - Important;3 - Neutral;5 - Very important;4 - Important;4 - Important;3 - Neutral;No opinion;;Much;Ce label devrait impérativement être adapté aux startups. Traditionnellement, les industriels et géants du numérique ont plus de ressources pour obtenir rapidement les labels et certifications: ils en tirent un avantage concurrentiel considérable au dépens des startups. Il faut prendre en compte le coût des audits externes lors de l’élaboration des normes du label. La mise en en conformité ne doit pas représenter des coûts trop importants, au risque de pénaliser les startups et PMEs.;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;Faire reposer le régime de contrôle de conformité sur un système d’accountability, sur le modèle de la protection des données personnelles, plutôt que sur une évaluation de conformité ex ante qui risquerait de ralentir considérablement les avancées en matières d’IA, voire de les limiter aux grandes entreprises qui pourraient supporter le coût d’une telle procédure. De la même manière, des délais de mise en conformité devraient être prévus, en particulier pour les start-ups et les PME.;;;No opinion;;No opinion;;No opinion;;;
F530133;12-06-2020 17:15;English;Company/Business organisation;ludovic;AIGROT;;Nasdaq;Yes, 76648765687-29;Large (250 or more);Sweden;The feedback can be published with your personal information;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;As indicated in the Commission´s White Paper on AI, continued work in international fora to foster common views on the ethical use of AI is also an important aspect. Targeted actions to this aim could be useful.  ;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;3 - Neutral;;4 - Important;4 - Important;5 - Very important;;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;Exchanges have created junior markets on which startups can raise funds for growing, for instance in the Nordics, this is the case of the First North market. In addition to Initial Public Offers, companies can raise funds with secondary issuances. The EU is envisaging the creation of an EU IPO funds, it would be important that such a fund is flexible enough to support secondary issuances, especially given the market conditions created by the Covid-19 crisis. ;4 - Important;4 - Important;4 - Important;3 - Neutral;3 - Neutral;3 - Neutral;Any AI application needs clear & well-designed rules to minimize the associated risks. Nasdaq considers that consumers of AI do not necessarily face unique challenges & risks. The consumer protection for AI should be similar to other data based services in the financial sector, including classic investor protection, ensuring fair treatment & safeguards for data protection and against abuse & misselling. A technology neutral environment should also prevail with respect to the protection framework;Other;The current legislation may have some gaps. Authorities should in the first place try to apply to the fullest extent possible the existing protection rules to consumers of products based on AI.;Yes;;Yes;;AI applications endangering safety or able of limiting human rights.;4 - Important;4 - Important;4 - Important;4 - Important;3 - Neutral;4 - Important;;;Rather not;For non-high-risk AI applications, we consider that a labelling system, if any, should follow and implement general guidelines stemming from industry standards and practices.  ;Other enforcement system;For high-risk AI applications, a combination of ex-ante assessments, based on an external conformity procedure, as well as ex-post market surveillance could be warranted.;"The list of requirements for high-risk AI applications should be reviewed and updated timely and frequently (e.g. without the requirement for a Level 1 change of the regulatory framework) to keep up with technological innovation. The review of the criteria should take the form of guidelines published by supervisory authorities and could be updated on a more regular basis.
It is crucial that the necessary capacities are in place to assess the AI.";;Every AI provider needs to put in place sound internal processes (i.e. modelling, training of data, handling of critical/sensitive situations, handbooks, documentation, etc.) according to industry standards and practices.  Internal processes should be reviewed on a regular basis.;;It is important to highlight the differences between AI applications operating in “open systems” (e.g. road traffic) or “closed systems” (e.g. playing chess). In “open systems”, the AI does not possess the required ability to cover all eventualities, as the training data is by nature limited. In these cases, humans should be required as final decision-making actors. This would also be true for high-risk AI applications in “closed systems”.;;As indicated above, Authorities should try to apply to the fullest extent possible the existing protection rules to consumers of products based on AI. Regarding product liability, legislation is not deemed necessary but some regulatory guidance could be beneficial.;;;;
F530132;12-06-2020 17:13;English;Academic/Research Institution;Mechteld;Bous;;Radboud AI, Radboud University Nijmegen;806205726728-39;Large (250 or more);Netherlands;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"Provide dedicated, long-term funding for fundamental research into the foundational aspects of AI. Only then can Europe become a world-leader in AI.
Education is an important task of the public sector that can gain much from AI. We are missing an explicit mention of the need for the uptake of AI in this public sector.
";5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;"The EU should increase financing of excellent fundamental AI research.
The promotion of uptake of AI by the business and public sector should only be done with the right safeguards in place (ethics and security) and should be preceded by law; i.e. the EU should promote the ‘careful’ uptake of AI.
Promoting the uptake of AI in these various sectors should not be done blindly/for its own sake. One should always ask if it is really necessary, and (truly) socially beneficial 
";3 - Neutral;5 - Very important;5 - Very important;"The EU should provide dedicated, long term funding for fundamental AI research.
We do not consider a lighthouse research centre essential for a successful implementation of AI in Europe. If it does come about, we endorse the CLAIRE vision that a lighthouse research centre should not drain talent from existing centres.
Next to the focus on industrial uptake of AI, the public sector has to be included as a relevant testing place, e.g. for including the human-rights perspective.";3 - Neutral;5 - Very important;5 - Very important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"We are missing the following risks regarding the deployment and application of AI:
•	Risk of exclusion, i.e. exclusive access to AI technologies. Training and (re-)skilling can help to ensure inclusion.
•	Risk of outsourcing public services to private partners (who serve different interest) and the associated risk of monopolization
•	Impact on the Quality of Life (e.g. fulfilling employment, frustrations of engaging with machines vs. humans, dependency on technology)";Other;"
We think that additional rules for AI are probably needed. Sector-specific rules seem more appropriate than cross-sector rules.
We need to understand the impact of AI on liability, safety, protection first. AI is part of a more encompassing socio-technical system that changes the system in which it operates: social norms may be impacted on what is deemed acceptable (in whatever sense) as well. 
";Other;The identification of high risk and low risk will turn out to be very problematic, lacking a contextual dimension (only two exceptions mentioned). We support and strengthen the argumentation of ALLAI (see attached) that it will be very difficult to identify any low risk sector that may be void of high risk AI applications. The approach adopted by the EC seems to be one that relies as much as possible on already existing structures but wrongly assumes a kind of product-style essence of AI.;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;The definition of biometric identification needs to include (automatic) speaker recognition (identification by voice) here, since voice differs in uniqueness from e.g. fingerprints and DNA.;No opinion;;No opinion;;"The ‘notified bodies’ that are mentioned lack the expertise and competences to assess the risk of an AI application, which will impact any kind of conformity assessment. The Commission seems to presuppose that this will be solved by the notified bodies themselves.
Issues regarding data selection and data synthesis as part of the data management in AI projects need further attention in the white paper. The same holds for error analysis into false positives and false negatives of AI systems.
";Mental health risks;;Yes;;Yes;Liability aspects specific to the health care system that employ continuous learning need to be elaborated.;No opinion;;As mentioned above, we think that the approach to AI as a kind of product that requires the usual liability, safety and consumer protection is too limited, as it misses considerations as to how AI will impact the system (social norms) within which it operates. How to understand liability, safety and consumer protection for AI if we do not understand how AI will impact upon these categories?;General_response_on_the_EC_White_Paper_on_AI_by_Radboud_AI_incl_FdR.pdf
F530131;12-06-2020 17:12;English;Consumer Organisation;Ernani;Cerasaro;;BEUC - The European Consumer Organisation;9505781573-45;Medium (< 250 employees);Belgium;The feedback can be published with your personal information;5 - Very important;4 - Important;5 - Very important;No opinion;No opinion;No opinion;;5 - Very important;5 - Very important;No opinion;No opinion;4 - Important;4 - Important;;No opinion;No opinion;No opinion;;No opinion;No opinion;No opinion;No opinion;No opinion;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;AI and Algorithmic Decision Making (ADM) may endanger consumers not only when it comes to their fundamental rights. AI is changing consumer products and services. Attention should be paid to the use of algorithms for monitoring and influencing consumers' behaviour and activities. There are also concerns relating to the transparency and explainability of algorithms, increase of power asymmetries, difficulty to monitor legal compliance, societal and environmental impact of AI tech, etc.;There is a need for a new legislation;;No;;;;We are concerned that some risky applications for consumers would fall outside the high-risk category. Algorithms can be used to exploit consumer’s weaknesses and biases in order to convince them to purchase products limiting their choices. In this sense, the potential damage suffered by a consumer or by a group of consumers can be very high. However, the White Paper seems not to embrace “everyday” activities such as shopping activities within its definition of high-risks.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;Considering the high risk of abuse, discrimination and violation of fundamental rights to privacy and data protection, the EU must develop a strong, privacy-protective approach for biometrics systems before they are largely used in public spaces. ;Rather not;Labels are as good as the requirements and enforcement systems they are based on. If there are no mandatory rules behind them, labels would bring very limited benefits. Once clear legal rules and enforcement mechanisms will be in place, the role of a trustworthy label could be considered. Another important element to consider is the inherent information asymmetry in the AI context, making the role of a label different and more complicated than in other sectors.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;A combination of independent ex-ante verification mechanisms and continued ex-post compliance checks by authorities during the use phase are necessary in presence of high-risk applications. Reliance on self-assessment of compliance by operators should in principle be avoided, at least for high risk applications.;Mental health risks;;Yes;;Yes;The Product Liability rules are not fit for consumers in the digital age and need to be updated. For more details on the necessary changes please see BEUC's recent position paper on ‘’Product Liability 2.0’’available on our website.  ;Yes, for all AI applications;;The need for targeted changes in liability rules based on a risk-based approach should be considered carefully. When it comes to liability, such an approach is problematic from a consumer perspective. In particular, as liability rules mostly intervene downstream, it doesn’t matter whether the product that caused the harm was classified as low- or high-risk. What truly matters for consumers is fair compensation for the damage suffered, regardless of what type of risk ultimately materialised.;
F530130;12-06-2020 17:11;English;Business Association;Louis;DANCOURT;;The European Tech Alliance;189607519323-76;Micro (< 10 employees);Belgium;The feedback can be published with your personal information;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;It is of paramount importance to address EU’s skill gaps in A.I. The Commission should boost the emergence of AI training modules, encourage special residence permits for AI specialists coming from third countries, promote attractive university programmes and bring girls or women to these fields of expertise, and create public-private chairs like the Joint Industry-Academia Postdoctoral Fellowships which exist in the United States. ;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;It is urgent to boost the emergence of AI training modules across the EU and facilitate researcher mobility between the public and private sectors. It will also be crucial to identify a common approach for public-private partnerships (differently addressed in Member States today) to nurture the network of existing AI research centers. The public-private partnership for industrial research shall allocate Intellectual Property rights on the private partner or give extensive exploitation rights.;3 - Neutral;5 - Very important;5 - Very important;EUTA members are supportive of building up a voluntary AI platform serving as a central point to gather and provide access to all publicly funded AI related knowledge and algorithms. For publicly funded datasets to be accessed by a wider community of users, the French Etalab is a good initiative that could be replicated across EU Member States and potentially merged to optimise common resources.;3 - Neutral;4 - Important;5 - Very important;5 - Very important;4 - Important;;4 - Important;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;The above factors are already tested for in the development phase and continuously. It is vital to consider the context in which AI is used and how these factors might arise (the importance of inaccuracies/discriminatory outcomes will vary significantly depending on the use case). A proportionate risk-based framework requires periodic reviews of these factors to understand the scale of a problem. Based on a situational assessment, any new requirements should only apply to high-risk application.;Current legislation may have some gaps;;Yes;;Yes;;The distinction between high and low risk applications should be made clearer to ensure an effective application and avoid putting SMEs in an untenable situation. The sector list and the application areas should be focused to address prevalent problems and avoid over-spill to low-risk applications. Considering any practices affecting consumer rights as a high-risk application is too broad and doesn’t help making a clear distinction between high and low risk applications.;3 - Neutral;3 - Neutral;4 - Important;4 - Important;4 - Important;3 - Neutral;No opinion;"We believe that face recognition should not be treated as high risk application when they are not used in public spaces. Some biometric recognition tools can present low-risk applications. For example: the identification and verification of the customer accordingly to anti-money laundering laws or for body scans before purchasing outfit online.
";No opinion;More clarity on the future effects and the functioning of a voluntary labelling scheme is needed. It should not require compliance with an identical set of requirements as for high-risk applications, but rather a set of requirements defined more narrowly and sufficiently flexible to take into account various use cases. This will help making the label accessible to SMEs, start-ups and scale-ups. A single EU point of contact should be created to avoid regulatory fragmentation.;Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;As a general principle, we understand and support the importance of effective regulatory enforcement. However, the options listed above require more thought and discussion. Requiring an assessment by an external authority would be highly burdensome for SMEs and delay innovation getting to the market. High-risk applications must be easily identifiable through clear and narrow definitions and criteria, without any ex-post judgement by of third parties (courts, independent organizations etc.).;Personal security risks;;No opinion;While some EUTA members can address any risks inherent in their software on a rolling basis, others (e.g. working in e-commerce) can’t realistically address risks inherent in AI products sold to consumers, especially if the product’s software is updated by the manufacturer subsequent. We therefore would like to ask for clarity on the allocation of responsibility for any harm caused across the AI life cycle and want to ensure that undue responsibility is not placed on intermediaries. ;No opinion;;No opinion;;Please refer to addendum paper. ;European_Tech_Alliance_-_Addendum_Paper_to_EU_Public_Consultation_on_AI.pdf
F530129;12-06-2020 17:07;German;Business Association;Felicitas;von Bredow;;German Association of Industry and Commernce;;Large (250 or more);Germany;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;2020-06-12-DIHK_Stellungnahme_zum_EU_Wei_buch_KI.docx
F530128;12-06-2020 17:05;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;;No opinion;No opinion;No opinion;;4 - Important;No opinion;4 - Important;4 - Important;4 - Important;;3 - Neutral;4 - Important;4 - Important;3 - Neutral;3 - Neutral;3 - Neutral;;No opinion;;Yes;;Yes;;;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;No opinion;;No opinion;;Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;;;;Yes;;No opinion;;No opinion;;;Vodafone_Group_Response_AI_White_Paper.pdf
F530127;12-06-2020 17:02;English;NGO (Non-governmental organisation);Vanja;Skoric;;ECNL- European Center for Not-for-Profit Law Stichting;052513138393-62;Small (< 50 employees);Netherlands;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;2 - Not important;2 - Not important;Include civil society into the above-mentioned efforts by integrating a participatory approach throughout EU AI efforts. Partnership with civil society should in particular include those that work on human rights and represent vulnerable and marginalized groups, before, during and after development and deployment of AI applications in order to gain a holistic understanding of its effects on society and prevent fundamental rights violations. This partnership includes providing resources to do so.;4 - Important;3 - Neutral;2 - Not important;3 - Neutral;3 - Neutral;3 - Neutral;AI literacy for public should be added. As AI will impact numerous aspects of life, AI literacy should help develop a basic understanding of what AI is, technological and social aspects, how AI works and how it is currently playing a role in our daily lives, in addition to the potential future ones. The objective is to eradicate the misconceptions around AI and to create an all-inclusive ecosystem where the public is empowered with basic skills needed to discuss and pursue further learning. ;3 - Neutral;4 - Important;;Priority should also be given to strengthening networks between AI research centres and human rights organisations and civil society community, to embed human rights impact assessments in research and innovation. Research priorities should include the human rights and societal implications of the development and use of AI, fairness design, discrimination risks and transparency. ;3 - Neutral;3 - Neutral;3 - Neutral;4 - Important;3 - Neutral;EU should include civil society stakeholders in promoting partnerships, as civil society can help bridge the gaps between industries, academics, institutions and the public, especially the vulnerable communities that might get left behind with AI technology. This is also in line with Sustainable Development Goals promotion and implementation that EU has subscribed to.  ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;"Additional concerns include AI environmental impact; AI impact on particularly vulnerable groups or communities; AI used in sensitive areas (public services) without democratic oversight, transparency or sufficient evidence to justify the need/ purpose. ";Other;Current EU regulatory framework is insufficient for AI purpose and needs adapting EU must develop and promote a rights-based approach, then within the rights framework there can be graduation of risk. Legislation must strengthen, not replace, GDPR. ;Other;The risk-based assessment should be preceded by a human rights-based assessment. The determination of the level of risk (“high” or “low”) should not be based on the specific sector in  which he AI application is developed, used and implemented.  Any risk assessment of AI must centre on the potential harm for the individual as well as for society at large, and must follow clear and transparent rules.  Assessment should also be conducted with the inclusion of relevant civil society groups.;;;In general, any impact on human rights and fundamental freedoms is most concerning. Current AI applications highly concerning are biometric processing ones and data profiling of individuals to identify political opinions, activisms, or similar exercise of different rights and freedoms. Problematic is potential use of data profiling apps by governments to deny registration to associations. EU should proactively ban AI in areas where the fundamental rights and societal impact are at risk.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;"The use of biometrics for remote identification in publicly-accessible spaces significantly
contributes to unlawful mass surveillance so should never be deployed.";Not at all;We strongly caution against incorporating voluntary, self-regulatory and ethics based approaches in AI regulations. Such approaches provide scope to cirucmvent accountability and soften fundamental rights obligations. Further, they reduce certainty and impede access to justice for those harmed. ;Other enforcement system;Should a labelling system be adopted, it should be mandatory, not voluntary, and based on rigorous human rights impact assessments. The labelling should also be periodically reviewed by repeating the impact assessment throughout the lifecycle, use and implementation of the AI-based application. All impact assessment and labelling procedures should mandatorily include participation of relevant civil society organisations and groups potentially affected by the use and implementation of AI.;Aside from including relevant civil society organisations and affected groups in the labelling and impact assessment procedures, we support setting up independent multi-stakeholder centres of expertise on AI. It is crucial to have independent bodies to monitor, assess and report on the societal and rights implications of the use of AI and that provide advice to regulators, government and industry according to public interest criteria. These centres should involve academic experts and CSOs.;Mental health risks;;Yes;;No opinion;;Yes, for all AI applications;;We believe national liability rules should be adapted and harmonised to ensure proper compensation and allocation of liability for the operation of AI. However, when establishing standards for liability, we recommend taking account of the findings of the Council of Europe Study DGI(2019)05, (https://rm.coe.int/responsability-and-ai-en/168097d9c5). See more in our contribution.;ECNL_Stichting_EU_AI_White_Paper_submission.pdf
F530126;12-06-2020 16:59;English;Company/Business organisation;Antoine;LARPIN;;Panasonic Europe;31253607851-84;Large (250 or more);Netherlands;The feedback can be published with your personal information;5 - Very important;3 - Neutral;4 - Important;No opinion;5 - Very important;5 - Very important;It is also important to ensure that the proposed legal framework for high-risk AI applications provides legally certain, fully harmonised, implementable and enforceable rules. This includes the development of a clear definition of AI as well as of a clear assessment-method of high-risk applications (please refer to Panasonic below comments on section 2). Panasonic  also welcomes the EU AI public policy to support the availability and access to qualitative data sets to train AI systems in Europe.;5 - Very important;4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;"Enhancing the coordination between EU Member States is important. The EU (digital) Single Market remains the main EU asset to deliver an environment of excellence and trust. Therefore, the White Paper’s follow-up actions should be designed as to ensure a truly functioning and competitive EU Single Market for AI (and consequently avoid each EU country to develop its own public policies and laws). 
Panasonic also welcomes the EU AI policy to support the availability and access to data sets. ";5 - Very important;5 - Very important;5 - Very important;"Centralisation of AI research and attracting the best researchers is of importance to strengthen innovation and support EU future needs. This also allows to set up direction and clear priorities. 
We support more standardisation activities in favour of AI technologies (in line with international developments). 
We support funding support to develop data sets (make data available and accessible).";5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;While individual Digital Innovation Hubs are important to ensure each member state participates equally in the advancement of AI, it is also strongly recommended that some encouragement or requirement for collaboration across Hubs occur, for efficient cross-pollination of knowledge discovery.;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;"Building trust can only be achieved with legally certain, fully harmonised, implementable and enforceable rules. This includes having a clear definition of AI and a clear assessment-method of high-risk applications. 

We support the White Paper approach to focus on specific use of AI systems and on high risk applications. ";There is a need for a new legislation;;Yes;;Yes;;"Despite their contribution to greater safety, security, and efficiency, we agree that remote biometric identification solutions are high-risk applications. Legal uncertainty exists today amongst local regulations defining the use of biometric identification for country safety (incl. against pandemics) and security purposes. Harmonisation would be beneficial. 
Legal certainty to distinguish between high-risk and non high-risk AI application is essential.  
";5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);"In addition to above question, for biometric identification systems: 
-	they should be allowed in publicly accessible spaces with a clear legal framework defining standards and conditions; 
-	EU Regulation should  address the lack of ethical framework for the choice and use of algorithms.  EU Guidelines for trustworthy AI can be turned into a mandatory legal requirement. A strong framework for labelling, standardisation and governance (e.g. third-party audits) is necessary; 
-	The European Data Protection Board should provide guidelines on video recording for data processors. ";Much;"A pan-EU voluntary labelling system would help to build an environment of trust. As for any successful voluntary labelling system, it requires a trusted and sound governance, enforcement and public recognition.  
The future EU regulatory framework may want to refer to relevant widely accepted international standards (e.g. ISO/TR 22100-5 “Safety of machinery — Relationship with ISO 12100 — Part 5”, IEC 62998-3 Safety of Machinery Safety-related sensors used for the protection of persons[...]).";A combination of ex-ante compliance and ex-post enforcement mechanisms;;"The evaluation of AI systems would need to respect: 
-	the intellectual property of the AI systems developer / integrator / vendor; 
-	cost efficiency  ; 
-	specific AI technologies deployed. 
Panasonic encourages the EU and Japan to adopt a mutual recognition mechanism for AI systems. 
Harmonized software defined safeguards can help to provide a balanced flexibility for AI deployment considering products and services. ";Risks related to the loss of connectivity;"We see a need for more legal certainty re.: 
-	Risks related to malfunction of connected other security systems (access control, fire alarm)
-	Risks related to databases (databases need to be secure (encrypted) 
-	Risk of linking databases (biometric data as unique identifier could enable to follow a person through different databases if all these databases contain biometric data).
We also observe an increase of the speed in which risks can impact individuals and society. ";Yes;"Current product placement regulations should be preserved when introducing an EU regulatory framework for AI. 
AI systems should undergo a regular lifetime assessment procedure with time to time checks. 
Machine learning training has the potential to introduce systematic faults. The data collection and learning system should be developed according to well specified safety standards, with attention given to reducing hazards (e.g. unintended bias). ";Yes;For the growing number of products involving hardware and software, there is actually no clear rule for allocation of liability. In case of damage caused by software, the producer may seek recourse from the software provider on grounds of warranty. However, it is subject to strict time limits. Therefore a joint and several product liability of hardware/product manufacturers and software providers for products that have mainly software driven purposes and risks should be codified. ;Yes, for specific AI applications;National legislation should consider expanding product liability to software as such, independent from the product it is integrated in. This should apply in particular to AI software whose malfunctioning could cause damages to life, health, or property of the user or third party.;A mandatory product liability insurance granting the damaged person a right to directly claim to the insurer could be considered. Everyone who puts AI products (software and finished goods) on the market could be required to take out such insurance. It could be further considered to limit this to certain sensitive purposes/deployment of AI.;
F530125;12-06-2020 16:56;English;Business Association;Caroline;BEDRAN;;AENEAS;8508278146-57;Micro (< 10 employees);France;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;"AI is the new opportunity as well as the new challenge in Europe. Technological leadership in AI requires mastering of new digital technologies, in particular new computing architectures, software algorithms and semiconductor production. This calls for a deep, sustained partnership on Key Digital Technologies (KDT) beyond the current levels of cooperation up and down the value chain. The KDT partnership proposal will be available on the EC website on “European Partnerships in Horizon Europe"".";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Keep up with developments in the future KDT partnership;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
F530124;12-06-2020 16:55;English;Company/Business organisation;Aleksandra;Appelfeld;;Philips;035366013790-68;Large (250 or more);Netherlands;The feedback can be published with your personal information;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;Securing access to data and computing infrastructures: it is important to improve access to data to enable the development of AI. Data localization restrictions must be removed. Investment (e.g. Digital Europe Programme) in infrastructures and computing technologies is necessary. For the health sector, it is crucial to support the compilation and curation of high quality datasets for scientific research and development. International cooperation is necessary for regulatory alignment.;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;"Support is needed for federated learning and training of AI systems and cross-border innovation in health data; also with trusted parties outside the EU, and multi-language semantic interoperability to be able to pool resource cohorts across Member States.";4 - Important;5 - Very important;5 - Very important;"Innovation-friendly policies and instruments to translate R&I into new opportunities; Specific healthcare mission in Horizon Europe R&I framework program aimed at integrated care; Level playing field through continuation of EU funding for companies of all sizes; Risk-proportionate, predictable, science-based policies and regulations that do not hamper competitiveness and innovation;Promotion of AI uptake by enabling trust, communicating benefits for society, putting risks in appropriate context ";4 - Important;4 - Important;4 - Important;4 - Important;No opinion;"Digital Innovation Hubs should be strengthened further and foster collaboration within the whole ecosystem, including SMEs, start-ups and larger companies. As such, they would best contribute to the creation of the level playing field in order to ensure fair competition in the EU single market.
Larger companies can play an important role in supporting the uptake of innovations from SMEs to get integrated into the entire health continuum.
";4 - Important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;4 - Important;Concerns of ‘over trust’ (too confident about AI functions) and ‘under trust’(too pessimistic, ignoring potential benefits,exaggerating threats and risks of AI).Trust should be balanced and proportionate with the evidence for the validation of AI tools.All concerns need to be taken seriously and addressed properly.AI applications need proper validation over exact explanation of the algorithmic functioning.There is a concern about possible inhibitors of AI innovation by being over cautious.;Current legislation is fully sufficient;;Other;Healthcare is a well-regulated industry.AI solutions may be separate products/be built in medical devices(MD).When they constitute a MD,applicable MD regulations would apply.Existing regulations generallycover AI appropriately.Introducing AI specific legislation should be done very carefully not to create conflicting regulations and new barriers for development of AI-supported MDs.If gaps are clearly identified,they should be addressed as much as possible in existing regulations for consistency.;;;;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;No opinion;;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;In addition to comments above, regulatory approach in healthcare is risk-based (e.g. medical devices framework). Risk classification and ex-ante conformity assessment (also during the entire lifecycle of the device) are the fundamental principles and are even  reinforced under the MDR. These rules should apply to software (with embedded AI) as medical device. GDPR also applies to AI(any processing of data through algorithms) and requires to ensure compliance with its principles.;;;No;As stated above, in the health sector, the regulatory approach is risk-based (e.g. medical devices framework). Risk classification and ex-ante conformity assessment (also during the entire lifecycle of the device) are the fundamental principles and are even reinforced under the MDR. These rules apply to software (which incorporates AI) as medical device. No new rules for assessment are needed.;No;Principles of PLD have been proven in last decades and fit for use/analogy in diverse situations. Many liability aspects are already included for medical devices and explicitly reinforced (MDR Art 10.16). Under the GDPR, the principle of accountability requires to ensure compliance and ability to demonstrate it.Already now, under current legislation, all potential risks that the use or creation of algorithms can pose to the rights and freedoms of persons must be considered and properly addressed;No;;Vendors will be responsible for the correct functioning of the product according to existing legislation and regulations. ;Accompanying_document_AI_White_Paper_consultation.pdf
F530123;12-06-2020 16:53;English;Other;Evgeni;Aizenberg;;Academic researcher;;Micro (< 10 employees);Netherlands;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;Considering the impact of AI on the nature of work, in particular with regard to automation, it is essential to equip and retrain workers with the skills necessary for a smooth transition from jobs that may go away, as a result of introduction of AI system, to other existing jobs or new jobs that may emerge.;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;Incentivize socio-technical multi-disciplinary research and innovation that will promote the development and use of responsible, socially-aware design methods.;2 - Not important;5 - Very important;5 - Very important;Instead of creating a new independent lighthouse research center, in my view it would be more effective to unite the existing centers in an inter-dependent network and promote that network as the European lighthouse for such research. This would facilitate exchange of knowledge and resources across different member states.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Help to raise SME’s awareness about potential risks of AI. And coupled with that, help promote knowledge transfer on responsible, socially-aware design methods. ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;AI can have disruptive impact on the job market by changing the nature of work in many sectors through automation. ;There is a need for a new legislation;;Yes;;Other;The approach should be more explicitly grounded in human rights by making human rights impact assessments part of the process to determine the level of risk. ;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Other special requirements in addition to those mentioned in the question above should be imposed (please specify);Considering the capabilities of this technology, an in-depth examination of legitimate exceptions, safeguards, and oversight is required together with stakeholders from various disciplines and civil rights organizations. Therefore, I support the Commission's call for a broad European debate on this topic.;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;The listed mandatory requirements do not explicitly cover the following critical features (explained in detail in the attachment): performance of human rights impact assessments, individuals’ autonomy over self-representation, and justification and contestation of AI-based decisions.;Mental health risks;;Yes;;Yes;AI-based decisions must be explainable in a way that is legible to individuals who are impacted by these decisions. This is essential for the ability of individuals to contest AI-based decisions. Furthermore, the responsibility for legible justifications has to be placed on producers and operators of the AI system.;Yes, for all AI applications;;;
F530122;12-06-2020 16:51;French;Company/Business organisation;Blandine;EGGRICKX;;Le Groupe La Poste;01890906437-84;Large (250 or more);France;The feedback can be published with your personal information;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;Des partenariats dans l’éducation afin de faciliter les liens entre l’école, les laboratoires de recherche et les entreprises privées. ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Comme prévu dans la stratégie européenne pour les données, il convient de noter qu’harmoniser les différentes politiques de partage et d’accès aux données pourrait représenter une opportunité au niveau européen. Les espaces européens de données tels que prévus par la Commission viendraient nourrir l’intelligence artificielle. Cette dimension qui fait partie intégrante de la stratégie globale de la Commission en matière de données devrait également être prise en compte ici.;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;"En tant que partenaire des PME, notamment via son programme French ioT, La Groupe La Poste soutient l’innovation et les entrepreneurs. Nous pensons que : 
Plus que fournir des informations sur les modes de financement aux start-ups, il conviendrait de les accompagner.
Une taxation avantageuse pour les PME qui souhaitent développer des produits fondés sur l’IA pourrait également être envisagée.
Ces pôles pourraient également traiter la problématique de l’empreinte écologique de l’IA
";5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;Veuillez vous référer à notre document joint, point 1.;Current legislation may have some gaps;;Other;Veuillez vous référer à notre document joint, point 2.;;;"-	Les diagnostics / scoring qui guident la décision humaine dans des domaines variés comme la santé, la justice, l’orientation scolaire, le scoring social ou encore dans le domaine de la sécurité. Dans domaines sensibles, la délivrance d’un résultat par u";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);;Much;Un label transsectoriel serait pertinent. Ce label permettrait principalement d’informer les utilisateurs des systèmes d’IA, notamment les consommateurs, pour leur permettre de faire des choix éclairés. Pour qu’il soit mis en place par le plus grand nombre, il conviendra de mettre en place un cadre incitatif (mise en valeur des entreprises ayant adopté ce label).;Other enforcement system;Il serait intéressant de prévoir un système d’évaluation qui permette la prise en compte de l’avis des utilisateurs de l’IA. Les associations de consommateurs auraient donc un rôle tout particulier à jouer.;"Afin d’assurer une stabilité, la grille d’évaluation de la conformité devra être valable pour plusieurs années et ne pas être modifiée trop fréquemment.
Elle devra tenir compte des avancées technologiques à venir (exemple : l’informatique quantique), de la qualité, la richesse et la quantité des données disponibles dans le futur. Cette grille devrait être intersectorielle, applicable de la même manière à tous les acteurs, qu’il s’agisse de son élaboration, son utilisation ou sa maintenance. ";Personal security risks;;No;Veuillez vous référer à notre document joint, point 4.;No;Veuillez vous référer à notre document joint, point 5.;No opinion;;;Contribution_le_Groupe_La_Poste_consultation_UE_IA.pdf
F530121;12-06-2020 16:49;English;Company/Business organisation;Andreas;Brunsgaard;;Confederation of Danish Industry;5749958415-4;Large (250 or more);Denmark;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"- Promote a clear definition of AI, along the line of the proposed definition by the HLWG, though with certain modifications.
- Ensure the respect by Member States of internal market  principles in relation to circulation of goods (no technical barriers t";5 - Very important;3 - Neutral;5 - Very important;4 - Important;5 - Very important;5 - Very important;"- Adapt an ambitious EU Multiannual Financial Framework in which funding for notably Horizon Europe and Digital Europe Programme are prioritized.
- Promote public funding in research focusing on AI applications for industry.
- Invest in high-speed infrast";3 - Neutral;5 - Very important;5 - Very important;;5 - Very important;4 - Important;5 - Very important;5 - Very important;4 - Important;"SMEs & start-ups need to be supported in developing, accessing and using AI. Their variety and divergence in terms of digital literacy, sector of activity and size create different needs. Digital Innovation Hubs should provide point of contacts as well as services and tangible support in SMEs’ transformation, incl. helping assess what technologies to adopt & advising how to implement them. Legal certainty and simple rules are key, within a proportionate principles-based regulatory framework.
";4 - Important;4 - Important;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;All of the listed concerns are valid. However, the level of concern is directly linked to the context in which AI applications are used. AI used in production facilities pose quite different risks, if any at all, than AI used in for instance the healthcare sector where patients are directed effected by the decisions of the AI. ;Current legislation may have some gaps;;Yes;;Other;Our support for the proposed risk-based approach is based on the premise that the criteria as to what constitutes high-risk is defined in a such a way that only truly harmful AI applications fall under the scope. At present this amounts to a limited number of AI applications. To provide legal certainty, an exhaustive list of specific applications covered should be created.;;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;No opinion;DI has no opinion on the use of biometric identifications systems in public spaces. It is a strictly political decision as to whether, and under what circumstance, biometric identifications systems should be used in public spaces. However, on a more general note, from an industry perspective, one should never in advance rule out the use of certain new technologies as long as we use it responsible, as we might not yet fully understand what economic and social benefits such new technologies may bring to society and industry.    ;Rather not;One should be careful introducing too many and specific labelling schemes – especially in related areas. Consumers and customers etc. have difficulties differentiating different labels. Moreover, businesses are forced to prioritize 1) space in marketing to display the labels and 2) resources to comply with the requirements of the labelling scheme. Like in Denmark, the Commission should instead consider establishing a broader labeling scheme focusing on data and digital responsibility -incl. AI. ;Other enforcement system;If, at some point an enforcement scheme was to be put in place, the EU has a long history of relying on self-assessment via the NLF scheme (CE-marking) to ensure free circulation of goods within the EU Internal Market. Self-assessment would keep the market open for European SMEs, to which certifications or audits may prove too challenging both in economic and administrative terms. In certain cases, self-assessment could be complemented with ex-post requirements.;If high risk AI applications are defined as those who truly pose a risk of significant harm (which will be a limited number), then if follows that external conformity assessments are needed.  ;Cyber risks;DI is in favor of a horizonal cyber security regulation based on NFL principles.;No opinion;One could consider to require manufactures of AI embedded products that undergo substantial modifications during their life cycle - either to due to machine-learning capabilities or software updates - to include an assessment in their risk assessment as to how long the risk assessment is valid for, and when, and under which circumstances (substantial modifications), a new risk assessment will need to be carried our in order to reflect the nature and risks of the modified product.;No opinion;;No opinion;;If EU legislation regarding liability law and procedural law is deemed necessary - which we are not convinced - then due consideration must be paid to the national regimes and their specificities. Therefore, the starting point should be a more detailed study as to the applicable national regimes, to identify whether or not “liability gaps” do indeed exist. In any case, there is a need for a close monitoring of developments in Member States and an EU coordination.;
F530120;12-06-2020 16:48;English;NGO (Non-governmental organisation);Roderick;Beudeker;;World Economic Forum's Centre for the Fourth Industrial Revolution;049060636194-17;Large (250 or more);Netherlands;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;There is a need for a new legislation;;;;;;;;;;;;;;;;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;;;;;;;;;;Briefing_Paper_-_Employment_Law_and_AI-Based_Recruitment-no_images.pdf
F530119;12-06-2020 16:44;English;Company/Business organisation;Erik;Dahlberg;;Scania CV AB;3305029916-47;Large (250 or more);Sweden;The feedback can be published with your personal information;3 - Neutral;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;"- Investment in and promotion of key technologies in the field of ""trustworthy"" AI are welcomed
- A strong collaboration between large European companies and public sector should be considered, as this could foster the uptake of AI in Europe
- Looking at ";5 - Very important;4 - Important;4 - Important;3 - Neutral;4 - Important;4 - Important;#NAME?;3 - Neutral;4 - Important;4 - Important;- Set up large scale EU research missions that will mobilize industries' efforts and with set up in different parts of Europe. E.g. sites for autonomous vehicles (AVs) at large freight terminals or mines and longer cross-border transport corridors with AV;4 - Important;4 - Important;4 - Important;4 - Important;No opinion;- While it is important to support the development of AI expertise for SMEs, there is a need to clarify what is meant by knowledge transfer;4 - Important;4 - Important;4 - Important;4 - Important;3 - Neutral;3 - Neutral;"- While all these concerns shouldn't be underestimated, they can result also from human actions and decisions: the potential of AI use is precisely that of assisting in making such threats measurable
- AI in vehicles has to, and will be, so accurate that ";Other;"Current legislation may have some gaps, but
- to ensure that an AI framework doesn't duplicate/invalidate existing certification requirements/regulatory frameworks, it should be carefully considered where these and industry standards are better instruments to address gaps
- different AI applications may require diversified regulatory intervention, hence a specific assessment of AI use-cases is prerequisite for the identification of any regulatory intervention";Yes;;Other;"In general, the high-risk sector approach and the classification of transport sector as high-risk are inadequate, since this may hamper AI development in the sector
There is a need to define AI more clear and strict. From the white paper, it can be interpreted as autonomous vehicles are per se defined as AI. However, as long as ""rational AI"" is used in the development and the vehicles are not ""learning rational AI systems"", they should fall out of at least the high-risk definition.";- Learning rational AI systems, that adapt its algorithms within a very wide, multi-dimensional and non-transparent space and that makes automated decisions with impact on human rights and safety;2 - Not important;4 - Important;3 - Neutral;4 - Important;2 - Not important;4 - Important;Other special requirements in addition to those mentioned in the question above should be imposed (please specify);"Bio-metric identification systems are covered by the General Data Protection Regulation and the processing of such data for uniquely identifying purposes is forbidden. Facial recognition can only take place if it falls under the scope of a listed exemption. 

The use of bio-metric data with machine learning is technically rather new and application-wise only marginally explored. Face recognition may be needed in autonomous vehicles for maximizing safety by determining the intention of other road users such as pedestrians and other drivers. A careful hand-in-hand of regulatory and technical development is required. Using an ethical principle framework can serve as a preparatory approach for regulation.";Not at all;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;Assessment of compliance should be carried out ex-ante preferably through third party. The existing third party type approval system in automotive industry works well and could be applied here. In particular for vehicles that use AI during development but still those vehicles do not self-learn in traffic, i.e. learning rational AI systems. ;;"- There is a need to distinguish between vehicles that “self-learn” during driving and those that may have used AI in the development but do not self-learn while driving (rational vs learning rational systems)
- In the automotive sector, cyber security an";Yes;In Automotive sector, when OEMs introduce new SW and HW in vehicles on the field, those should undergo the procedures of Software Updates (or new approval if considerable changes that affects performance covered by current approval). Third party SW or HW changes affecting safety functions must be prohibited. ;No;"-Current PLD is efficient as it provides both legal certainty and compensation to consumers: OEMs are held liable because of a defective AI-based product and can later call upon their supplier
- PLD is a horizontal regulation covering many sectors, any as";No;;"The current liability regime is based on a fair distribution. It is important that liability continues to be adequately allocated and that national law is governed/influenced by European law. 
There is not either any reason to have specific compensation policy as long as the system meets its requirements.";
F530118;12-06-2020 16:40;English;Company/Business organisation;Aurelie;Caulier;;Zalando SE;877966419254-70;Large (250 or more);Germany;The feedback can be published with your personal information;3 - Neutral;5 - Very important;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;Building research communities and skills are rightly-identified priority areas. Unnecessary visa restrictions for non-EU nationals should also be a priority area for action. It is a current obstacle to the recruitment of the best worldwide talents (ex: from the US or China). To tackle the current scarcity of adequately trained AI researchers and to build world-class research capability in Europe, the EU should facilitate the entry of non-EU nationals to study and work in AI in the EU. ;4 - Important;3 - Neutral;4 - Important;3 - Neutral;5 - Very important;3 - Neutral;The pioneers of AI in the 1990s and 2000s and today’s young generation of AI leaders were self-taught in AI. They learnt AI skills coming from other study fields or through their own academic research and networks. Today, it is still not adequate that so few universities propose study programs in Data Science or that AI skills are acquired on the job. Creating academic curricula for AI should be a top priority. It is the foundation for AI innovation in Europe.;3 - Neutral;5 - Very important;3 - Neutral;The development of  applied AI research today is the result of research projects conducted in public research institutions or private research labs over the past 10 years, across Europe. In order to strengthen knowledge sharing within the AI community, the EU could further support the establishment of pan-European networks of AI researchers from the public and private sectors. One such example is the ELLIS network. ;3 - Neutral;3 - Neutral;4 - Important;4 - Important;3 - Neutral;/;4 - Important;4 - Important;4 - Important;3 - Neutral;3 - Neutral;3 - Neutral;A concern often raised in AI debates is that of information, especially linked to enabling users to know they are closely interacting with an AI system. For example, while the use of technology may be obvious when filling in an online questionnaire, the addition of the mention of a “chat bot” could be a relevant piece of information in the context of what may look like an online discussion. The EU framework for AI could consider user information as a key element for transparency. ;Current legislation may have some gaps;;Other;Zalando supports a risk-based approach for regulating AI. Differentiating between low-risk, high-risk and “extremely high-risk” would ensure the proportionality of the EU’s approach. External conformity assessments may impair the innovation capacity and the competitive advantage of European AI companies through lengthy, costly procedures, while AI experts are scarce. In this context, external conformity assessments should be reserved to extremely high-risk AI use cases.;;;A gradation of risks would reflect the potential numerous applications of AI: many AI use cases are likely to be low-risk (logistics, personalised fashion outfits, etc), while a few AI applications could potentially create “extreme high risk” (ex: for life and limb, for the loss of personal liberty, etc). Defining high-risk by combining specific sectors and/or precise AI use cases may not be sufficient. Adding qualitative criteria to this definition would bring further clarity.;3 - Neutral;4 - Important;5 - Very important;3 - Neutral;3 - Neutral;4 - Important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);We understand concerns regarding biometric identification in public spaces for surveillance or control. It is important to separate these cases from other much less risky uses of biometric elements, for example when pictures or measurements are used in controlled environments for entirely different purposes than identification. ;Rather not;Labelling systems can lead to longer projects, extra administrative burden and costs for SMEs. In our view, AI innovation can primarily be fostered in the EU through a clear set of simple rules, enabling as many European companies as possible to use AI.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;A gradation of risks (low, high, extremely high risk) would allow to focus ex-ante compliance requirements and ex-post enforcement on clearly identified risks. For the majority of low-risk AI cases, entities could therefore identify and mitigate any potential issue themselves based on simple criteria, while additional rules would apply to high-risk, and ex-ante conformity assessments to a narrow list of extremely high risk AI use cases. This would foster innovation while addressing risks.;Mental health risks;In the context of product safety, risks to life and limb could form a basis for defining harm to consumers. Generally, a definition of risks for AI use cases could - in our view - be based on a mix of factual criteria (sectors, use cases), adding qualitative cross-sectoral criteria. It will be important to evaluate whether these definitions indeed cover the small percentage of risky use cases or whether they are too broad and may deter companies from innovating and using AI as a consequence.;Yes;Please see our answer on the assessment of compliance, at the end of section 2.;No opinion;In our view, a comparison between AI and softwares better encompasses what AI is technologically than a parallel between AI and products. Similarly to softwares, AI systems can’t be 100% bug free or entirely bias free, but excellence in the development of AI leads to controlling inputs, outputs and feedback loops, as well as to making updates to the AI systems when necessary, among others. On the contrary, products are intended to be flawless at the end of their production process.;No opinion;;/;Zalando_-_Addendum_to_AI_public_consultation_June_2020.pdf
F530117;12-06-2020 16:37;English;NGO (Non-governmental organisation);Jessica;Galissaire;;Renaissance Numérique;388341731904-25;Micro (< 10 employees);France;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;Other special requirements in addition to those mentioned in the question above should be imposed (please specify);In a report published on 10 June 2020, Renaissance Numérique calls for the creation of a strong EU-wide standardisation system for facial recognition technologies. The EU standards should take into account both the technological aspects of said technologies and crucial legal aspects like the protection of fundamental rights. The system should apply to all use cases, including for experimentations. For more information on how the system could work, see the attached report.  ;;;;;;;;;;;;;;;200612_RN_Report_Facial_Recognition_FR.pdf
F530116;12-06-2020 16:33;English;Company/Business organisation;Marco;CANTON;;FUJITSU;441732425040-02;Large (250 or more);Belgium;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"Data access and quality is critical to the design, training and use of AI models. 
Cybersecurity by design must be addressed. 
AI ‘Skills’ (including soft & ethical skills) are key factor 
International cooperation and harmonization are crucial and we encourage EU to take a leadership to make a cooperative environment among global players including countries that do not share same values to find a way to harmonize different rules and values.
";5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;Point 4 is marked as important because the EU should get the support from MS.;4 - Important;5 - Very important;5 - Very important;"Not necessarily need for a new AI REC but more important is to ensure coordination among existing ones. 
AI should have a prominent place throughout Horizon Europe, funding core AI research as well as stipulating AI components in research projects.
";4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;Digital Innovation Hubs should be the right ecosystems where SMEs, Large and Academic world can cooperate. It is important to avoid duplication of efforts with similar initiatives or organisations working at local, national or European level.;5 - Very important;5 - Very important;5 - Very important;4 - Important;2 - Not important;2 - Not important;"Accuracy of AI is very important for developers, but not for rule makers. 
Discussions on AI must take into account different perspectives, on ethics, quality assurance, governance and legal responsibilities.
While AI has made significant advances, it has not yet reached the point where it is required to impose a ""excellent"" standard on AI over humans. Therefore, we believe that a phased approach should be taken after gaining a consensus by, for example, sharing a roadmap. ";Other;"No need for new Legislation but working on existing one to see where improvements can help to more trustworthy AI solutions i.e. Liability 
Approach such as rules and insurance scheme for redress are necessary because it is assumed that no one, including AI developers, distributors and users, can be held criminally or civil responsible.
We need a wide approach including business such as insurance scheme not considering only AI as a Silo
";Yes;;Other;In principle, we agree with the two cumulative criteria proposed. It will be important to provide clear rules on how the criteria would be applied. The “use” should weigh more heavily in this assessment whilst further granularity is needed within each sector listed. Existing definitions of “harm” & “risk” that may differ in various sectors must be taken into account. Risk assessments should be designed specifically for AI.;"Where human lives could be in danger;
Where there could be high risks for environment, society, peace and human rights;
Autonomous lethal weapon system

";5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);"Once Fundamental Human Rights are not in danger and existing Privacy Regulation GDPR are applied there is no need “overregulate” 
Data to be processed by AI should be subject to GDPR and regulated from a privacy perspective.
It would be useful to clarify the conditions to use of remote biometric identifications and the measures to be taken during operation by providing reader-friendly guidelines.
";Much;The labelling scheme should be consistent with international standards.;Other enforcement system;Since AI systems are diverse and their usage and perception will change due to technological development and social changes in the future, flexible regulation using soft law is preferable to rigid regulation. It is also desirable to support users and consumers in making rational choices so that high-quality AI systems are selected in the market.;"Requiring retraining in the EU may lead to performance requirement to foreign companies which should be strictly avoided.
In order to promote the selection of appropriate AI systems in the market, it is important for developers and distributors to provide appropriate labelling and information disclosure to users. In addition to enforcing existing laws and regulations, it is necessary to develop guidelines for self-assessment, third-party’s evaluation, labelling and information disclosure.


";Personal security risks;;No;We need to consider how the risks arising by inputting new data will be covered during the operations phase.;No opinion;"More clarity is needed in particular with concrete examples.
The existing liability framework is solid and technology neutral, making it flexible enough to cover the challenges arising with AI. Expanding the scope of the PLD to software and all AI applications must be done in a way that will avoid that anyone involved in making an AI system could be held liable for problems for which they had no control. This would significantly burden AI system developers with and stifle innovation
";Yes, for specific AI applications;For highly automated/autonomous systems;In AI systems, it may be virtually impossible to determine the cause of an accident or damage due to the characteristics of the technology. Even in such cases, placing full responsibility on the developer or provider (hold accountable, especially for negligence) is not appropriate because it reduces the use of AI systems. From the viewpoint of the balance between the benefits and risks of AI, it is necessary to further discuss how compensation and responsibility should be in society as a whole.;AI_White_Paper_Consultation_Fujitsu_FINAL.pdf
F530115;12-06-2020 16:27;German;NGO (Non-governmental organisation);Anne;Wagenführ-Leroyer;;Deutscher Caritasverband e.V.;04903991238-83;Large (250 or more);Germany;The feedback can be published with your personal information;4 - Important;4 - Important;5 - Very important;4 - Important;4 - Important;3 - Neutral;Die (organisierte) Zivilgesellschaft wird zu wenig mitgedacht. Da Weiterbildung vielfach am Arbeitsplatz stattfindet und manche unterdurchschnittlich von Weiterbildung profitieren, besteht das Risiko, dass bestimmte Personen (z.B. Langzeiterwerbslose, Niedrigqualifizierte, ältere Menschen) vergessen werden. Hier spielen Akteure der non-formalen und informellen Bildung eine wichtige Rolle. Digitale Angebote/Produkte müssen außerdem von den Bedürfnissen und Möglichkeiten der Menschen her denken.;4 - Important;4 - Important;3 - Neutral;4 - Important;5 - Very important;4 - Important;Die gemeinnützige Sozialwirtschaft muss verstärkt in den Blick genommen werden und in ihren Innovationsbemühungen unterstützt werden.;4 - Important;4 - Important;4 - Important;Die (organisierte) Zivilgesellschaft sollte aufgrund der erheblichen Auswirkungen, die KI auf unsere Gesellschaft hat, wo möglich und so weit wie möglich einbezogen werden. Um zu optimalen Ergebnissen für die Gesellschaft zu kommen, müssen insbesondere die NutzerInnen von KI und deren Bedürfnisse und Möglichkeiten von vornherein berücksichtigt oder auch aktiv eingebunden werden.;3 - Neutral;4 - Important;4 - Important;4 - Important;4 - Important;Digitale Innovationszentren müssen auch Akteure im Bereich sozialer Innovationen wie die freie Wohlfahrtspflege, gemeinnützige Sozialunternehmen bzw. die Zivilgesellschaft unterstützen.;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;Mit Blick auf selbstbestimmte Teilhabe in der digitalen Welt ist es zwingend notwendig, die Menschen für den Umgang mit diesen neuen Formaten zu befähigen, da manchen die Risiken (psychische Risiken, Datenschutz, Privatsphäre, etc.) nicht richtig ermessen können.;There is a need for a new legislation;;No;;;;;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);"Gerade der Einsatz biometrischer Fernidentifikationssysteme (wie z.B. die Gesichtserkennung) birgt besondere Risiken für die Grundrechte der EU-BürgerInnen. Insbesondere sollte der Gefahr des ""racial profiling"" entgegengewirkt werden. Die bestehenden EU-Rechtsvorschriften (bspw. DSGVO) sind aus unserer Sicht noch nicht ausreichend.";Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;Mit Blick auf selbstbestimmte Teilhabe in der digitalen Welt ist es zwingend notwendig, die Menschen für den Umgang mit diesen neuen Formaten zu befähigen, da manchen die Risiken (psychische Risiken, Datenschutz, Privatsphäre, etc.) nicht richtig ermessen können.;Yes;Gerade bei den immateriellen und daher vielleicht weniger offensichtlichen potentiellen Risiken und Schäden  müssen die Interessen besonders benachteiligter Gruppen berücksichtigt werden, da diese die Risiken für sich selbst nicht immer sicher einschätzen können.;Yes;Parteien, die von negativen Auswirkungen von KI-Systemen betroffen sind, müssen wirksame Rechtsbehelfe zur Verfügung stehen. Informationen zur Haftung müssen leicht und verständlich sein, damit insbesondere auch benachteiligte Personen in der Lage sind eine Entschädigung zu erhalten.;Yes, for specific AI applications;Mindestens für hochriskante KI-Systeme braucht es angepasste nationale Haftungsvorschriften.;Es braucht für Haftungsfrage beim Einsatz von KI eine klarere Abgrenzung von fahrlässigem und nicht fahrlässigem Verhalten.;DCV_Sozialpolitische_Positionen_2019_final.pdf
F530114;12-06-2020 16:24;English;Business Association;Clemens;Otte;;Federation of German Industries (BDI);1771817758-48;Medium (< 250 employees);Germany;The feedback can be published with your personal information;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;The definition of SMEs must not be too narrow. Many German SMEs, which have a high potential in the implementation of AI, do not fall under the EU definition of SMEs, but need support as well.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;The EU Commission should back up the Coordinated Action Plan with as concrete, measurable targets as possible in order to be able to monitor and evaluate the impact of the individual measures. The EU Commission should develop and continuously collect appropriate indicators in cooperation with science and Industry.;5 - Very important;4 - Important;5 - Very important;The EU Commission should specifically promote research topics with strategic importance for industry, such as robotics, low-data AI or hybrid AI. Particular emphasis should be placed on the topic of Edge AI and the interplay between AI and cybersecurity.;5 - Very important;4 - Important;5 - Very important;5 - Very important;4 - Important;The definition of SMEs must not be too narrow. Many German SMEs, which have a high potential in the implementation of AI, do not fall under the EU definition of SMEs, but need support as well.;3 - Neutral;4 - Important;4 - Important;4 - Important;3 - Neutral;3 - Neutral;- The concerns mentioned above have to be seen in relation to other Technologies.  Positive aspects must also be highlighted. Compared to other technologies, AI can e.g. increase security and counteract discrimination by human decisions.;Current legislation may have some gaps;;Yes;;No;;Any application with a black-box algorithm, that is self-learning and makes automated decisions with high impact on human rights.;4 - Important;3 - Neutral;5 - Very important;5 - Very important;4 - Important;4 - Important;No further guidelines or regulations are needed;;Rather not;"Voluntary labelling could rather confuse the user. Furthermore, if an AI application does not pose an increased risk, voluntary labelling does not create any additional added value. It is also questionable whether ""horizontal"" voluntary labelling can do justice to the large number of specific applications. More important than voluntary labelling are clear, transparent rules based on international standards.";A combination of ex-ante compliance and ex-post enforcement mechanisms;;For many high-risk applications, extensive conformity assessments are already taking place. Within this framework, it should be checked whether the overall system meets the corresponding requirements, regardless of whether AI is used or not.;Cyber risks;The EU product safety regulation is largely fit for purpose and covers AI as a risk cause sufficiently even though there may still be gaps to be closed. For example, product safety legislation should make clear to what extent it also covers software. In particular, it is in particular an open question whether stand-alone software is covered by EU product safety legislation. Apart from that, EU product safety legislation should be extended to security and, if necessary, also to services.;No;;No;We would advocate for a careful approach with regard to a potential update on the legislative liability framework. According to our assessment, we consider the current liability framework as sufficient and balanced. Products based on AI are adequatly addressed. ;No;;;BDI-Statement_-_White_Paper_on_AI_final_EN.pdf
F530112;12-06-2020 16:20;English;Trade Union;Magnus;Lundberg;;TCO - Tjänstemännens Centralorganisation;86242309652-84;Small (< 50 employees);Sweden;The feedback can be published with your personal information;4 - Important;4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;A legal framework on ethical norms and recourse to collective bargaining and collective agreement with trade unions are key. Therefore, negotiations with social partners need to be included in this process. Social partners play a key role on all levels in shaping the economic and organisational perspective of industrial relations. Core trade union functions and social dialogue are key pillars of an inclusive digital transformation, including artificial intelligence in the workplace.;5 - Very important;4 - Important;4 - Important;3 - Neutral;5 - Very important;5 - Very important;The European Commission should further support the European social partners capacity building activities, fulfilling its mission in the frame of the role to promote the European social dialogue. This will provide trade unions more opportunities to enhances their competencies in doing capacity building and empower workers and workers representatives to adapt AI to the workplace and respect decent work organisation, including occupational health and safety.;4 - Important;4 - Important;4 - Important;A key aspect is the involvement of European and national social partners and sectoral trade unions, as they bring expertise and experience of situations of real workplace exposure. They further contribute to shaping sustainable AI technologies with possible ways forward in the development of research and innovation. The lighthouse structure for innovation needs to have a space for trade unions, in their role as European and national social partners.;4 - Important;3 - Neutral;4 - Important;4 - Important;3 - Neutral;"Digital Innovation Hubs need to be equipped in order to (a) give support to carry out risk assessment and managing data protection to the different SMEs across Europe. It is key to upgrade their capacity in these two issues that are impactful for work and employment; (b) to allocate trade unions equal access and participation to shape and monitor AI technologies at work and to take part to related employment discussions with the related national authorities.";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;While the regulatory framework should prevent that introduction of AI at the workplace leads to discriminatory outcomes, it is crucial to acknowledge all the risks that AI might mean for workers’ rights. AI must be compliant with fundamental and human rights related to the employment context. Human must remain in command of AI technology. This can only be reflected in establishing a specific governance approach that includes European and national social partners, and in particular trade unions.;Current legislation may have some gaps;;;;;;When discussing regulatory options for AI, it is important to recognise collective agreements as a regulatory instrument. Given the versatility of the collective agreement, it is effective, easily adaptable to different sectors and a very suitable tool to regulate AI in the labour market context,;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;The issues above are all important, but regardless of how a regulatory framework on AI is devised, such a framework must be without prejudice to national labour market models and the autonomy of national social partners and must encourage collective bargaining.;Much;A voluntary labelling system could be useful if the private organisation behind it provide for independence, transparency, quality and trust.;No opinion;;;;AI related risks are still largely unknown, can emerge in many circumstances and can be completely new. More certainty is needed to address new risks like the “deepfakes”, risks related to self-learning applications, bias and discrimination. Also, there will be various factors to take into account to attribute “fair” liability. A business that uses a technology with a certain degree of autonomy, should remain fully liable for any harm that results from using this technology. ;Yes;There is a need for a holistic approach, including societal, organizational and individual scopes and to identify invisible risks, like risks related to semi-automated decision-making, including discrimination, related to data analytics, personal data, cyber-security, infrastructure, human-machine interaction, Internet of Things (IoT) etc. In adapting frameworks to carry out risk assessment and management, a multidisciplinary approach is required, by which trade unions should always be involved.;Yes;In amending the EU liability framework, trade unions need to be properly consulted and involved.;No opinion;;;
F530113;12-06-2020 16:20;English;Company/Business organisation;Mosche;Orth;;Deloitte (on behalf of all firms in the EU);782529817171-84;Large (250 or more);Germany;The feedback can be published with your personal information;4 - Important;2 - Not important;5 - Very important;3 - Neutral;4 - Important;4 - Important;;5 - Very important;5 - Very important;4 - Important;3 - Neutral;4 - Important;4 - Important;#NAME?;4 - Important;5 - Very important;4 - Important;- Transfers of basic research performed by the centre to SMEs to expand the proposed AI technology through the applied research phase, in essence giving SMEs a boost to enter the AI space. This could also happen after applied research is complete and the ;4 - Important;4 - Important;4 - Important;4 - Important;3 - Neutral;;4 - Important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;"- National security, political stability, economic stability, financial performance risk, reputational risk, infrastructure integrity
- Issue of transparency
- AI increasingly empowers companies, which collect large amounts of data, which may concentrate ";There is a need for a new legislation;;Yes;;Yes;;"- AI applications that determine the course of a person’s life, rights and freedom e.g. access to health, employment, education or financial services
- Healthcare diagnoses without human verification
- Decisions on judicial rulings, financial sanctions an";5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;#NAME?;A combination of ex-ante compliance and ex-post enforcement mechanisms;;#NAME?;Mental health risks;#NAME?;Yes;#NAME?;Yes;;Yes, for all AI applications;;;EC_consultation_AI_Whitepaper_-_Deloitte_cover_letter.pdf
F530111;12-06-2020 16:18;English;;;;Local;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;"Authorize and regulate public sector experiments with new technologies for security purposes.The Objective, in addition to ""by design"" security, is to allow territories to experiment with technologies in compliance with the GDPR and assess the impacts of the implementation of technologies as well as the results";5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;;5 - Very important;4 - Important;4 - Important;Allow the territories to use the product tested and developed in partnership with companies outside of conventional public markets, if technology authorized for use by the territories.;3 - Neutral;5 - Very important;4 - Important;5 - Very important;;Allow digital innovation centers access to the BIG DATA of communities to develop software with real data;4 - Important;4 - Important;4 - Important;3 - Neutral;3 - Neutral;4 - Important;Need to develop very large-scale experiments and allow companies to use public data to develop AI with fibal data and adapted to the context.;There is a need for a new legislation;;Yes;;Yes;;"the threshold for the mention ""high risk"" must be defined in a specific and exhaustive manner";3 - Neutral;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;;;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Risks related to the loss of connectivity;;Yes;;Yes;;Yes, for all AI applications;;;General_Response_from_the_City_of_Nice_and_ua_security_partnership__to_the_European_Commission.pdf
F530110;12-06-2020 16:18;English;NGO (Non-governmental organisation);Martyna;Balciunaite;;European Union of the Deaf;;Micro (< 10 employees);Belgium;The feedback can be published with your personal information;4 - Important;4 - Important;5 - Very important;No opinion;4 - Important;4 - Important;;4 - Important;5 - Very important;2 - Not important;4 - Important;4 - Important;4 - Important;;5 - Very important;4 - Important;4 - Important;;No opinion;No opinion;No opinion;No opinion;No opinion;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;Other;It is important to consider that current EU equal treatment and accessibility-related legislation is not comprehensive. So, specific rules guaranteeing accessibility and protection of fundamental rights for AI systems are needed.;No opinion;;;;;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;;No opinion;;Other enforcement system;;;Mental health risks;;Yes;Any assessment of risk to fundamental rights, non-discrimination, equality, accessibility, privacy, personal security and mental health should be based on an intersectional approach, taking into account full diversity of affected persons/groups in society. DPOs should be involved in such assessment procedures.;No opinion;;Yes, for all AI applications;;;
F530109;12-06-2020 16:15;English;;;;National;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;On top of creation fo nerw entities, it is very important to make the most out of existing academic institutions and researche centers on AI.;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;;3 - Neutral;5 - Very important;4 - Important;5 - Very important;4 - Important;4 - Important;;Current legislation may have some gaps;;Yes;;Yes;;"A few  examples:
-	Credit scoring
-	Fraud detection 
-	Insurance products pricing:  mutualization is of essence to insurance, small granularity in insurance products segmentation have negative impact on the principle of insurance itself plus the price can be the one that the clthe client is led to believe he should pay and not the fair price of the product actually being sold to him/her.";5 - Very important;3 - Neutral;4 - Important;4 - Important;5 - Very important;4 - Important;;"Strictly speaking in terms of financial regulations, there is no need to have specific ones, although some guidelines are welcome. In a more generic sense, it would be helpful to have some safeguard in terms of ethical use of AI and in terms of privacy protection.
so neither of the above
";Much;It would be useful to have a labelling system, especially a crowdsourcing one which would be more time sensitive and fair as long as a reputed community is behind the labelling system.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;While studying the way financial processes involving AI should be regulated, we identified the importance to define precise explainability criteria proportionate to the caracteristics of the product and the protection requirements for the targeted customer. The governance structure should also be assessed. ;;;;;;;;;;
F530108;12-06-2020 16:13;English;Company/Business organisation;Kate;Brightwell;;Adobe;575371411483-96;Large (250 or more);United States;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;Section 4H, International Aspects, is key to any future framework for AI. Where possible, a consistent approach should be taken with other key markets outside of the EU, such as the US, as this will be vital to encourage an ecosystem of excellence in the EU.;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;Access to EU data for AI research purposes by the private sector should be encouraged, and EU laws that support this should be implemented by Member States, for example the Copyright Directive (EU) 2019/790. The text and data mining exemption in the Directive enables the private sector to use EU data for AI research and innovation initiatives in certain circumstances. Having access to a wide variety of data sets enables the potential for wider applications of AI that can benefit society.;4 - Important;4 - Important;5 - Very important;5 - Very important;3 - Neutral;Whilst it is important to support SMEs, in doing this, Digital Hubs should also support and build strong relationships with multi-national companies with a substantial EU footprint and interest. Not only will this benefit the broader commercial AI ecosystem, but benefits could indirectly flow down to SMEs and assist with the above tasks. ;4 - Important;4 - Important;5 - Very important;2 - Not important;4 - Important;4 - Important;Explainabilty is a complex area of ongoing research and should not be regulated prematurely to avoid being ineffective. Our concern lay in the fundamental trade-off between increased transparency requirements and privacy and data protection principles and regulations. Instead of focusing on explaining the rational for decisions made by AI, (often impossible from a technical perspective), we would prefer to see the regulations focus on the outcomes with ex-post enforcement and supervision. ;Current legislation may have some gaps;;Yes;;Yes;;Adobe has no AI applications that fall in the ‘high-risk’ category. However, areas that have a significant impact on people’s lives, such as housing, credit, employment, health, and criminal justice deserve focus.;4 - Important;2 - Not important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No further guidelines or regulations are needed;;Not at all;Adobe acknowledges the Commission’s ambition to provide a level of consumer confidence. However, voluntary labelling needs to be reassessed because the initiative set out in the White Paper would not meet the objectives it seeks to address. We would urge the Commission not to pursue the creation of a blanket voluntary labelling system. Given the diverse range of AI products and services that will fall into this category, a one-size-fits-all labelling scheme would, be unworkable, and ineffective.;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;Instead of focusing on ‘unbiased’ data sets and ex-ante analysis, the focus should be on testing outcomes of the AI systems before deployment or applying safeguards against biased outcomes after deployment. Transparency and disclosure requirements would be premature, as the industry is still exploring what is possible as the technology develops. Any over-the-top requirements from policy oversimplify the problem and are likely to add risk in other areas e.g. privacy, IP rights, data security.;;While Adobe acknowledges that the above risks may arise in some specific high-risk scenarios, it does not see its use of AI as being leading to increased risk in these areas.;No;Because of the difficulty defining an ‘important change’ and the potential for several changes and improvements to be made to a product, having ongoing risk assessment procedures would not be practical or effective. Therefore, a focus on monitoring the output on ex-post enforcement measure would be a better way to protect consumers.    ;No; ;No;;;Adobe_-_EU_AI_consultation_response_FINAL_June_2020.pdf
F530107;12-06-2020 16:11;English;NGO (Non-governmental organisation);Pasquale;Esposito;;Center for Democracy and Technology;57305017757-64;Small (< 50 employees);United States;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;Please see the supporting document.;Other;The hardest problems posed by AI applications concern the risk of unfair discrimination. EU law proscribes such discrimination. It applies to development and use of AI, and should be enforced effectively. CDT supports legislation for AI applications where the risk of harm to individuals and fundamental rights is material. Such legislation should build on the current framework, notably the GDPR, and ensure that AI in high-risk cases meets transparency, accountability and explainability standards.;Other;We consider that the bifurcation of AI applications into high and low-risk could fail to capture the spectrum of AI risks. CDT suggests a more scaled approach. Legal obligations should gradually increase with the identified risk level. In the lowest risk category (e.g. music recommendation systems) there may be little need for regulation, but as risks to people or their rights increase, so should the level of regulatory oversight. ;;;AI applications could cause risks to individuals and fundamental rights in areas where decisions and outcomes are highly consequential. Examples are healthcare, law enforcement, employment and education. These applications should be subject to robust scrutiny and control. Another factor is whether citizens can choose to engage with automated decision making, or not. AI systems deployed by or for the public sector are likely to involve risks that most private sector applications do not.;4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;Biometric identification systems should never be allowed in publicly accessible spaces;"CDT considers that several legislative instruments already prohibit the use of remote biometric identification systems in public spaces: the ECHR, the Charter of Fundamental Rights and the GDPR. It is difficult to imagine a scenario in which the use of remote biometric identification in public spaces would meet the necessity and proportionality threshold. We urge the Commission to explicitly discourage deployment of biometric identification in public spaces. 
";Rather not;It is unclear what value a voluntary labelling system would bring for either companies or   citizens. If AI-related risks associated with a product or service are sufficient to warrant labelling, then it should be subject to mandatory requirements. If the Commission proposes a voluntary labelling system, it should consider carefully what meaning labels would have for consumers, and whether the costs of such a system - especially for SMEs - would be outweighed by the benefit to the public.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Legislation should ensure that ex-ante human rights impact assessments related to high-risk algorithmic systems are carried out regularly and submitted for independent expert review. Robust auditing is necessary to foster trust in AI technologies. CDT does not have views on what regulatory authority should carry out this oversight, and whether they should be Member State or EU-level. In some sectors, already existing authorities may have or be able to develop the necessary expertise. ;;;;;;;;;;CDT_Supporting_Document_for_the_EU_Commission_Open_Consultation_on_the_AI_White_Paper_June_2020.pdf
F530106;12-06-2020 16:02;English;NGO (Non-governmental organisation);Andrei;Frank;;SOLIDAR Foundation;310206914824-45;Small (< 50 employees);Belgium;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;1 - Not important at all;1 - Not important at all;Private partnerships are dangerous at this stage given the unregulated tech space and how tech giants have been mired in scandals related to non-consensual usage of personal data. Public sector adoption is too soon as it is under-prepared when considering skills and digital infrastructure. Focus on developing basic digital infrastructure and skills, closing inequality gaps and ensuring democratic oversight (people must have a say over something that influences their lives greatly).;5 - Very important;No opinion;1 - Not important at all;3 - Neutral;5 - Very important;1 - Not important at all;Yes, regulations on the Internet, which should be provided as a public good, and of the virtual space in general, to provide transparency, clarity and certainty over how individuals’ data is used and the opportunity to opt out of any violation of their privacy. Include checks on AI systems for compliance with human rights, democratic oversight over the phase out of AI (ensuring citizens have a say on this) and collaboration with civil society actors when making AI-related public decisions.;3 - Neutral;4 - Important;1 - Not important at all;Prioritising this without considering implications on human rights is misleading. Before investing in research in an AI tool, there must be an assessment whether they meet human rights considerations. Extensive evaluation of the human rights compliance must be done, while the Commission must ensure that all funded research meets EU’s own ethical standards for AI and fundamental rights laws. Ensure diverse recruitment for research and innovation teams to offset underlining biases in AI tools.;1 - Not important at all;5 - Very important;5 - Very important;4 - Important;No opinion;Avoid technosolutionism, considering that AI should be implemented in all SMEs and would be essential. There needs to be more research on the benefits of AI, better mitigation of its risks, and insurance that SMEs will also apply a human rights perspective when using AI. Scandals on misuse of AI technologies was not only applicable to tech giants, but also to SMEs.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;Working in education&training, we are concerned of AI’s opacity in influencing learning processes. Also about the overburdening of teachers/educators who need AI skills with currently insufficient training for digital skills or adequate remuneration for increasing workloads. AI’s implicit biases are dangerous for various groups of learners, increasing academic achievement and equality of opportunities gaps. The implications of private interests intruding on educational concerns are worrying.;Other;The current legislation must be revised, starting with the GDPR, to cover AI’s extensive use of non-personal data, but also to ensure that inequalities, surveillance, predictive policing, especially for human rights organisations and NGOs, are covered by law. A broad legislative framework for the Internet’s operation is needed, ensuring that people have control over their participation in the virtual space, over data sharing and over how predictive algorithms limit online freedom.;Other;This binary approach is inadequate. AI has broad and unexpected implications that can be entrenched so strongly. It must be evaluated on a case-by-case basis and not on general assumptions. A human rights-based approach and not a risk-based approach is needed. Some AI applications should simply be banned, and risk-based approach would only try mitigating them. What is classified as low-risk for a majority, can be exceptionally high-risk for a minority due to discrimination concerns in AI.;;;Any use that would shrink the civic space or impact large numbers of citizens is problematic: predictive policing, indiscriminate biometric surveillance, use of AI to determine delivery of essential public services. The implicit bias existent in AI implications will serve to reinforce societal biases, and this is an incredibly high-risk for a society who wishes to progress.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;-;Not at all;This would create a false sense of security in consumers for a product on which they should be exceptionally vigilant, especially considering the lack of basic digital skills across the EU. Tech companies have also a spotty record on adequately labelling themselves, and this issue has too many implications to allow for this. ;Other enforcement system;All systems should undergo a mandatory ex-ante human rights impact assessment from an external body. The human rights-based perspective is missing from the foreseen plans, and this must be addressed.;This must be done by an external body as self-regulation cannot be trusted on such an important and complex topic.;Mental health risks;Discrimination is the biggest concern, as this would create inequalities, would impact democratic processes in a state, would create filter bubbles (in a context when media and digital literacy is still an issue that the Europe at large confronts with). Minorities or disadvantaged groups would be further impacted by this, damaging social cohesion.;Yes;-;Yes;-;Yes, for all AI applications;;The EU address copyright and database protections which prevent proper oversight of AI applications. Liability rules should provide incentives for openness.;
F530105;12-06-2020 15:59;English;Business Association;Typhaine;Beaupérin;;The Federation of European Risk Management Associations (FERMA);018778010447-60;Micro (< 10 employees);Belgium;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;FERMA_s_Position_Paper_on_Artificial_Intelligence.pdf
F530104;12-06-2020 15:56;English;Company/Business organisation;Elise;Cachin;;Ingka Group / IKEA;1095068839-59;Large (250 or more);Netherlands;The feedback can be published with your personal information;4 - Important;No opinion;5 - Very important;No opinion;5 - Very important;No opinion;A strong emphasis should be put on (i) enhancing collaboration between public authorities, the private sector and academia, (ii) achieving general data literacy across the EU, (iii) securing AI training in all education programmes, and (iv) include ethics and fundamental rights in AI training curricula.;4 - Important;No opinion;5 - Very important;No opinion;5 - Very important;3 - Neutral;Future approaches on AI should secure effective coordination at national level to prevent the fragmentation of the European Single Market. Developing competitive and ethical AI capabilities requires extensive investments that diverging rules across Member States would threaten. Homogeneous frameworks will greatly contribute to the EU’s attractiveness for AI development and support a value-driven approach to AI innovation boosting post-COVID recovery.;No opinion;No opinion;No opinion;-;No opinion;No opinion;No opinion;No opinion;No opinion;-;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;The relevance of these concerns is greatly dependent of the context in which we develop and use AI. Particular attention should be paid to potentially vulnerable people (those traditionally at risk of exclusion). For instance, risks attached to imprecise AI-powered product recommendation engines are mainly reputational and would lead customers to switch retailers.;Other;Guidance on how to apply existing rules to AI applications would ease the uptake of Artificial Intelligence. Current EU legislation, such as the Product Liability Directive, offers a comprehensive framework securing adequate remedies against most risks AI can pose. However, clear guidelines and, where needed, adjustments to current rules will be required to mitigate risks associated to emerging AI applications (e.g., how to apply GDPR purpose limitation to support data reuse for AI training).;Yes;;Other;Whether Artificial Intelligence is deemed ‘high risk’ should be equally based on the type of AI application being used (i.e., likelihood and magnitude of adverse outcomes) rather than on sectoral use. Applications only aiming at improving business processes do not have the same implications for people and society than AI-fuelled autonomous driving solutions for instance. An application-focused approach would secure proportional requirements across sectors and support innovation.;“High-risk” applications should be assessed based on the type of AI applications, i.e., based on whether they are intended for (i) internal business processes, (ii) consumer-facing use, (iii) decision tools, and (iv) physical use that presents safety risks including harm to human body. ;4 - Important;5 - Very important;4 - Important;5 - Very important;4 - Important;4 - Important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);EU rules for remote biometric identification systems should balance privacy concerns with opportunities with consumer experience improvement. The GDPR has already created a clear framework for remote biometric identification systems in which biometric data processing should be a last resort option. Retailers would welcome the opportunity to explore innovative biometric-based services for our consumers and visitors, such as cashier-less check-out processes.;Not at all;A voluntary labelling system for ‘no-high-risk’ AI applications would likely overflow consumers with information and refrain them from using AI technologies by putting forwards the notion of risks rather than benefits. Such a scheme would also increase administrative burdens for developers and reduce the attractiveness of the European Union for AI innovation and development.;Other enforcement system;"Businesses have a role to play in achieving a human-centric approach to AI that relies on ethical data use. Strong internal policies should be put in place to: 
- empower people to understand why, how and when we use AI,
- set accountability mechanisms for data and its usage, including the resulting decisions and outcomes, 
- design, develop and deploy technology based on ethical standpoints.
";The future framework for AI should achieve clear and harmonised guidance across the EU. Should a dedicated authority be created to oversee AI, such authority should focus on coordinating public input and AI-specific guidelines issued by existing authorities (e.g., data protection and product liability enforcement authorities).;Cyber risks;-;No opinion;Risk assessment procedures should be conducted before an AI solution is introduced in the market and after any major updates that will change (i) its purpose, (ii) the way it is use and/or powered, or (iii) its capabilities.;No;As a company, the division of liability is regulated in contracts. Liability provisions and exemptions for improper use/update apply to technology sourcing and updates, similarly to product liability. Ex ante rules to lean on when developing AI solutions would ease compliance and secure a homogeneous approach across sectors. ;No;;-;2020.06_COM_AI_White_Paper_Open_Statement_Ingka_Group.pdf
F530103;12-06-2020 15:54;French;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;Toutes ces actions sont indispensables, la crise sanitaire a révélé l'importance des infrastructures numériques et le fait que l'Europe puisse maîtriser tous les aspects technologiques numériques et l'intelligence artificielle en particulier afin qu'elles soient mises à la portée de toutes les entreprises quelle que soit leur taille. Le MEDEF est partie prenante de la e-skills coalition et mène un projet sur ce thème avec la Commission.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;Sur les données, les adhérents du MEDEF soulignent que ce n'est pas tant la quantité de données que la qualité de celles-ci qui compte. Sur l'établissement d'un cadre européen des données ils sont favorables à un cadre commun qui mettent les entreprises à jeu égal mais souhaitent des clarifications en droit de la concurrence, la maîtrise du partage et conditions financières attachées car la production, l'entretien et l'enrichissement des données nécessite des investissements importants. ;3 - Neutral;5 - Very important;4 - Important;Les adhérents s'accordent à dire que ce n'est pas tant le nombre de centres mais le fait qu'ils puissent rivaliser avec leurs alter ego qui est important. Le fait de savoir s'il faut en créer un de niveau mondial ne semble pas être le bon sujet mais plus celui de  faire fonctionner en réseau les centres existants. Enfin le MEDEF est favorable à ce que les entreprises privées soient associées aux programmes de recherches.;4 - Important;4 - Important;5 - Very important;4 - Important;5 - Very important;Le MEDEF a mené une enquête en collaboration avec l’Association des Centraliens sur les usages de l’IA et les attentes des entreprises. Elle montre que la pénétration de l'IA dans les TPE, les PME et ETI est encore assez limitée : elles sont davantage utilisatrices que créatrices d’IA. L’impact de l’IA sur l’amélioration de la productivité est pourtant bien identifié par les entreprises. Nous sommes pour que les PME soient incluses dans les programmes de sensibilisation et d'accès à l'IA.;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;L'IA couvre des technologies différentes et ne peuvent pas se résumer à une seule technologie. Ce n'est pas tant l'IA elle-même qui pourrait poser des problèmes ou créer des risques mais l'usage qui en est fait. A cet égard les travaux du HLEG soulignent l'importance des données d'entrées choisies et la formation des programmeurs et des équipes qui vont l'utiliser. Enfin, des régulations de protections des consommateurs, de non-discrimination ou  sectorielles fortes existent déjà (ex banque);Other;Le RGPD  couvre beaucoup des sujets souvent soulevés? : on ne voit pas bien quels manques il y aurait dans la législation actuelle qui ne seraient pas déjà couvert? En plus du RGPD des législations sectorielles apportent aussi beaucoup de contraintes ciblées et préviennent déjà les risques communément soulevés?;Other;La définition de l'IA à hauts risques nous semble pas assez précise et recouvre un nombre de situations qui est très large : peu ou prou toutes les IA comportent un des critères définis.Sur les questions de responsabilité: le changement éventuel de la charge de la preuve doit être analysé et dans le cas où la preuve de la faute de l’IA devrait être apporté pour être d'indemnisé (preuve impossible et  donc un obstacle sans doute insurmontable) devrait être résolu par une approche assurantielle.;;;Le risque majeur est un risque démocratique, des experts soulignent le risque d’aller vers une « algocratie » où de plus en plus de tâches bénignes et de décisions seraient confiées à l'IA qui mises bout à bout constituent un nouveau risque.  Si la technocratie est le système critiqué dans lequel les décisions sont prises en privilégiant les données techniques par rapport aux facteurs humains et sociaux, l’« algocratie » serait celui où les décisions sont prises sur le fondement d’algorithmes ;4 - Important;2 - Not important;4 - Important;5 - Very important;3 - Neutral;4 - Important;No opinion;Comme actuellement prévu par les dispositions du RGPD les données biométriques sont des données sensibles et leur collecte et traitement ne peut répondre qu'à des cas limités d'usage dont la finalité est hautement justifiée.;Rather not;"Il nous semble trop tôt pour créer des labels pour les IA ne présentant pas de ""hauts risques"". La confiance ne peut être réduite à un jeu de contraintes et de « liste à cocher » qui serait inadaptée et  incompréhensible, et donc inutilisable pour les équipes opérationnelles. De plus de nombreuses normes de conformité déjà existantes dans certains secteurs (bancaire par ex). Il nous semble plus judicieux que les entreprises soient encouragées à avoir une gouvernance de l'éthique des IA.";A combination of ex-ante compliance and ex-post enforcement mechanisms;;Il nous semble trop tôt pour créer des labels pour les IA. La confiance ne peut être réduite à un jeu de contraintes et de « liste à cocher » qui serait pesante, inadaptée et  vite obsolète,  donc inutilisable pour les équipes opérationnelles. De plus de nombreuses normes de conformité déjà existantes dans certains secteurs.?En matière de certification il y a des domaines où la elle existe déjà sur le produit (avions, automobile) et donc l’IA va être soumise de facto.;Cyber risks;Dans la définition de la responsabilité , le retour à l’application pure et simple d’une responsabilité pour faute est examinée mais elle fait porter aux victimes preuve de l’imputabilité du dommage ce qui risque de devenir particulièrement complexe en raison de la sophistication de l’intelligence artificielle. Comme dans le cas de l'automobile une obligation d'assurance est peut-être à examiner pour certains risques.;No;La notion d’ « important change » n’est pas suffisamment définie et cela risquerait d'avoir pour conséquence de considérer une machine ancienne comme une machine neuve, avec la nécessité de mettre en œuvre l’ensemble de la procédure d’attestation de la conformité.;No;"pas à ce stade du développement des IA qui restent encore dans le domaine des IA dites ""faibles"".";No;;A ce stade de développement de l’intelligence artificielle, il semble que le droit de la responsabilité civile actuelle reste opérant, mais un travail en continu dans ce domaine est nécessaire afin de ne pas laisser d’éventuelles victimes dans des situations inacceptables;Position_MEDEF_sur_l_IA_-_2020.pdf
F530102;12-06-2020 15:53;English;Company/Business organisation;Andres;BELLMONT ROLDAN;;ESBG (European Savings and Retail Banking Group);8765978796-80;Small (< 50 employees);Belgium;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Please refer to the document attached to this consultation. ;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;3 - Neutral;Please refer to the document attached to this consultation. ;4 - Important;4 - Important;5 - Very important;Please refer to the document attached to this consultation. ;5 - Very important;4 - Important;5 - Very important;5 - Very important;4 - Important;In a post-Covid context, helping to raise awareness among SMEs of the potential benefits of AI can be particularly relevant, especially in the field of automation.;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;Please refer to the document attached to this consultation. ;Current legislation is fully sufficient;;Other;"We consider that new rules are not necessary for AI systems, but if there were to be new ones, they should be restricted to high-risk applications.
Worth taking into account here is the differentiation between applications and sectors. We do not support the specific regulation of AI, but if it is regulated, then it should be done by applications, not by sectors, which are too broad categories for regulation to be efficient.

For additional information, please refer to the document attached.";;;;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;No opinion;Please refer to the document attached to this consultation. ;Much;We believe that this framework should provide for lighter requirements than the regulatory framework for high-risk applications, as the labelling scheme will only cover AI systems that are not considered as high-risk. In a nutshell, the labelling scheme should be easy to implement and to update as it is an evolving technology and it should not be too costly. ESBG encourages the involvement and dialogue with the private sectors regarding the design and implementation of such a scheme. ;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Please refer to the document attached to this consultation. ;;;No opinion;Please refer to the document attached to this consultation. ;No opinion;Please refer to the document attached to this consultation. ;No opinion;;As a European association, ESBG does not take a specific position on national rules. ;0258_ESBG_response_to_EC_White_Paper_on_AI-_attachment.pdf
F530101;12-06-2020 15:49;German;Business Association;Elisa;Brummel;;Verband der TÜV e.V. (VdTÜV e.V.);45013506457-28;Small (< 50 employees);Germany;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;4 - Important;;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;"1. Methoden der Integritätssicherung für Online-Learning sind als Marktzugangsbedingung zu entwickeln. 
2. Konformitätsbewertungen sind an relevanten Stellen zu implementieren.
3. Für Systeme entsprechender Kritikalität sind vom Gesetzgeber entsprechende Zertifizierungen vorzusehen.";4 - Important;5 - Very important;4 - Important;;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;Other;Der bestehende Rechtsrahmen muss um die KI-Spezifika ergänzt werden. Darüber hinaus müssen dort noch nicht adressierte Risiken in neuen Rechtsrahmen berücksichtigt werden. Normung und Standardisierung sowie gegenseitig anerkannte Regelwerke für die Überprüfbarkeit von KI-Systemen durch unabhängige Dritte sind zu etablieren.;No;;;;Ein substanzielles, also hohes Risiko besteht immer dann, wenn Gefahr für Leib und Leben, bzw. die Umwelt besteht, die Integrität, Vertraulichkeit oder Verfügbarkeit von kritischen Infrastrukturen gefährdet wird.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Not at all;Insbesondere dann, wenn KI-gestützte Anwendungen in der Lage sind während der Nutzung ihr Verhalten zu ändern, sollte dies durch ein verpflichtendes Kennzeichnungssystem erkenntlich gemacht werden. Die Erfüllung der Vorschriftsmäßigkeit der KI-Systeme muss durch eine unabhängige Drittprüfung nachgewiesen werden. ;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F530100;12-06-2020 15:48;English;Business Association;Björn;Olsson;;Swedish Bankers' Association;53517281038-30;Small (< 50 employees);Sweden;The feedback can be published with your personal information;5 - Very important;5 - Very important;2 - Not important;4 - Important;5 - Very important;5 - Very important;"-	Developing the skills necessary to work in AI and upskilling the workforce to become fit for the AI-led transformation is of course pivotal, however we do not believe that the EU of today has the resources and the mandate sufficient with the ambition of";5 - Very important;5 - Very important;4 - Important;4 - Important;2 - Not important;5 - Very important;-?In order to leverage the wealth of industrial data available in Europe, there should be a greater focus on industrial applications. The EU should encourage the creation of reference datasets and well-defined benchmark industrial problems which the resea;4 - Important;3 - Neutral;3 - Neutral;What in the lighthouse research centre would attract the “best minds”? Would there be a continuation to continue to attract these best minds even after their visit at the lighthouse would end, or is this a position without end of employment? How to secure that the “best minds” are working in important research areas for the EU? This will most likely change over time, how to handle adding new minds to the lighthouse?;1 - Not important at all;3 - Neutral;2 - Not important;5 - Very important;3 - Neutral;;4 - Important;5 - Very important;5 - Very important;2 - Not important;3 - Neutral;2 - Not important;"The human-in-the-loop approach should always be ensured, so that the results of a system can always be overruled by a human. In finance the current legislation for finance covers many of the potential risks with AI systems, since they are not physical systems but rather virtual. 

Banks already monitor and assess systems with high – to medium business impact, to ensure that any system that is deployed is monitored, validated and re-evaluated on an appropriate recurring basis.";Current legislation may have some gaps;;Yes;;Other;Designating high-risk sectors could harm the level playing field where regulatory arbitrage is possible, e.g. where an activity can be performed by companies that fall outside a designated sector. To avoid uncertainty the Commission should designate the applications that are considered high-risk and develop clear and objective criteria to ensure that only those applications that could cause serious harm to citizens are captured.;;3 - Neutral;4 - Important;3 - Neutral;4 - Important;5 - Very important;4 - Important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;No opinion;;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;Creating expert centres around ethical and fundamental rights questions, where eg. SMEs and get information and education on “best practice”, analyse the threats and risks in their systems and products and get guidance on how to mitigate those.;Personal security risks;The following risks could be highlighted: expansion of existing threats (increase in the complexity of cyber-attacks, and a reduction in costs to launch them), introduction of new threats, and an increase of the speed and success rate of cyber-attacks. ;Yes;Yes, since this type of system may have a varying level of autonomous decision making incorporated it needs to be monitored to ensure robustness and that the decisions do not start to stray from the intended scope.;No opinion;;No opinion;;;200612_SBF_Position_EC_AI_White_Paper.pdf
F530099;12-06-2020 15:48;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;4 - Important;5 - Very important;No opinion;5 - Very important;4 - Important;;1 - Not important at all;3 - Neutral;5 - Very important;4 - Important;5 - Very important;No opinion;An additional reflection on interactions between AI and IPR will be needed, with the aim to promote innovation and legal certainty in a balanced way. A common definition of IPR is key to avoid fragmentation of internal market and legal challenges on the rules. Intellectual property rights issues should also be examined to ensure that the related regulatory framework properly addresses a number of challenges that are specific to AI and is thus ableto promote its development.;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;3 - Neutral;;Other;"The regulatory framework needs to be reviewed and adapted in order to consider the use of AI, but it is hoped that the Commission will avoid an overregulation which may penalise investment and business development. Specific requirements for the development of AI may not be fully met by the current regulatory framework. In that case an accurate analysis of the possible risks may be needed in order to find the right solution.
";Other;It is far from certain that new rules are necessary. It is a possibility that must be carefully evaluated and only if deemed necessary new requirements should be limited to high risk applications.;;;;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;Other special requirements in addition to those mentioned in the question above should be imposed (please specify);Poste believes that the use of remote identification systems (such as face recognition) and other technologies which may be used in public space should be formally regulated. Face recognition in public spaces should be authorized as it is not compliant with the GDPR provisions.;Much;Mandatory regulation should be fully assessed and be limited to high risk applications. For all the rest, a voluntary labelling system among operators may be a useful option to avoid mandatory regulation and increase users’ trust.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;The transparency requirement should be proportionated to risks in order to avoid excessive compliance costs or limitations in firms trials. Where AI systems might have a negative impact on users’ fundamental rights, an obligation to provide transparency is appropriate.AI is an important tool, but it should not replace human intervention.The governance of the systems must remain in the hands of human, who defines the processes and chooses how to optimize the use of AI.;Personal security risks;;No opinion;;No opinion;The Report “Liability for Artificial Intelligence and other emerging technologies” puts forward digital technologies which move in public spaces, such vehicles or drones as candidates for additional basis of strict liability (p 40). The strict liability regime to be applied is a relevant issue that should be tackled in a broaden way involving all the main stakeholders, the industry as first.;No;;A further analysis of liability rules related to the actors operating in the value chain is needed.The operators should be able to assess the possible risks related to new projects before launching them in order to include, if necessary,the right safeguards in the contracts with providers.Poste is both user third parties AI-based systems and developer and considers that adequate safeguards and appropriate responsibilities also ensure a greater guarantee of transparency on the end-customer side.;
F530098;12-06-2020 15:47;German;Public authority;Georg;Eisenreich;National;Bayerisches Staatsministerium der Justiz;;Large (250 or more);Germany;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;Beitrag_Staatsminister_Georg_Eisenreich_-_Justizminister_des_Freistaates_Bayern_-_Haftungsrecht.pdf
F530097;12-06-2020 15:46;English;Business Association;Luca;CASSETTI;;Ecommerce Europe;867433111414-11;Micro (< 10 employees);Belgium;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;Ensuring access to general and practical knowledge on AI and AI applications to the public and enhance trust in safe AI-application is fundamental. In addition, a strong emphasis should be put on (i) enhancing collaboration between public authorities, the private sector and academia, (ii) achieving general data literacy across the EU, (iii) securing AI training in all education and re-skilling programs, and (iv) include ethics and fundamental rights in AI training curricula.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;A revision of the Coordinated Plan would be an opportunity to strengthen cooperation and ensure effective coordination between Member States to prevent the fragmentation of the Single Market. This would in turn boost the EU’s attractiveness for the development of AI solutions, notably as part of the post-COVID recovery. ;4 - Important;4 - Important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;4 - Important;4 - Important;4 - Important;4 - Important;3 - Neutral;4 - Important;Compensation of damages caused by the AI-application to individuals should be based on the risk-based principle that compensation should be provided for by the group that benefits from or uses the application (e.g. general insurance obligation). There should be a clear difference in the compensation of personal harm and material damages caused by AI. Assessment of liability should consider the context in which the harm was done. Specific attention should be given to potentially vulnerable people;Other;The EU and Member States have a well-functioning legal framework for products and services brought on the EU market addressing concerns for data protection and privacy and liability for harm caused by unsafe products that could arise in relation to the development and use of AI. National framework can be further assess to ensure consistency, legal certainty and the highest level of harmonization.  Adjustments related to the role of new operator like developers could also be considered ;Yes;;Other;We broadly agree with the approach to determine “high-risk” applications. Concerning the periodic revision of an annex “function of relevant developments in practice”, such a revision should carefully assess how much time would be reasonably necessary for businesses to adapt to new additions to the list. Assessment should be based on a combination of both sectoral and application criteria in the White Paper, as there should be a case-by-case assessment for all AI applications.;;5 - Very important;4 - Important;3 - Neutral;4 - Important;4 - Important;5 - Very important;No further guidelines or regulations are needed;Companies should have the opportunity to explore further the possibilities opened by remote biometric identifications to develop biometric-based services for their customers and visitors (e.g. smooth checkout processes). The creation of regulatory sandboxes would be crucial to leave space for innovation and understand the possibilities under the GDPR.;No opinion;There is no need for voluntary labeling scheme as the necessary regulatory framework exists to allow consumers and citizen to rely on the fact that product including AI-applications are compliant and safe. Additional information is not necessary to ensure consumer safety, hence why labeling should be limited to “high-risks” applications. In addition, new schemes could increase administrative burdens and reduce the attractiveness of the EU when it comes to development of new AI solutions.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;The future framework for AI should achieve clear and harmonized guidance across the EU. Should a dedicated authority be created to oversee AI, such authority should focus on coordinating stakeholders’ input and AI-specific guidelines issued by existing authorities (e.g. data protection and product liability enforcement authorities).;Personal security risks;;No;As AI products by nature develop over time, new assessments will be required over time as new risks may appear. Risk assessment procedures should be conducted before an AI solution is introduced in the market and after any major updates that will change (i) its purpose, (ii) the way it is use and/or powered, or (iii) its capabilities.;No;The current EU framework is providing for an effective allocation of responsibilities and liability for product information, risk-assessment and damages caused by unsafe products. The principle of liability of the producer of the AI and minimal liability for the seller of AI-application should be used as a basis to address the role of the deployer of the AI-application and the importer of the AI application in the EU. Any changes to the liability rules should not result in undue burden for SMEs.;No opinion;;They should be adapted to ensure that national approach, regardless of the AI application, are harmonized. The principle of full harmonization across Member States should apply to limit possibilities of gold-plating and secure the growth of businesses cross-border.;
F530096;12-06-2020 15:38;English;NGO (Non-governmental organisation);Scott;McCulloch;;Counter Extremism Project (CEP);990159532140-35;Small (< 50 employees);Belgium;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;;Other;"Considering the DSA it is necessary to highlight the need for transparency and explainability of automated decision-making (ADM) systems. An essential part of transparency is the ability to explain the technical processes of the applied ADM-systems and the related human decisions. Reporting mechanisms must include explanations of the individual automated moderation tools; the technical compliance system as a whole; and a better understanding of the practical application of moderation policies. ";No;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);The United Nations considers facial recognition systems to be necessary in the fight against terrorism, including the identification of returning terrorist fighters, as stated in UNSC Resolution 2178 (2014): departing foreign terrorist fighters (FTFs), see here: https://undocs.org/en/S/RES/2178(2014), and UNSC Resolution 2396 (2017): returning foreign terrorist fighters, see here: https://undocs.org/en/S/RES/2396(2017). ;No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;A more transparent reporting mechanism is needed to explain the individual automated moderation tools, the technical compliance system and better understand the application of moderation policies in practice. A transparent system ought to require: (1) the designation of a suitable entity, with technical and domain expertise, as an external observer with full access to moderation policies, and (2) provide comprehensible information (see CEP's 15 features of comprehensible transparency attached). ;;;No opinion;;No opinion;;Yes, for specific AI applications;Particularly in the for-profit context, e.g. social media, video sharing platforms or online marketplaces that allow for illegal content to be uploaded, shared, promoted or recommended.;;
F530095;12-06-2020 15:25;English;Company/Business organisation;Peter;Kofler;;Danish Entrepreneurs;;Micro (< 10 employees);Denmark;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;"Good idea to create a lighthouse centre for AI research, innovation and expertise, but important to work closely with AI practitioners from the private sector.  
Target more support for SMEs to help them benefit more from AI and specifically for their type of business via good best practice examples.
Ensure the right skills to prosper from the AI benefits: Educate the next generation for a digital era; support the current workforce to reskill; and ensure to attract and retain talent. ";4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;2 - Not important;"A positive AI vision. AI must be demystified in the public eye to help businesses receive more investments for AI projects much needed to develop real-world evidence and data essential for high quality AI and high quality regulations. 
Digital learnings on all levels, incl. policymakers and politicians, for better AI understanding and vision. Applying AI successfully often requires a complete rethinking of a business, and startups need easy access to AI practices and tools to succeed.";5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;Most importantly, any new specialised Digital Innovation Hub must be built with respect for the existing AI-ecosystem, and function in close collaboration with the AI-industry and -practitioners already on the market to build on top of existing knowledge and expertise.  ;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;Innovation comes first. The primary goal is to create an ecosystem for companies and technologies to succeed in Europe, not push them to relocate to the US. But it’s impossible if concerns and risks about AI outweigh potentials and benefits. Too much emphasis on danger and risks will discourage the use of AI in Europe. Instead focus should be on creating a framework that allows data to flow freely, helps countries address skills shortages and supports SMEs to embrace AI. ;Current legislation may have some gaps;;Yes;;No;;;2 - Not important;2 - Not important;2 - Not important;2 - Not important;2 - Not important;2 - Not important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);;Not at all;A new labelling scheme, even voluntary, will only burden startups with more red tape, and give larger, well-established companies a bigger advantage to outscale them, as larger companies will be better prepared and financially fit to accommodate it and use it as a competitive edge. With so much of the tech industry’s great ideas coming from startups, this is a major concern. ;Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;Please see attachment.;;;No;Avoid new 'software liability’. Liability shouldn’t be applied for all kinds of damages to software. It’s practically impossible because software is not a physical product. You can’t treat an app like a washing machine. Startups can’t survive being liable for all sorts of software errors. Software vendors don’t have full control over all updates and can’t force a user to accept them. Offline products don’t change in the same way as software, e.g. via generic updates or bug fixes. ;No;"We accept liability to be able to go to market. But we don’t see a need for a regulatory review of the Product Safety Directive to include AI where a security threat does not directly entail a significant safety issue.
Only changes to the functionality of a product that alter it in such a way that it could be classed as an “important change” may require a new risk assessment. Generic updates such as security and bug fixes, or simple improvements, should be excluded. ";No;;;
F530094;12-06-2020 15:24;French;;;;Regional;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Compétences : accent sur la formation (création d'un module IA du CAP au BAC+5 et de lieux dédiés : showroom pour démonstrations, cas pratiques) mais aussi sur la sensibilisation aux enjeux de l’IA (vers étudiants, salariés utilisateurs, professionnels/experts de l’IA, grand public). Ne pas oublier l’implication des Régions : accompagnement notamment financier via différents dispositifs, essentiel pour les PME-TPE et compétence en matière d’enseignement supérieur, de recherche et de formation.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"1. Dédier des financements à la mise en place d'un programme d'accompagnement à l’intégration de l'IA pour les TPE/PME
2. Prise en compte de la protection et la sécurité des données (cybersécurité) dans le déploiement de l'IA au sein des entreprises ";5 - Very important;5 - Very important;5 - Very important;"1. Dédier des financements pour le transfert  de technologies
2. Accès des chercheurs aux données des entreprises ";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Soutenir des plateformes de démonstration pour évoluer vers une industrie 4.0 intégrant l'IA ;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;;There is a need for a new legislation;;No;;;;applications dans les secteurs de la santé et des transports;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;No further guidelines or regulations are needed;;Very much;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Risks related to the loss of connectivity;;Yes;;Yes;;Yes, for all AI applications;;;
F530093;12-06-2020 15:23;English;;;;National;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;"Working with civil society bodies in order to examine the needs of the citizens/consummers/elderly/youth/women etc; Building an ecosystem of excellence, beginnung from Research and Innovation up to the whole production chain and taking into account the needs and fears from civil Society in order to create trust and acceptance. That also correlates to the concept of the human centered approach.";5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;4 - Important;3 - Neutral;Concretisation of the crucial importance of research: an increased focus also on further research into the societal, ethical and psychological dimensions and implications of AI will be crucial for its benefit to society as a whole.;5 - Very important;5 - Very important;3 - Neutral;;;;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;The functioning/the results of AI is usually not transparent regarding the criteria on which algorithms are based. AI may also cause security Problems.;There is a need for a new legislation;;No;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;"Right of affected party to defend itself against automated decision procedures;obligations to identify use of adaptive algorithms;obligations to justifiy decisions regarding structure and algorithm, disclosure of program Code; documentation obligations (logging of program sequences);reversal of burden of proof;regular review of technical/organisational measures to ensure fair algorithmic decision-making prodedures; stadardisation/self-regulation; Right of action for consumer associations.";Mental health risks;;Yes;As AI will usually be updated like all digital content applications it is absolutely necessary to ensure safety during the whole lifetime (see also Directive on digital content which obliges the seller to update digital content). Furthermore, the concept of product safety has to be broadened: not only products but also services and digital content should be in the scope of GPSD. Alternately it should be examined if a horizontal legal act on AI would be more preferable than amending several acts.;Yes;"The concepts of PLD have to be amended: Notion of defect has to be extended (for instance non conformity/violation of data protection principles like privacy by design/default); digital content has to be included regardless whether it is embedded in a product or not; broader concept of damage (also pure financial loss); Extension of liable persons (not only Producer, but also deployer, delveloper, other key actors in the production chain).";Yes, for specific AI applications;;;
F530092;12-06-2020 15:20;German;Other;Gabriela;Schneider;;Kommissariat der deutschen Bischöfe - Katholisches Büro in Berlin -;524375510752-92;Small (< 50 employees);Germany;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;"Der Exzellenzbegriff greift zu kurz. Das Ziel einer ""ethischen KI made in Europe"" setzt auch ethische Exzellenz voraus. Daher bedarf es auch Maßnahmen, die eine interdisziplinäre Ausbildung von Fachkräften ermöglichen, die sowohl über exzellente technische als auch über exzellente ethische Kompetenzen verfügen. Darüber hinaus sollte die Kommission Räume und Instrumente für eine gesamtgesellschaftliche Debatte über die für eine ""ethische KI made in Europe"" erforderlichen Merkmale schaffen.";5 - Very important;3 - Neutral;4 - Important;5 - Very important;5 - Very important;4 - Important;Eine Angleichung politischer Strategien und eine Verstärkung der Kooperation sollte auch in bestimmten grenzüberschreitend relevanten Anwendungsfeldern für KI vorangetrieben werden. Dies gilt insbesondere für den Schutz globaler Gemeingüter, da dort Marktmechanismen, die KI voranbringen könnten, nicht oder nicht hinreichend greifen. Anwendungsfelder sind insbesondere der Schutz des Klimas, der Artenvielfalt und letztlich die Erreichung aller SDGs der Agenda 2030.  ;4 - Important;5 - Very important;4 - Important;Angesichts der vielfältigen ethischen Herausforderungen durch KI, schließen wir uns der Forderung der Hochrangigen Expertengruppe an, 720 Lehrstühle für KI-Ethik in Europa einzurichten. Wünschenswert sind zudem Maßnahmen zur Stärkung der interdisziplinären KI-Forschung und KI-Ethik, z.B. durch Einrichtung von Lehrstühlen oder die Förderung von Exzellenzclustern/-Initiativen an Universitäten europaweit. Auch Horizon Europe könnte für eine Förderung der Forschung zu KI-Ethik genutzt werden.;3 - Neutral;4 - Important;5 - Very important;5 - Very important;5 - Very important;Digitale Innovationszentren können dazu beitragen, für die ethischen Risiken bestimmter Anwendungen zu sensibilisieren. Auch können sie personell und technisch so ausgerüstet werden, dass sie Hilfestellungen (Beratung, Tools) zur ethischen Überprüfung von KI-Anwendungen - von deren Entwicklung bis zu deren Einsatz - geben können. ;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;Die Anwendung und der Einsatz von KI können neben konkreten/sofortigen, auch abstrakte/langfristige Folgen für die Gesellschaft und den Einzelnen haben. Die Mensch-Maschine-Beziehung kann auf Dauer zu einer Veränderung sowohl unseres Menschen- und/oder Gesellschaftsbildes als auch unseres Selbstverständnisses führen und Verschiebungen im Werte- und Normengefüge einer Gesellschaft verursachen.;There is a need for a new legislation;;Other;Nein. Es sollte unabhängig vom Risiko ein Mindestmaß an verbindlichen Qualitäts- und Transparenzanforderungen für KI geben, dies kann nicht erst vom Einsatzbereich der KI abhängig gemacht werden. Wir halten den sektorspezifischen Ansatz zur Ermittlung von „Anwendung mit hohem Risiko“  für unterkomplex. Ethisch sensible Anwendungsbereiche finden sich sektorübergreifend. Eine Differenzierung nach Schädigungspotenzial (vgl. die deutsche Datenethikkommission) wäre schon eher denkbar. ;;;"Hochproblematisch ist die Nutzung von KI: im Kontext demokratischer Entscheidungsprozesse auf allen (lokal, regional, national, europäisch) Ebenen, in denen Bürger*innen politische Präferenzen einbringen oder ihr Lebensumfeld gestalten, bspw. in der kommunalen Städte- und Verkehrsplanung; bei nicht-rückholbaren Umweltveränderungen; im Bildungs-,Gesundheits- und Justizwesen, in den Bereichen Verteidigung, Militär, Polizei- und Ordnungsrecht, soziale Sicherung, Asyl, Migration, Grenzkontrollen.   ";5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No further guidelines or regulations are needed;Art. 6 Abs. 1 (e) und Abs. 3 DSGVO und die EU-Grundrechtecharta schränken den Gestaltungsspielraum der EU-Mitgliedstaaten (bzw. je nach Verfasstheit ihrer unterstaatlichen Struktureinheiten) angemessen ein, ohne ihnen die Möglichkeit zu nehmen, situativ und an die jeweiligen, insbesondere lokalen Erfordernisse angepasst über die Nutzung solcher Systeme im öffentlichen Raum zu entscheiden. Die nationalen Grundrechte bleiben als zusätzlicher Schutz anwendbar.;Much;Es bedarf eines verpflichtenden Kennzeichnungssystems zur Ausweisung aller KI-Anwendungen oder KI-basierte Anwendungen, ein freiwilliges Kennzeichnungssystem reicht nicht aus. Darüber hinaus wäre eine weiterführende Kennzeichung bestimmter Aspekte von KI wünschenwert, die insbesondere ethische Kriterien (etwa Transparenz, Fairness, etc.) ausweist und die entweder freiwillig oder differenziert nach Schädigungspotential auch verpflichtend sein könnte. ;A combination of ex-ante compliance and ex-post enforcement mechanisms;;"Es bedarf eines verpflichtenden Vorab-Überprüfungssystems sowie einer laufenden Überprüfung mit Abhilfemechanismus während des gesamten Nutzungszyklus von KI. Teil dieser Überprüfung muss auch die ""ethische"" Konstruktion und ""ethische"" Aspekte der KI sein, wofür Kriterien festgelegt und für den Nutzer - etwa im Rahmen des vorgeschlagenen, weiterführenden Kennzeichnungssystems - transparent gemacht werden müssen.";Mental health risks;"Risiken bzgl. der Einhaltung spezifischer ethischer Standards einzelner EU-Mitgliedstaaten im Querschnittsthema KI; Risiken digitaler, insbesondere auch sozioökonomisch oder demographisch begründeter Blindspots und deren Auswirkungen auf das Training und den Nutzen von KI; Risiken bzgl. der Berücksichtigung spezifischer, mitgliedstaatlicher Grundrechtausprägungen und -gewichtungen; Risiken für die individuelle informationelle und digitale Selbstbestimmung.";Yes;;Yes;Die Haftungsrisiken müssen eindeutig und transparent zugewiesen und deklariert werden. Unsicherheiten dürfen nicht zulasten von Verbraucher*innen gehen, denkbar sind insofern Beweiserleichterungen. Sozial benachteiligte Personen müssen angesichts des durch KI noch verschärften Macht- und Wissensgefälles u.U. zusätzlich befähigt und unterstützt werden, ihre Rechte in KI-betreffenden Produkthaftungsfällen wahrzunehmen. ;Yes, for all AI applications;;;
F530091;12-06-2020 15:16;English;Public authority;Martha;STICKINGS;International;European Union Agency for Fundamental Rights (FRA);;Medium (< 250 employees);Austria;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;FRA-input-to-the-European-Commission-AI-White-Paper.docx
F530090;12-06-2020 15:16;English;Company/Business organisation;Pierre;LUCAS;;ORGALIM, Europe's Technology Industries;20210641335-88;Small (< 50 employees);Belgium;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;See complementary answer;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;5 - Very important;4 - Important;See complementary answer;3 - Neutral;4 - Important;4 - Important;See complementary answer;5 - Very important;4 - Important;5 - Very important;5 - Very important;4 - Important;See complementary answer;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;See complementary answer;Other;See complementary answer;Other;See complementary answer;;;See complementary answer;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;No further guidelines or regulations are needed;See complementary answer;Rather not;See complementary answer;A combination of ex-ante compliance and ex-post enforcement mechanisms;;See complementary answer;;See complementary answer;;See complementary answer;No;See complementary answer;No;;See complementary answer;
F530089;12-06-2020 15:15;English;Trade Union;Marco;Sartori;;Federation of European Securities Exchanges - FESE;71488206456-23;Micro (< 10 employees);Belgium;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"FESE believes that the cooperation between authorities and market participants has the potential to bring valuable outcomes. This includes mutual understanding of benefits and risks associated with the technology and lays the ground for a wider ecosystem. These should be promoted and supported.
Furthermore, training in Artificial Intelligence for market members and regulators should be a priority.";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;The creation of training and professional centres of excellence, along with the European Data Space are two key elements for a meaningful adoption of AI in the financial and stock markets sectors. ;5 - Very important;5 - Very important;5 - Very important;The establishment of benchmark training and research centres in constant communication with regulators of each sector could foster and strengthen innovation.;4 - Important;5 - Very important;4 - Important;4 - Important;5 - Very important;Digital Innovation Hubs could benefit from a homogeneous development of AI tools. Acknowledging the existence of biases and addressing them, as well as working with common data sets for development should be amongst the main tasks of specialised Digital Innovation Centres. ;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;Any AI application must have clear rules/objectives (AI assessment) to minimize the risks. Given AI’s low capacity for explanation, it is crucial that the original data sets are unbiased. Most activities performed by AI in the financial sector would be regulated by already existing harmonized rules, leading to a minimal risk of violation of fundamental rights. Sandboxes are a possible solution in the technical testing phase. However, where services are offered to retail clear rules need to apply;Other;Current legislation may have some gaps in the application of AI in the financial sector, there are no accessible and unbiased data sets. There are no consistent backtesting, each entity having its own. The development of AI tools in this type of environment make it difficult to guarantee the reliability of products that emerge. ;Other;Yes, FESE agrees with the proposal. For high-risk AI applications, a combination of ex-ante assessments, based on an auditable external conformity procedure (backtesting with homogeneous requirements), as well as ex-post market surveillance would be necessary. Care should be given as labelling high-risk application could be a barrier to entry (e.g. SMEs). ;;;;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;For non-high-risk AI applications it should be allowed for companies to receive a voluntary certification (similar to ‘quality labels’). This labelling system, if any, should follow and implement general guidelines stemming from industry standards and practices. Moreover, FESE does not support “self-certification”.;Other enforcement system;"Combination of ex-ante and ex-post mechanisms, with public data sets, clear and homogeneous rules, periodic audits and a voluntary certification system (quality seal).
Clear identification of high-risk applications and the requirements that will be asked of them.";"The list of requirements for (high-risk) AI applications should be reviewed and updated timely and frequently (e.g. without the requirement for a Level 1 change of the regulatory framework) to keep up with technological innovation. The review of the criteria should take the form of guidelines published by supervisory authorities and could be updated on a more regular basis.
It is crucial that the necessary capacities are in place to assess the AI.";;Every AI provider needs to put in place sound internal processes (i.e. modelling, training of data, handling of critical/sensitive situations, handbooks, documentation, …). A voluntary certification system for non-high-risk systems and a correct auditing system in the case of high-risk systems would be adequate to guarantee legal certainty. Internal processes should be reviewed every 5 years, if deemed necessary. Adaptation in the sectorial law in which AI has an impact could also be necessary.;Yes;Need to highlight the differences between AI applications operating in “open” or “closed” systems. In open systems, the AI does not possess the required ability to cover all eventualities, as training data is limited. Therefore, humans should be required as final decision-making actors. This is also true for high-risk applications in closed systems. However, reinforcement learning is designed to work in open systems, leading to an increased number of eventualities covered by AI.;Yes;;Yes, for all AI applications;;A revision of the regulatory framework is necessary, taking into account the impacted sectors, in order to review/regulate, inter alia, the responsibility that would be required for the use of these systems, their imputation (to which multiple agents involved in the design, release, use), during what phases (release of the product on the market and afterwards), any new risks derive from learning which were not foreseen in the marketing moment and they appear a posteriori.;
F530088;12-06-2020 15:13;English;Business Association;Maggie;Henkin;;Computing Technology Industry Association (CompTIA);;Medium (< 250 employees);United States;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;CompTIA_EC_AI_White_Paper_FINAL_SH.pdf
F530087;12-06-2020 15:12;English;Business Association;Claudia;Russo;;International Association of Scientific, Technical and Medical Publishers (STM);98356852465-08;Micro (< 10 employees);Netherlands;The feedback can be published with your personal information;3 - Neutral;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;(1) Building on existing initiatives and coordinating with stakeholders to ensure alignment, minimise burdens and streamline activity. (2) Ensuring an appropriate intellectual property framework to protect innovative AI tools and the holders of pre-existing or purpose-built IP when used to generate or improve AI systems. (3) Promoting positive, understandable examples of AI applications to build public understanding and support and underpin sustainable long-term government investment in AI.;4 - Important;3 - Neutral;4 - Important;4 - Important;4 - Important;3 - Neutral;(1) Utilise existing standards and protocols, respect intellectual property, and develop an understanding of ethical and IP-compliant practices in AI. (2) Supporting and promulgating community-based ethical standards and practices for AI development across the EU, to provide quality, trust, and integrity and transparency.;3 - Neutral;4 - Important;4 - Important;Ensuring an appropriate intellectual property framework to protect innovative AI tools and the holders of pre-existing or purpose-built IP when used to create, learn, calibrate, repair or improve AI systems, as well as intellectual property that may be adapted or purpose built with use in AI applications and systems in mind. Moreover, rather than one centre, any investments should be in a network that works together on the three above-mentioned actions, rather than isolated activities.;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;(1) Ensuring skills and training efforts include training in ethical practices for AI, respect for intellectual property rights in IP that might feed into and improve AI systems, and transparency to ensure trust and integrity of AI tools. (2) Respect for IP is particularly important for any testing and reference facilities.;3 - Neutral;4 - Important;4 - Important;4 - Important;3 - Neutral;4 - Important;(1) Fundamental rights include IP: IP protection incentivises investment in high-quality proprietary content, datasets, metadata, curated items or databases fed to AI. (2) AI needs clear practices on content and data acquisition, use, and sharing: Marketplaces for innovation depend on transparent use and terms. (3) AI may be vulnerable to manipulation: The potential for abuse/compromise of AI systems, including by “adverse examples,” must be addressed. (4) Data quality is critical for AI inputs.;Other;It may be too early to tell what gaps there are and therefore if new rules are needed. The first approach should be to consider the applicability of the existing frameworks, including for IP for privacy (GDPR), competition law etc. One area that might need examination may be product liability. ;No opinion;;;;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;3 - Neutral;No opinion;;No opinion;Given that the evolution and application of AI systems are still in their early stages, the utility and efficacy of labelling standards are not yet clear. Creating a voluntary labelling system at this point could stand in the way of innovation. Standards and labelling systems should instead be developed over time by practitioners and communities of use, including industry.;No opinion;;;Cyber risks;(1) Risks to IP protection should be addressed: IP protection incentivises investment in high-quality proprietary content, datasets, metadata, curated items or databases ingested into AI. (2) Product liability needs to be addressed, but it may be too early to determine full outlines of liability concerns. (3) In general, certainty needs to be balanced with the need to allow for innovation. ;No opinion;;No opinion;;No opinion;;;
F530086;12-06-2020 15:07;English;Company/Business organisation;Ville;Virtanen;;CSC - IT Center for Science Ltd.;098297335667-27;Large (250 or more);Finland;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;Investing in research infrastructures such as HPC and data spaces is crucial in order to reap the full potential of AI. Convergent use of HPC and AI techniques requires a holistic approach and more interaction between the two communities. In addition, more data must be made available by implementing common data policies, such as FAIR principles and European Interoperability Framework, and opening up public data sets, including the ones that are not currently completely public.;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;;3 - Neutral;5 - Very important;4 - Important;CSC emphasises that HPC and AI are interdependent and tightly connected and should not be competing for funding, but be jointly developed. There is a need for sufficient funding for the digital and research infrastructures and HPC as well as interoperable data infrastructures as a part of them. The EU should build on existing infrastructures and make sure that data can be moved across the sectors (private-public-research) so that there is a real opportunity to develop a European data economy.;5 - Very important;4 - Important;4 - Important;4 - Important;3 - Neutral;;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;Legislation must prevent harm but at the same time enable innovation and development of AI applications. Building European digital capacities is essential in order to ensure that AI development is done according to European values.;Other;Current legislation must be examined not only from the perspective of possible gaps but also from the point of view that it might unnecessarily hinder the development of AI. For example, the Digital Single Market directive concerning copyright is not providing sufficient exceptions for text and data mining and is thus harmful for the commercial use of text and data mining techniques.;;;;;;4 - Important;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;;;Much;;;;;;;;;;;;;;CSC_Position_Paper_AI_White_Paper.pdf
F530085;12-06-2020 15:02;English;Academic/Research Institution;Petri;Myllymaki;;Finnish Center for Artificial Intelligence (FCAI);039049138497-12;Large (250 or more);Finland;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;Establishment of ecosystems, platforms and protocols for utilization and sharing of diverse data sets, and for preventing monopolistic accumulation of digital data.  ;5 - Very important;4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;;2 - Not important;5 - Very important;4 - Important;Distributed but closely interlinked networks of AI excellence centers are the key. ;4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;;3 - Neutral;4 - Important;4 - Important;5 - Very important;4 - Important;5 - Very important;;Current legislation may have some gaps;;Yes;;Other;Limit to high-risk applications, but this should apply to ALL systems, whether they are AI or not.;;3 - Neutral;3 - Neutral;3 - Neutral;5 - Very important;4 - Important;4 - Important;;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;No opinion;Again, this should not apply to AI only, but all digital products and services.;Yes, for specific AI applications;E.g. decision-support systems in governmental decision making (regardless of whether they are considered AI or not).;;FCAI_consultation_on_the_AI_White_Paper_signed.pdf
F530084;12-06-2020 14:59;English;NGO (Non-governmental organisation);Michele;Calabro;;European Patients' Forum (EPF);61911227368-75;Small (< 50 employees);Belgium;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;EPF welcomes the Paper on AI and its approach based on excellence, trust, human rights, and fundamental values. The EU can develop a strong framework that benefits people, businesses and governments, matching innovation, safety and trust. The EU can achieve this goal by involving patient organisations as key stakeholders in shaping policy to ensure trustworthy, ethical, safe, inclusive AI. We invite you to read our attached statement, which addresses in detail the key elements of our response. ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Improved coordination at European level will be key to advance together on AI while limiting inequalities and harmonising innovation and accessibility to AI-related benefits for patients. For more details, please refer to our accompanying statement, in particular section 2 to 5 (pp. 3-6);5 - Very important;5 - Very important;5 - Very important;Creating an ecosystem of excellence for research on AI in Europe is key, and the actions mentioned are surely important. Innovation in AI, as in others, should be valued for its potential to improve the quality of care and of life, over and above mere potential for putting a product on the market. EPF would also like to emphasise the importance of sustainability and of addressing health inequalities. For more details, please refer to our accompanying statement, in particular section 4 (p. 5);5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;/;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"The application of AI in healthcare raises a series of concerns, in terms of ethics, safety and fundamental rights for citizens and patients. EPF calls for particular attention in ensuring that AI in healthcare enhances society, and is an enabler of – and not a threat to – patients’ rights and wellbeing, guaranteeing that the value of real human contact is not minimised or entirely replaced by technological alternatives.
For more details, please refer to our accompanying statement, section 1";Other;AI-specific risks, safety and liability, should be addressed specifically in EU legislation, avoiding duplication with existing regulatory frameworks (from data protection to medical devices) while ensuring adequate protection for individuals. EPF calls for inclusion of patients’ views from the very beginning in the process of adaptation and update of the current legislation or, where necessary, the development of new legislation. For more details, please refer to our statement, section 7, 8, 9;Other;The adoption of new and specific rules addressing specific risks related to the application of AI should surely be considered. However, especially for AI applications in the healthcare sector, whether the introduction of new compulsory requirements should be limited to high-risk applications depends on how the future EU rules on AI will detail the risk-based approach and therefore the list of high-risk applications uses. For more details, please refer to our statement, in particular section 7;;;"EPF calls for particular attention on the definition of high-risk AI in healthcare and a dedicated discussion on this topic inclusive of the views of patients. Since the EU approach to define high-risk will be linked to more restrictive or relaxed assessment procedures, it will be necessary to carefully evaluate what could be considered to be low-risk AI application in healthcare. For more details, please refer to our accompanying statement, in particular section 7 (p. 7)
";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Other special requirements in addition to those mentioned in the question above should be imposed (please specify);For more details, please refer to our accompanying statement, in particular section 7, 8 and 9 (pp. 7-8);Very much;As concerns the proposal of a voluntary labelling system for non-high-risk applications, such an approach could indeed be useful to facilitate the identification of trustworthy applications for both patients and professionals. As previously mentioned, however, the key question here is how to define high-risk and non high-risk applications. For more details, please refer to our accompanying statement, in particular section 7, 8 and 9 (pp. 7-8);A combination of ex-ante compliance and ex-post enforcement mechanisms;;High-Risk AI applications in healthcare should be subject to strict assessments procedures, harmonised at EU level, to fully ensure patient’ safety and address ethical questions. In EPF’s view, the best solution would be a combination of ex-ante compliance and ex-post enforcement mechanisms, with the important specification that ex-ante compliance should be assessed by an independent body. For more details, please refer to our accompanying statement, in particular section 8 and 9 (pp. 7-8);Mental health risks;For more details, please refer to our accompanying statement, in particular section 1, 7, 8 and 9 (pp. 2-3, 7-8);Yes;Given the speed of technological advancement in this field, any new AI-related legislative framework should incorporate requirements for systems to be re-evaluated, and for the new legislation to be adapted, in a rapid and nimble way in response to technological evolution – thereby ‘future-proofing’ the legislation. For more details, please refer to our accompanying statement, in particular section 7, 8 and 9 (pp. 7-8);Yes;Clarity and transparency on liability and responsibility, therefore on who should respond to potential harm caused by AI applications, will be also crucial to increase trust in AI. People should always have effective and transparent mechanisms for redress. For more details, please refer to our accompanying statement, in particular section 7, 8 and 9 (pp. 7-8);Yes, for all AI applications;;For more details, please refer to our accompanying statement, in particular section 7, 8 and 9 (pp. 7-8);
F530083;12-06-2020 14:56;English;Academic/Research Institution;Uwe;MOELLER;;Deutsches Zentrum für Luft- und Raumfahrt e.V. (DLR);21280626733-05;Large (250 or more);Germany;The feedback can be published with your personal information;4 - Important;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;Regulatory aspects and actions to improve acceptance within the public;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;;4 - Important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;;3 - Neutral;4 - Important;4 - Important;4 - Important;3 - Neutral;3 - Neutral;"In the coming years, AI will be used in both defensive and attacking roles. Sophisticated attacks will evolve faster, but at the same time security experts will have better insight, knowledge and scenarios for protection planning, which should lead to faster results. Technological advances therefore pose risks because the algorithms are as perfect,
flawed, benign or malicious as the people who programmed them";There is a need for a new legislation;;No;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;"For areas in which AIs violate ehtical principles and, for example, discriminate against
parts of society etc.";Yes;;No opinion;;No opinion;;;
F530082;12-06-2020 14:48;English;Trade Union;Manon;Van Thorre;;ACV-CSC Belgium;80866452991-55;Large (250 or more);Belgium;The feedback can be published with your personal information;5 - Very important;4 - Important;5 - Very important;3 - Neutral;3 - Neutral;3 - Neutral;"Legally binding framework on ethical norms and recourse to collective bargaining and collective agreement with legitimate trade unions and workers representatives are key. Therefore, negotiations with social partners need to be included in this process. 
 It is crucial to establish a regulatory framework to ensure that private investments in AI are directed to the fulfilment of public utility. Safeguards for fundamental rights are needed before implementing AI in the public sector.";4 - Important;4 - Important;3 - Neutral;2 - Not important;5 - Very important;3 - Neutral;The Commission needs to further develop training schemes for workers at all levels in order to make the workers “AI literate”. Trade unions and workers’ representatives shall be enabled to engage in effective negotiations regarding the use of AI. Moreover, attention should also be given to the impact of AI on the existing EU acquis on workers’ rights and human rights in general. The EU strategy on AI should focus on the societal impacts of AI and respect the existing legislation on personal data;4 - Important;4 - Important;3 - Neutral;European and national social partners and sectoral trade unions should be involved as they bring expertise and experience of situations of real workplace exposure.They further contribute to shaping sustainable AI technologies that are of public interest. The lighthouse structure for innovation needs to have a space for trade unions, in their role as European and national social partners. And it should focus on avoiding risks such as discrimination.;3 - Neutral;2 - Not important;3 - Neutral;3 - Neutral;3 - Neutral;Digital Innovation Hubs need to be equipped in order to give support to carry out risk assessment and managing data protection to the different SMEs across Europe.Trade unions should have equal access and participation to shape and monitor AI technologies at work and to take part to related employment discussions with the related national authorities.There should be no exceptions of fulfillment of obligations by SMEs.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;It is crucial to acknowledge all the risks that AI brings along in relation to workers’ and citizens' rights. There should be zero tolerance for AI in breach of compliance with fundamental and human rights, especially in the public sector or in the workplace. There is also a lack of transparency in the research and development of AI. ;There is a need for a new legislation;;No;;;;"
";5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;GDPR states that processing biometric data for the cause of identifying individuals is prohibited, except for specific circumstances. But biometric identification is not possible thanks to the GDPR as it is now, and it should remain so. One of the most probable risks for society is that facial recognition creates mass surveillance across the world, incompatible with human rights and democratic principles. It will raise inequalities and exacerbate biases. ;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Having a mandatory AI framework can improve the level of compliance of business operating in the EU. The EU should have the ambition to leave up to the fundamental rights values anchored in the Treaty and to set up a legal system for AI. If GDPR is open for revision to further regulate personal data for AI applications, European and national trade unions need to be included on the table of negotiations.;Mental health risks;The AI related risks are still highly unknown and they can emerge in many circumstances. More legal certainty is needed to address new risks like the  “deepfakes”, risks related to self-learning applications, bias and discrimination. Also, there will be various factors to take into account to attribute “fair” liability.  ;Yes;;Yes;"More discussion needed on whether AI application is product or service; legal scholars have different views and the distinction is not clear cut. Unions to be part of this discussion. Manufacturers should make sure that the AI application works safely before it is applied, using AI should not be an excuse to breach the duty of care. In amending the EU liability framework, trade unions need to be properly consulted and involved.";Yes, for all AI applications;;National legal regimes might require adaptation too. Again, national legal regimes provide different liability considerations to the supply of services and to the supply of products. It is necessary to clarify the scope of potential liability of designers, hardware manufacturers, operators, network service providers, etc. Further discussion in relation to the probe of harm or psychosocial issues at the workplace is also required.;
F530081;12-06-2020 14:47;English;Company/Business organisation;EPIF;European Payment Institution Federation;;European Payment Institutions Federation (EPIF);461826311486-83;Large (250 or more);Belgium;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;EPIF_response_to_Commission_Consultation_on_White_paper_on_AI.pdf
F530080;12-06-2020 14:46;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;4 - Important;5 - Very important;3 - Neutral;5 - Very important;3 - Neutral;;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;3 - Neutral;3 - Neutral;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;;Current legislation may have some gaps;;Yes;;Yes;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No further guidelines or regulations are needed;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Risks related to the loss of connectivity;;Yes;;Yes;;No;;;Liberty_Global_position_paper_EC_White_Paper_on_AI.120620.pdf
F530079;12-06-2020 14:42;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;Current legislation may have some gaps;;Yes;;Yes;;the concept of high risk activities or dangerous activities is regulated in some member states. For example, in Italy liability for dangerous activities is expressly regulated in article 2050 of the civil code. So I would agree to limit part of the regulation to those activities which, according to the laws of the member states, can be defined as dangerous. I believe we need a regulatory ladder that allows us to modulate ex ante responsibility and rules so as not to suffocate the new businesses ;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Cyber risks;;Yes;;Yes;;Yes, for all AI applications;;;
F530078;12-06-2020 14:41;English;Company/Business organisation;Andrea;Przybyla;;CEDEC European Federation of Local Energy Companies;54829912208-85;Micro (< 10 employees);Belgium;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;;5 - Very important;3 - Neutral;5 - Very important;4 - Important;5 - Very important;4 - Important;Even more important are clarification and simplification of the legal framework for the provision and use of data.;4 - Important;4 - Important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;5 - Very important;No opinion;4 - Important;"""AI is not always accurate"" - AI provides recommendations based on probabilities. Based on these, decisions can be better compared to ""uniformed"" decisions. However, it is important to understand the mechanics. If certainty is needed, AI is the wrong tool. Thus, lack of accuracy is less a concern but a misunderstanding of the capabilities of the technology.";Current legislation may have some gaps;;Yes;;Yes;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);Uncontrolled and not explicitly authorised biometric identification should only be possible for a narrow range or for sovereign purposes such as criminal prosecution under judicial control and supervision in each single case. Otherwise it should be prohibited in general.;Very much;;Other enforcement system;Some high-risk products should be assessed ex-ante by means of an external conformity assessment procedure.;It should be avoided that the EU framework is too prescriptive. A risk-based approach must also take into account the size of entities that will have to comply with mandatory requirements, meaning that it should be less prescriptive for (smaller) entities with a more limited risk.;Risks related to the loss of connectivity;"Risks connected to (public) services where AI is used; risk of social discrimination by AI.";No opinion;;Yes;;No opinion;;;
F530077;12-06-2020 14:37;English;NGO (Non-governmental organisation);Patricia;MUNOZ;;European Society of Cardiology;1,15045E+11;Large (250 or more);France;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;The adoption of AI in healthcare needs particular attention, as it involves possible changes in the patient-physician relationship, different levels of liability, and specific needs that have to be addressed. It will be essential to involve medical professional stakeholders in the development of proposals for example by engaging with the Biomedical Alliance in Europe and its members including the European Society of Cardiology.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Alignment and coordination among European countries is also required in the healthcare sector, as the recent COVID pandemic has shown. This needs a specific effort. Medical professional organizations could play a role of facilitators in highlighting major differences among systems that could constitute a barrier for AI adoption in healthcare.;5 - Very important;5 - Very important;5 - Very important;"The development of digital skills in AI, when the application is for healthcare, is required to enable medical professionals to understand and evaluate the role(s) of AI in their clinical practice; and to give specific understanding to developers that healthcare data cannot be considered as any other kind of data. Medical professional organizations could help in this process. These actions should be preceded/accompanied by efforts to assure the quality of datasets for AI.";4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;"Establishing an Expert Laboratory under the MDR (EU) 2017/745  to test AI and machine learning (as SaMD) and the impact of new iterations of AI reference datasets; it might also conduct  vulnerability testing of implanted electronic devices that are internet-enabled. SMEs should involve medical professionals in the co-design of AI that is intended for healthcare applications, to ensure its relevance and increase its probability of adoption.";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"How will managing uncertainty in medical decisions be changed by AI? How will decision-support influence the doctor-patient relationship? Will it impact on learning by new physicians? For some diagnostic tasks, AI may be useful even if uninterpretable; in complex scenarios, transparency will be essential. In teleconsultations, how could interaction with a real physician/nurse or an algorithm change outcomes? How can freedom of choice to have/not have AI involved in one’s own care be guaranteed? ";There is a need for a new legislation;;Yes;;Yes;;Every application of AI in healthcare that could have an effect on the diagnosis, treatment or prediction of an individual patient’s health should be considered as a high-risk application – since poorly performing algorithms could have serious or critical consequences. Professional medical organizations should be involved to better delineate and stratify the relevant risks connected to the particular healthcare domains. ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;Biometric identification of patients (for example in hospitals) would collect sensitive information. Clear criteria/examples of situations where such use is justified and when it is not, are needed. In addition, having the possibility to track where a person goes, while he/she transits in public spaces, could create stigma and possible financial consequences (such as an increase in a health insurance premium).;Much;A voluntary labeling system will apply to AI systems that are not considered as high-risk. Therefore, the system will not apply to medical applications of AI (rather than managerial ones).;A combination of ex-ante compliance and ex-post enforcement mechanisms;;For medical applications of AI, physicians should have access to post-market surveillance to disclose any issues related to the software and to give transparent access to problems related to the software that they are using. There is also a need for physicians to know exactly which version of AI is utilized (in cloud computing, this information is hidden to the user), as well as to be informed about which differences in performance are present between different versions.;Personal security risks;Particular care should be taken to ensure transparency for individuals and users, and good governance, of individual-level data held in nationwide, administrative registries. These data are expected to increase and can include interactions with the healthcare system, sociodemographic data, potentially genetic data and other sensitive information, thereby posing particular risks within the field of trustworthy AI.;Yes;"Guidance is needed about what would constitute appropriate clinical follow-up for medical applications of AI.In our opinion the European Commission should seriously consider establishing an Expert Laboratory under the MDR for artificial intelligence in medicine. In addition, medical professional organizations should be included in the process, to give appropriate feedback at every step.
";Yes;For AI healthcare applications, the current MDR includes AI as software, but guidance is needed on how to test an AI-based system (what constitutes sufficient clinical evidence?). For liability, specific legislation is needed for healthcare, taking into account specific traits of AI (model update, training dataset diversity, etc). Legal responsibility in the case of self-learning AI medical applications needs to be defined. ;Yes, for specific AI applications;Medical applications;How to define the entity accountable for medical decisions, in case of medical errors? Clear liability rules are needed in healthcare, in case of problems or missed benefits due to AI. If the physician trusts the AI system and an error occurs, who is responsible: the physician, the software user, the AI developer, or the provider of the training datasets?  How does the chain of responsibility work? Medical associations could help to define national regulatory gaps and AI-related problems.;
F530076;12-06-2020 14:36;English;Company/Business organisation;Matthew;HOULIHAN;;Cisco;494613715191-85;Large (250 or more);United States;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;We welcome the focus on creating an AI ecosystem of excellence.  Coordination of EU Member State actions in this space will be highly valuable. Businesses will be critical players in driving the development and adoption of AI and therefore a collaborative approach between the public and private sectors will be needed, as will engaging independent universities and research centres.  Efforts on all these areas should include collaboration beyond the EU to promote international collaboration.;4 - Important;3 - Neutral;5 - Very important;4 - Important;5 - Very important;3 - Neutral;AI adoption by businesses and the public sector should be prioritised so that the EU becomes a world leader in AI use as well as development. Business advisory services, financial incentives, AI productivity vouchers, skills programmes should be considered. Certainty on the legal framework for AI applications and related data will be a prerequisite here.  We would also advocate work to facilitate open data models to stimulate sharing of more government and industrial data.;2 - Not important;5 - Very important;5 - Very important;R&D&I and its commercialisation should be at the heart of any EU AI strategy.  We would favour supporting existing R&D&I excellence over creating a new centre. The EU should partner with other regions to build global AI innovation centres.  Engaging industry in AI research should be prioritised to encourage application of AI research – initiatives to foster collaboration through a wide number of smaller PPPs (rather than one large one) should be encouraged.;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;Encouraging SME development and use of AI will be critical.  In addition to the actions above, providing independent advice to on what AI-related technology is available and how that can be deployed to enhance productivity, innovation etc would be useful.  Specific focus should be given to analysing and overcoming barriers SMEs face to adopting AI technologies and how additional help can be provided to under-preforming regions with a view to promoting inclusive use of AI across the continent.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;4 - Important;Ensuring an effective, risk-based, proportionate and targeted regulatory approach will be important to driving up trust in and use of AI.  We agree in principle with a risk-based approach.  Although all of the issues outlined above are important, they will not all be relevant for all AI use cases and a well-targeted approach is needed. Access to remediation mechanisms will be important.  International consistency in approaches to regulation is needed. ;Current legislation may have some gaps;;Yes;;Other;High-risk applications should be ones where the AI would be considered as a crucial factor in a decision that has a legal or otherwise substantial impact on an individual's or group's human rights.  A huge number of AI applications will not be in this category.  For example, many B2B applications and those that have little consumer engagement are unlikely to come under this category. ;;4 - Important;4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);Although facial recognition technologies can offer benefits in terms of safety alongside, potentially, some economic benefit.  Care needs to be taken over the use of these technologies in public spaces.  Oversight is needed to protect the rights of inviduals, including the right of privacy, freedom of expression, freedom of association, non-discrimination and other human rights. Codes of conduct may be needed to set out the circumstances under what circumstances facial recognition can be used.  ;Much;A voluntary labelling system could be useful for certain use cases where knowledge and understanding of using an AI application is important.  Applying labelling to applications where a physical or digital label would be difficult to apply could be challenging.  Given the global nature of the technology sector, the Commission should work with international partners and businesses on any labelling scheme to ensure international consistency.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;As long as clear rules and standards are in place, an ex-post assessment would be business friendly, still achieve high-levels of trust-worthiness and be consistent with approaches to conformity assessment in other areas. An ex-ante approach could hold back innovation from being introduced in the EU market and deter innovation investment but may be appropriate for extremely high-risk AI use cases. Care needs to be exercised to respect IP in any conformity assessment procedures. ;Risks related to the loss of connectivity;Legal liability in the event of an IT vulnerability, data breach etc. ;Yes;Market surveillance procedures will need to consider AI that has the ability to significantly change in nature over the course of its lifetime.  Consideration should be given to the trustworthiness of supply chains involved in developing AI solutions.  Lighter-touch regimes should be applied to those companies with supply chains and development processes that consistently demonstrate best practice. ;No opinion;As pointed out in the White Paper, establishing an effective liability regime for AI applications will be complex and challenging.  Some of this may be determined by contracts between suppliers, contracts with end users and existing legislation.  However, the nature of AI may mean specific approaches will be needed for specific applications.  Again, this should be targeted, specific and focused on high-risk applications. ;No opinion;;;
F530075;12-06-2020 14:34;English;Non-EU Citizen;Thomas;Douglas;;;;;United Kingdom;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;3 - Neutral;;5 - Very important;5 - Very important;3 - Neutral;4 - Important;5 - Very important;3 - Neutral;;3 - Neutral;4 - Important;3 - Neutral;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;;4 - Important;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;4 - Important;;Current legislation may have some gaps;;No;;;;AI-based manipulation of behaviour (e.g. through social media);5 - Very important;5 - Very important;;4 - Important;4 - Important;4 - Important;No opinion;;No opinion;;No opinion;;;Mental health risks;;Yes;;No opinion;;No opinion;;;EU_consultation_AI-based_manipulation_response__12_June_2020.pdf
F530074;12-06-2020 14:34;English;NGO (Non-governmental organisation);Julia;WADOUX;;AGE Platform Europe;16549972091-86;Small (< 50 employees);Belgium;The feedback can be published with your personal information;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;;No opinion;No opinion;No opinion;;No opinion;No opinion;No opinion;No opinion;No opinion;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;The problems are pretty well described in the White Paper and does cover the key issues to be considered. Still in relation to the risk of discriminatory outcomes, it is very important to pay attention to proxies. This is particularly true for automated decision-making: used criteria will be neutral and respect the regulation, but proxies can be used and generate discrimination, e.g. using place of residence, or health status, or salary level could also be seen as proxies to race, age, gender.;Other;There are indeed gaps notably in relation to discrimination: the  Equal Treatment Directive COM(2008)0426 is still blocked in the Council while an ambitious text would help to cover all discrimination grounds outside the labour market. See recent Equinet Report (June 2020). It is also important to ensure a proper enforcement and empowerment of citizens in relation to their rights, considering the additional challenge of digital literacy and inequalities in a fast moving environment. ;Other;Where to draw the line between high- and low-risk apps? E.g what about apps used in relation to employment, either to pre-select candidate for a job or in context like the COVID-19 to determine the vulnerability of an employee to the disease for him/her to return back? The risk can be low some and highly significant for others, there might be competitive risk, e.g reassuring an informal carer v/ protecting the autonomy and freedom of an older person. See Equinet Report on AI for concrete cases.;;;"All AI app have to be treated carefully and respect human rights. They use and generate data and have detrimental effect if not well-trained and not reflecting the complex social-realities of our world. Protecting against data breach or unauthorised use is still a challenge making any AI application sensitive. Some application might not be defined as ""high risk"" but still are crucial on a daily basis (e.g. application used in car parks) and need to be fully reliable and accessible.";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;;Rather not;Voluntary labeling system does not really help citizens and protect them in a reliable way, bearing in mind the sensitivity of most AI application, the inequalities of citizens in terms of understanding and being able to deal with the complex mechanism of these applications. ;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;;;;;;;;;;AI_WhitePaper_Contribution-Addendum_AGE_2020-06-12_FINAL.pdf
F530073;12-06-2020 14:27;English;NGO (Non-governmental organisation);Ioanna;Psalti;;European Alliance for Vision Research and Ophthalmology;221589017973-83;Micro (< 10 employees);Belgium;The feedback can be published with your personal information;4 - Important;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;"Engage with investors;
Working with countries where the EU has established commercial relations to ensure continuity of relations;
Work with civil organisations and all kinds of communities such as grassroots AI and informal groups, conferences and associations to achieve an aligned AI agenda with priorities that reflect citizens needs.";4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;"Harmonisation of EU ethical guidelines for all development stages & uses of AI to enable pan-European AI services and prevent divergence at national level.

Comprehensive mapping of who’s involved, where they’re based and what they’re doing (studying AI, producing/using AI solutions) at both public and private domains (startups & companies, products, research labs, public administrations, universities and research centres, communities and investors)";5 - Very important;5 - Very important;5 - Very important;"Harmonisation of EU ethical guidelines for AI development/use;
Sector-specific approach with incentives for compliance;
Empowerment of public bodies to oversee/audit/monitor AI technologies by domain (e.g. health, education);
Increase awareness among data generators on strategies for creating value from data & generating revenue streams for sustainable AI research;
Specific support to SMEs to overcome fragmentation of suppliers/consumers & disruption by major data players (Amzon,Google,etc";4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;"Constant reviewing of: state of AI application, implications to national security & industry, implementation weaknesses & changing needs in AI ecosystem evolution.
Adaptive security measures & awareness among SMEs of risks as data are targets for distortion/corruption.
Target support to innovation segment of AI ecosystem (hyperscalers,technology enablers application-focused solution providers,platform vendors & professional service providers as the drivers of real-world apps & niche solutions.";4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;"Perpetuation of existing inequalities & prevent access to health,accommodation,credit, employment justice due to reliance on faulty metric for assessing need; https://science.sciencemag.org/content/366/6464/447
Failure to improve from real-world use due to lack of ability to facilitate future adaptation as in COVID-19 diagnosis/prognosis with ‘locked’ algorithms,Ref attach). 
Insensitivity to impact. More specific healthcare concerns: https://qualitysafety.bmj.com/content/28/3/231";There is a need for a new legislation;;Other;"New requirements should be compulsory for any type of technology that puts our European fundamental values at risk as such societal impact constitutes harm in a broader sense e.g. online services &social media platforms/websites/apps to also abide by EU values in how they are developed/what they deliver & have mechanisms for manual intervention & override;
Compulsory to create bias-resistant tools with disclosure of measures taken to reduce bias towards non-representative populations   ";;;"Healthcare needs:
clarity in output& algorithm aimed at users; validation with input data on population characteristics,systemic diseases,diverse disease’ phenotypes;different settings/conditions, e.g.diverse ethnics/environments;  
Compulsory for AI models to integrate continuous monitoring and adaptation strategy to changing data to maintain predictive accuracy.
Quality systems/good machine learning practices to be established with
relevance of available data to clinical problems/practice;";5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;"Harmonise and enable the uniform implementation of the four European instruments which prohibit biometric mass surveillance: the European Convention on Human Rights and the EU Charter of Fundamental Rights,, the (Modernised) Council of Europe Data Protection Convention, the GDPR and its sister instrument, the LED. 
Any planned activities and deployment that amount to mass surveillance in public spaces should be publicly disclosed
Revise EU legislation to include guidelines on how to assess targeted identification checks that are proportionate to the issues and context, with recommendations on the provision of effective remedies against abuse.";Much;"Collect & identify good current practices; Sector-specific legislation in health sector to ensure rigorous implementation of the ethical rules retaining human control over algorithms &decision-making systems. Improve clarity in GDPR implementation in health research. Voluntary labelling system to include physical and psychological health, data protection & protection of privacy and not cause injustice/damage/suffering to individuals or groups of people. ";A combination of ex-ante compliance and ex-post enforcement mechanisms;;The choice of convenient, seemingly effective proxies for ground truth can be an important source of algorithmic bias in many contexts.;Personal security risks;All the risks related to individuals such as physical safety, financial health, and equity and fair treatment. For details on such risks: https://www.mckinsey.com/business-functions/mckinsey-analytics/our-insights/confronting-the-risks-of-artificial-intelligence#description;Yes;The traditional paradigm of medical device regulation was not designed for adaptive, autonomous technologies, which have potential to adapt & optimize device performance in real-time to continuously improve healthcare for patients. A new, total product lifecycle (TPLC) regulatory approach is needed that facilitates a rapid cycle of product improvement & allows these tools to continually improve and safeguard.;Yes;;Yes, for specific AI applications;"Health applications
FinTech applications
Agriculture applications";;
F530072;12-06-2020 14:26;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;3 - Neutral;The Commission should develop public awareness campaigns and enhance education and training efforts to prepare EU citizens to face changes resulting from the uptake of AI. Moreover, the introduction of a proper data ecosystem supporting data exchange is key to support AI innovation and use. There also needs to ensure that Europe remains competitive globally while respecting EU values and principles, by promoting the development of an ecosystem with AI systems responsible by design.;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;While all the actions mentioned above are critical to promote AI innovation (e.g. sandboxing approaches, tax incentives, one-to-one academic-private partnerships), the Commission should consider additional levers to support the full-scale value chain and ultimately help risk takers to scale up. For this purpose, the private sector needs incentives to support investments (e.g. to create business innovation jobs), facilitate partnerships as well as an easier access to data in order to scale up.;4 - Important;5 - Very important;5 - Very important;There needs to have focused research and innovation programs gathering very few experts partners (e.g. to identify risks specific to AI systems or to develop labels). Similarly, cooperation between private companies should be encouraged to support the development of best practices. We are indeed convinced that the development of secure, robust and fair AI systems needs to be tested in real context, before industry deployment and we are willing to support the European R&I community to that end.;4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;;4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;AI systems will be a major target for cyber-attacks and hacking risks raise important data privacy concerns. These are directly related to the robustness and control specificities of AI because (i) an insufficiently robust AI system will not detect issues before and during a cyber-attack and (ii) a lack of control over an AI solution poses a threat for the system itself and data processed.;Other;Trustworthy AI needs to be translated in concrete tools and not just guidelines to effectively protect EU citizens' rights while not impeding innovation. More precisely, (i) more structured investment in R&D are needed to produce tangible tools enabling industry uptake of AI systems and (ii) legislation must avoid overlap or contradictions with partially covered topics. For instance, actuarial work and models are already strictly regulated and relevant existing regulations should not be ignored.;Other;The introduction of new compulsory requirements raises many questions regarding the (i) type of AI usage targeted, (ii) the stage(s) of the life cycles of AI systems they intend to apply to and (iii) how to cope with temporal issues since updates could change the high-/low-risk profile of an AI system. We consider that to cope with the evolving nature of AI systems, these requirements should be proportionate to the risk and keep human control over the different stages of AI development.;;;;3 - Neutral;2 - Not important;4 - Important;5 - Very important;5 - Very important;5 - Very important;No opinion;;Much;To be truly beneficial to our customers, a voluntary labelling system needs to be identified as a pledge of quality and a differentiator with other competitors, which raises questions in terms of assessment (audit, compliance requirements, etc.). It will be even more difficult to define a relevant labelling system because there are no clear stands yet on explainability and fairness issues, for instance.;Other enforcement system;There is a need to define a clear governance scheme, with clear human responsibilities, to ensure the deployment of robust, secure and fair AI systems across the EU. The Commission can play a critical role in assisting organizations to establish such a governance. For instance, the adaptation of the model designed by the GDPR for data protection (e.g. implementation of Data Privacy Officers) should be discussed.;If the Commission wants to perform risk assessments on AI fairness for example, clear metrics will need to be defined and implemented. Similarly, benchmarks will be of great value to promote the deployment of fair AI systems across the European Union.;;From an insurance perspective, we consider that the most concerning risks posed by AI systems are non-material losses. In addition, the deployment of AI systems raises concerns in terms of systemic risks, since the market is developed into monopolies, therefore creating risks of accumulation and raising concerns regarding the insurance industry's funding capacity.;Yes;AI systems can change during their lifetime and the risk assessment procedure proposed by the Commission could help to ensure product safety. However, given the rapid pace of digital developments, the Union product safety framework must be adaptative. For that reason, a principles-based approach supplemented by a proportionate and incremental risk assessment procedure, would bring more flexibility to adapt to the level of maturity of AI solutions.;Yes;"Due to the complexity of AI systems, we agree that some concepts and definitions of the Product Liability Directive should be reviewed to address AI-specific challenges, namely revisit the definition of ""product"" and modalities for ""putting [it] into circulation"". In addition, we consider it would be in the interest of our customers to harmonize national liability rules so that product liability insurance covers claims for non-material damage.";No;;We think the current EU liability Framework is adapted to cope with AI. Introducing an AI-specific regime with a reversal of the burden of proof or strict liability coupled with mandatory insurance for high-risk AI systems would result into higher premium for consumers given risks of accumulation for insurers, creating barriers to AI innovation while complicating liability claims. It could even result in uninsurable high-exposure sectors (e.g. energy) since proving causation will be difficult.;
F530071;12-06-2020 14:23;English;Other;Evanna;FRUITHOF;;"General Council of the Bar of England & Wales ""The Bar Council""";39850528734-23;Medium (< 250 employees);United Kingdom;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
F530070;12-06-2020 14:22;English;Business Association;Juliette;PRISSARD;;EUROCINEMA, association of films and television producers;4,32457E+12;Micro (< 10 employees);Belgium;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;EC_consultation_Artificial_intelligence_EUROCINEMA-contribution_12.06.2020_final.pdf
F530069;12-06-2020 14:12;English;Public authority;Niklas;Johansson;Regional;European Forum North Sweden;;Large (250 or more);Sweden;The feedback can be published with your personal information;No opinion;No opinion;No opinion;5 - Very important;No opinion;5 - Very important;The European regions are key actors within the move towards digitalization and can in cooperation with academy, research institutes and commerce play an even larger role in the implementation of the EU´s strategies and support. In Sweden, the regions carry the responsibility for regional growth policy and the strategies for the regional development funds which constitutes a resource for the dissemination and inclusion of digital solutions all over Europe with its existing organizations. ;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;;No opinion;No opinion;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;The regions work with Smart Specialization enables strategic initiatives and implementation of EU strategies in combination with an inclusive bottom-up process. The work should be one of the platforms for the EU's ambitions in digitalization, for example in building a network of EDIH within the DEP. There are obvious benefits and synergies to achieve by linking the EU:s efforts for digitalization to the region's support to the regional innovation system and strategies for Smart specialization.;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;;No opinion;;No opinion;;;;;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;;No opinion;;No opinion;;;;;No opinion;;No opinion;;No opinion;;;Postition_Digitalization_North_Sweden_European_forum.pdf
F530068;12-06-2020 14:10;English;Company/Business organisation;Elisa;Gastaldi;;Siemens Healthineers AG;982823533509-58;Large (250 or more);Germany;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;5 - Very important;4 - Important;3 - Neutral;4 - Important;5 - Very important;5 - Very important;For the development of data spaces, high-quality, standardized, structured data will be key. Legal European-wide certainty and alignment in the approach towards incentives, financing, and reimbursement of digital health technologies, including AI in healthcare, would be beneficial for the efficient deployment of already existing AI solutions. ;5 - Very important;3 - Neutral;4 - Important;Defined rules and collaboration principles concerning access to data;3 - Neutral;5 - Very important;3 - Neutral;3 - Neutral;3 - Neutral;;4 - Important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;No opinion;We are concerned that the discourse has been focused on risks rather than the potentials and benefits of AI. We should rather focus on the benefits of using AI compared to not using it. ;Other;It is worth noting that the safety of stand-alone software, when it is considered a medical device, is already addressed by both the MDR/IVDR sectorial legislations. We believe exisitng legislation is adequate, but helpful to have guidance on concrete cases. ;Other;Introduction of new rules should be done based on a risk/benefit balance, in alignment with sectorial rules. In medtech, MDR and IVDR address the need to safeguard patient safety in response to technological progress.;;;Any future categorization of “risks” posed by AI systems should be specified under sectorial legislation, for instance in IVDR/MDR which have their own risk classification and respective regulatory requirements, to avoid duplication and confusion in regulated sectors.;3 - Neutral;4 - Important;3 - Neutral;5 - Very important;3 - Neutral;4 - Important;;Biometric identification is  not AI related ;Not at all;"Medical devices are only used by doctors and trained personnel, a label wouldn´t bring any benefit to them.
we don´t think that a label system on medical devices would bring benefit to patients and citizens.
";Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;In healthcare applications currently the certification process is done ex/ante and it works  really well. ;;"These are not AI-related risks, these risks are associated to digital solutions in general. 
we believe that risks should be balanced against the opportunities provided by using AI.";No;there should be a sector specific approach  - in MDR risk assessment is already part of core assessment, there should be an assessment on whether the existing risk-mitigation assessment processes for different categories of medical devices are sufficient .;No;PLD contributes to a reasonable balance between protecting those who suffer injury and ensuring fair competition. Where PLD should not manage to address AI specific challenges, we see value in developing guidance to clarify certain issues under the Directive (e.g. placing on the market) or amending the existing directive (e.g. in respect to software). We do not see a gap of liability, in case the overall consultation process comes to such a conclusion, exemption for medtech should be considered.;No;;The German Ministry for Justice has recently looked into this topic (more in the annex)https://www.justiz.nrw.de/JM/schwerpunkte/digitaler_neustart/zt_fortsetzung_arbeitsgruppe_teil_2/2019-06-06-JuMiKo---Beschluss.pdf;Consultation_on_AI_Strategy_-_SHS_annex_I.pdf
F530067;12-06-2020 14:07;English;NGO (Non-governmental organisation);Pascale;Vandenbussche;;European Confederation of Institutes of Internal Auditing (ECIIA);TR84917001473652;Micro (< 10 employees);Belgium;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;4 - Important;4 - Important;Based on our experience, as internal auditors and as users and auditors of artificial intelligence, we believe that it is important to develop the right skills (and manage job transformation) but also to define the data sets (labelled by whom?) and the development of European infrastructures (cloud,network,..).It is also key to decide the strategic dimension of AI regulation: do we need more European protectionism or not ?;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;AI governance refers to the structures, processes and procedures implemented to direct, manage and monitor the AI activities of the organisation,ensure that there is ownership and accountability over AI activities, that there are controls in place to manage the associated risks,that the objectives of these activities are ultimately met . Data quality is also key and norms must be set up at European level for data and infrastructure. ;4 - Important;4 - Important;4 - Important;The pool of talent for technology professionals with AI expertise is reportedly small. Organisations who want to participate in the AI revolution need to grow or acquire talent with competencies in a multitude of areas such as:  Natural language processing, ? Application program interfaces (APIs) such as facial recognition, image analytics, and text analytics, algorithms and advanced modeling,probabilities and applied statistics,Data analytics,Software engineering, Programming...;4 - Important;3 - Neutral;3 - Neutral;3 - Neutral;4 - Important;It is important to develop facilities for artificial intelligence development in Europe. It gives a lot of opportunities in term of processing large amounts of data and automate repetitive and burdensome steps.It should be available for all types of companies.;4 - Important;5 - Very important;3 - Neutral;3 - Neutral;3 - Neutral;4 - Important;It is important that the organisation  clearly articulates its AI strategy and clearly expresses the intended result of AI activities. The foundation upon which AI systems are built are key:Data assets (inventory, quality, security,privacy,management and infrastructure technology). Third party risks and cyber resilience are important to consider too.;Current legislation may have some gaps;;Yes;;Yes;;"We recommend a risk based approach to define the ""high risks"" AI applications. The approach should be global and assess all types of risks involved (strategic and environmental, business,operational,reputational).";5 - Very important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;No further guidelines or regulations are needed;;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;"In terms of risks assessment, it is important that the different parties involved assess the AI process based on a clear AI governance framework. Once AI is implemented, it is worth assessing the compliance based on ""real data"".Internal audit provides objective assurance that the company’s use of AI is in compliance with all relevant industry standards and régulations and will make recommendations. ";Personal security risks;;No opinion;;No opinion;;No opinion;;;letter_NFR_DG_connect_AI.pdf
F530066;12-06-2020 14:00;English;Public authority;Riikka;ROSENDAHL;National;Finnish Competition and Consumer Authority / Finnish Consumer Ombudsman;;Medium (< 250 employees);Finland;The feedback can be published with your personal information;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;;No opinion;No opinion;No opinion;;No opinion;No opinion;No opinion;No opinion;No opinion;;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;See our opinion (attachment).;Other;See our opinion (attachment).;Other;See our opinion (attachment).;;;;4 - Important;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;No opinion;;No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Risks related to the loss of connectivity;See our opinion (attachment).;Yes;;Yes;;No;;;AI_public_consultation_Finnish_Consumer_Ombudsman_120620.pdf
F530065;12-06-2020 13:57;Spanish;;;;;;;;;The feedback can be published in an anonymous way;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
F530064;12-06-2020 13:54;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;Increase awareness in public debate. ;5 - Very important;4 - Important;5 - Very important;3 - Neutral;5 - Very important;4 - Important;;5 - Very important;4 - Important;5 - Very important;;5 - Very important;4 - Important;4 - Important;5 - Very important;3 - Neutral;SME don't necessarely have to develop their own AI expertise. Knowledge transfer is more importan and efficient. ;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;;Current legislation may have some gaps;;Yes;;Yes;;;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;Yes;;Yes, for specific AI applications;;;
F530063;12-06-2020 13:52;Spanish;Business Association;Cristina;Cartes Andres;;ADIGITAL;972127919039-72;Small (< 50 employees);Spain;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;ADIGITAL_contribution_public_consultation_White_Paper_AI.pdf
F530062;12-06-2020 13:40;English;Company/Business organisation;Jacinto;Pico;;Naturgy Energy Group;67833029261-54;Large (250 or more);Spain;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"It is also important to take into account the views of Expert Teams and Working Groups that have already been working on data issues and advanced data analyses (e.g. Chief Data Officers).

Additionally, it is important to support national administrators in the adoption of new frameworks being designed at EU-level.

Investments, Working groups and support for European regulation implementation are essential to accelerate the adoption at public and private level.";5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;4 - Important;To promote a tailored plan for science, technology, engineering and mathematics (STEM) students. A European initiative to promote post-doctoral studies and specific programmes in Artificial Intelligence ;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;Scale of adoption of Artificial Intelligence is very constrained by the lack of deployment of digital platforms and, for SMEs, this access would not be possible without Cloud facilities. Therefore, it is critical to ensure access and promote cloud SaaS services for Artificial Intelligence.;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;;Other;We need to be transparent and assign responsibilities to ensure the protection of citizens;Other;Europe should try to avoid establishing a complex and burdensome framework for Artificial Intelligence;;;These are well addressed in the EC's 'White Paper';4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);Only in carefully weighed cases of general interest, where the pursuit of common good prevails over the interests of the individual;Much;;Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;;Personal security risks;It is necessary to build a civil responsibility framework to cover all the risks for the individual and to make sure it is clear enough so that the stakeholders are aware of who is responsible for what.;Yes;Regulate products and services not only at the time of marketing and supply but all throughout their entire life cycle;Yes;A clear European regulatory framework would build trust between consumers and businesses and accelerate their adoption;Yes, for specific AI applications;Yes, in those AI applications where the model could generate a direct or indirect risk for people;;
F530061;12-06-2020 13:36;Portuguese;EU Citizen;Marise;ALMEIDA;;;;;Portugal;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Promote partnership with civil society organizations;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;4 - Important;4 - Important;;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;3 - Neutral;;Current legislation may have some gaps;;;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;No opinion;;No opinion;;;
F530060;12-06-2020 13:35;English;Other;Katharina;Sehnert;;National standardization committee for Artificial Intelligence in DIN e.V.;;Large (250 or more);Germany;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Please see attached document;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;2 - Not important;5 - Very important;5 - Very important;Please see attached document;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;;;;;;;Please see attached document;Other;Please see attached document;Other;Please see attached document;;;Please see attached document;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Not at all;Please see attached document;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Please see attached document;Cyber risks;Please see attached document;Yes;;Yes;;No opinion;;;NA043-01-42AA_20200612_EU_Consultation_on_the_White_Paper_on_Artificial_Intelligence.pdf
F530059;12-06-2020 13:33;Swedish;Trade Union;linda;larsson;;Landsorganisationen i Sverige, The Swedish Trade Union Confederation;;Large (250 or more);Sweden;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;Landsorganisationen i Sverige har valt att besvara samrådet genom bifogat dokument;LOs_svar_p__EUs_vitbok_om_AI.pdf
F530058;12-06-2020 13:32;English;Company/Business organisation;Carlos;Rodriguez Cocina;;Telefonica;52431421-12;Large (250 or more);Spain;The feedback can be published with your personal information;3 - Neutral;3 - Neutral;4 - Important;4 - Important;5 - Very important;4 - Important;"Access to data is fundamental for development of AI: to enable access to data EU the Data Strategy or DSA should foster voluntary data sharing based on contractual freedom, the development of European Data Spaces and tackle positions of dominance where a competitive advantage is reached through data.
Public procurement of AI systems should be considered as lever for ecosystem of excellence. A roadmap on instruments and allocation of funds among sectors / priorities is needed.
";4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Policy makers should have a meaningful understanding of new technologies, including AI and big data to define AI policies, to enable them identifying, and thus prioritizing, AI areas subject to greatest impact. Focus on start-up and innovation should include scale-up programs and instruments to foster their development into business for a more efficient use of resources.;2 - Not important;4 - Important;5 - Very important;"Having a practical and business-oriented approach to investigation of AI should be a priority; as such the development of public-private partnerships to jointly coordinate efforts should be top priority among the proposed actions. Cooperation initiatives should expand beyond EU, collaborating with technological allies sharing the same human centric values of the EU.";3 - Neutral;4 - Important;3 - Neutral;5 - Very important;4 - Important;"Fostering SMEs access to data for AI development should be a priority, especially when positions of data dominance result in limiting restrictions to access data. EU Data Strategy should tackle dominant positions leveraging data access to expand dominance in AI, as well as to enable B2B data sharing on voluntary basis.
Antitrust authorities should enable knowledge transfer, so it is not considered anticompetitive. Support of partnerships should include funds and adoption of regulatory sandboxes";5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;3 - Neutral;"Unintended negative consequences on individuals are difficult to predict but still need to be carefully managed; more even when benefits of AI could still be greater than its negative side effects comparing it to current practices without AI. 
Relevant issue is comparing AI-driven decision making with current human decision making, which is also subject to bias an error
AI could increase inequalities if fostering dominant positions of companies or if less developed regions have no access to AI";Other;"Current legislation can address most uses.. For high risk AI applicable legislation is to be clearly identified and if needed, expanded. Overlapping among legal instruments should be avoided to : obligations for high risk AI could end up applied to non-high risk via GDPR or Liability and Safety.
In cases it is unclear if the WP refers to personal or non-personal data. For personal data, we consider the EU GDPR to be sufficient. New regulation should be limited to clearly identified problems.
";Yes;;Other;The double cumulative criteria and conformity assessment are well focused but leaves room for interpretation. A more precise assessment for identifying high risk is needed: clear criteria for identifying high risk are missing, it is unclear who will be responsible for applying such criteria. How often this assessment process is to be modified can have a significant negative impact on the industry. The high risk definition should be aligned with new liability provisions to assure consistency.;"Combination of massive surveillance, biometrics & AI could have severe negative effects on societies and human rights.
Decision making systems impacting many people's lives (housing, jobs, judicial, allocation of public resources) with (biased) data subject to constant updates that results in changing outcomes for the same incomes. The question is if AI decision improves current human biased decisions. Therefore, the focus should be on analyzing and limiting bias in AI systems and training data";4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;Biometric identification systems should only be used in public spaces for national security of health emergencies and should not take place until specific guidelines or legislation on legitimate usage are in place at EU level.;Much;Labeling systems should be defined through multi stakeholder processes including the participation of industry, academia, government and civil society. Its adoption should be completely voluntary and labelling schemes should not become mandatory or turned into law. Rightly defined labelling can become a de-facto standard because it is much appreciated by consumers, in this way it becomes a priority for business to adopt them without the need for legislation.;Other enforcement system;High-risk AI should fall under “ex ante” products safety rules that address new challenges. A balanced combination of ex ante and ex post is also reasonable as risk resulting from some AI uses is not always fully predictable. Clear provisions should define who decides an application is high or low risk. Using 5 layers of risk as German Data Commission can be more efficient than all or nothing approach. ;"Clear regulatory roles and cooperation mechanisms are needed to ensure enforcement effectiveness
Risk assessments should be tested with use cases with input from product owner's side. Any ex-ante conformity assessment should be carefully designed to avoid creating a burden for developers of AI in Europe versus other regions of the world
Evaluation of AI risk and GDPR could be too onerous for SMEs and for bigger non-native AI specialist businesses, being a huge deterrent for development of AI
";Mental health risks;"Risk assessment of AI should be focused on direct risk resulting in harm to individuals, not on effects of other systems which could indirectly affect individuals; otherwise accountability & liability of AI systems would be unlimited
Specific uses of AI can result in relevant risk not directly affecting health of individuals: AI-generated fake news, AI to increase usage of app and services leading to addictions, AI systems for reinforcing echo chamber effects; this should be carefully assessed";Yes;;Yes;Current Product Liability Directive and new legislations, such as the Code, the Omnibus Directive, Sales of Goods Directive and Digital Content Directive, are enough to address most legal concerns and the needs of consumers, suppliers and business. Only for cases where AI subject to important changes over lifetime, such as machine learning continuously being trained, risk assessment and liability should be adjusted to avoid legal uncertainty.;Yes, for specific AI applications;Self-driving vehicles and other very high-risk applications with direct negative effect on health and lives of individuals could be a case. But in general liability should be similar to the one of the human based system replaced by AI. Otherwise, if there is huge liability on AI systems, in particular on start-ups, the entire AI ecosystem could break down - it won't even get started.;"It is neither practical nor future-proof to create an AI-specific regime for compensation and allocation of liability. This should remain covered as much as possible by horizontal rules to ensure consistency.
";
F530057;12-06-2020 13:31;English;Company/Business organisation;elizabeth;CROSSICK;;RELX;338398611148-62;Large (250 or more);United Kingdom;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;These actions will help build the excellence ecosystem, RELX would add to this education and the promotion and sharing of best practice to help citizens understanding of AI which in turn will help its uptake. This includes giving them tools to understand risk and how to evaluate it. Nothing has zero risk & there can equally be a risk in not doing something. We also would like to see the work of AI to achieve the SDGs elevated to an EU action.;5 - Very important;No opinion;5 - Very important;4 - Important;5 - Very important;3 - Neutral;Whilst we are not ourselves a startup, we work with startups and recognise their value and importance in the ecosystem, hence our responses above. Re the data space there are potential benefits to a European data space if it focused on voluntary and collaborative approaches to data sharing, but would be counterproductive if it created a closed European space as it would significantly hamper innovation, which is borderless.;4 - Important;4 - Important;4 - Important;"We support all these actions and see them as a package that work together to encourage coordination & collaboration across Europe as opposed to picking one ""winner"".  It is also important to ensure quality data, for which a robust IP framework helps.";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Whilst we are not ourselves an SME, we work with startups and SMEs and recognise their value and importance in the ecosystem, hence our responses above.;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;AI is a technology and is not in and of itself harmful, it depends on the use case. It is the decisions that can be harmful. Key to this is keeping human oversight as a prerequisite. We would also note that for the same list of concerns, AI can also have beneficial outcomes and its important not to lose sight of the positives AI can bring to society. Furthermore the machine is only as good as the data it gets, so the quality of data is also relevant. ;Other;most EU legislation is technology neutral so whilst current legislation may have gaps we would suggest a good starting point is to do a review of existing legislation to evaluate whether amendments to these (eg product liability, civil liability, GDPR) would be both efficient and sufficient.;Yes;;Other;We support a risk-based approach and believe the Commission’s suggestion is a useful starting point for this discussion, although we have some reservations that a sector-based approach could become overly prescriptive. However high-risk is defined, the context of an application is key, and risk assessments can help provide that context. An exceptional circumstances clause undermines the Commission’s efforts and would create significant legal uncertainty. ;;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;overt use of facial biometrics provides unique opportunities (eg identifying criminals) but also unique risks (eg social profiling) and these should be clearly understood;No opinion;business/sector led labelling schemes can help build trust of users so could be helpful but at the same time they can also add significant burdens to companies and as the technology keeps evolving may become out of date very fast. ;Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;It could be helpful for companies and organisations to be able to reach out to their local regulator for assistance and guidance;Personal security risks;there is a danger that predesignating risks is a snapshot in time approach that could quickly become outdated;No opinion;;Yes;;No opinion;;;20200611_RELX_AI_Position_Paper_Final.pdf
F530056;12-06-2020 13:28;English;Academic/Research Institution;Petri;Myllymäki;;European Laboratory for Learning and Intelligent Systems (ELLIS);;Large (250 or more);Germany;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;Establishment of ecosystems, platforms and protocols for utilization and sharing of diverse data sets, and for preventing monopolistic accumulation of digital data.  ;5 - Very important;4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;;2 - Not important;5 - Very important;4 - Important;Distributed but closely interlinked networks of AI excellence centers are the key. ;4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;;3 - Neutral;4 - Important;4 - Important;5 - Very important;4 - Important;5 - Very important;;Current legislation may have some gaps;;Yes;;Other;Limiting to high-risk applications is OK, but this should apply to ALL systems, whether they are AI or not.;;3 - Neutral;3 - Neutral;3 - Neutral;5 - Very important;4 - Important;4 - Important;;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;No opinion;Again, this should not apply to AI only, but all digital products and services.;Yes, for specific AI applications;E.g. decision-support systems in governmental decision making. ;;ELLIS_Comments_on_the_EU_white_paper_on_AI_signed.pdf
F530055;12-06-2020 13:27;English;Company/Business organisation;Bernard;GEORGES;;Société Générale;34369111614-57;Large (250 or more);France;The feedback can be published with your personal information;4 - Important;4 - Important;4 - Important;3 - Neutral;4 - Important;4 - Important;An increase in the awareness of European citizens about AI would help by improving the understanding of benefits and limits of IA. Significantly increase investment in AI, to achieve the same level of excellence and development as North America and Asia. Treat related subjects (data, cloud, etc.) in coherence and with the same level of ambition. Encourage multidisciplinary and transdisciplinary training. Support the development of academic initiatives aimed at integrating AI into IT programs.;4 - Important;3 - Neutral;4 - Important;3 - Neutral;4 - Important;4 - Important;Increase investment in research is key. Develop “applied research” to bring out European champions. The subject of tests should be clarified: it is difficult to separate the production of AI algorithms from an effective testing policy. We are sceptical, in terms of re-search, on the effectiveness of large-scale funding of specialized AI start-ups. Few of them can bring a truly transformative innovation, most of them focusing on vertical businesses and will be integrated in larger structures.;4 - Important;4 - Important;5 - Very important;"Put in place incentive tax policies to facilitate the creation of partnerships (public-private; (inter)sectoral, with a critical size; with or between large research institutions). Review the rules of competition and dominant market position. EC policy should better connect research to the business world, to get both academic excellence and industrial performance, and promote industrialization. Increase demand for AI, by taking initiatives to strengthen its use by corporates and individuals.";3 - Neutral;3 - Neutral;3 - Neutral;4 - Important;3 - Neutral;To achieve ambitious objectives, the creation of specialized digital hubs by member state, proposed by the EC, should be based on “majors”, and not on the articulation or participation of SMEs and/or startups. More generally, we must rely on companies with a signifi-cant strike force, versus a network of small innovative players (startups, SMEs, etc.) who will have difficulty in having sufficient means.;3 - Neutral;4 - Important;3 - Neutral;4 - Important;4 - Important;3 - Neutral;Rely mainly on existing regulations. We recommend avoiding duplication and overly prescriptive regulations to keep agility, given the very evolving nature of AI. The banking sector is already highly regulated, and supervised, and subject to very high internal and ex-ternal control requirements for all of its types of risk, including the specific risks that could result from AI. Take inspiration from the regulatory, governance and supervision framework of the models used in the financial sector.;Other;Adopt with respect to AI (whatever its forms, ranging from algorithms based on predictive logic (ML, big data, Analytics) to more forward-looking algorithms (machine reasoning, exploratory algorithms (MAS), deep learning), with the exception of a very hypothetical “strong AI”) a technology neutral principle, considering AI as a technology among others, however presenting some specificities requiring special attention justifying, if necessary, to adapt or supplement existing regulations.;Other;In the long term, it would be competitively fair, consistent and sustainable over time, to retain at the first level a risk-based approach, and to adopt possible specific measures only in proportion to the foreseeable or proven risks linked to the uses and users of AI. The concept of risks should be extended to all risks that could have serious consequences, ranging from the (physical …) risks mentioned in the WP to the risks of manipulation of opinion, risks to democracy, psychosocial risks…;;;"In any sector, any actor (GAFA, etc.) using AI and who may present high-risks or systemic risks (impacts on opinion, public liberties, health, etc.) must be required to act within an internal and external control framework, inspired by the best international standards. The banking regulation relating to technology is already very comprehensive and could in-spire all actors, in particular all those who, without being banks, carry out ""financial"" or related activities.";5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;No further guidelines or regulations are needed;The current legal framework (GDPR, CNIL, ACPR, etc.) seems sufficiently robust to grasp the main risks linked to the use of AI, especially from the consumer's point of view, and to avoid any risk of excess or slippage.;Rather not;Labelling systems are difficult to set up, and likely to complicate the implementation of AI systems, which are often scalable or self-learning, or embedded into larger systems in the form of internal or external components that are difficult to isolate. Another option would be to label, not products or services or specific algorithms, but the ability to set up and comply with an appropriate “algorithm governance framework” allowing, upstream and downstream, to have appropriate risk control.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;For the banking sector, the implementation of AI is taken into account through existing governance frameworks (data, models) and regulations (compliance requirements, very strict internal control, many customer protection obligations, supervision by independent authorities). The banking sector is significantly ahead about AI, and could even be a model for other sectors, given its ability to grasp systemic risks and to maintain the customer confidence which is a key issue for the sector.;;;;;;;;;The current liability rules are already well suited to the use of AI. However, it can be assumed that some specific applications or uses (to be identified) may, by exception, require special attention. Thus, some difficulties could remain in determining the origin of possi-ble problems, in particular to determine the causal links, to identify the person responsible vis-à-vis the end user, to bring an action (against whom in the chain of responsibility, if external supplier, if third party, etc);
F530054;12-06-2020 13:26;English;NGO (Non-governmental organisation);Maja;Lænkholm;;Danish Academy of Technical Sciences (ATV);;Small (< 50 employees);Denmark;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;4 - Important;;4 - Important;Increased focus on research and development of a European approach to AI, and that the approach puts man at the center of technological development, the role of technologies in society. Debate the use of AI in critical areas such as health, social work, transport, climate, etc. Increased use of data creates positive effects for humans;5 - Very important;5 - Very important;4 - Important;4 - Important;;3 - Neutral;Create platforms where citizens, experts, authorities and industry can be in dialogue and that the dialogue is also action oriented. However, we also experience some national political voices, including that the government has so far completely failed to discuss and shape a Danish strategy. Past successes are mostly due to our quick internet access. Most countries have the Internet now and the competition is now on educating and attracting talent. Large investments are needed to compete.;5 - Very important;4 - Important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;;3 - Neutral;4 - Important;4 - Important;5 - Very important;3 - Neutral;5 - Very important;That we focus to much on the ethical concerns and not the possibilities;There is a need for a new legislation;;Yes;;Yes;;;5 - Very important;4 - Important;4 - Important;3 - Neutral;5 - Very important;4 - Important;No opinion;;Much;;Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;;Mental health risks;;Yes;;Yes;;No opinion;;;ATV_English_comment_final_whitepaper_AI.pdf
F530053;12-06-2020 13:26;English;Academic/Research Institution;Jeanette;Nilsson;;Research Institiutes of Sweden, RISE;;Large (250 or more);Sweden;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"Invest in and facilitate lifelong learning in AI. Focus on inter- and multidisciplinary research with AI in focus that can drive both disciplinary and AI research forward.
Establish a large-scale cellular test system for IoT and applied AI 
New Businessmodells and value-based procurement
Important to secure fundamental research longtime funding without any application requirements  ";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;" Inter- and multidisciplinary research with AI in focus. AI advisor and change manager for the public sector and for different industries. Increase digital insight  and inclusion to enable residents to take part in digital services and services through popular education efforts. 
Encourage conversations between associations, trade unions and civil society on issues of AI and the impact of automation on society, working life and the required transition.
";5 - Very important;5 - Very important;5 - Very important;"AI is defined as a separate research area among research financiers. Create conditions and incentives for shared employment between companies and academia to strengthen knowledge transfer and ensure access to skills. Build infrastructure for industrial research in AI and applied AI. Develop national test beds. Mandate authorities to structure and make data available in collaboration with industry so that it is accessible for further use and innovation. 
";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"
“Use case factory"" - a place where good examples of applications of AI in different industries are continuously updated and created. ""Upskill and reskill"" to cope with the transition of working life.  
Develop professional and role-specific training in AI, and expand incentives to create more business (small and large) and application-oriented AI training. 
Ensure availability of data and skills 


";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;" 
Create access for new inputs of  AI companies to the public sector and government through innovation platforms. Ensure that citizens  are given sufficient knowledge and understanding of AI and its consequences so that they can be active and critical members of society. Develop a digital AI guide, which sets out guidelines and practices that support the efficient and sustainable development and use of AI solutions in public administration and business 


";No opinion;;No opinion;;;;"


";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;"Develop guidelines and best practices to apply AI in a legally secure manner. The legal challenges are many – data protection, patents, legal liability, product safety, etc. – and create great uncertainty that can hamper the implementation of AI. Guidelines and best-practices could significantly increase usage. Produce simple and clear examples of how personal data can be handled in a legally secure way. The value of the data is lost if we do not dare to use it 
";No opinion;"AI infrastructure/ applications  with common standards, where data and services can be shared with ensured privacy and data protection. 
";A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;Bias;No opinion;;No opinion;;No opinion;;"Everyone's equal value, understand and follow decisions and be able to appeal ex government decisions 
";
F530052;12-06-2020 13:21;English;Company/Business organisation;Jesús;LOZANO BELIO;;Banco Bilbao Vizcaya Argentaria S.A.;090350412968-04;Large (250 or more);Spain;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;;3 - Neutral;3 - Neutral;5 - Very important;4 - Important;5 - Very important;4 - Important;Regulators and supervisors’ expectations and practices should be harmonized in order to ensure a common approach to AI by each EU Member State and by sectoral and cross-sectoral authorities;3 - Neutral;5 - Very important;5 - Very important;"2 additional actions should be taken:
- Make available training datasets based on anonymised data from the real digital footprints generated by millions of citizens or IoT devices to avoid the barriers of fragmented/low scale datasets compared to those in the hands of researchers in other geographies (mainly USA and China).
- Create standards, applications and quality assurance methods to generate such cross-country datasets throughout key sectors (health, transport, environment, finance, etc)";5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;Although supporting SMEs is convenient for their AI adoption, access to such facilities should also be possible for larger firms;2 - Not important;3 - Neutral;3 - Neutral;4 - Important;2 - Not important;3 - Neutral;"AI entails risks and opportunities as any other technology. Identifying AI with higher discrimination or safety risks is undermining trust on this technology and AI adoption. This could divert vital resources from deployment to compliance.
Citizens and customers do not trust AI or any other technology, but they trust the firms, organisations or services using it. Risk assessment criteria should be defined linked to applications and use cases, not to an specific underlying technology such as AI.";Other;"The EU regulatory framework is already comprehensive and strongly oriented to the protection of citizens’ rights, making the EU a global benchmark on this.
Therefore, authorities should focus on providing guidance on how to comply with existing regulations to avoid unfair discrimination and achieve suitable levels of explainability or interpretability, and creating a harmonized framework of regulators and supervisors’ expectations on each AI application depending on their criticality.";Other;"Current legal framework is sufficient. The proposal is not technology neutral and does not solve legal uncertainties on AI use.
Designating high-risk sectors could harm the level playing field in those cases where an activity can be performed by companies from different sectors.
If finally developed, AI regulation should focus on AI activities, regardless of the sector, rely on a definition that only covers complex ML techniques and set clear criteria to designate high-risk applications.";;;;4 - Important;3 - Neutral;4 - Important;4 - Important;3 - Neutral;4 - Important;No opinion;;Much;"A voluntary labelling system could be useful as a reference for the development of internal procedures and as a global reference.
However, if finally developed, this labelling system should be oriented to the certification of activities and not firms as a whole, requirements should be proportionate to the actual risk posed by the activity, and the labelling system should not be favoured in ways that make it de facto mandatory.";Other enforcement system;"Citizens do not trust AI or any other technology, but the firms, organisations or services using it. Therefore, it is essential that clear and technology-agnostic governance and risk mitigation requirements are in place.
Enforcement and oversight tasks should be undertaken, to the extent possible, by current supervisors in order to avoid overlapping or contradictory practices.";We feel the oversight of these requirements should be ex-post, as ex-ante conformity checks would delay the launch of products/services to the market.;Mental health risks;A security baseline of technologies or products that are developed should be established, detailing the minimum security measures to be implemented to avoid misuse or tampering of these technologies. ;Yes;"A risk-based and proportionality approach should be taken.
Additional risk assessments should be performed during a product/service lifecycle. It should be guaranteed that in case of a significant change, all (new and existing) risks are being considered and addressed.
Also, there must be clear rules to allocate liabilities when multiple parties take part in the development or evolution of a product/service  
";No opinion;Any amendment should be as technology neutral as possible, that is, should not be AI-specific.;No opinion;;;
F530051;12-06-2020 13:20;Spanish;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;5 - Very important;No opinion;5 - Very important;4 - Important;Nos parece relevante que las empresas que no dispongan de conocimientos o capacidades en materia de IA, puedan obtener las ayudas a través del programa HORIZONTE con el objetivo de ganar en competitividad en el futuro.;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;Las plataformas on line, entre otras, recaban datos de información de consumo mediante la IA de los ciudadanos europeos. Nos parece relevante, por tanto, que las administraciones públicas tengan la capacidad y autoridad para tener acceso a los mismos con el objetivo de control e información.;5 - Very important;5 - Very important;5 - Very important;Somos una compañía audiovisual y la IA aplicada al sector audiovisual tiene sus propias especificidades y hay que dar prioridad a que las empresas de dicho sector puedan disponer de las misma herramientas que otras empresas intencionales, sin perjuicio de defender el talento y evitar su fuga fuera de Europa.;4 - Important;4 - Important;4 - Important;5 - Very important;3 - Neutral;;4 - Important;5 - Very important;5 - Very important;No opinion;4 - Important;No opinion;Creemos que en el marco regulador de la IA, debe especificarse que los propietarios de los datos y algoritmos deben hacerse responsables legalmente del uso que hacen de los mismos con el objetivo de garantizar la seguridad jurídica.;No opinion;;No opinion;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;;No opinion;;No opinion;;;Risks related to the loss of connectivity;;No opinion;;No opinion;;Yes, for specific AI applications;"La piratería de contenidos audiovisuales genera daños morales y económicos muy importantes a los propietarios de los derechos de IP. El uso de inteligencia artificial permitiría detectar los tipos de infringements y sus autores y por lo tanto, proveer de la información necesaria para la reparación de los daños generados.

";;
F530050;12-06-2020 13:16;Dutch;Public authority;Kim;de Vries;Local;Dutch Association of Municipalities / Vereniging van Nederlandse Gemeenten (VNG);32705794105-55;Large (250 or more);Netherlands;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
F530049;12-06-2020 13:07;Finnish;;;;Local;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;;4 - Important;4 - Important;5 - Very important;4 - Important;5 - Very important;4 - Important;;4 - Important;4 - Important;4 - Important;;5 - Very important;;5 - Very important;5 - Very important;4 - Important;;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;;Current legislation may have some gaps;;No opinion;;;;;5 - Very important;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;No opinion;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Personal security risks;;Yes;;No opinion;;No opinion;;;Data-agile_economy__From_reactive_to_proactive_approach_for_the_benefit_of_the_citizens.pdf
F530048;12-06-2020 12:59;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;The European Commission should take a leading role on the international governance of AI and promote a shared understanding and joint approach on the ethical development and use of AI in platforms such as the OECD and G20.This will ensure a global level playing field and open AI markets for European industry worldwide.The success of AI will depend on the availability of data therefore, technical, administrative and cultural hurdles must be overcome to make more data available for AI applications;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;As the Multiannual Financial Framework entails various funding programmes with different priority areas, the synergies between these programmes should be strengthened for the next 2021-2027 programming period to better support AI specific projects across Europe. ;3 - Neutral;5 - Very important;5 - Very important;The Commission should offer programmes on “applied AI” focus using ML & AI in industry, public services with distinguished focus on start-ups, SME, large enterprises and development and use of controlled training data pools with an open-source approach. The Commission should have a greater role in gathering information and provide information about AI specific jobs & new roles such as “data scientist” or “AI auditor”. Public funding should focus on the usage of trustworthy and ethical AI. ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;In order to accelerate the uptake of AI by SMEs, they must have a conceptual understanding of the technology and the means they are able to leverage its benefits and develop use cases which fit their business. Digital Innovation Hubs should play a more significant role and provide additional services.;4 - Important;4 - Important;4 - Important;3 - Neutral;3 - Neutral;4 - Important;Concerns around AI needs to be addressed, but the context and purpose will be key to determine the implications and relevance of ethical and legal challenges that may emerge in specific use cases.Clear distinction shall be made between AI frameworks providing a set of technical approaches (which have individual technical challenges e.g. in regards of accuracy, explainability)and AI solutions created and used by human developers and users (being responsible for their created systems and actions).;Other;We support the review of EU legislation potentially applicable for AI in transparent manner and make them fit for AI.We recommend not to rush into an AI specific legislation that could lead to legal inconsistencies, over-prescriptive rules that will hinder investments in AI as well as the use of innovative AI solutions.A potential AI regulation should analyze risks from the perspective of the consumer.We support a principle-based framework limiting the scope to high risks arising in B2C context.;Yes;;Other;We recommend setting up clear and precise criteria for high-risk AI systems based on probability of occurrence and consequences of the expected risk.The Commission should rely on existing risk assessment processes and classification defined in existing legislation.For most B2B AI use cases,AI Ethics concerns are often not primary relevant concern in B2B context as they are inB2C.Potential risks related to B2B AI applications can be addressed via contractual arrangements between business partners;;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;4 - Important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;It could serve as flexible and market-driven approach to develop Trustworthy AI and it should be established for medium & low risk. Its acceptance will highly depend on the overall scope of high-risk AI systems, the operational nature of its requirements and governance structure. The Ethical Guidelines created by AI HLEG could serve as a foundation. Auditors should be involved to confirm the requirements are auditable and it should be supervised by the Commission and endorsed by EU Member States;Other enforcement system;We would caution against new ex-ante conformity assessments that could cause significant delays in releasing AI products to the EU market.We suggest the application of existing self-assessment tools such as the data protection impact assessment under GDPR building on companies’ existing practises.Lack of expertise needed to evaluate algorithms should be addressed. A process -based certification scheme for high-risk AI systems should be applied instead of product or algorithm-based certifications;Re-training algorithms in a specific location will not guarantee AI systems with higher quality or even a different output, as previous training on non-EU data would still remain in any re-trained algorithm. Relying solely on EU trained algorithms and EU data sets will also cause challenges with regards to the diversity of datasets. We must have a global focus to ensure a diverse and fair user experience and avoid burdensome requirements for companies serving markets across the globe. ;;;No;;No;The PLD works well because it sets out clear, well understood and time-tested rules which are technology neutral in relation to products and consumers. The burden of liability risks is on the producer but without demanding anything impossible or unreasonable, leaving room for innovation. ;No;;It is crucial to identify gaps first before assuming that existing liability rules are not sufficient. Contractual liability or other compensation regimes will apply alongside or instead of tortious liability. This must be taken into account when determining to what extent the latter needs to be amended. For strict liability: it is key to understand who has the economic or social benefit from applying the risk/using the technology and for what use was the technology foreseen.;FINAL_SAP_position_paper_for_AI_public_consultation.pdf
F530047;12-06-2020 12:48;German;Other;Mathias;Grandosek;;Bundesarbeitskammer Österreich - BAK;23869471911-54;Large (250 or more);Austria;The feedback can be published with your personal information;5 - Very important;4 - Important;4 - Important;3 - Neutral;4 - Important;3 - Neutral;;5 - Very important;5 - Very important;4 - Important;3 - Neutral;5 - Very important;4 - Important;;4 - Important;4 - Important;3 - Neutral;;3 - Neutral;4 - Important;5 - Very important;4 - Important;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;No;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;Die Technologie wirft massive datenschutzrechtliche Bedenken auf, die auch von Organisationen wie Privacy International oder Bits of Freedom ausführlich dargestellt worden sind. Eine Fehlerrate von 1% bedeutet etwa: Sind 10.000 Menschen einer Gesichtserkennung ausgesetzt, die polizeilich gar nicht gesucht werden, dann werden 100 von ihnen dennoch als gesucht markiert.Für den Grundrechtsschutz in der EU ist es das falsche Signal, wenn das nun veröffentlichte Weißbuch lediglich eine Debatte anstößt, statt sich für ein (zumindest temporäres) Einsatzverbot auszusprechen.;Rather not;Es braucht für alle KI-Anwendungen einen Rechtsrahmen, der Transparenz, Zurechenbarkeit von Rechtsverstößen, Haftung usw sicherstellt. Der Rechtsrahmen kann zwar risikoabhängig abgestufte Regelungen vorsehen, muss aber in jedem Fall verbindlich sein. Eine bloße Branchen-Selbstregulierung reicht zum Schutz der Verbraucher auch in Bezug auf Kennzeichnungs/Transparenzvorschriften keinesfalls aus.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Eine Risikobewertung ist grundsätzlich oft schwierig zu erstellen, da auch keine absoluten Kriterien im Weißbuch definiert werden. Zudem kann sich (insbesondere bei lernenden Systemen) das Risiko auch über die Zeit ändern. Es ist daher dringend erforderlich, dass zur Risikobewertung Kriterien erstellt werden, die unabhängig zu bewerten sind und dass zudem auch ex-post eine Kontrolle und Durchsetzung der Vorschriften möglich sein muss.;Mental health risks;Verstöße gegen Grundrechte, unsachliche Benachteiligung/Diskriminierung;Yes;;Yes;RL muss für alle materiellen und nicht materiellen Sachen, digitale Dienstleistungen u digitalen Inhalte gelten. Als „defekt“ sollten auch Produkte gelten, von denen Cybersicherheitsrisiken ausgehen, die keine Updates erhalten oder nicht DSGVO-konform sind. Die Fähigkeit, selbst zu lernen und autonome Entscheidungen zu treffen, sollte als „Defekt“ gelten, wenn sie Schäden bei NutzerInnen verursacht. Falls Schaden: keine Beweispflichten für Konsumenten, welche Komponente dafür ursächlich ist;Yes, for all AI applications;;Erforderlich ist eine Solidarhaftung für alle in der Wertschöpfungskette der KI-Anwendung beteiligten Unternehmen gegenüber KonsumentInnen;Positionspapier_der_Bundesarbeitskammer_zum_Weissbuch_KI.pdf
F530046;12-06-2020 12:41;English;NGO (Non-governmental organisation);Dimitar;DIMITROV;;Wikimedia (Free Knowledge Advocacy Group);191538712765-84;Medium (< 250 employees);Belgium;The feedback can be published with your personal information;4 - Important;3 - Neutral;5 - Very important;2 - Not important;4 - Important;3 - Neutral;"We believe the European Commission should follow the principle ""Public Money, Public Code"". Public sector should use open source and open data exclusively and promote it. All research and start-ups funded by public money, should also abide by the open source principle.";5 - Very important;3 - Neutral;2 - Not important;2 - Not important;4 - Important;2 - Not important;The European Commission can help by promoting the best practices in the field of AI, i.e. the projects based entirely on free software and open data, which are instrumental for open science, open government and more generally for transparency, citizen empowerment and human rights. We can provide examples of best practices like Wikimedia software, Moses MT, Tesseract, GROBID and others useful for the promotion of culture. ;4 - Important;4 - Important;4 - Important;Public-private partnerships should not favourise for-profit over non-for profit groups and organisations. ;;;;;;It is important to underline that the commercial side of the private sector should not be favorised more than non-profit associations (for example partnerships between SMEs, larger enterprises and academia around AI projects is not considering non-profit organizations as Wikimedia). ;4 - Important;5 - Very important;5 - Very important;3 - Neutral;4 - Important;4 - Important;"A bad use of AI may endanger fundamental rights and may lead to discriminatory outcomes. 
Citizens should never be subjected to any state power making use of closed source or closed data AI. Any usage of secret or unpublished methods and data to limit a citizen's freedoms would infringe articles 20, 21, 41, 47 and 49 of the Charter of Fundamental Rights of the European Union.";Current legislation may have some gaps;;No;;;;;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;No opinion;"It is hard for us to answer such a question in a sensible and responsible way here. We would be happy to participate in a stand alone consultation targeted solely at this and similar issues.
";Rather not;We are afraid that voluntary labelling can create confusion among users, it could give disadvantage to who want to honestly show that is using a low-risk AI. An example where voluntary labelling does not work well are coffee labels. An example where it works rather well is the German TÜV labelling system. Systems that are underpinned by some sort of binding legislative framework might work better.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;"A combination of both mechanisms should be better in order to protect both users (who have been damaged) and makers (who should not be incriminated for having made bad use of AI only after several milion operations done at AI velocity. They should at least have the possibility to not run this risk by making a (low-cost) official conformity test before running the system publicly).
";;Risk to endanger other people that are persecuted by unfair governments. Worst-case example: I shoot a photo of a crowd and upload it online on Wikimedia Commons. I don't know that there are people that are persecuted. An unfair government can immediately find where they are by a face-recognition algorithm and find a way to kill them.;Yes;"In principle yes. It seems to us extremely challenging to define ""imortant change"" in a comprehensive way for legislative purposes. 
";Yes;">Even the producer of the software doesn't know what they are doing if it is not open source. Otherwise it is the same as regular software.
>It should be re-affirmed that companies are liable for everything they make. So if they can't prove the mistake is not with them (e.g. because of closed source code), the liability is still with them.
>A problem with the PLD is that it is based on user expectations for products. There are only vague expectations for new systems like AI.";No opinion;;We believe in some jurisdictions product liability also covers immaterial damages caused by software. We encourage the Commission to look into this and try to harmonise the situation in favour of immaterial damages.;_Public_Money__Public_Code__for_AI_in_Europe.pdf
F530045;12-06-2020 12:39;English;Trade Union;Ignacio;DORESTE;;European Trade Union Confederation (ETUC);06698681039-26;Large (250 or more);Belgium;The feedback can be published with your personal information;4 - Important;4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;Legally binding frame on ethical norms and recourse to collective bargaining and agreements with trade unions are key. Social partners need to be included in this process - they are key pillars of an inclusive digital transformation, including AI at work. They are best placed to understand policy and organisational needs. A potential EU framework on AI must be without prejudice to national labour market models and the autonomy of national social partners and must encourage collective bargaining.;5 - Very important;4 - Important;4 - Important;3 - Neutral;No opinion;5 - Very important;Developing AI skills at workplace should be better included in AI policies. EC needs to further develop training schemes for workers, including adult learning and “AI literacy” for workers. EC should further support EU social partners capacity building activities to enhance their competencies in doing capacity building and empower workers and representatives to adapt AI at work and respect decent work, including OSH, human rights values, provide for security, data protection, privacy. ;4 - Important;4 - Important;;A key aspect to be included withing the R&I realm, is the involvement of European and national social partners and sectoral trade unions, as they bring expertise and experience of situations of real workplace exposure. They further contribute to shaping sustainable AI technologies with possible ways forward in the development of research and innovation. The lighthouse structure for innovation needs to have a space for trade unions, in their role as European and national social partners.;4 - Important;3 - Neutral;4 - Important;4 - Important;3 - Neutral;"Digital Innovation Hubs need to be equipped in order to (a) give support to carry out risk assessment and managing data protection to the different SMEs across Europe. It is key to upgrade their capacity in these two issues that are impactful for work and employment; (b) to allocate trade unions equal access and participation to shape and monitor AI technologies at work and to take part to related employment discussions with the related national authorities. ";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;"In setting up a regulatory framework for artificial intelligence, there should be zero tolerance for AI in breach of compliance with fundamental and human rights related to the employment context including impacts on the nature of work. 
Human should remain in command of any AI technology. 
This can only be reflected in establishing a specific governance approach, to be reflected in policies and rules, that includes legitimate European and national social partners, and in particular t.u. reps.";There is a need for a new legislation;;No;;;;New rules are needed. High-risk applications need to be defined. They should be governed based on the precautionary principle as anchored in EU Treaty. Clarification of horizontal or vertical rules needs to happen with the participation of EU&national social partners. Sectoral unions should contribute from their specific expertise / competences. One concern is how to measure harm of any high/low risk application. EU legally binding risk assessment frame needs to be developed with social partners;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;"The White Paper on AI should have had a reference to a ban on facial recognition in public spaces, at least ""for up to five years until safeguards to mitigate the technology's risks are in place"". 
The lack of such a ban is inacceptable in the face of the rash development of AI with little to no public control and no legally binding rules on ethics and references to human and fundamental rights instruments. 
The moratorium should be reconsidered, as facial recognition and other remote identification systems are intrusive technologies that can be used in multiple harmful and disruptive ways. The impacts of AI technology and its possible risks should be assessed. GDPR states that processing biometric data for the cause of identifying individuals is prohibited, except for specific circumstances. Yet, there are still unsolved dilemmas about their implementation in policing and enforcement. One of the most probable risks for society is that facial recognition creates mass surveillance across the world, incompatible with human rights and democratic principles. It will raise inequalities exponentially and exacerbate biases.
Facial recognition should remain exceptional and reduced to clearly specific circumstances fixed in law. Any aspect of AI collection and processing of personal data should be based on sound, public and democratic rules, taken in cooperation with legitimate social partners. 
";Rather not;"Voluntary labelling raise concerns: They are granted by private companies with little/no public control; become profitable business that not provide for independence, quality and trust and lead to bias; rely on voluntary will for implementation and compliance; driven by marketing instead of safety&quality; lack of official public &valuation and verification schemes. Instead, mandatory labelling schemes are required and accountable and should be in place by public law with public control+sanction";A combination of ex-ante compliance and ex-post enforcement mechanisms;;Mandatory AI framework improves level of compliance of business operating in EU.  Precautionary principle set up by TFEU ensures that EU secures & reinforces AI via fundamental rules+values. This is true for technologies which risks are unknown+uncertain. EU has export. rights and values of GDPR. EU should ambition to leave up to the fundamental rights values in TFEU and set up a legal system for AI. If GDPR open for revision to further regulate data for AI, EU+national unions to be involved. ;Mental health risks;AI related risks are still unknown and can emerge in many circumstances and can be new. More legal certainty needed to address new risks like the  deepfakes, risks related to self-learning apps, bias and discrimination. Various factors should be considered to attribute fair liability.  A business/employer that uses a technology with a certain degree of autonomy, should remain liable for harm that results from use. Using a semi-autonomous technology should not be excuse to reduce liability. ;Yes;Need to review and adapt existing processes for AI-related risk identification, assessment, control + monitoring.  Need for a holistic approach, including society/organization/individual. Need to identify invisible risks [related to semi-automated decision-making, including discrimination, data analytics, personal data, cyber- security, infrastructure, human-machine interaction, internet of things (IoT), etc.] A multidisciplinary approach is required, for which trade unions should be involved.;Yes;"More discussion needed on whether AI application is product or service; legal scholars have different views and the distinction is not clear cut. Unions to be part of this discussion. Manufacturers should make sure that the AI application works safely before it is applied, using AI should not be an excuse to breach the duty of care.  In amending the EU liability framework, trade unions need to be properly consulted and involved.";Yes, for all AI applications;;National legal regimes might require adaptation too. Again, national legal regimes provide different liability considerations to the supply of services and to the supply of products. It is necessary to clarify the scope of potential liability of designers, hardware manufacturers, operators, network service providers, etc.  Further discussion in relation to the probe of harm or psychosocial issues at the workplace is also required.;
F530044;12-06-2020 12:33;English;Business Association;Marta;Marazzi;;ACEA -European Automobile Manufacturers Association;0649790813-47;Small (< 50 employees);Belgium;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;"-A strong collaboration between large European companies and public sector must be also considered, as this could foster the uptake of AI in Europe.
-Looking at non-EU countries, while an in-depth scrutiny of US and China AI ecosystems is necessary to ben";5 - Very important;4 - Important;5 - Very important;3 - Neutral;5 - Very important;4 - Important;A focus on AI safety / robustness issues to meet possible forthcoming high-risks requirements is welcomed.    ;2 - Not important;5 - Very important;4 - Important;"-Set up large scale EU projects that will mobilise industries' efforts 
-EU-wide minimum knowledge on AI should be ensured to increase trust in AI technologies
-A single lighthouse research centre might be limiting and could jeopardise the work of the exi";4 - Important;4 - Important;4 - Important;5 - Very important;No opinion;"-List the various industries' needs and establish sectoral roadmaps.
-While it is very important to support the development of AI expertise for SMEs, there is a need to clarify what is meant by knowledge transfer. (See attached response)";4 - Important;4 - Important;4 - Important;4 - Important;3 - Neutral;No opinion;"While all these concerns shouldn't be underestimated, they
primarily emerged with the raise of new technologies and should as such be addressed in a wider scope;
can result also from human actions and decisions: hence, the potential of AI use is precisely that of assisting in making such risks measurable. 
AI has to be so accurate that applicable laws/traffic rules are obeyed and passengers/road user feel comfortable.Training AI on broader/better data using ML will produce better outcomes.";Other;"Current legislation may have some gaps. But
-to ensure that an AI framework doesn't duplicate/invalidate existing certification requirements/regulatory frameworks(Type Approval;UNECE work-streams), it should be carefully considered where these and industry standards are better instruments to address gaps
-different AI applications may require diversified regulatory intervention, hence a specific assessment of AI use-cases is prerequisite for the identification of any regulatory intervention ";Yes;;Other;"No new rules are needed, but if developed they should be limited to AI high-risk app.
High-risk sector approach/classification of transport sector as high-risk are inadequate, for this may hamper AI development in the sector; the variety of AI applications within transport is too wide.
The only suitable criteria for a risk-based approach is the assessment of high-risk AI applications, but it must be defined clearly who evaluate risk level of applications and based on which specific criteria.";-Any application with a black-box algorithm, that is self-learning and makes automated decisions with high impact on human rights and safety. ;2 - Not important;3 - Neutral;2 - Not important;3 - Neutral;2 - Not important;4 - Important;Other special requirements in addition to those mentioned in the question above should be imposed (please specify);Biometric identification systems are already covered by the General Data Protection Regulation (GDPR). The processing of biometrics data for uniquely identifying purposes is forbidden pursuant to Article 9(1) of GDPR. Facial recognition can only take place if it falls under the scope of one of the exemptions listed in such article. However, the use of biometric data with machine learning is technically rather new and application-wise only marginally explored. Hence, a careful hand-in-hand of regulatory and technical development is required. Using an ethical principle framework can serve as a preparatory approach for regulation. ;Rather not;Voluntary labelling, as long as it is standardised and certified, can be an addition to quality assurance and could contribute to gaining user trust in AI technology. However, at this stage a voluntary labelling system would be very burdensome, particularly for SMEs (additional requirements to be met, approval process ex-ante/ex-post) and may result in confusion for the end-user. ;;;Any assessment of compliance should be carried out ex-ante either through self-assessment or through third party assessment and approval. In the case of third party ex-ante assessment and approval in the automotive industry, it is recommended that this will be encompassed in the current vehicles-homologation process.The latter is the right framework to trace how the system fulfils the safety/trustworthy AI requirements.The creation of a standalone assessment scheme for AI systems must be avoided;;" -In the automotive sector, cyber security and connectivity loss will be tackled in other regulations. 
-In general, security issues are technology open and already covered by legislation.";No opinion;;No;"-Current PLD is effective, as it provides both legal certainty and compensation to consumers: OEMs are held liable because of a defective AI-based product and can later call upon their supplier
PLD is horizontal regulation which covers many sectors, any a";No;;The current liability regime is based on a fair distribution. It is important that liability continues to be adequately allocated and that national law is governed/influenced by European law. Moreover, there is no reason to have specific compensation policy as long as the system meets its requirements.;20200612_ACEA_Written_Response_AIConsult.pdf
F530043;12-06-2020 12:26;English;Company/Business organisation;JUNYA;UKAI;;Hitachi Corporate Office, Europe (Hitachi, Ltd.);50213201578-64;Large (250 or more);Japan;The feedback can be published with your personal information;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;"It is important to: 
-Ensure business profitability to maintain effective data, computer infrastructure, applications and other tools which are required for social benefits.
-Consider balance between saving people’s lives, securing safety and health of individuals while securing privacy as the COVID -19 crisis highlighted.
-Consider international cooperation and coordination to promote common rules on the ethical development and use of AI, which preserve an active environment for innovation.";5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;"-Build a reliable framework between companies inside and outside Europe through free and secure cross-border data flows.
-A project development methodology for using AI to address data-driven issues.
-Support for the improvement of skills required for bus";4 - Important;4 - Important;5 - Very important;#NAME?;5 - Very important;4 - Important;5 - Very important;5 - Very important;3 - Neutral;;4 - Important;4 - Important;4 - Important;3 - Neutral;3 - Neutral;3 - Neutral;"-As we believe the definition of AI proposed is over-inclusive, we would like the definition to be limited and identified properly.
-AI is a technology continues to evolve rapidly. Its risks vary in context, purposes and applications. We strongly expect f";Other;Many AI use cases, e.g. railway systems, are assessed and regulated by the existing regulatory bodies in each sector in the EU which regulates physical safety measures and operations to eliminate or minimize risks. As a new cross-sectorial regulation and regulatory bodies on AI may cause double legislations and legal contradictions and uncertainties, we would like the EU to review the existing EU regulatory framework which may cover most of the common concerns before creating a new regulation.;Other;AI’s impact on individual rights varies depending not only on “sector/manner” or “using remote biometric identification or not”, but also on context of its use such as purposes and way of its uses. What makes it more difficult to assess is the detail underneath of these broad criteria and how to apply them. We consider these are likely to be subject to significant debate and interpretation. ;;;"Related to AI use, we would like the Commission to consider the requirements on high-risk AI to be wholly or partly exempted or eased in following cases:?
-In case that governing authorities confirmed that the risks of an AI, which was designated as high risk AI initially, are sufficiently low in its way of use or purpose.
-In case that the risks of the whole system utilizing AI are sufficiently eliminated or mitigated by countermeasures such as independent and redundant fail-safe systems.";4 - Important;4 - Important;3 - Neutral;3 - Neutral;4 - Important;4 - Important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;We believe this requirement could possibly be regulated by existing laws such as GDPR. In case of this requirement to be introduced uniformly to remote biometric identification, it is necessary to clarify the definitions of “public place”. Also, the specific requirements for remote biometric identification should only be applied to massive systematic surveillance case in the open public.;Rather not;-The reliability and quality of AI is important, but it should be taken into account that the market/users will not be confused by several global self-labeling scheme which are similar or incompatible with each other. We would also like the Commission to ;Other enforcement system;-We would like the assessing criteria to be consistent with the international standard on trustworthiness and on data quality, such as ISO/IEC, JTC1, and SC42 etc. It is also important that the standards of evaluation are unified in the each Member States;"It is important that the assessment will not overly burden companies and assessment mechanism to be pragmatic. Also, we would like the assessing criteria to be consistent with the international standard on trustworthiness and on data quality, and to be unified in all the Member States.
In case a high risk AI is assessed as unqualified and needs to be retrained, it is important that place or method for retraining will not be regulated (e.g. retraining place should not be confined to the EU)";;As AI is a technology continues to evolve rapidly, which means that it may cause unpredicted potential harms in future. We believe that introducing rigorous regulations from the very beginning and maintaining them can be resulted as authorities to limit the possibilities of their own abilities to regulate AI related technologies. We expect the EU to review the regulation continuously and amend flexibly to reflect comments from industry side even after the regulations become effective.;No;-If an AI is confirmed that its risk is sufficiently low as a result of a risk assessment, we would like the requirements to be exempted and eased (e.g. requirement on human oversights should be exempted or eased if a machine control exceeds, or product d;;As for the framework on liability and AI risk, it is important that governance entities have adequate knowledge and expertise to perform investigations and assessments. We suggest investigating authority on AI to be delegated to existing national authorities in the Member States. If independent AI regulatory bodies are newly established, there will be concerns over double legislations, and inefficiencies and ineffectiveness in assessment due to lack of collaborations between the authorities. ;No opinion;;;
F530042;12-06-2020 12:22;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;"- We believe that transparency is important in countering mistrust. The AI & Ethics document already describes the importance of trust. 
- Encourage the development of technologies, especially  the use of AI for sustainability and helping society, e.g. en";4 - Important;4 - Important;5 - Very important;3 - Neutral;4 - Important;3 - Neutral;;2 - Not important;4 - Important;5 - Very important;"- The study of networks: 5G and 6G networks will serve as a basis for many of those AI solutions and therefore it is important to ensure their robustness.  
- Data sharing. 
";4 - Important;5 - Very important;4 - Important;5 - Very important;3 - Neutral;Defining standards, in particular, network and IT interaction to improve interoperability.;5 - Very important;5 - Very important;4 - Important;4 - Important;1 - Not important at all;3 - Neutral;It is crucial that the regulatory framework also provides confidence that AI applications can be adopted without exposing enterprises to disproportionate risk. Different risks faced by enterprises, such as liability, should be addressed in an appropriate framework. This could be addressed, e.g. in the review of the General Product Safety Directive.;Other;Any potential new legislation should be well-balanced, i.e. not limit AI deployment, however, set some basic principles which would need to be ensured in some cases. Case-by-case assessments and regulatory sandboxes could also be considered in some cases.;No;;;;"We propose an alternative approach that already exists in EU law; the approach taken by the GDPR. This would require enterprises using AI to undertake a risk assessment and apply mitigating measures appropriate to any risks identified. In practice, many enterprises deploying AI will have those processes already in place through the ISO 27001. It would lead to a graduation of mitigating measures, appropriate to the level of risk. ";No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;GDPR is sufficient for this area. ;Rather not;As explained already, a binary approach to AI applications would not be well suited as it is unlikely to reflect the range and different degrees of risk arising from AI. It is also likely to be difficult to allocate, with any degree of accuracy, applications to one or other categories in advance. In addition, the full range of regulatory obligations seem to fall on high risk applications, whereas applications not considered high risk get none.  ;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;Enterprises deploying AI applications should undertake their own ex-ante risk assessments. However, an approach following GDPR principles would require a body, such as the EDPB, to adopt recommendations and promulgate guidance. This could respond more rapidly to new developments than legislation and ensure surveillance.  ;Risks related to the loss of connectivity;;Yes;Enterprises deploying AI should undertake their own risk assessments as they already do under GDPR and ISO 27001. ;Yes;"The responsibility of the electronic communications operator is limited to being a mere conduit. There are several responsibilities for manufacturers, creators and end-users when using AI.
Changes to the electronic communications code are needed to protect operators and to implement what will emerge here.
Devices connected to networks must provide guarantees including periodic reviews. Manufacturers should certify their products in order to be able to use ECNs. ";Yes, for all AI applications;;Any rules will need to consider the technical possibilities. Human oversight should not mean that human intervention is required for each output since this will defeat the purpose of AI applications that can take decisions faster than humans. Instead, human oversight should mean oversight of the system through security by design. It should mean human oversight of the methods but not human intervention.;CK_Hutchison_Group_Telecom_s_position_paper_on_AI.pdf
F530041;12-06-2020 12:09;English;Business Association;Ond?ej;Ferdus;;Confederation of Industry of the Czech Republic;785320514128-81;Medium (< 250 employees);Czech Republic;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;3 - Neutral;;5 - Very important;4 - Important;4 - Important;;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;4 - Important;;4 - Important;4 - Important;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;;Current legislation may have some gaps;;Other;The Commission has rightly identified a need for a well-defined risk-based approach to AI regulation that doesn’t apply ‘one size fits all’ logic across AI’s myriad applications. With that in mind, a number of adjustments are needed to ensure that any potential regulation is targeted at the right use cases, provides legal certainty, and does not discourage the development and diffusion of AI. See attached joint position of 15 organizations and associations from 10 CEE countries.;;;;3 - Neutral;3 - Neutral;5 - Very important;3 - Neutral;5 - Very important;3 - Neutral;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);;Rather not;The additional requirements and costs of a voluntary labelling system, and the resulting impact on SMBs, should be examined. Existing self-regulatory approaches should also be taken into account.;Other enforcement system;Ex ante conformity assessment requirements as recommended by the White Paper do not strike the right balance. A combination of ex-ante risk self-assessment and ex-post enforcement for high risk AI applications would likely achieve similar results within much faster timeframes and without risking unduly stopping innovation and creating unnecessary burdens. See attached joint position of 15 organizations and associations from 10 CEE countries.;;Mental health risks;;Yes;;No;The current liability framework remains fit for purpose, being both effective and technology neutral, so sweeping changes are not needed. The long-standing provisions of European liability law have worked well, and there has been no showing of problems sufficient to warrant changing them and introducing the risk of unintended consequences. See attached joint position of 15 organizations and associations from 10 CEE countries.;No;;;
F530040;12-06-2020 12:09;English;Business Association;Jan Willem;Knibbe;;ESOMAR - The World Association for Market, Opinion and Social Research;29952722795-07;Small (< 50 employees);Netherlands;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"In our view the most important action would be to generate big high quality data sets that:
1. are generated with respect to privacy concerns; 
2. are tested for biases, especially for potential discrimination of certain groups; and 
3. that can be used by everybody.";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;AI should be on the timetable of universities for the training of social research, economics, political research, law sciences, statistics and computer sciences. Furthermore bodies such as chambers of commerce should develop skills to assist their members in building  AI capabilities for SME.;5 - Very important;5 - Very important;5 - Very important;#NAME?;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"Work with the national and European Trade Associations to understand how existing areas of strength within the EU, such as Market Research, can be leveraged within the emerging field of AI.
To advise on ethical and legal frameworks
Generate lighthouse projects for SMEs that show the potential of AI. Enhance data partnerships between SMEs. Use existing data sets that are already available to set up AI projects.
Ensure that SMEs have access to open data sources and support them in accessing it";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;Yes;;;;"Compulsory requirements should be limited to applications where there is a risk that people can suffer harm. If the risk is especially high there can be even stricter rules. It should be handled the same way as with data privacy where we there are under Art. 9 GDPR special categories of personal data benefiting form additional protection.
Areas that can be considered high risk would include facial recognition, biometric data, individual tracking and automated decision making. ";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;Scientific research may need to use these techniques but that if they are used then they must be undertaken only on the basis of there being a legal grounds for such data processing.;Very much;It is important that there is an independent person in the company or an oversight body that is not the developer or the operator of the system needs to certify that it is in fact not high-risk.  ;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;Assessments could be built into Sector Codes of Conduct similar to the GDPR Article 40 Codes. We believe strongly that there is a need for sectoral codes to assist and refine the application of the EU data principles. We would therefore suggest that there is a need for some additional ethical, quality or sectoral data approval marks (e.g. ESOMAR membership, or adherence to our Code), on which any algorithmic development is founded.;;Our view is that there should be risk assessments to products during their lifetime if they undergo important changes.;Yes;;Yes;;Yes, for all AI applications;;;
F530039;12-06-2020 12:03;English;Trade Union;Angela;Pfister;;Austrian Trade Union Federation / Österreichischer Gewerkschaftsbund;43246044354-41;Large (250 or more);Austria;The feedback can be published with your personal information;3 - Neutral;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;3 - Neutral;Labour rights and employees. See contribution attached.;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;3 - Neutral;Labour rights and employees. See contribution attached.;4 - Important;4 - Important;4 - Important;;3 - Neutral;4 - Important;4 - Important;4 - Important;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;;There is a need for a new legislation;;No;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;The technology is problematic in terms of fundamental rights and data protection.;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;Violation of fundamental rights and labour rights.;;;Yes;;Yes, for all AI applications;;;2020_06_10_OEGB_Kuenstliche_Intelligenz.pdf
F530038;12-06-2020 12:00;English;Business Association;Jelle;Hoedemaekers;;Agoria vzw;68004524380-10;Medium (< 250 employees);Belgium;The feedback can be published with your personal information;5 - Very important;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;Provide a clear definition of AI. Cooperation in and across the EU is required to achieve the full potential of AI. Main points here are Focus on SME’s and the Skills. Consider setting up regulatory sandboxes around AI. Work on Data access and removal of localization restrictions.;4 - Important;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;We feel that the promoting the uptake of AI by businesses and public sector is of major importance. This will be done by creating trust in and developing skills for AI. A European data space will provide the data and computing power to properly use these AI skills.;3 - Neutral;5 - Very important;5 - Very important;Creating a network around existing research and innovation seems much more important and feasible than establishing a ‘new’ lighthouse research center. There are a lot of existing organisations and consortia who can take up the defined roles. The funding given to this ‘lighthouse’ center would be much better spend directly on R&I. AI should be one of the major subjects in the European research & innovation programmes.;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;Support needs to be provided for SME’s to develop and implement AI. There is a huge potential for SME’s to improve their products and processes using AI, but there are hurdles such as understanding the technology, funding and legal uncertainty. We question the effectiveness of giving these task towards specialized Digital Innovation Hubs, we would rather give these to organisations or mechanisms already known to SME’s. More clarification is needed on the exact way the DIHs should support the SME;3 - Neutral;4 - Important;3 - Neutral;5 - Very important;3 - Neutral;3 - Neutral;There are relevant concerns around AI, but they should be looked at realistically and not based on emotions. Most of these concerns are not caused by AI, but by human actions. All of the concerns listed above are not inherent only on AI. It is also important to look at the potential impact of these concerns relating to the frequency of how much they will appear. AI is proving to be a tool to, at first identify and pinpoint these concerns, and afterwards assisting in addressing these concerns. ;Current legislation may have some gaps;;Other;We would like to suggest the approach where first a detailed study of applicable regulations on AI is performed. After publishing the results a gap analysis can be done together with the relevant stakeholders. If there would be a gap, try to fit this into the existing regulations rather than developing a specific AI regulation.The High risk approach has a lot of promise, however regulation would need to be proportionate and provide legal certainty.;;;"Pinpointing one ""most concerning"" application is difficult from our point of view, but everything relating implications to human rights infringements should be looked at carefully.";5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);We feel like there is no need to ban a specific technology, only based on the fact that some uses are questionable. We need to look towards what this technology is used for. We feel that it has more value to open the societal discussion, and see what we would like to use said technology for. ;Much;The voluntary labeling system is something which could be of value for the market, both for consumer products as well as B2B. Considerations should be made however to not over complicate the process of achieving compliance as to not limit innovation. Industry standards should be used to support this labeling system. If worked out correctly, could work as a differentiatior for EU AI, if not done properly this has the potential to become an empty label or even a false flag.;Other enforcement system;We do not feel that the enforcement procedure is closely linked to ensuring that AI is trustworthy. Skills and awareness around AI are the main points in our opinion. If at some point an enforcement system should be put into place we feel that the most suitable approach would be to look towards existing enforcement mechanisms, and try to fit items relating to AI enforcement into these existing regimes. In this way an ex post enforcement with a prior self-assessments sounds the most logical.;The question here is that, similarly to the voluntary labeling, there are standards needed to evaluate certain systems. First these standards should be developed by the stakeholders. When this is done, methods for checking compliance against these standards should be looked at. ;Cyber risks;We think that cyber security is one of the critical areas for the future, however that does not mean that certain of the safety regulation will not need further clarification, A specific example is that there are certain risks which need to be defined more clearly. Specifically we are wondering what would be considered a mental health risk.;No;;No;The main comment is that a number of assumptions are made as to the AI applications, their developers and their users, while the substantiation of those assumptions is unclear. Currently there seems to be no evidence that existing frameworks, both at EU level (Product Safety Directive, Machinery Directive) and at national level are not working and cannot cope with the specificity of AI applications. We would like to see a more practice-based approach to liability, limiting the EU interventions;No;;If EU legislation regarding liability law and procedural law is deemed necessary, and we are not convinced that it is, then due consideration must be paid to the national regimes and their specificity. Therefore, the starting point should be a more detailed study as to the applicable national regimes, to identify whether or not “liability gaps” do indeed exist. ;
F530037;12-06-2020 11:55;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;4 - Important;5 - Very important;4 - Important;3 - Neutral;5 - Very important;"- Collaboration with all stakeholders in the healthcare system, including health insurance funds and health mutuals, doctors, hospitals and patient organisations
- Link with compulsory health insurance is important
- Working on citizens trusting the new t";5 - Very important;4 - Important;4 - Important;3 - Neutral;5 - Very important;4 - Important;;4 - Important;5 - Very important;4 - Important;;3 - Neutral;4 - Important;5 - Very important;4 - Important;4 - Important;- AI could contribute help gaining time in the diagnostic in some health sectors such as dermatology, ophthalmology, cardiology and oncology, using algorithms. By saving time for the diagnostic, health professionals could focus more on the relation with t;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;"- The importance of human agency and human oversight may not always be considered/possible (see recommendation 2 of AIM position paper).
- The need to improve eHealth literacy may not always a priority (see recommendation 8 of AIM position paper).
";There is a need for a new legislation;;No;;;;#NAME?;4 - Important;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Rather not;"- Labelling would only make sense, if it is done by a public authority
- Healthcare stakeholder, such as health insurance funds, health mutuals, doctors, hospitals, when concerned
- A democratic and transparent procedure for the labelling process would be";A combination of ex-ante compliance and ex-post enforcement mechanisms;;- Involvement of health insurance funds and health mutuals, doctors and hospitals in the assessment of compliance, when concerned.;Mental health risks;;Yes;;Yes;;Yes, for specific AI applications;#NAME?;;Position_Paper_AI_final_version.pdf
F530036;12-06-2020 11:51;French;Business Association;Christiane;Féral-Schuhl;;Conseil national des barreaux (French national bar Council);;Medium (< 250 employees);France;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;20200611_-_LIvre_Blanc_IA_-_CNB_-_Contributionvdef2.pdf
F530035;12-06-2020 11:51;English;Company/Business organisation;Thomas;LYMES;;Renault-Nissan Alliance;946343776-69;Large (250 or more);France;The feedback can be published with your personal information;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;3 - Neutral;;5 - Very important;4 - Important;4 - Important;3 - Neutral;4 - Important;2 - Not important;;2 - Not important;4 - Important;4 - Important;"While the idea of having a cutting-edge lighthouse research centre in Europe may sound attractive, it should not be developped in the detriment of the existing network of research centre on AI, which may be specialized in different fields of A.I. 
The focus of E.U actions on AI research should be to strenghthen capacity building of the existing AI research excellence centres.";4 - Important;4 - Important;4 - Important;4 - Important;3 - Neutral;;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;It must be stated that all these concerns do not only derivate from a wider use of A.I applications but primarly emerged with the raise of new technologies. Thus, they should be addressed in a wider scope.;Other;The needs for regulatory interventioin is hard to assess when taking into account the general concept of A.I. Different A.I applications may require different levels of intervention by policy-makers. That is why, if a need for regulation would be identified, it should be so after a specific assessment of the relevant A.I use-cases.;Yes;;Other;Defining a sector as such as high-risk does not include the variety of AI applications used by a sector. The distinction between “high risk” and non “high risk AI” could lead to conflicting rules  (e.g. AEB regulation).  Policy-makers should also take into account that in certain sectors, products already go through validation processes to ensure among other things safety. Better defining high-risk AI is needed, in light of the potential of deep-learning and its impact on AI categorization.;;3 - Neutral;3 - Neutral;4 - Important;3 - Neutral;2 - Not important;4 - Important;Other special requirements in addition to those mentioned in the question above should be imposed (please specify);Biometric identification systems are already covered by the General Data Protection Regulation (GDPR). The processing of biometrics data for uniquely identifying purposes is forbidden pursuant to Article 9(1) of GDPR. Facial recognition can only take place if it falls under the scope of one of the exemptions listed in such article. However, the use of biometric data with machine learning is technically rather new and application-wise only marginally explored. Hence, a careful hand-in-hand of regulatory and technical development is required. Using an ethical principle framework can serve as a preparatory approach for regulation. Specific safeguards should be put in place in order to ensure the confidentiality and security of the data processed by these systems. An opinion or guidelines from the European Data Board would suffice to supplement the GDPR regulation and clarify the safeguards to be put in place for this specific use case;Not at all;;Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;If the high risk approach should be retained in future regulations, the latters should ensure that users would not be able to modify the systems relying on A.I. Any assessment should be carried out ex-ante and in consistency with the existing type-approval framework.;Risks related to the loss of connectivity;"Risks related to the loss of connectivity could emerger when the AI is not embedded in the product but hosted in the cloud, and activating functionalities that without instant response from the cloud could put the individual in danger or impact his rights.

Yet we believe that in the automotive sector, cyber security and connectivity loss will be tackled in other regulations.
-In general, security issues are technology open and already covered by legislation.";No;The variation of the environment compared to the one in which the A.I systems were trained could be considered;No;"The current regulation allows vehicle manufacturers to act as a ""one-stop shop"" when seeking liability of OEMs because of a defective product. OEMs can later call upon the supplier of the parts that caused the damage. The PLD provides legal certainty to consumers and allow for their effective compensation in such a situation. We deem that the current develoments do not justify any modification of the current framework on product liability. ";No;;The current liability regime is based on a fair distribution. It is important that liability continues to be adequately allocated and that national law is governed/influenced by European law. Moreover, there is no reason to have specific compensation policy as long as the system meets its requirements;
F530034;12-06-2020 11:50;English;Business Association;Christian;BORGGREEN;;Computer & Communication Industry Association (CCIA Europe);15987896534-82;Small (< 50 employees);Belgium;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"We welcome the goal of achieving an ‘ecosystem of excellence’. 
Given the skills gap in the EU we encourage more emphasis on STEM education for students, re-skilling of workers, etc. Adoption of AI by the public sector will stimulate demand for AI applications. Startups will have an important role in developing new AI solutions. The EU’s work should lead to more harmonisation, enabling startups to scale up, while remaining consistent with globally harmonised standards and best practices.";5 - Very important;2 - Not important;5 - Very important;5 - Very important;5 - Very important;2 - Not important;"We welcome the emphasis on excellence in R&D and skills, the promotion of uptake of AI, and financing for startups.
Compliance and testing of AI solutions shouldn’t be outsourced to authorities, which are not best equipped to do so. Testing should be done by AI developers before market launch.
We encourage the making available of data, e.g. by public authorities. Creation of common EU data spaces could be helpful where there is clear market demand. Participation shouldn’t be discriminatory.";4 - Important;4 - Important;3 - Neutral;"Europe can play a leading role in AI research and innovation. We support the idea of establishing a lighthouse research centre and a network of AI research excellence centres. 
The EU should however avoid spending scarce funding to replicate existing data and computing infrastructure. 
We support the EU’s ambition to build global alliances with countries and institutions which share the same values, e.g. the USA, the OECD as well as universities and tech companies. ";5 - Very important;3 - Neutral;5 - Very important;4 - Important;5 - Very important;"The Digital Innovation Hubs will be important to ensure that SMEs and startups can access and use AI. 
We support more access to finance for startups to promote the takeup of leading technology solutions such as AI. 
CCIA members already provide tools for SMEs and startups to take advantage of AI opportunities. 
We welcome the inclusion of the private sector, European and international, in setting the research and innovation agenda and provide the necessary level of co-investment.";4 - Important;4 - Important;4 - Important;2 - Not important;2 - Not important;2 - Not important;"We agree that “AI can help protect citizens' security and enable them to enjoy their fundamental rights”. We welcome the proportionate and risk-based approach to balance potential harms with the benefits AI will provide. 
Like other technologies and human behaviour, AI can't always be expected to be 100% accurate, e.g. in low/no risk situations.
Compared to existing complex technologies there is not clear evidence showing that AI is a paradigm changer with regards to liability issues.";Current legislation may have some gaps;;Other;"We welcome the risk-based approach but suggest a more nuanced definition of “high-risk” applications of AI. The risk assessment criteria should be limited to ensure proportionality. It should moreover build on existing understandings of risk from current European risk management frameworks, e.g. EU risk management frameworks and processes for network security, transport of dangerous goods, etc. 
The “exceptional instances” clause is too open and should be removed.";;;;2 - Not important;2 - Not important;4 - Important;4 - Important;3 - Neutral;3 - Neutral;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);"The EU’s approach must adhere to existing EU legislation and fundamental rights. It must in particular respect the EU’s strict data protection rules and guidance provided by the EDPS and data protection authorities. 
The EU’s approach should also recognize the AI technologies already governed under the EU’s General Safety Directive and subject to both current and pending safety regulations for their general operation, to avoid duplicative or conflicting requirements.";No opinion;We take note of the range of suggested tools such as the voluntary labelling system. The system should remain voluntary, to not overburden developers including European startups. We hope a labelling system will not place a significant administrative load on smaller companies and thus favour larger players who can afford to meet the requirements.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;"Companies should be responsible for self-assessing high risk applications prior to introducing new applications on the EU market. Authorities should conduct ex post intervention when required and in limited circumstances only. 
Any requirement for AI applications to undergo a lengthy, bureaucratic approval process would, by default, keep Europe behind in the global AI race. It would also encourage European startups to launch innovations outside of the EU.";;"We do not see major deficiencies in the existing framework. 
The EU’s product safety framework should remain focused on use cases for health and safety not addressed in existing instruments, such as the Cross-Border Health Care Directive or the General Safety Directive, and it should not be overly complicated by expanding it to other notions such as ethics, data protection or cybersecurity which are already dealt with by other EU legal instruments. ";No;All AI models have specific parameters for operation based on their design, and regardless of their training or modification, those boundaries cannot be changed unless the product itself is fundamentally changed. Only material changes, that alter the risk assessment performed before product launch, should be considered an “important change”. Minor remote updates, e.g. for security reasons, do not constitute important changes.;No;"We would caution against an overhaul of the Product Liability Directive (PLD). For instance, introducing strict liability for AI would mean that anyone involved in creating an AI system, including SMEs, could be liable for any future problems they had no influence over. This would have a chilling effect on innovation, increase development costs and the uptake of AI.
The PLD defines damage as death, personal injury or damage to property and we would suggest similar wording be used.";No;;To safeguard the EU Single Market and ensure harmonisation of services, any possible changes to liability rules should happen at EU-level, rather than at national level to avoid fragmentation. We are moreover not aware of any consensus among EU Member States on the need for specific liability framework changes or the model that should be used for such a change.;
F530033;12-06-2020 11:48;English;Company/Business organisation;Francesca;PASSAMONTI;;Intesa Sanpaolo;24037141789-48;Large (250 or more);Italy;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;;4 - Important;4 - Important;4 - Important;;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;;1 - Not important at all;3 - Neutral;4 - Important;4 - Important;3 - Neutral;2 - Not important;Our response “neutral” considers the that concerns are already mitigated by existing regulations. Moreover, many concerns do not depend on the technology used.;Other;We agree with the EBF response. ;No opinion;;;;;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;;Rather not;We consider that more clarity on the requirements and functionality is needed. Please, refer also to our position paper.;No opinion;;;;;No opinion;;No opinion;;No opinion;;;Intesa_Sanpaolo_response_-_AI_White_Paper_-_June_2020_Def.pdf
F530032;12-06-2020 11:46;English;Other;Rosa;Oektem;;German Institute for Human Rights;;Medium (< 250 employees);Germany;The feedback can be published with your personal information;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;;No opinion;No opinion;No opinion;;No opinion;No opinion;No opinion;No opinion;No opinion;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;No;;;;The German Institute for Human Rights considers face recognition/biometric identification systems as among the most concerning AI applications with regard to their human rights impact and strongly recommends banning the use of such technology in the EU.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Very much;The labelling system should be mandatory and include more than two levels of labelling (risk/non-risk).;Other enforcement system;Ex-ante compliance and ex-post enforcement mechanisms should be combined and in both cases verified by relevant (external) competent authorities.;Audit procedures should not be voluntary or based on ethical principles, but should be mandatory and implement international human rights obligations of states and also of companies.;Mental health risks;;Yes;;No opinion;;No opinion;;;
F530031;12-06-2020 11:44;English;NGO (Non-governmental organisation);Stephen;ABBOTT PUGH;;Open Knowledge Foundation;970709235835-50;Small (< 50 employees);United Kingdom;The feedback can be published with your personal information;5 - Very important;4 - Important;4 - Important;4 - Important;3 - Neutral;1 - Not important at all;;5 - Very important;1 - Not important at all;1 - Not important at all;4 - Important;4 - Important;3 - Neutral;;3 - Neutral;5 - Very important;4 - Important;;3 - Neutral;No opinion;3 - Neutral;3 - Neutral;No opinion;;5 - Very important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;4 - Important;;There is a need for a new legislation;;Other;Categorising AI by risk level is a bad idea because even seemingly benign systems can become harmful when used in the wrong contexts or are implemented in a problematic way.;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;Remote biometric identification systems change the nature of the surveillance from an event-based alert (someone is committing an illegal act) to an individual-based alert (someone appears to be suspicious). The main user of those systems in public places, the police, is currently in the public eye due to the all too common abuse of power and use of excessive force. And that is the case even when they are the most regulated among the potential users of this technology. It is consequently irresponsible to promote the use of this technology in public places until more fundamental changes are made in how policing and public surveillance is made.;Not at all;What is needed is a legal and technical framework that allows auditing by all stakeholders. The legal framework would define what should be auditable (purpose, effectiveness etc) and the technical framework would define standards for making systematic auditing possible by third parties.;Other enforcement system;"Each AI system should document its purpose, accuracy, safe contexts of use, limits, risks and technical specifications. They should also provide indicators against which their claims can be verified. Regulators would then vet AI systems ex-ante and log them into a central open database; civil society could also audit the claims of the AI ex-post.";;Cyber risks;Deceptive commercial practices hidden by the trust in AI.;Yes;;No opinion;;No opinion;;;Open_Knowledge_Foundation_response_to_the_EU_AI_White_Paper.pdf
F530030;12-06-2020 11:39;German;;;;Local;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;Aufgrund der konsolidierten Beantwortung der Fragen, war eine Beantwortung der Freitextfelder in nur 500 Zeichen nicht möglich. Siehe daher Seite 2 der Anlage.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"- Stärkung der Exzellenz im Transfer von KI-Forschungsergebnissen in den
Wirtschaftssektor für mehr Wertschöpfung. Dies gilt auch für die frühe Verzahnung von
Forschung, Entwicklung und Innovation mit KI-relevanten Schlüsseltechnologien wie
dem im Weißbuc";5 - Very important;5 - Very important;4 - Important;Aufgrund der konsolidierten Beantwortung der Fragen, war eine Beantwortung der Freitextfelder in nur 500 Zeichen nicht möglich. Siehe daher Seite 3 der Anlage.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;-;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;Aufgrund der konsolidierten Beantwortung der Fragen, war eine Beantwortung der Freitextfelder in nur 500 Zeichen nicht möglich. Siehe daher Seite 3 f. der Anlage.;Other;"Bitte oben „Neue Rechtsvorschriften sind erforderlich“ werten!
Die Ausführungen unter „Sonstiges“ sind dennoch auf Seite 4 in der Anlage aufgeführt und zu berücksichtigen.";Other;"Bitte oben „Nein“ werten!
Die Ausführungen unter „Sonstiges“ sind dennoch auf Seite 4 in der Anlage aufgeführt und zu berücksichtigen.";;;"- KI im Gesundheitswesen, wenn die Daten Krankenkassen zur Verfügung gestellt
werden.

- KI-Einsatz durch Nachrichtendiensten fremder Staaten oder krimineller Organisationen";5 - Very important;5 - Very important;5 - Very important;No opinion;4 - Important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);Aufgrund der konsolidierten Beantwortung der Fragen, war eine Beantwortung der Freitextfelder in nur 500 Zeichen nicht möglich. Siehe daher Seite 5 der Anlage.;Very much;"- KI-Systeme sollten als sicher und vertrauenswürdig gekennzeichnet werden können,
wenn sie bestimmte Voraussetzungen erfüllen, da der Anwender keine eigene Prüfung
vornehmen kann.

- Es sollte erwogen werden, die Kennzeichnung so in der Öffentlichkeit zu";Other enforcement system;"Die Vor- und Nachteile einer ex-ante und ex-post Konformitätsprüfung müssen intensiv
geprüft werden und eine schnelle und unkomplizierte Prüfung durch die zuständigen
Stellen sichergestellt wird.";-;Mental health risks;"Vgl. „Anlage 3.- Ergänzende Stellungnahme des Ministeriums der Justiz und für
Europa Baden-Württemberg“";Yes;Aufgrund der konsolidierten Beantwortung der Fragen, war eine Beantwortung der Freitextfelder in nur 500 Zeichen nicht möglich. Siehe daher Seite 7 der Anlage.;Yes;"Eine Klarstellung wäre im Hinblick auf die streitige Frage wünschenswert, ob Software
als Produkt im Sinne des Artikel 2 Satz 1 der Produkthaftungsrichtlinie gilt und somit
von deren Regelungen umfasst ist. Die diesbezüglich bestehende rechtliche
Unsicherheit steht in Widerspruch zu der Bedeutung, die Software für Anwendungen
Künstlicher Intelligenz zukommt. Im Zuge dessen sollte auch die Abgrenzung von
digitalen Produkten zu digitalen Dienstleistungen in den Blick genommen werden.";Yes, for specific AI applications;"Bitte oben mit „Ja, für alle KI-Anwendungen“ werten!
Für die weiteren Ausführungen zu „Ja, für Bestimmte KI-Anwendungen“ sind dennoch auf
Seite 8 aufgeführt und zu berücksichtigen:";Aufgrund der konsolidierten Beantwortung der Fragen, war eine Beantwortung der Freitextfelder in nur 500 Zeichen nicht möglich. Siehe daher Seite 8 f. der Anlage.;FINAL_Anlagen_-_Konsultation_KI_Wei_buch_der_EU_KOM.pdf
F530029;12-06-2020 11:36;English;Business Association;Jochem;de Boer;;World Employment Confederation-Europe;7438623236-16;Large (250 or more);Belgium;The feedback can be published with your personal information;No opinion;5 - Very important;5 - Very important;4 - Important;5 - Very important;No opinion;To create and leverage value-based recruitment AI, WEC-Europe calls for the creation of a safe space – including through public-private collaboration - to test and train AI to deal with data that currently cannot be processed in the framework of GDPR, including sensitive personal data.;5 - Very important;5 - Very important;4 - Important;No opinion;5 - Very important;4 - Important;To create and leverage value-based recruitment AI, WEC-Europe calls for the creation of a safe space – including through public-private collaboration - to test and train AI to deal with data that currently cannot be processed in the framework of GDPR, including sensitive personal data.;No opinion;4 - Important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;5 - Very important;No opinion;;No opinion;5 - Very important;5 - Very important;4 - Important;No opinion;4 - Important;WEC-Europe believes proposals to classifying all recruitment and workers’ rights related AI applications as ‘high risk’ is insufficiently substantiated and specific. Argumentation is missing on ‘why’ the Commission has classified it as such. Although WEC-Europe recognizes that AI application in human resources might contain risks, further materialisation and proportionality are needed to tailor a more specific framework on “high-risk” AI application on the EU labour market. ;Current legislation may have some gaps;;Other;WEC-Europe believes proposals to classifying all recruitment and workers’ rights related AI applications as ‘high risk’ is insufficiently substantiated and specific. Argumentation is missing on ‘why’ the Commission has classified it as such. Although WEC-Europe recognizes that AI application in human resources might contain risks, further materialisation and proportionality are needed to tailor a more specific framework on “high-risk” AI application on the EU labour market.;;;;5 - Very important;4 - Important;3 - Neutral;5 - Very important;5 - Very important;4 - Important;No further guidelines or regulations are needed;;No opinion;;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;WEC-Europe would have great concerns with external ex-ante conformity assessment of all ‘high risk’ AI application for recruitment and ‘workers’ rights’ related application. Lack of sufficient resource and expertise result in direct level playing field issues as well as hampering of the adoption of value-based AI.;;;No opinion;;No opinion;;No opinion;;;
F530028;12-06-2020 11:36;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;It is essential that the ecosystem of excellence fosters competitiveness of EU companies vis-a-vis firms from other geographies. Thus, authorities should promote and support AI adoption by firms of any size and create incentives that allow EU firms to create and retain talent, avoiding brain drain. Also access to data will play a fundamental role. The Com should take measure to allow data sharing and reuse across sectors to foster a data-driven economy and all its benefits to society;3 - Neutral;3 - Neutral;5 - Very important;4 - Important;5 - Very important;4 - Important;Regulators and supervisors’ expectations and practices should be harmonized to ensure a common approach to this technology by each EU Member State and by sectoral and cross-sectoral authorities;3 - Neutral;5 - Very important;5 - Very important;"2 additional actions should be taken:
Make available training datasets based on anonymised data from the real digital footprints generated by millions of citizens or IoT devices to avoid the barriers of fragmented/low scale datasets compared to those in the hands of researchers in other geographies (USA and China)
Create standards, applications and quality assurance methods to generate such cross-country datasets throughout key sectors (health, transport, environment, finance...)";5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;Although supporting SMEs is convenient for their AI adoption, access to such facilities should also be possible for larger firms;;;;;;;"AI entails risks and opportunities as any other technology. Identifying AI with higher discrimination or safety risks is undermining trust on this technology and AI adoption. This could divert vital resources from deployment to compliance.
Citizens and customers do not trust AI or any other technology, but they trust the firms, organisations or services using it. Risk assessment criteria should be defined linked to applications and use cases, not to a specific underlying technology such as AI";Other;While the current regulation may be sufficient to cover the risks generated by the application of AI, some adjustments are likely to be needed to address the barriers to its use. In this regard, consideration could be given to the possibility of EU authorities working with industry to develop guidance on how to comply with existing rules in order to avoid some of the unintended consequences of the use of IA and to achieve appropriate levels of explicability and accountability.;Other;Although we don’t think new rules are needed, we support the risk-based approach to focus on apps that could cause serious harm to citizens. But it must be considered that the approach to determine high risk apps may leave outside of the scope cases where regulatory arbitrage is possible, and an activity can be carried out by firms that do not belong to a designated sector. In these cases, the Com should ensure that the exception foreseen (application regardless the sector) should be applied;;;;;;;;;;No opinion;;Rather not;"From the point of view of its usefulness, it raises many questions.
If a voluntary labelling scheme is finally developed, we would recommend that the framework be simpler and based on a risk-based approach and be oriented to the certification of activities and not firms. A labelling scheme that adopts the same requirements as those in the regulation for high-risk applications could discourage companies of all sizes from applying for the label to the detriment of consumers. ";Other enforcement system;"A strict ex-ante oversight could delay the launch of products leveraging AI to the market.
Enforcement and oversight tasks should be undertaken by current supervisors to avoid overlapping or contradictory practices. Adequate ex-post enforcement mechanisms should be required for firms providing high risk apps not submitted to specific supervision. Citizens do not trust AI, but the firms using it. Therefore, it is essential that clear and technology-agnostic governance exists.
";"A strict ex-ante oversight could delay the launch of products leveraging AI to the market.
Enforcement and oversight tasks should be undertaken by current supervisors to avoid overlapping or contradictory practices. Adequate ex-post enforcement mechanisms should be required for firms providing high risk apps not submitted to specific supervision. Citizens do not trust AI, but the firms using it. Therefore, it is essential that clear and technology-agnostic governance exists.";Mental health risks;"Among cyber risks, the following could be highlighted: Expansion of existing threats (increase of the complexity and reduction of the costs of cyber-attacks), introduction of new threats, increase of the speed and success rate of cyber-attacks, hinder the detection of malware or hackers.
A security baseline of technologies or products that are developed should be established, detailing the minimum-security measures to be implemented to avoid misuse or tampering of these technologies.";No opinion;;No opinion;;No opinion;;;AEB.20.06.12__Final_AEB_annex_proposal_to_the_European_Commission_AI_White_Paper.docx
F530027;12-06-2020 11:34;English;Company/Business organisation;Owen;Bennett;;Mozilla Corporation;174457719063-67;Large (250 or more);United States;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;;Other;The concept of AI and the concerns outlined above are extremely broad. We encourage the Commission to first define precisely the problems to be tackled, before identifying the potential gaps in existing legislation. In case the updates to existing legislation and a strict enforcement of the GDPR are not enough to ensure transparency and accountability rules for AI systems, then smart and targeted interventions (especially for high-risk applications) might be needed. See our document attached.;Other;A focus on high-risk AI applications and the conditions (including potential market authorisation) that they should comply with is very relevant. However we strongly encourage further clarification of what is meant by risk, by whom will assessments be carried out, and when in the product cycle would risk be assessed. Please see our attached document for more details on the risk assessment. ;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);The collection and use of biometric data comes with significant privacy risks and should be carefully considered in an open and evidence-based process. Biometric data is unique to each individual. People should have the ability to reveal only specific attributes associated with their identity depending on the legitimate requirements of the context. AI applications using such data should be subjected to a high justification threshold for deployment. As a baseline, use of biometric ID systems must satisfy Article 9 of the GDPR. We would welcome further EU-level guidance on deployment for these systems, particularly given the recent developments around COVID-19.;Rather not;Individuals should know when decisions with legal/social impact on them are made by algorithms. A labelling system is at best one part of the safeguarding approach. Trust labels often give a false sense of security to consumers, especially when some of the threats (e.g. to security and privacy) can be difficult to anticipate or model. Crucially, the primary burden should not fall on the consumer to assess and mitigate harm. A labelling scheme should be a complement, not a cornerstone.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;;;;;;;;;;
F530026;12-06-2020 11:32;English;Company/Business organisation;Michael;STRÜBIN;;MedTech Europe, the European trade association for the medical technology industry including diagnostics, medical devices and digital health;433743725252-26;Small (< 50 employees);Belgium;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;"A clear and agreed definition of AI is essential. Also, to boost uptake of AI in medtech the following will be critical:
- Appropriate access, including to companies, to representative datasets to ensure robust, accurate and safe AI solutions
- Investing in skill development of healthcare professionals to ensure an optimal use of AI technologies, build trust with patients and foster AI uptake
- Prioritising public-private partnerships to test new AI and their integration in healthcare systems";5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;"For medtech, funding and reimbursement (= national competences) are essential to ensure efficient deployment. This will be important to consider for AI medtech solutions. Member States should coordinate to ensure development of best practices, e.g. setup of a European database containing information on how AI is assessed for reimbursement purposes, along with the pathways for reimbursement and funding.

Increase efforts to study (and counteract) cyber-attacks aimed at AI systems.";5 - Very important;5 - Very important;4 - Important;"Striving for excellence in the rapidly evolving field of AI requires speed. Therefore, any EU framework for AI should be based on agile processes. In addition, there will be a need for mechanisms to shorten the application-to-granting duration for EU R&D funding.
AI research excellence centres should associate with all healthcare players to test AI solutions in real operational environments. The private sector would welcome further discussions on how they can support such an initiative.";4 - Important;4 - Important;5 - Very important;5 - Very important;3 - Neutral;"DIH can play an important role in the uptake of AI technologies in healthcare by building models of integration in healthcare organisations, addressing their services, organisational arrangements and workflows as well as contributing best practices regarding accessing testing and reference facilities, AI standards, development of AI expertise, and partnership support; which could then be consolidated and refined at EU level to develop a set of guardrails for AI innovation.";4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;MedTech Europe is concerned that the discourse has been focused on risks rather than benefits. The medtech industry is already heavily regulated. The ‘additional risk’ that may come with AI seems generally speaking relatively limited and mainly relates to situations where an AI solution is not developed because of barriers erected that inhibit the deployment of AI, even though it is proven to show better health outcomes and developed according to applicable standards and regulations.;Other;"In the medtech sector, specific regulatory guidance documents (e.g. produced by MDCG) would be useful to support the regulatory authorities’ assessment as well as harmonising the requirements for manufacturers.
Furthermore, it is worth noting that the safety and effectiveness of stand-alone software, when it is considered a medical device, is already addressed by both the MDR/IVDR sectoral legislations.";Other;New rules should only be introduced on a risk/benefit balance, in alignment with sectoral rules. In medtech, MDR/IVDR address the need to safeguard patient safety in view of technological progress. Both regulations address risk classes and as such an AI software may in principle fall in any of the risk categories (low to high). Our concern is that while the EC recognises that EU legislation may class “risks” differently to the Paper, it is unclear how to reconcile these classifications.;;;Any categorisation of “risks” posed by AI should be specified under sectoral legislation (e.g. IVDR/MDR have their own risk classification and respective regulatory requirements) to avoid duplication and legal uncertainty. Furthermore, where those regulatory authorities exist, such as in medtech, these bodies are best positioned to judge the level of risk that an AI system may entail for people or systems, and provide specific guidance to address these risks (e.g. defining “significant risks”).;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;No opinion;With regards to the question above about “mandatory requirements for a possible future regulatory framework for AI”, we responded “neutral” because we believe that the specific regulations applying to the medtech industry generally cover medtech AI in an appropriate manner. ;Very much;As far as healthcare is concerned, the medtech industry would welcome voluntary recommendations as well as industry standards (e.g. ISO guidelines). ;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Compliance assessment of an AI framework should factor in existing provisions in other pieces of legislation which require certain classifications and risk assessment. Therefore, such compliance provisions should be structured to refer to existing requirements to contribute to enhanced legislative clarity and to avoid any inconsistencies, through the development of sector specific guidance documents issues by regulatory authorities (e.g. Medical Devices Coordination Group).;Risks related to the loss of connectivity;"The risks highlighted above are associated with digital solutions in general and are not specific to AI. 

Legal certainty is essential, regardless whether this is achieved via the interpretation of existing rules or the implementation of new rules. New rules will need to be limited to specific risks that are not covered by other existing regulations.";Yes;"Risk mitigation should include company-wide and risk-specific controls to guide the development and use of AI systems, ensure proper oversight, and put into place strong policies, training, and contingency plans, as part of the overall AI life cycle management process.
Many healthcare products are already obliged to perform a continuous assessment of risk, whether the product contains AI or not. Yet, different risk assessment methodologies may be necessary for continuous learning AI systems.";No;The Product Liability Directive contributes to a reasonable balance between protecting those who suffer injury and ensuring fair competition. Where the Directive does not manage to address AI specific challenges, we see value in developing guidance to clarify certain issues under the Directive (e.g. “placing on the market”). ;No;;"A pan-European framework would be preferable to different, potentially overlapping, national liability rules. An EU-wide framework would also prevent local AI ""safe havens"".   For similar reasons, it is also important to find common ground internationally and not only within Europe, given that many of the AI challenges are global in nature.";
F530025;12-06-2020 11:29;English;Business Association;Axelle;D'heygere;;"The European Committee for Interoperable Systems (""ECIS"")";32238324913-44;Micro (< 10 employees);Belgium;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AI_consultation_note_final.pdf
F530024;12-06-2020 11:24;German;Public authority;Alexander;BANFIELD-MUMB;National;Bundesministerium für Digitalisierung und Wirtschaftsstandort;;Large (250 or more);Austria;The feedback can be published with your personal information;4 - Important;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;"Förderprogramme
Datenzugang
Innovationsräume schaffen (Reallabore, Testing Facilities, regulatory sandboxes)";4 - Important;4 - Important;5 - Very important;3 - Neutral;3 - Neutral;5 - Very important;"gesellschaftliche und ethische Zielen sollen stärker angeglichen und beachtet werden. 
Grundlegende Ziele festlegen für die KI einen positiven Beitrag leisten können. Definition von gemeinsamen roten Linien hinsichtlich Einsatz bestimmter KI-Systeme.";3 - Neutral;4 - Important;4 - Important;;4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;"Bereitstellung von Testdaten und KI-Tools für KMUs (""KI-Toolbox"") bzw. Möglichkeit, dass KMUs ihre KI-Lösungen/Algorithmen anbieten (""KI-Marktplatz"")";3 - Neutral;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;Entwickler von KI-Systemen sind eine sehr homogene Gruppe (sozialer und technischer Bias);Current legislation may have some gaps;;Other;es Bedarf einer genauen Definition was hohes Risiko bedeutet, bzw. ob diese Definition auch angepasst werden kann bei zukünftigen (technologischen) Entwicklungen. Es soll eine einheitliche europäische Lösung gefunden werden und eine Fragmentierung vermieden werden.;;;"biometrische Fernidentifikation im öffentlichen und privaten Raum (Gesichtserkennung)
Generell Anwendungen die sich auf die Grundrechte auswirken oder eine große Anzahl von Personen einschließt. Eine Liste von ""hochriskanten"" Anwendungen wäre in regelmäßigen Abständen zu aktualisieren.";5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Risks related to the loss of connectivity;u.a. autonomes Fahren;Yes;;Yes;es sollten u.a. vorhandene Rechtslücken im Haftungsbereich (Schadenersatz) geschlossen werden und die Verantwortlichkeit vom KI-Anwendungen geklärt werden.;Yes, for specific AI applications;;In Fällen wo die konkrete Anwendung durch den Hersteller nicht vorhersehbar ist, sollen Haftungsfragen überprüft werden.;
F530023;12-06-2020 11:22;English;Company/Business organisation;Fiona;WILLIS;;Association for Financial Markets in Europe (AFME);65110063986-76;Medium (< 250 employees);United Kingdom;The feedback can be published with your personal information;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;4 - Important;Please see our paper attached for more detail.;4 - Important;4 - Important;5 - Very important;4 - Important;5 - Very important;4 - Important;Please see our paper attached for more detail.;4 - Important;4 - Important;5 - Very important;Please see our paper attached for more detail.;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;Please see our paper attached for more detail.;1 - Not important at all;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;2 - Not important;Our response to this question is for capital markets only, which is a highly regulated industry. Applications of AI, not the technology itself, should be risk assessed, and this should take into account existing mitigants, such as regulation. Please see our paper attached.;Other;We believe that authorities have a range of supervisory tools that are generally more appropriate than legislation. Please see our paper attached.;No opinion;;;;AFME has no comments in response to this question.;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;No opinion;;Not at all;Please see our paper attached for more detail.;No opinion;;Please see our paper attached for more detail.;;;;;;;;;;20200612_AFME_EC_AI_CP_Response_-_Final_.pdf
F530022;12-06-2020 11:07;English;Public authority;Noleen;Campbell;National;National Standards Authority of Ireland - Standards Committee on Artificial Intelligence;;Medium (< 250 employees);Ireland;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Advise adding a separate action on Standardization & Interoperability as removes barriers to entry and enables international trade and excellence networks. Important to ensure Public & Private Sectors are aligned and progressing at approximately the same pace. Any ecosystem of excellence should be cognisant of Open Innovation 2.0 with SMEs and Micro Enterprises in mind. The inclusion of Ethics as part of the points raised would link back to the other discussions around ethics that AI touch upon.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Explainable AI and Trustworthiness in decision making are extremely important in both the Public and Private Sector. ;2 - Not important;5 - Very important;5 - Very important;Building on the networks of existing AI research excellence centres creates a robust ecosystem of trust and is better aligned with an open ecosystem of trust.;3 - Neutral;5 - Very important;5 - Very important;5 - Very important;4 - Important;"Access to standardised testing facilities/sandbox environments and open data (including synthetic data) with supporting guidance are key enablers for SMEs.
The interaction of SMEs with the larger private organisations must be considered also.";4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;"The preservation of fundamental rights principles is critical for trustworthy AI and will ensure the preservation of the particular principles such as safety and fairness.
Questions 1, 3, 4, 5, 6 require more context to be graded.";Current legislation may have some gaps;;No;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Rather not;Ethics parameters are not measurable at the moment and cannot be until standards are in place.;Other enforcement system;The risks of AI impact on several areas of legislation so that a single piece of legislation on AI may give a false sense of security or interferes with the proper implementation or enhancement in specific domains. A sub-legislative approach involving guidance and common standards could be developed that more directly tracks different areas of legislation e.g. human rights, workers, consumers, the environment, business partners and local communities from social responsibility ISO26000;;Mental health risks;Risks to Political and Civil Rights (misuse of social networking data for political profiling and targeting). Cyber risks is too vague.;Yes;Wait for ISO/IEC JTC 1 /SC 42 Standards to be published in this area.;Yes;Wait for ISO/IEC JTC 1 /SC 42 Standards to be published in this area.;No;;The gap doesn't appear to be in legislation but in the investigation methodology.;
F530021;12-06-2020 10:58;English;Other;Franco;Garbelotto;;Arbeitsgemeinschaft berufsständischer Versorgungseinrichtungen e.V. (ABV);427924533404-31;Small (< 50 employees);Germany;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;;4 - Important;No opinion;4 - Important;3 - Neutral;4 - Important;5 - Very important;;3 - Neutral;4 - Important;4 - Important;;2 - Not important;4 - Important;4 - Important;4 - Important;4 - Important;;5 - Very important;5 - Very important;3 - Neutral;4 - Important;No opinion;3 - Neutral;Development and deployment  of AI should always take into consideration issues around data security, privacy and transparency towards end users. A properly regulated legislative environment and easily enforceable rules should be the cornerstone of any Commission Proposal.;Current legislation may have some gaps;;Yes;;Yes;;;3 - Neutral;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;No opinion;;No opinion;;No opinion;;;
F530020;12-06-2020 10:58;English;Company/Business organisation;Jasmien;CESAR;;Mastercard;58204758673-16;Large (250 or more);Belgium;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;It is essential to invest in R&D in key areas, such as Privacy Enhancing Technologies (“PETs”). Investment in anonymization methodologies can preserve the value of datasets by generating non-personal data insights. There is currently legal uncertainty as to what it takes for companies to anonymize data in light of GDPR. PETs may include homomorphic encryption, synthetic data, differential privacy, or solutions provided by anonymization providers like Truata (co-founded by Mastercard and IBM).;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;4 - Important;Education of workforce and university curricula on AI and on how to use PETs is key. Additionally, education efforts should focus on the wide variety of possible applications of AI, including those with limited to no risk, and the potential benefits of AI (in addition to the potential risks). ;4 - Important;5 - Very important;5 - Very important;Private-public sector partnerships that promote knowledge transfer between academia, industry, Small-Medium Enterprises and public authorities should be encouraged in order to ensure upskilling in a way that will ultimately benefit the digital economy and European society. This will create AI “literacy”, jobs, and incentives for AI professionals to stay in Europe. A data marketplace to share data across private-public partnerships should also be considered.;5 - Very important;4 - Important;5 - Very important;5 - Very important;No opinion;Providing tools and guidance to SMEs and their ecosystem to protect themselves from security vulnerabilities and cyber-attacks is an additional task. SMEs are often the most vulnerable part of the AI value-chain given that they lack means and resources to keep their technology and infrastructure secure. ;5 - Very important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;"The so called “reticence risk”, i.e. what would be the consequence, including the lost benefits, to individuals and society of not going forward with a specific AI-related project due to potential concerns, should also be part of the equation. 
Accuracy is not always an end goal. In the financial sector, reducing accuracy on purpose in order to increase false positives (e.g. flag more payment transactions as potentially risky) allows for extra due diligence steps in assessing fraud.";Other;Current legislation may have some gaps. It is important to note that e.g. GDPR or existing competition law mechanisms already establish a good threshold for trustworthy AI. Any new AI normative framework should not contradict or duplicate this, but rather build upon existing legislation. It should also take a principle-based and flexible approach to ensure it is technology-neutral, and to provide a level-playing field to enable all participants across industries to compete and innovate.;Yes;;Other;The high risk approach outlined in the White Paper does not allow for the necessary flexibility for market actors and society to benefit from AI without producing onerous requirements. Qualifying certain sectors as high-risk does not consider that sectors evolve and AI applications may be used across sectors. Qualifying certain uses as high-risk as such, e.g. use of AI for recruitment, is too rigid and ignores the potential benefits of the AI application and measures available to mitigate risk.;With respect to remote biometric identification being considered high-risk as such, clarification is needed on what risks the European Commission is aiming to mitigate with this qualification (risks differ in nature and significance when remote biometric identification is used e.g. in context of mass surveillance vs. a service requested voluntarily by user). Also, further clarification is needed about the difference between remote biometric identification and remote biometric authentication.;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);To ensure consistency and legal certainty, it is important to note that the use of remote biometric identification systems and other technologies which may be used in public spaces, is already covered by the existing data protection framework. Any further guidelines in relation to such technologies should build on the existing framework and focus on practical mechanisms to foster compliance with the existing principles, as well as accountability tools in line with the GDPR.;Much;A voluntary labelling system at EU level to allow AI actors to demonstrate their compliance with the AI normative framework for no-high risk AI applications may serve to establish consumer trust in the use of AI technologies. To avoid significant administrative burden, maintain efficiency of a labelling process and technology neutrality, the labelling should focus on AI controls and safeguards, rather than labelling of specific AI tools, solutions and/or personnel.;Other enforcement system;An apt enforcement approach combines an ex-ante self-assessment and an ex-post compliance assessment similar to what is established in GDPR. A self-risk assessment process should be based on the structure and content of a Data Protection Impact Assessment under GDPR (as the use of AI often requires a DPIA) but could include additional AI-specific sections related to bias, explainability and auditability. Questions on safety, infrastructure stability and social impact should be considered too.;If a self-risk assessment indicates an AI application is likely to create high risk (in light of severity and likelihood of potential harm), an organization should take steps to reduce the risk, for instance by implementing technical & organizational controls. If after the remediation steps have been taken, the high risk remains, companies should consult with the relevant competent authorities prior to deploying the AI application. This is similar to the consultation process in Article 36 GDPR.;;Given that the risks highlighted above are generic, they may be relevant depending on the context but cannot be addressed effectively in horizontal legislation. In any event, they would not only be relevant for AI, but are also relevant for other new technologies. Any further guidance on product safety legislation should therefore be technology-neutral, to allow for sufficient flexibility in its application.;No opinion;It is important to clearly spell out what would constitute an “important change” of a “product” in the context of AI, and technology more generally, before considering establishing new risk assessment requirements for products during their lifetime. A material change in the risk may be a more suitable trigger for a new risk assessment. Also, any safety risk assessment should be part of the general AI (self-)risk assessment framework discussed above.;Yes;Some liability aspects of AI are already covered by existing regulations, e.g. GDPR when personal data is involved. Whereas some aspects of the Product Liability Directive should be clarified, i.e. its applicability to AI & how liability should be allocated, more clarity is needed on what a review of the Directive would cover. Any EU liability framework should be risk-based, focusing on high-risk applications to ensure consistency with other regulations and the proposed AI regulatory framework.;;;The Digital Single Market would benefit more generally from a harmonization of national liability rules. Such harmonization should not be AI-driven but should rather be future-proof and technology-neutral.;Mastercard_Towards_a_European_AI_Framework.pdf
F530019;12-06-2020 10:57;English;Company/Business organisation;peter;ekelund;;Streamanalyze AB;;Small (< 50 employees);Sweden;The feedback can be published with your personal information;3 - Neutral;4 - Important;4 - Important;5 - Very important;5 - Very important;3 - Neutral;Europe´s AI potential lies at the heart of EU´s SMEs. For companies to quickly and resolutely implement AI in their product offerings requires a regulatory and legislative long term predictable and secure enviroment. The Commissions proposal give to much arbitrary power to regulatory and political bodies. Lack of incentive to invest in AI technology will be major problem for Europe;3 - Neutral;3 - Neutral;5 - Very important;3 - Neutral;5 - Very important;4 - Important;AI should be particulariy considered as part an edge network strategy for localised and energysaving strategy for data from IofT and MtoM.;4 - Important;5 - Very important;2 - Not important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;2 - Not important;;2 - Not important;4 - Important;3 - Neutral;3 - Neutral;3 - Neutral;;The questions above relating to trust are deeply misleading as trust for AI is dependent on the individual applications  which there will be millions. To create a moral abstract handbook of rules at this stage would severly hamper the implementation of AI technologies in industries.;There is a need for a new legislation;;Yes;;No;;Existing product liability regulations should be agood launching plan for also the inclusion of new AI technologies;2 - Not important;3 - Neutral;5 - Very important;5 - Very important;4 - Important;5 - Very important;No opinion;;;Shoe part of normal product liability agreements;No opinion;;The Commission is entirely focused on consumer applications for AI which is politically maybe understandable but risks by default to block the implementation of AI tools in the industry whereas the productivity gains are considerable and key for Europe´s long term competitiveness .;Personal security risks;;No;;No;;Yes, for all AI applications;;AI is not a stand alone its part of everything we produce and use;
F530018;12-06-2020 10:56;English;Business Association;Erik;O DONOVAN;;Ibec;479468313744-50;Medium (< 250 employees);Ireland;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Further international co-operation can deepen our mutual understanding of the opportunities and may overcome shared technical and policy challenges in further digital transformation. The need for an overarching, market-friendly data strategy.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;All areas can be seen as equally important, but some could be prioritized in the short term for example investment in research and data infrastructure. Adoption and skills development will be long-term projects.;5 - Very important;5 - Very important;5 - Very important;"Prioritise the PPP for industrial research and the co-ordination of existing AI research excellence centres in the short-term with clear structures for leadership and co-ordination; a clear vision of purpose and objectives; and the necessary funding for delivery. The proposal to create a lighthouse centre is an excellent initiative that could help in the medium to long-term, although it is vital to strike the right balance in the degree of centralisation and funding. ";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Leverage the EU Digital Innovation Hub (DIH) network further. Develop a coherent network that is enterprise led. DIHs should support further awareness, development, collaboration and deployment of digital applications across the European economy for small and big firms alike. SMEs & start-ups have different requirements and need to be supported in developing, accessing and using AI.;4 - Important;4 - Important;5 - Very important;4 - Important;2 - Not important;3 - Neutral;Concerns must be viewed in context and linked to a demonstrable risk. Explainability and accuracy are important but may vary according to application or sector and can be improved through intensified R&D. A strong liability framework should be risk-based and clearly allocate responsibility among the AI chain operators. The opportunity cost of not using AI needs to be sufficiently reflected in policy debate. The PLD provides a strong liability framework.;Current legislation may have some gaps;;Other;We support a risk-based approach to a new regulatory framework but it is important to ensure   that any potential regulation is targeted at the right use cases, provides legal certainty and does not discourage the responsible development and diffusion of AI. The Commission must be clear in its risk assessments that it is taking into account the likelihood of harm and not just the severity of the harm, as well as a nuanced consideration of the opportunity cost of not using AI.;;;;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;No opinion;;No opinion;A voluntary labelling system could risk placing a significant burden on SMEs to comply. This could favour large players who can afford to meet the requirements whilst delivering minimal benefit to consumers. There should be broad agreement on standards to ensure such a scheme would be feasible or helpful. Given the pace of change, any scheme would have to be very flexible to work as intended. Existing self-regulatory approaches should also be taken into account.;Other enforcement system;Ensure extensive and meaningful engagement between actors across the AI value chain and relevant competent authorities to develop the assessment (including self-assessment), compliance and enforcement mechanisms to achieve shared outcome of trustworthy AI. A combination of ex-ante and ex-post mechanisms would be useful. For the vast majority of applications conformity assessment may be best done on a self-certified ex-ante basis with ex-post third party market surveillance to ensure compliance. ;The assessment regime must be pragmatic, well-resourced and transparent to ensure it is not overly burdensome for application providers, and also practical for designated assessment bodies to deliver. Ensure an international outlook in the development and implementation of mechanisms. It is also important to remember that there is no “one size-fits-all'' approach to compliance, and thought should be given to the context in which the AI technology is operating.;Mental health risks;"Difficult to answer this. It depends on the context of use of specific systems.
The focus should be solely on areas where the unique properties of AI, IoT, or robotics heighten the risk to the physical and mental integrity of consumers. The selections of risk (relating to cyber, connectivity, mental health) are not intended to imply that we believe additional laws are needed - existing laws are often sufficient, but it would be useful to provide greater legal clarity as to their interpretation.";Yes;Carrying out a new risk assessment should only be required when there has been a significant change to the functionality of the product which is likely to materially alter its performance in testing or the safety disclosures made. Generic over-the-air updates (OTAs) such as security fixes, bug fixes, or simple improvements after placing a product on the market should not trigger a renewed risk assessment.;No;;No;;The existing liability framework is solid & technology neutral, making it flexible enough to cover the challenges arising with emerging technologies. Changing such a foundational legal & societal framework should be done thoughtfully & only in response to significant & demonstrable shortcomings with the current legislative framework. A strict liability regime for software would stall innovation in Europe, stifling economic growth. Please find attached Ibec paper that expands on our responses.;Ibec120620_ECconsultation_AI_wp.pdf
F530017;12-06-2020 10:54;English;Business Association;Raquel;LOPES;;eu travel tech;70614728635-77;Micro (< 10 employees);Belgium;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;;No opinion;No opinion;No opinion;No opinion;No opinion;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;;Current legislation may have some gaps;;Yes;;Yes;;We urge clarity about which ‘exceptional instances’ AI applications could be classified as high-risk outside of the cumulative criteria proposed. Notably, the notion of ‘applications affecting consumer rights’ is very broad and could lead to legal uncertainty. Also, further clarification on the intention to include transport as a high – risk application: transport is a far-reaching sector and while some AI applications may pose ethical or fundamental rights implications, many of them do not. ;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);eu travel tech also seeks clarification regarding the specific regulation that biometric solutions will be submitted to as a high – risk application. Post COVID-19 travel will accelerate the need for touchless travel, and some of our members are developing solutions that will drive the recovery of the sector through several technology advances, including biometrics. A clear regulatory framework for biometric solutions will be essential for the recovery of the travel and tourism sector. ;Rather not;While we see benefit in a voluntary labelling scheme for low-risk applications, we question the added value of setting standards for high- risk. Such scheme that from the outset is based on extreme connotations of AI would not provide meaningful information to consumers. Alternatively we could opt for a voluntary scheme to recognize the processes and techniques a company has in place to ensure legal and ethical compliance, making company bound to act in accordingly and ‘certified’ as such.;No opinion;;;;;No opinion;;No opinion;;;;;ett_Artificial_Intelligence_POP_submission_consultation_110620.pdf
F530016;12-06-2020 10:44;English;EU Citizen;Alexandra;GEESE;;;;;Germany;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;2 - Not important;1 - Not important at all;Involve NGOs and the broader public, particulary groups negatively affected by bias in AI;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;4 - Important;"Develop leading techniques in de-biasing, bias analyses, cooperation between AI and gender/racism experts, establish world-reference transpency and participation standards
in order to build trust";5 - Very important;5 - Very important;3 - Neutral;Offer the possibility to researchers to have two careers within one family - reconciliation of work and family is a factor for higly qualified researcher to choose Europe over the US. ;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;Intransparency of decision processes, citizens feeling manipulated. That would promote conspiration theories.;There is a need for a new legislation;;No;;;;Biometric recognition, automatic weapons, recruitment tools, credit scoring, medical AI (because of gender and possibily race bias and lack of unbiased data);5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Rather not;I'd rather have a mandatory systems that allows for a first risk category without specific legal obligations. ;A combination of ex-ante compliance and ex-post enforcement mechanisms;;The determination of the risk of algorithmic systems in a future horizontal regulation on artificial intelligence should be based on a combination of the severity of the potential damage and the probability of its occurrence. An increased risk potential of an algorithmic system must be accompanied by a higher degree of regulatory intervention. For the algorithmic systems in the lowest risk category, no further legal obligations should apply. ;Cyber risks;;Yes;Must be dynamic and flexibel to adjust to new insights.;Yes;Consider reversal of burden of proof for consumers and citizens in order to ensure access to the knowledge necessary for legal proceedings or prima facie evidence.;Yes, for all AI applications;;;Proposal_for_risk-based_approach.docx
F530015;12-06-2020 10:34;German;Company/Business organisation;Manfred;Kurz;;Adolf Würth GmbH & Co. KG;690399725323-59;Large (250 or more);Germany;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;4 - Important;;4 - Important;4 - Important;5 - Very important;4 - Important;4 - Important;;4 - Important;4 - Important;4 - Important;3 - Neutral;3 - Neutral;2 - Not important;;Current legislation is fully sufficient;;No;;;;;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;No opinion;;Much;;Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;;Cyber risks;;Yes;;No;;No;;;
F530014;12-06-2020 10:29;English;Academic/Research Institution;Christiaan;van Veen;;Digital Welfare State and Human Rights Project (based at the Center for Human Rights and Global Justice at New York University School of Law);;Small (< 50 employees);United States;The feedback can be published with your personal information;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;In light of the work my project has done on the potentially severe implications of the uptake of AI and other digital technologies by public authorities (see attachment), I am concerned about Chapter 4, section F of the White Paper where the adoption of AI by the public sector is encouraged. The development and use of AI by the public sector carries significant human rights risks and both the benefits and risks of such development and use are best debated separately from private innovation.;No opinion;No opinion;No opinion;;No opinion;No opinion;No opinion;No opinion;No opinion;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;One concern not mentioned here is that of AI systems used by public authorities operating in secrecy (see, for example: https://bit.ly/2zqtCSV). A more general worry relates to how the language in Chapter 5 is muddled. AI may affect the enjoyment of European and international human rights law, which contains specific human rights norms. These are not equivalent to the ethical concerns set out in the non-binding Guidelines of the HLEG, such as relating to fairness, human agency or diversity.;There is a need for a new legislation;;No;;;;Generally speaking, the use of AI by public authorities in areas that directly or indirectly affect the human rights of individuals. This applies especially to AI use that relates, directly or indirectly, to decision-making by government authorities. But since the application of this technology can be so wide-ranging, it is difficult to say which is 'most concerning'. Facial recognition cameras in public spaces are concerning, but so is the use of AI to decide on eligibilty for social benefits.;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;First, while the use of biometric identification systems by public authorities is highly problematic, it is not the only use of AI by public authorities which is troublesome. In footnote 52, the White Paper distinguishes remote biometric identification from biometric authentication, but that application is also problematic (see, e.g. https://bit.ly/2Aju1qY). The most important step is to separate the discussion of private use of AI from its public use and address such public use under a separate regulatory framework and process. ;No opinion;;No opinion;;;;;;;;;;;;
F530013;12-06-2020 10:19;English;Academic/Research Institution;Margherita;Galassini;;Center for Religious Studies of Fondazione Bruno Kessler (FBK-ISR);488774727597-70;Small (< 50 employees);Italy;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Please see our response paper (section 2.1).;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;We support the coordination of European policies not only among Member States but also with non-EU states that subscribe to values and goals in the development and adoption of AI which are coherent with the European approach. ;3 - Neutral;5 - Very important;5 - Very important;We believe that priority should be given to the creation and the strengthening of multi- and interdisciplinary European research networks that bring together experts in the fields of humanities, social sciences, engineering and science (see section 2.1.1 of our response paper). In this context, efforts should be made to facilitate the coordination between academic institutions, private research centres and enterprises. ;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;We support the provision of fiscal incentives for SMEs' contribution to research hubs.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;We highlight the importance of ensuring the protection of the rights and freedoms of  religious or belief communities and individuals (see section 2.2 of our response paper).;Current legislation may have some gaps;;Other;The definition of high-risk applications should be subject to continuous review and refinement, since it is impossible to predict risks with certainty. Only on this condition, compulsory requirements can be responsibly limited to high-risk applications. ;;;Biometric identification systems (see section 2.2.2 of our response paper).;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;Please see our response paper (section 2.2.2).;Very much;We recommend the involvement of stakeholders of the impacted sectors in the development of the voluntary labelling system in order to facilitate its adoption.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Besides the respect of data protection norms with regard to AI deployment, transparency in the structuring and use of datasets should be a key concern. ;Personal security risks;Applicable provisions to AI should be coordinated in order to avoid gaps and risks with regard to legal certainty. ;Yes;Ethical impact assessment and data protection impact assessment frameworks for AI development should be elaborated. ;Yes;The framework should be amended in consultation with associations representing manufacturers and consumers. ;Yes, for all AI applications;;National legal frameworks may widely diverge. AI applications in e-health are particularly relevant in this context. ;FBK-ISR_Response_Paper.pdf
F530012;12-06-2020 10:11;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;4 - Important;4 - Important;3 - Neutral;4 - Important;3 - Neutral;;4 - Important;3 - Neutral;4 - Important;4 - Important;4 - Important;3 - Neutral;;3 - Neutral;5 - Very important;5 - Very important;The Commission should focus on supporting existing centres of excellence and facilitating public-private partnerships in order to foster innovation, collaboration and research.;4 - Important;4 - Important;4 - Important;3 - Neutral;4 - Important;Digital Innovation Hubs should provide guidance to SMEs on AI best practice and ethics.;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;3 - Neutral;"It is right to consider the concerns listed above; however, many are most pertinent in certain specific applications of AI, where there is a risk of loss of life (for example when AI is applied to the healthcare and transport sectors), and are less concerning in others, where there is not a risk of harm (for example when AI is applied to content recommendation). As such, it is important to contextualise both the potential harms and potential benefits of AI and not take a generalised approach.";Other;The need for new legislation should be considered on a case by case basis, taking into account different applications of AI, rather than applying a horizontal framework. The Commission should explore the potential of codes of practice to provide a more flexible mechanism for achieving the same outcome as additional legislation. ;Yes;;Yes;;;3 - Neutral;3 - Neutral;4 - Important;3 - Neutral;3 - Neutral;3 - Neutral;No opinion;;Much;The voluntary labelling system will have to take into account that the supply chain for many AI technologies is long which can present issues when seeking to confidently label the end system. ;Other enforcement system;"Ex-ante compliance can slow down innovation and does not easily allow for add-on development; however, it is necessary for certain high-risk AI applications. For example, in sectors like healthcare and automotive, ex-ante compliance requirements should be in place to guarantee safety.  Ex-post compliance is preferable for lower risk applications such as content recommendation as it is outcome focussed and more readily allows for innovation."; ;;;No opinion;;No opinion;;No opinion;;;
F530011;12-06-2020 10:07;English;Non-EU Citizen;Koichi;Takagi;;;;;Japan;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;The_Conference_toward_AI_Network_Society_of_Japan.pdf
F530010;12-06-2020 10:01;English;Business Association;Marek;Kysela;;SEMI EU Machinery Directive Working Group;;Micro (< 10 employees);Belgium;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;SEMI_EU_Machinery_Directive_WG_Comments_on_safety_and_liability_implications_of_AI__IoT_and_robotics_June_2020.pdf
F530009;12-06-2020 09:48;English;NGO (Non-governmental organisation);Mokrane;BOUSSAÏD;;European Blind Union (EBU);42378755934-87;Micro (< 10 employees);France;The feedback can be published with your personal information;4 - Important;4 - Important;5 - Very important;No opinion;4 - Important;4 - Important;See EDF response;4 - Important;5 - Very important;2 - Not important;2 - Not important;4 - Important;4 - Important;See EDF response;5 - Very important;4 - Important;4 - Important;See EDF response;No opinion;No opinion;No opinion;No opinion;No opinion;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;As developed in the EDF position paper attached to their response and for which we have been consulted, we are in general concerned that AI will in most cases not consider people who are largely out of societal ideas of “the norm”, like persons with disabilities, and consequently risks aggravating discrimination.;Other;"We fully support the EDF message that the current EU legal framework does not sufficiently address the risks, that current EU equal treatment and accessibility-related legislation is not comprehensive: EU equal treatment legislation protects rights of persons with disabilities only in employment and vocational training, leaving out vital aspects of daily life; the European Accessibility Act is very digital products and services-centered.";Other;We fully support the EDF message that limiting the proposed requirements to a list of ‘high-risk’ AI applications is not the right approach, as risks may evolve over time and vary, considering human diversity, and that consequently the focus should be on setting boundaries to the domains in which AI can be deployed, based on the risks of discrimination.;;;See EDF response;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;Biometric identification systems in publicly accessible spaces risk being highly discriminatory, often inaccessible for persons with disabilities, and deployed without the consent of affected persons. Sensitive data about an individuals’ chronic illness or disability can be gathered without their consent, and later be used to discriminate against the person.;Rather not;See EDF response;Other enforcement system;See EDF response;See EDF response;Mental health risks;See EDF response;Yes;See EDF response;No opinion;;Yes, for all AI applications;;See EDF response;
F530008;12-06-2020 09:38;English;NGO (Non-governmental organisation);Fotios;FITSILIS;;Hellenic OCR Team;451914138191-77;Small (< 50 employees);Greece;The feedback can be published with your personal information;4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;Besides cooperation among sectors, a topical focus to a landmark issue could be considered as a beacon for development. AI and Democracy and the infusion of the democratic principles in AI (the latter is part of the Hellenic national strategy on AI) could serve as such. ;5 - Very important;4 - Important;4 - Important;3 - Neutral;4 - Important;3 - Neutral;Enabling of a interdisciplinary approach across all actions described is considered critical for the design and implementation of future trustworthy AI applications. This so-called Interdisciplinarity-by-design is necessary when considering that AI-related technologies and applications cover a wide range of disciplines. ;5 - Very important;4 - Important;4 - Important;The white paper is often referring to citizens as data subjects but misses to address their collective power. In this regard, the Hellenic OCR Team, a unique crowdsourcing platform for the processing and analysis of parliamentary data, is proposing the establishment of an ‘algorithmic observatory’ for baseline screening of AI applications for potential risks upon a predefined set of standards, see Fitsilis, F. (2019) Imposing Regulations of Advanced Algorithms. Cham: Springer. ;3 - Neutral;4 - Important;3 - Neutral;4 - Important;3 - Neutral;Specifically in relation to SMEs and in the framework of the Digital Innovation Hubs, the definition of focal points, e.g. sectors that are critical or essential for preserving EU’s technological leadership, in tandem with increased access to financing opportunities specifically for tackling those sectors, could spark EU-wide interest. ;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;4 - Important;Closely related to fundamental rights, but again a distinct issue, is the possible breach of core democratic principles through AI applications. The word ‘Democracy’ and its derivatives are not to be found in the text (the term ‘democratic’ is used once), an absence that we consider serious. Democracy, which forms the fundament of the European structure, is already critically threatened by the unregulated operation of digital platforms and existing AI tools. ;There is a need for a new legislation;;No;;;;AI applications that intervene in the political dialogue, particularly within digital platforms, and related profile-based political marketing. ;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;;Legal values and rights always have to be balanced. In this sense, biometric identification systems may indeed represent a beach of privacy. However, the rule of law and the generic feeling of a just society might very well be impossible to be maintained in multimillion societies that are getting more diversified, often at the cost of public safety and security. Hence, from today’s point of view, the widespread use of AI-powered biometric  tools seems inevitable.;Rather not;The general idea of a voluntary labeling system is attractive and could be linked with an oversight mechanism for cross-check. The already proposed public Algorithmic Observatory could monitor whether such labels are trustworthy. ;A combination of ex-ante compliance and ex-post enforcement mechanisms;;AI technologies and AI-based services are fundamentally different compared to ‘traditional’ technologies and products, but they should be treated equally nevertheless. A combination of ex-ante and ex-post compliance with regulations, standards and guidelines at the Union and member state level should always be envisaged. ;Mental health risks;AI-based products do have distinct characteristics compared to other types of products. The mentioned risks seem justifiable. The Hellenic OCR Team does not claim expertise in the field to propose more such risks. ;Yes;New or updated risk assessment procedures seem inevitable for AI-based products and services.  The role of an EU AI-agency could potentially be beneficent into designing and certifying such procedures. ;Yes;The risks specifically linked to the use of AI tools should be mentioned in regulatory acts. The type of act (EU or national) and the amount of detail need to undergo careful and multi-level assessment.;No opinion;;We cannot express a specific opinion at the moment. When concerning such an important issue, all member states should align their approach. As a consequence, they might want to await EU response that shall serve as a point of reference.;
F530007;12-06-2020 09:36;German;NGO (Non-governmental organisation);Jason;Chumtong;;Konrad-Adenauer-Stiftung e.V.;;Large (250 or more);Germany;The feedback can be published with your personal information;3 - Neutral;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;Though the introduction of the white paper mentions that the development and use of AI for military purposes does not appear in the text, the aspect of utilizing AI for state and public security is missing. Questions likely to be raised in this regard concern the deployment of AI for instruments countering organized crime, terrorism and general police work. It is important to address if AI systems used for these purposes are dealt with in similar ways to those proposed in the paper.;5 - Very important;3 - Neutral;4 - Important;5 - Very important;4 - Important;5 - Very important;;4 - Important;4 - Important;5 - Very important;"While attracting experts from outside is a legitimate way to expand expertise within the field of AI, a more sustainable and effective alternative is to foster knowledge about computer science (CS) on the early educational level. Europe must start building a generation of professionals acquainted with the basic understanding of CS in order to
fill in the gap of already missing professionals for the AI market, similar to ERASMUS+, but including tech hubs such as Israel, Singapur, Japan etc.";5 - Very important;3 - Neutral;5 - Very important;4 - Important;5 - Very important;;2 - Not important;4 - Important;3 - Neutral;3 - Neutral;4 - Important;2 - Not important;As the performance of AI systems depends on the quality and content of the data their algorithms process it is necessary to improve the monitoring and controlling mechanisms of this process in order to prevent unwanted results and adverse effects. This aspect of human oversight is acknowledged in the white paper, however, risks inherent to human supervision must be further addressed in terms of liability, responsibility, educational standards and more.  ;Current legislation may have some gaps;;No;;;;Every AI application which ultimately results in an outcome with (almost) irreversible consequences could be seen as highly risky because its decisions come with lasting effects on the user. ;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;No opinion;;Much;AI systems will become more sophisticated over time which is why it is reasonable to assume that the majority of technological artifacts in the future will inhabit some sort of automated systems. Thus the act of labelling a broad variety of products only makes sense when it indicates important information in a simple and direct way so that a lay person understands 1.) WHAT the system does and 2.) HOW and WHICH data will be used to execute respective action.  ;A combination of ex-ante compliance and ex-post enforcement mechanisms;;According to the definition of HIGH RISK in the white paper autonomous driving would be categorized as HIGH RISK. Yet, in the automotive sector the usage of AI is said to reduce the risk of human errors, consequently leading to less casualties from traffic accidents. Therefore, the proposed use of HIGH RISK is misleading for providers and customers alike. For a practical approach the concept of HIGH RISK should be reconsidered in consultation with the private sector.;Personal security risks;As mentioned above in SECTION 2, the correlation between data used for an AI system and the desired outcome may lead to complex issues, especially when the data and its categorization appear to be compliant with regulations, yet the results may cause negative outcomes for the user. Therefore, it is important to manifest solutions for these scenarios. ;Yes;Rather a question: what about AI systems that are already in use within take part in areas ;No opinion;;No opinion;;;
F530006;12-06-2020 07:18;English;Consumer Organisation;Miika;Blinn;;Verbraucherzentrale Bundesverband - vzbv (Federation of German Consumer Organisations);2893800753-48;Medium (< 250 employees);Germany;The feedback can be published with your personal information;5 - Very important;5 - Very important;No opinion;No opinion;No opinion;No opinion;;5 - Very important;5 - Very important;No opinion;No opinion;No opinion;No opinion;;5 - Very important;5 - Very important;No opinion;The efforts to support R&D should focus on fields of research that correspond to European values and further a European, human-centered approach to AI. These include R&D of procedures and standards for anonymization as well as explainable AI. This of particular relevance, as developers/deployers of high-risk AI applications must inform consumers and explain the result of the individual case in a comprehensible, relevant and concrete manner.;No opinion;5 - Very important;5 - Very important;No opinion;No opinion;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Smart assistants/consumer interfaces can undermine individual self-determination and autonomy: Digital assistants increasingly take decisions for consumers, the criteria according to which these decisions are made are often opaque. Manipulating consumer decisions via biased rankings is already a reality on some large online marketplaces. Some systems analyse human emotions in real time or evaluate personality profiles of people. This weakens the consumers’ bargaining position vs. enterprises.;There is a need for a new legislation;;Yes;;No;;Applications in any sector must be subject to legal requirements if they pose a high risk/can cause significant damage, not only applications in the predefined sectors. With the cumulative approach many high-risk applications fall out of the legislation’s scope. The cumulated approach should be replaced by a horizontal approach, complemented with sector-specific criteria. Risk and corresponding risk-mitigating legal obligations should be assigned gradually and grouped into a number of categories;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;"The operation of remote biometric identification systems by public and non-public operators should be prohibited in public places (including publicly accessible places such as supermarkets) until the associated risks and consequences for individuals and society have been adequately researched. The same should apply for the analysis of metabolites (see below).

In addition to the much-noticed biometric identification and analysis, the collection and analysis of so-called “metabolites” can be particularly risky. This involves the analysis of particles that people leave behind/give off, for example sweat, dust, breath, etc. The analysis of metabolites can allow conclusions to be drawn about individual behaviour, consumption and habits and is thus highly sensitive. Therefore, it should be included in the scope of the regulation and be subject to strict legal limits. 

The European Commission clearly distinguish between the use of remote analysis of biometric/metabolite information between public and private spaces as well as public and private uses.";Rather not;Voluntary labelling is no substitute for mandatory requirements for AI Applications that can cause significant damage. Any voluntary labelling system must be based on the same requirements as those mandatory for higher risk AI applications. Different requirements risk undermining consumers’ trust in the voluntary label and the mandatory regulation. To generate trust in the voluntary labelling system, strict ex ante/ex post enforcement must ensure compliance of operators with all requirements.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Compliance with a European horizontal piece of AI regulation complemented by sector-specific rules must be ensured with ex ante/ex post audits. A European governance structure or agency should support the sector-specific national and European competent authorities directly in supervising AI systems with methodological and technical expertise, e.g. with checking whether the obligatory conformity assessment for high-risk applications had been carried out in a diligent manner.;Mental health risks;"All those risks are relevant and have to be addressed in order that consumers can rely upon safe products. In light of risks stemming from AI, privacy and data protec-tion should be furthermore added. 
Also, new production processes more generally - such as 3D printing potentially using AI-based models - need to be covered by product safety legislation.";Yes;"With respect to connected devices and digitally enabled goods security by design and by default should gain more prominence. Also todays product safety legislation is still centered around the placing in the market of products, regarding the above mentioned products a ""continued conformity"" should be considered.";Yes;The Product Liability Directive (PLD) should cover devices and their function and effects, including software and digital content. Developers and deployers should bear strict liability when a product causes damage while used as intended. In ecosystems of connected AI-powered products/services, all developers and deployers involved should bear joint liability. EU law needs a new definition of damage covering immaterial damage to consumer data or their digital environment as well as data leakages;No opinion;;;20_06_11_vzbv_EC_Whitepaper_AI_Comment.pdf
F530005;12-06-2020 02:10;English;Academic/Research Institution;Joshua;Meltzer;;The Brookings Institution;Yes;Large (250 or more);United States;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;3 - Neutral;4 - Important;4 - Important;;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;;5 - Very important;5 - Very important;4 - Important;;4 - Important;4 - Important;4 - Important;4 - Important;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;3 - Neutral;;There is a need for a new legislation;;Yes;;No;;;5 - Very important;4 - Important;4 - Important;4 - Important;3 - Neutral;4 - Important;No opinion;;Rather not;;No opinion;;;Personal security risks;;Yes;;No opinion;;No opinion;;;AI_White_Paper_Submission_Final.pdf
F530004;11-06-2020 23:41;English;Academic/Research Institution;Daniel;Dols;;The London School of Economics;;Large (250 or more);United Kingdom;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;Mandate research into human psychological ramifications on behavior and decision making altered by AI. ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Promoting ethical-by-design AI systems and encouraging SMEs to avoid using AI merely as tools for consumer data collection. ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Companies may employ Dark Patterns in AI used to nudge consumers to self-disclose private information or purchase more items from them. ;There is a need for a new legislation;;No;;;;The usage of AI as a mere data collection tool employed to further the large US-based tech company reach fueling the data economy. The employment of Dark Patterns in AI raises serious questions about how legislation will dictate human-machine interactions and to what extent data gleaned from voice assistant and chatbot interactions can be used by companies to increase their capital. ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;There must be strict legislation established extensively limiting the ability of biometric technology used on citizens in public spaces by both governments and corporations.;Much;"There should not be a ""voluntary labelling"" option since all AI should be regulated. Both categories of high-risk and low-risk should be equally regulated accounting for the specific functions that they perform. The creation of a voluntary label vs. no label assumes that most organizations will opt to get the voluntary certification to improve the consumer trust awarded to them, at which point it makes sense to enforce the ""voluntary"" framework proposed onto all EU AI tools and not just some. ";A combination of ex-ante compliance and ex-post enforcement mechanisms;;A specific focus should address Dark Patterns which may be used in AI by way of an independent regulatory body which assesses all AI tools put into the market and also provides best-practice guidelines which address ethics, value-sensitive design, and preserves user autonomy. ;Mental health risks;The interplay between Dark Patterns used to manipulate and deceive consumers into giving up their data, how AI can further exploit these already occurring tactics, and how this is connected to wider issues around the data economy. Therefore, there is a need to address these interconnected issues in future legislation in order to protect consumers from being mislead and coerced by AI applications into self-disclosing information which serves corporate purposes and hinders user autonomy. ;Yes;"A definition should be created by engaging various stakeholders to address how AI systems can promote a user's ""best interest"" whenever consumers interact with the system. This would help carve out a consideration within the risk assessment procedures which accounts for the preservation of user autonomy within the AI system. ";Yes;New frameworks must be created to address the emerging challenges of liability and must depart from existing ideations. In many ways, AI tools themselves should become liable, subject to financial compensation provided on behalf of the owning company to the plaintiff, the termination of the AI and/or an alteration which caused its behavior, and in extreme cases where negligence or clear disregard for AI regulations can be proven, the engineers who created the AI should be held liable.;Yes, for all AI applications;;;Consultation_Response_to_the_European_Commission_Daniel_Dols.pdf
F530003;11-06-2020 22:14;English;Public authority;Hannah;Obersteller;Regional;Ministry of Economics Lower Saxony (Germany);;Large (250 or more);Germany;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;1 - Not important at all;5 - Very important;5 - Very important;Assure possibility of exchange/partnership/cooperation with non-EU stakeholders;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;;3 - Neutral;5 - Very important;5 - Very important;3 - Neutral;4 - Important;1 - Not important at all;;There is a need for a new legislation;;No;;;;;5 - Very important;4 - Important;4 - Important;5 - Very important;3 - Neutral;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;"General supervision of public space not in line with fundamental rights; maybe to be considered in case of reasonable suspicion/tracing of persons at a certain place and time";Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Foster open source AI initiatives;;;Yes;Give guidance on documentation of ex-ante risk assessments;Yes;;Yes, for specific AI applications;"autonomous driving; generally it must be ensured that - where AI may be applied - there is a clear liability regime in place";;
F530002;11-06-2020 21:56;German;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;4 - Important;4 - Important;2 - Not important;1 - Not important at all;1 - Not important at all;Die Nutzung von AI kann kein Selbstzweck sein. Sie muss transparent und offen erfolgen und die Grundsätze unseres demokratischen Gemeinwesens achten. Themen wie predictive policing, profiling sowie die offensichtlichen Schwächen in der Erstellung von fairen und gerechten Modellen zeigen auf, wie wichtig eine demokratische Kontrolle, Überwachung von AI und der öffentliche Diskurs über den Einsatz von AI ist. ;5 - Very important;4 - Important;1 - Not important at all;2 - Not important;;1 - Not important at all;Es bedarf klarer wissenschaftlicher und politischer Kriterien für die Förderung von AI. Dabei müssen soziale Auswirkungen und die Auswirkungen auf Menschenrechte berücksichtigt werden. Eine demokratische Kontrolle der Aktivitäten in diesem Sektor ist unerläßlich. ;3 - Neutral;4 - Important;1 - Not important at all;"Nicht nur technische Möglichkeiten sind zu untersuchen. Soziale und humanitäre Auswirkungen sind ebenfalls entlang der selbstgesteckten ethischen Leitsätze zu betrachten. Damit verbietet sich auch die weitere Förderung von Massenüberwachung, angeblicher ""Lügendetektion"" im Rahmen der Grenzüberwachung und der blinde Glaube an die Effizienz von KI-Systemen ohne fundierten wissenschaftlichen Nachweis.";1 - Not important at all;4 - Important;1 - Not important at all;1 - Not important at all;1 - Not important at all;Gerade SME sind nicht dadurch aufgefallen, dass sie sich von ethischen oder rechtlichen Bedenken von Massenüberwachung, Gesichtserkennung und dem Missbrauch solcher Systeme abbringen lassen. AI kann daher kein Selbstzweck sein, der um sich selbst Willen gefördert werden muss. Datenschutz und freiheitliche Bürgerrechte haben Vorrang zu geniessen vor dem blinden Verfolgen von nicht abschätzbaren technologischen Entwicklungen.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;"Die Entscheidungskriterien für AI sind nicht immer nachvollziehbar, aber oft geprägt von falschen Grundannahmen und Voreingenommenheiten in der Auswahl von Parametern oder Daten. Damit sind die Grundlagen für eine faire und gerechte Gesellschaft bedroht, auch, weil man gegen die ""unbestechliche Logik und Unfehlbarkeit"" eines AI-Systems immer ""bergauf"" argumentiert. Hier müssen Algorithmen, Entscheidungskriterien, Lerndatensätze usw. in einem höchsten Maße transparent und nachvollziehbar sein.";Other;Gerade die erst beginnenden Verbesserungen im Datenschutz durch die DSGVO dürfen durch einen unbeschränkten Einsatz von AI auf der Grundlage von allen möglichen Daten nicht gefährdet werden. Hier ist sicherzustellen, dass durch die Möglichkeiten von AI nicht diese ersten Fortschritte zunichte gemacht werden, weil Daten in bisher nicht bekannter Menge und Verarbeitung neu Rückschlüsse zulassen oder Überwachung, Tracking und Identifikation ermöglichen.;Other;"Regularien müssen unbedingt für AI erarbeitet und fortgeschrieben werden. Eine Unterscheidung in ""high-risk"" und ""low-risk"" ist nicht nachvollziehbar und daher unredlich. Wie soll das denn unterschieden werden? Wie soll verhindert werden, dass Anwendungen als ""low-risk"" eingestuft werden, die einer näheren Betrachtung nicht standhalten würden? Wo soll die Grenze zwischen high- und low-risk liegen? Wo bleibt der kritische Blick auf den tatsächjlichen Nutzen solcher Systeme?  ";;;"- Autonome Waffensysteme,
- predictive policing,
- Gesichts- und Verhaltenserkennung,
- Biometrie
- ""Lügendetektion"" auf Basis der Auswertung von Emotionen";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;Diese Systeme schränken das Recht auf Privatheit, Bewegungsfreiheit, Versammlungsfreiheit uvm. in einem unegahnten Maß ein. Sie stellen alle Bürger unter Generalverdacht, führen zu chilling effects in der Wahrnehmung demokratischer Rechte und sind in einer freiheitlichen Gesellschaft nicht akzeptabel. Dem gegenüber steht ein erkennbar nur marginaler Nutzen überhaupt und wenig im Sinne einer Verhinderung von Straftaten.;Rather not;Eine Freiwilligkeit für die Klassifikation entspricht nicht meiner Forderung nach öffentlicher Überwachung und Kontrolle. Derart kritische Handlungsfelder dürfen nicht einer Freiwilligen Selbstkontrolle und -einschätzung seitens der Hersteller und Betreiber solcher Systeme unterliegen, um jedwedem Mißbrauch vorzubeugen. ;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Die frühzeitige Kategorisierung in high risk und low risk ist nicht nachvollziehbar. Jedweder Einsatz von AI muss sowohl im Vorfeld als auch nach der Inbetriebnahme von unabhängigen Einrichtungen geprüft und überwacht werden. Dazu gehört auch die unbedingte Forderung nach Untersuchung der Auswirkungen auf Menschen- und Bürgerrechte und deren unbedingte Gültigkeit.;Mental health risks;Die Diskriminierung von Personen und Gruppen auf Basis von schlecht ausgewählten, mangelhaft erforschten und fehlerhaften Daten. Die bisherigen Erfahrungen zeigen, dass hier bereits in den Ansätzen die wissenschaftliche Grundlage fehlt und falsche, damit diskrimierende Schlüsse gezogen werden. Auch verstärkende Effekte von Algorithmen, z.B. in Suchmaschinen und Sozialen Netzen bergen Gefahren (Filter bubble/ echo chamber);Yes;Selbstverständlich müssen die bereits etablierten Stellen zur Sicherstellung der Einhaltung von Bürgerrechten, z.B. Datenschutzbeauftragte usw. in der Erarbeitung aller Grundlagen für AI frühzeitig und umfassend eingebunden werden.;Yes;"Entwickler und Betreiber von AI-Systemen sollten umfassend für die Auswirkung solcher Systeme verantwortlich gehalten werden, um Fälle wie ClearView AI zu verhindern. Ein Rückzuck auf die Linie ""das war die AI, da wissen wir nicht, wie die entscheidet"" darf niemals möglich werden. Hier müssen klare Haftungsregeln durchgesetzt werden.";Yes, for all AI applications;;Genau hier muss die geforderte Transparenz greifen, um die notwendigen Nachweise führen zu können. Damit sind bisherige Regeln zu Schutz von Algorithmen, geistigem Eigentum usw. so anzupassen, dass eine Prüfung und Kontrolle offenmöglich ist.;
F530001;11-06-2020 21:48;English;Business Association;K.C.;Swanson;;Coalition of Services Industries (CSI);;Micro (< 10 employees);United States;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;20200614_CSI_comments_on_EC_White_Paper_on_AI.pdf
F530000;11-06-2020 21:09;English;;;;;;;;;The feedback can be published in an anonymous way;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;Test_Document.docx
F529999;11-06-2020 20:56;English;Company/Business organisation;Marie-Paule;ODINI;;Hewlett Packard Enterprise;;Large (250 or more);France;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;4 - Important;5 - Very important;No opinion;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;;There is a need for a new legislation;;No;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No further guidelines or regulations are needed;;Very much;Labelling should help a lot ;A combination of ex-ante compliance and ex-post enforcement mechanisms;;operated by external authorities, not by self-assessment means;Risks related to the loss of connectivity;risks related to the loss of connectivity are important as well. We won’t have fully autonomous AI agents since the beginning. There will be mixed architectures as well, such as most of AI on the edge, but still interacting with some remote data center (example: federated learning). What happens if we lose connectivity? Can we react to this? Who is going to pay if a car kills someone because of this connectivity loss? What if a remote surgery fails because of the same?;Yes;;No opinion;Products with AI inside that make them change over lifetime should be at a minimum be labelled as such, and some legislation shoudl apply to limit these changes – ex: a mobile phone is not supposed to change to a listening device with AI inside;Yes, for all AI applications;;as Europeans we are very good in regulating, hence we could lead on applying this to AI. But regulation should be lightweight and perceived as an added value, not as an additional complexity limiting our companies. A good legislation will ensure European companies can create high-quality and certified AI, and attract intercontinental customers and investments.;
F529998;11-06-2020 20:46;German;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;4 - Important;5 - Very important;2 - Not important;1 - Not important at all;1 - Not important at all;;4 - Important;5 - Very important;1 - Not important at all;2 - Not important;1 - Not important at all;3 - Neutral;;3 - Neutral;3 - Neutral;1 - Not important at all;;2 - Not important;2 - Not important;4 - Important;2 - Not important;1 - Not important at all;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;No;;;;;5 - Very important;1 - Not important at all;4 - Important;4 - Important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Very much;;Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F529997;11-06-2020 20:45;German;;;;;;;;;The feedback can be published in an anonymous way;No opinion;5 - Very important;No opinion;No opinion;2 - Not important;2 - Not important;;5 - Very important;4 - Important;3 - Neutral;2 - Not important;3 - Neutral;2 - Not important;;4 - Important;4 - Important;2 - Not important;;3 - Neutral;4 - Important;3 - Neutral;2 - Not important;2 - Not important;;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;There is a need for a new legislation;;No;;;;;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;;;Yes;;Yes;;;;;
F529996;11-06-2020 20:31;English;Business Association;Arthur;HILLIARD;;Insurance Europe;33213703459-54;Small (< 50 employees);Belgium;The feedback can be published with your personal information;5 - Very important;4 - Important;5 - Very important;3 - Neutral;5 - Very important;4 - Important;Limited access to public sector data that would offer invaluable data sources for developing AI, due to the comprehensiveness and quality of the datasets. To provide as much societal benefit as possible, such datasets should be made available free of charge and in a machine-readable format to would allow their subsequent use in AI applications. Technical issues such as interoperability and standardisation of data should be addressed to ensure that datasets can be used to their full extent.;4 - Important;4 - Important;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;The EC should focus on a strategy that ensures access to data from the public sector and supports data partnerships between public & private sectors. This could facilitate future prevention work in the insurance industry. Members states should align policies and strengthen coordination on the development of global labels or certifications regarding levels of data protection, privacy etc. An additional area could focus on the creation of European libraries of open source codes for AI algorithms.;3 - Neutral;4 - Important;5 - Very important;The EU should have a more ambitious approach to AI research and be able to retain or attract experts in this field. It could be useful to learn from the practices of other countries, to further develop European AI laboratories, etc. It could also explore how cooperation could be arranged with universities to attract new employees.;2 - Not important;4 - Important;3 - Neutral;5 - Very important;3 - Neutral;"Digital Innovation Hubs could promote and focus on use cases (being customer-minded and business-oriented to help move faster from R&D to practical and industrialised applications); promote customer-centric and ROI focus; and play a role in demystifying, explaining and evaluating the valuable contributions of AI. It is important that data protection authorities and financial regulators are also on board when creating digital innovation hubs.";3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;The future regulatory framework should not be an obstacle for innovation but an enabler. GDPR-related issues should be carefully considered and clarified during the legislative process to avoid overlaps or contradictions, eg limits on data usage affect reliable AI development and may inhibit the aim to make Europe a world leader in AI development & deployment. Further guidance on the application of GDPR to new technologies would be welcome (eg clarification of controller/processor).;Other;The development & use of AI is covered by a wide body of existing EU legislation addressing many of the potential risks & challenges, further complemented by national & sectoral regulatory frameworks. A proportionate, principles- and risk-based framework that builds on this, addressing potential gaps where necessary, will help support development & uptake of AI and avoid unnecessary burden. Policymakers should also examine where existing legislation may create barriers to AI use or development.;Yes;;Other;Insurance Europe believes it is important to ensure that the framework is risk-based and proportionate. Only those AI applications with proven high risk & producing significant effects for the rights of individuals should be in the scope of the future framework. New rules would be pertinent if, and only if, current rules and regulations are not sufficient. Any changes should be limited to clearly identified problems for which gaps exist and overlapping or contradictory texts should be avoided.;-;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);The use of such technologies in public spaces should be permitted but their use needs to be supervised, subject to strict legal control and limited to certain cases only. New technologies using biometric identification systems are evolving rapidly without any limitations or guidelines for their operation. Clear rules and signposting, including ensuring effective mechanisms for consent, would enhance trust and ensure greater legal certainty for both businesses and individuals.;Much;The voluntary labelling system should be legible, easy to use and recognised across the EU. It is also important to ensure proportionality is incorporated into such approach. If an AI application is considered non-high risk, and not subject to mandatory requirements, the same or similar requirements should not be introduced via a labelling system. Further issues also require clarification, eg responsibility for the system, industry participation, practical arrangements, etc.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;No.;Personal security risks;From an insurance perspective, major changes to the legal regime could result in challenging insurability issues. In general, we are of the view the existing system works well. In the future there may be a need to further explore the risks associated to the opacity of AI in-use, as well as the role & liability of the vendor/platform that sold AI goods to the consumer.;Yes;The ability to track software updates is worthy of consideration. This ability would assist in determining the version of software that was installed and in-use at specific points in time and would have use in determining liability. In addition, an adaptable and flexible governance will be relevant as the AI systems will change and evolve over time. ;No;In European insurers’ experience the current liability framework is fit for purpose, implementing a well-balanced system of liability that provides a high level of protection to consumers while taking into account manufacturers’ legitimate interests, thereby encouraging growth and innovation. Any change to this balance could negatively impact the cost and availability of insurance. Guidance documents may prove useful, addressing certain perceived risks posed by AI applications.;No;;Different national approaches are taken, and different national rules, contract or tort law are specific to individual markets. With the PLD at European level, the existing framework is broadly is fit for purpose. It should in any event be kept in mind that consumers have additional liability compensation options through co-existing civil liability regimes (contractual and extra-contractual) for cases where the PLD may, for one reason or another, not apply.;Artificial_Intelligence_-_views_of_the_European_insurance_industry.pdf
F529995;11-06-2020 20:11;German;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;1 - Not important at all;1 - Not important at all;in deutsch plz;4 - Important;No opinion;1 - Not important at all;2 - Not important;4 - Important;2 - Not important;deutsch plz;3 - Neutral;4 - Important;1 - Not important at all;deutsch plz;1 - Not important at all;2 - Not important;3 - Neutral;3 - Neutral;5 - Very important;deutsch plz;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;deutsch plz / AI used in sensitive areas (public services) without democratic oversight, transparency or sufficient evidence to justify the need/ purpose;Other;"• Legislation must strengthen, not replace, GDPR. AI presents issues for meaningful consent, objection, data minimisation, purpose limitation, explanation
• AI law must complement a broad interepretation of GDPR, including affinity profiling or sensitive inferences
• Current law does not address use of non-personal data, and collective impact of AI, such as furthering overpolicing, surveillance, inequality
• Current law does not prohibit AI discrimination on non-protected ground";Other;"• New rules are necessary to determine the criteria for when it should be legal to develop and deploy AI
- burden of proof is on the developer/deployer and not on impacted groups
- mandatory democratic oversight before deployment of AI in public sphere
• mandatory fundamental rights impact assessments for all uses
• The EU should proactively ban AI applications in areas where the fundamental rights and societal implications are too ";;;In DEUTSCH plz / The use of AI to determine delivery of essential public services, predictive policing, autonomous lethal weapons, identification/ analysis of emotion and identity traits, and indiscriminate biometric surveillance, are incompatible with fundamental rights and should be banned by default Determining ‘risk’ should be rights and outcomes focused, not sectoral. Major concern with systems which impact fair trial, in migration control and policing, and systems which may perpetuate ineq;No opinion;No opinion;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);"In DEUTSCH plz / The use of biometrics for remote identification in publicly-accessible spaces significantly contributes to unlawful mass surveillance so should never be deployed.
Such uses will transform public spaces into sites of continuous watching and irreversibly compromise fundamental rights to privacy, freedom of assembly, expression, non-discrimination, data protection, fair trials, democracy and the presumption of innocence. The EU must harmonise and enforce existing legislation to protect these rights.";Rather not;"In DEUTSCH plz / We should exercise caution deciding that some applications are inherently low risk and therefore do not require oversight to guarantee fundamental rights.
We strongly caution against incorporating voluntary, self-regulatory and ethics based approaches in AI regulations. Such approaches provide scope to cirucmvent accountability and soften fundamental rights obligations. Further, they reduce certainty and impede access to justice for those harmed";Other enforcement system;In DEUTSCH plz / Heightened risks of discrimination, in particular with reference to online products and services using data for targeted advertising. This poses risks of differentiated pricing, discrimination financial detriments, the risk of creating filter bubbles, interfence in the democratic process, based on sensitive inferences or associations. In addition, there are concerns related to accessibility or harms specifically to be experienced by people with disabilities.;In DEUTSCH plz / Insofar as possible compliance with regulatory measures should be guaranteed by external and independent entities, avoiding self regulation and ensuring there are no loop holes to fundamental rights protection;Mental health risks;In DEUTSCH plz / Heightened risks of discrimination, in particular with reference to online products and services using data for targeted advertising. This poses risks of differentiated pricing, discrimination financial detriments, the risk of creating filter bubbles, interfence in the democratic process, based on sensitive inferences or associations. In addition, there are concerns related to accessibility or harms specifically to be experienced by people with disabilities.;Yes;In DEUTSCH plz / Internal supervisors, such as Data Protection Officers under GDPR should be included and and asked for advice.;Yes;"In DEUTSCH plz / Liability should be centered around accountability and the extent to which faults can be redressed by users of technology.
There should be liability for producers of AI that do not disclose source code (including their algorithmic models/ datasets) and do not provide fixes for issues brought to their attention or otherwise hinder fixes from being applied, for example by not allowing third-party fixes based on any disclosed source code.";Yes, for all AI applications;;"In DEUTSCH plz / AI applications are covered both by copyright and database rights protections, which prevent 
users from assessing their quality and limit their ability to redress issues that have been observed. Moreover, current software liability is usually exonerated through license agreements. Liability rules should limit the ability to exonerate such liabilities by providing incentives for openness.";
F529994;11-06-2020 20:04;English;Academic/Research Institution;Jan;de Bruyne;;KU Leuven Center for IT & IP Law (CiTiP) & Leuven.AI;;Large (250 or more);Belgium;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;Feedback_on_White_Paper_on_Artificial_Intelligence__KU_Leuven_CITIP_-_Leuven.AI_.pdf
F529993;11-06-2020 19:34;German;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;;4 - Important;No opinion;2 - Not important;3 - Neutral;;4 - Important;2 - Not important;3 - Neutral;3 - Neutral;4 - Important;5 - Very important;;4 - Important;3 - Neutral;2 - Not important;;4 - Important;5 - Very important;4 - Important;4 - Important;3 - Neutral;;3 - Neutral;5 - Very important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;;There is a need for a new legislation;;Yes;;Yes;;;5 - Very important;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;4 - Important;Biometric identification systems should never be allowed in publicly accessible spaces;;Very much;;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;;Cyber risks;;Yes;;No opinion;;Yes, for specific AI applications;E.g. driving cars by AI;;
F529992;11-06-2020 19:28;German;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;Es ist wichtig anonyme offene und möglichst zentrale Datenpools zu generieren, die regelmäßig aktualisiert werden. Unternehmen, die in Europa ansässig sind und natürlich ihre Steuern zahlen, können diese Datenpools für verschiedene Anwendungsfälle  benutzen. Üblich sind API Kosten ab X Anfragen pro Monat. ;4 - Important;2 - Not important;3 - Neutral;4 - Important;3 - Neutral;5 - Very important;Die Qualität und Aktualität der anonymen Daten entscheidet darüber, wie präzise die AI reagieren kann bzw. Entscheidungen trifft. ;4 - Important;5 - Very important;4 - Important;"Die Nachvollziehbarkeit der Entscheidung von einer AI muss gegeben sein. Hier muss geforscht werden! Entscheidungsstränke müssen aufzeigbar gemacht werden: 
Welche Wahrscheinlichkeiten haben zu welchem Zeitpunkt das Endergebnis beeinflusst? Diese müssen gerichtlich anfechtbar sein, um die Menschenrechte zu wahren.";4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;Die Arbeiten sollten möglichst öffentlich bzw. für EU Mitglieder leicht einsehbar sein. Damit möglichst Viele von dem Wissen profitieren können. ;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;Die Qualität der Daten, mit der die AI Systeme arbeitet, sind entscheidend, wie präzise die Ergebnisse sein werden. Alle Daten die von Hand oder von Menschen hinzugefügt werden, können neutrale Handlung der AI verfälschen.;There is a need for a new legislation;;Other;"Die Anforderungen müssen für alle AI Systeme gelten und nicht nur auf die Systeme mit hohem Risiko. Je nach dem in welchem Gebiet die AI eingesetzt werden soll, sollten verschiedene Anforderungen formuliert werden.
Verschiedene Fragestellungen fallen mir dazu ein: 
- Wird durch die Entscheidung der AI Tier- oder Menschenleben riskiert?
- Werden durch die Entscheidungen der AI Menschen bewertet (z.B. Jobportale) und findet somit eine Diskriminierung statt?";;;"Kombinationen aus Zukunftstechnoligien wie KI und der Bioinformatik. Es dürfen keine naiven Experimente durchgeführt werden.


";5 - Very important;3 - Neutral;4 - Important;4 - Important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Not at all;Haha, Nein! Freiwillige Kennzeichnungssysteme funktionieren nicht.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;Die Menschen werden höchstabhängig von KI Systemen, die ihre Entscheidungen treffen. Der Mensch wird es dem Computer überlassen und verlernt die Fähigkeiten die gute und überlegte Entscheidungen zu treffen, z.B. Karten zu lesen/Navigation (Bsp. Google Maps).;Yes;Der Klimawandel ist um Längen wichtiger: Wie nachhaltig können die neuen KI Systeme sein?;No opinion;;Yes, for specific AI applications;Für Systeme die Leben gefährden. Nicht nur durch den Tod, sondern auch durch z.B. Diskriminierung oder falsche Verdächtigungen der Polizei durch z.B. Gesichtserkennung. ;Wichtig ist die Unterscheidung zwischen AI, wo mehrere Neuronale Netze Entscheidungen treffen und Algorithmen wo Entscheidungen anhand fester Regeln getroffen werden. Die Entscheidungen der Algorithmen können anhand der festen Regeln nachvollzogen werden und sind somit weniger Kritisch. AI Systeme können undurchsichtige Entscheidungen treffen die Niemanden glücklich machen. ;
F529991;11-06-2020 19:28;English;NGO (Non-governmental organisation);Maria;BIELIKOVA;;Slovak research center for artificial intelligence - slovak.AI;299938338124-40;Micro (< 10 employees);Slovakia;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;Further development of AI is through deeper cooperation worldwide, through developing European capabilities and continue cooperation with technology leaders around the world. Most countries where the rule of law and the democratic establishment exist, share concerns about data protection, cyber security and future industrial capabilities. The Commission should seek cooperation with credible int. partners such as OECD countries who can guarantee to comply with European rules and regulations.;3 - Neutral;5 - Very important;4 - Important;"Europe needs a ""lighthouse centre"" but it should serve for knowledge exchange instead of employing researchers. We should create a vibrant network of excellence labs which will present by itself the reference center for Europe.  This place for knowledge exchange should ""just"" facilitate the network.

Europe should take into account in all the activities small countries, which should not be excluded. ";4 - Important;5 - Very important;4 - Important;5 - Very important;4 - Important;The level of AI development in particular regions should be constantly evaluated. Strong support of partnerships with academy would bring higher quality of research and innovations. Support small enterprises by partnerships with larger enterprises or more experienced enterprises can accelerate adopting AI and innovations.;4 - Important;5 - Very important;4 - Important;5 - Very important;3 - Neutral;4 - Important;We should devote the same concern to ethical issues as to the development itself. Excellent research can lower a degree of fear. Responsibility should be clearly defined.;There is a need for a new legislation;;Yes;;Other;(1) Too much emphasis on training data quality, not enough on testing output. (2) A literal approach to reproducibility can aggravate situation and for many cases it will not provide satisfactory results. (3) Ex ante conformity assessment requirements do not strike the right balance. A combination of ex-ante risk self-assessment and ex-post enforcement for high risk AI applications would likely achieve similar results within much faster timeframes and without risking unduly stopping innovations.;"Automatic weapon systems; personal monitoring applications, biometry; applications that provide risk of life. ";5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);For example for the employee identification in entry to a building on condition of informed consent. In public spaces no person identification should be possible and strict rules for keeping data should be provided (which data - no source data, strictly anonymized). ;Much;We need flexible legislative.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;A combination would likely achieve similar results within much faster timeframes and without risking unduly stopping innovation and creating unnecessary burdens. This would also build on existing industry practices. Prior to any launch, for AI applications deemed to be high-risk, organisations could be mandated to carry out and document risk assessments based on articulated principles. This would be analogous to the requirement for data protection impact assessment under GDPR.;Personal security risks;;Yes;Procedures for risk assessment should not burden innovations and should be sufficiently reliable. End user should be informed whenever any (risk) function of the product is changed. ;Yes;;Yes, for specific AI applications;We should determine responsibility. Responsibility cannot be transferred to AI, always a person or legal subject should be responsible. It should be defined intelligibly with clearly named risks or constraints followed character of product or service.;;
F529990;11-06-2020 19:16;German;EU Citizen;Cordula;Helmig-Walker;;;;;Germany;The feedback can be published with your personal information;3 - Neutral;4 - Important;5 - Very important;4 - Important;5 - Very important;4 - Important;;;;;;;;;;;;;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;;3 - Neutral;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;;Current legislation may have some gaps;;Yes;;No opinion;;;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F529989;11-06-2020 19:10;English;Academic/Research Institution;Matteo;STOCCHETTI;;Arcada University of Applied Sciences;;Large (250 or more);Finland;The feedback can be published with your personal information;4 - Important;4 - Important;4 - Important;2 - Not important;2 - Not important;2 - Not important;Setting conditions for the institutional control on the usage of AI and related technologies;5 - Very important;4 - Important;2 - Not important;2 - Not important;4 - Important;5 - Very important;Coordinate initiatives for the institutional control on the usage of AI and related technologies;5 - Very important;4 - Important;1 - Not important at all;Support the independence of universities of applied sciences and their researchers from private funding;2 - Not important;5 - Very important;4 - Important;2 - Not important;2 - Not important;;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;AI will increase, rather than reduce, inequalities and ultimately the power of groups already influential in society, increasing the opaqueness of decision making on relevant social issues;There is a need for a new legislation;;No;;;;Finance, military affairs, welfare & education;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;General prohibition should stay in place until further considerations and knowledge can support an adequate legislation for the whole of EU space because there is too much that we do not know yet.;Not at all;Corporate culture around the globe has failed too many tests on other issues where self-regulatory practices were enforced. AI is too attractive to unethical (if not illegal practices) and too risky for the whole of society to be regulated on voluntary basis.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Assessment of compliance should be designed as a continuous, transparent and loop-proof system. ;Risks related to the loss of connectivity;The global weaponization of AI and related technologies.;Yes;;Yes;;Yes, for all AI applications;;National liabilities rules should be integrated as much as possible in the EU so to minimize differences among member states ;
F529988;11-06-2020 19:02;English;Academic/Research Institution;Stephanie;von Maltzan;;"FIZ Karlsruhe – Leibniz-Institut für Informationsinfrastruktur
Hermann-von-Helmholtz-Platz 1, 76344 Eggenstein-Leopoldshafen, Germany";;Large (250 or more);Germany;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;Adopting AI by the public sector should be assessed on the basis of necessity and proportionality;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;legal certainty for example;4 - Important;4 - Important;3 - Neutral;"providing ressources for legal and ethical issues in research; public-private partnership should be transparent";4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;"Providing legal and ethical advice; ensuring a transparent innovative and development process";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"rights of data subjects should be specified in relation to AI and automated decision-making systems; the risk of manipulative and biased outcomes should be considered";Other;"gaps in existing legislations; in some fields there is a need for a new legislation; see attached document";Other;"depending on the definition of high-risk applications; developing a more-level based risk approach is recommended; requirements should be specifically set in accordance to the different levels of automated decision-making systems; see attached document";;;"Profiling (PNR); remote biometric identification systems; automated lethal weapons; psychological manipulation/deep fakes; opaque systems in general";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Other special requirements in addition to those mentioned in the question above should be imposed (please specify);rights and remedies of data subjects should be strenghtened in regard to biometric applications in particular in the context of law enforcement;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;;Yes, for specific AI applications;robots for example;;
F529987;11-06-2020 19:00;Dutch;NGO (Non-governmental organisation);Margrietha H.;VINK;;Nederlandse AI Coalitie (NL AIC);Neder4618018393;Micro (< 10 employees);Netherlands;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;De NL AIC wil graag dat er een ‘lerende aanpak’ , waarbij we door onderzoek, experimenten en pilots, gerelateerd aan maatschappelijke uitdagingen kunnen beoordelen of (en waar) er problemen zijn met betrekking tot AI-toepassingen. Deze meer convergentie gerichte aanpak is van groot belang op het gebied van regelgeving en interpretatie van regels. Interpretaties kunnen verschillen per land en per sector en dit leidt tot marktstagnatie, wat onwenselijk is voor bedrijven. ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Definieer AI-beleid in relatie tot maatschappelijke uitdagingen. Tav onderzoek en innovatie moet de EC de oprichting/versterking van centra van excellentie en testcentra faciliteren die EU, nationale en regionale kennis/investeringen multidisciplinair combineren. De NL AIC verbindt alle NL nationale en regionale AI excellentie en expertise en ziet graag dat de EC door het  opzetten en verbinden van toonaangevende AI-centra/kennisinstellingen dmv netwerken en creëren van AI hubs faciliteert. ;3 - Neutral;5 - Very important;5 - Very important;De NL AIC wil dat de EC inspanningen zich richten op een top Europees netwerk van AI excellentie centers, verankerd in Hubs met bijbehorende spokes, om zo het versnipperende landschap van kennisinstellingen en centra op het gebied van AI tegen te gaan en beter te laten samenwerken. Gemeenschappelijke onderzoek en innovatie programma’s, opleidingsprogramma’s mobiliteitsinstrumenten en investeringen in infrastructuren kunnen dit bewerkstelligen. ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;Neerzetten ecosystemen van hubs-en-spaken gebouwd het bestaande systemen. Met nadruk stellen we dat activiteiten in de hubs niet alleen technisch wetenschappelijk zijn, maar ook sociaal  maatschappelijke aspecten omvatten en het betrekken van burgers. Alle activiteiten op andere geografische locaties (de spaken) verbinden zich met hubs, om geïsoleerde initiatieven te vermijden. De resulterende hubs-en-spaken structuur kunnen dan per EU lidstaat fungeren als dé nationale AI-hub in EU initiatieven;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;Er moet een lerende aanpak in een open consortium komen: het ruimte bieden aan lerende aanpakken die de dynamiek van de keten volgt in plaats van een vooraf volledig vastgelegde normen, waarden, regels ed. opleggen. Lerend in de zin dat zowel de technologie, toepassingen, het mensgericht ontwerpen, en de maatschappelijke reflectie welke in ontwikkeling zijn en hun onderlinge samenhang alleen door experimenteren in multidisciplinaire context ontdekt kan worden. Tevens  voor ‘human intelligence’.;Current legislation may have some gaps;;Yes;;Yes;;Steun voor de intentie van de EC om dmv cumulatieve criteria een specifieke reikwijdte voor het instrument te creëren. Wel twijfels over haalbaarheid van de sector- gebaseerde benadering. De afbakening sectoren is zeer lastig uit te voeren en kan leiden tot calculerend gedrag en het grote verschillen tussen de juridische regimes voor AI-toepassingen. Verdere invulling, rekening houdend met bijvoorbeeld de levenscyclus van AI, typen data, de menselijke factor en vormen van AI vraagt onderzoek.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);Bij Biometrische identificatie dient extra veel aandacht uit te gaan naar de ontwerp- en ontwikkelfase van AI in deze systemen voor de verwerking van bijzondere persoonsgegevens voor zover dat noodzakelijk is om discriminerende effecten tegen te gaan in het bijzonder ten aanzien van specifieke kwetsbare groepen.;Much;De NL AIC staat positief tegenover dit voorstel aangezien dit past in de eerder genoemde lerende aanpak en een gevarieerd instrumentarium, waar het ontwikkelen van zowel wetgeving als andere instrumenten zoals standaardisatie of certificering ook een optie is.  ;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;Aangezien toezichthouders thans in staat moeten zijn om het door de EC voorgestelde “conformiteitsassessment” uit te voeren, moet het uitgangspunt zijn dat er zoveel mogelijk moet worden aangesloten bij bestaande kaders en structuren.  ;Mental health risks;AVG is mogelijk onvoldoende is toegesneden op AI-toepassingen. En zeker de interpretatie en praktische invulling. Referenties zijn nodig want ook toezichthouders leren nog.;Yes;Een lerende aanpak en een daarop gebaseerd gevarieerd instrumentarium kan op termijn uitkomst bieden.;Yes;NL AIC pleit ervoor voort te bouwen op de AI levenscyclus zodat voor alle betrokken actoren (adressanten) duidelijk is aan welke regels zij onderhevig zijn in de diverse stadia van AI. Bovendien zouden de eisen van toepassing zijn op alle AI-toepassingen op de Europese markt, ongeacht of de producent gevestigd is buiten de EU.  Pas op voor de valkuilen van een risicogebaseerde aanpak.;No opinion;;Een proactieve betrokkenheid van burgers, wetenschappers en bedrijven in en co-creatie van normen en regels voor AI in een lerende omgeving, waarbij de regels uiteindelijk gedragen worden door stakeholders is een absolute ‘must’ en zal snel uitwijzen of het huidige kader voldoet.;
F529986;11-06-2020 18:56;English;Academic/Research Institution;Rafael;Calvo;;Imperial College London;;Large (250 or more);United Kingdom;The feedback can be published with your personal information;No opinion;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;;5 - Very important;4 - Important;3 - Neutral;4 - Important;5 - Very important;4 - Important;;4 - Important;4 - Important;4 - Important;;2 - Not important;4 - Important;4 - Important;4 - Important;4 - Important;;3 - Neutral;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;;Current legislation may have some gaps;;No;;;;;4 - Important;4 - Important;5 - Very important;3 - Neutral;4 - Important;5 - Very important;No opinion;;No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;No opinion;;No opinion;;;
F529985;11-06-2020 18:38;German;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;4 - Important;4 - Important;5 - Very important;1 - Not important at all;1 - Not important at all;"KI darf nicht als Selbstzweck gefördert werden. Bei Anwendung im öffentlichen Sektor muß es klare, veröffentlichte Gründe für den Einsatz der KI geben und wissenschaftliche Nachweise, dass die Technologie funktioniert; wenn es um unseren Zugang zu lebenswichtigen Dienstleistungen oder die Wahrnehmung unserer Grundrechte und -freiheiten geht, müssen wir ein Mitspracherecht haben, ob die KI in einer demokratischen Gesellschaft akzeptabel eingesetzt werden kann oder nicht.";4 - Important;5 - Very important;1 - Not important at all;4 - Important;4 - Important;4 - Important;"Der Plan zur künstlichen Intelligenz muß aktualisiert und wissenschaftliche und politische Kriterien aufgenommen werden, wie die Ressourcen der künstlichen Intelligenz zugeteil werden können.
Der Plan muß einen Abschnitt über Menschenrechte, gesellschaftliche Auswirkungen der KI und Automatisierung enthalten sowie Vorgaben, wie eine demokratische Kontrolle der Anwendung von KI-Systemen gewährleistet werden kann.";3 - Neutral;4 - Important;1 - Not important at all;"Die Finanzierung von EU-Projekten zur KI muß an die Einhaltung der ethischen Standards der EU für KI und der Grundrechtegesetze gebunden sein.
EU-Fonds müssen die Finanzierung von Projekten sofort einstellen, die eine Gefahr für die Grundrechte darstellen, wie der Einsatz von Gesichts- und Emotionserkennungstechnologie, oder die Massenüberwachung erleichtern könnten.";1 - Not important at all;4 - Important;4 - Important;4 - Important;1 - Not important at all;"SME dürfen keine Ausnahmen vom Schutz der Menschenrechte erhalten.
Es muß sichergestellt, dass kleine Unternehmen bei der Arbeit an KI unter allen Umständen Datenschutz, Privatsphäre und alle Grundrechte respektieren.";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;"Es besteht die Gefahr,
- daß KI in sensiblen Bereichen wie öffentlichen Diensten ohne demokratische Aufsicht, Transparenz oder ausreichende Beweise zur Rechtfertigung der Notwendigkeit oder des Zwecks eingeführt wird; 
-daß intransparente, privat entwickelte Technologie eingesetzt wird;
daß die Haftung für Schäden, die durch KI-Technologie verursacht werden, bewußt umgangen wird.";Other;"Die KI-Verordnung darf keine Schlupflöcher in Datenschutzgesetze oder andere Regelwerke, wie z.B. Diskriminierungsgesetze, öffnen.
Das geltende Recht berücksichtigt nicht die Verwendung nicht-persönlicher Daten für KI, also Datentypen, die nicht unter die DSGVO fallen.";Other;"Der vorliegende Ansatz ist zu stark vereinfacht und viel zu eng gefasst.
Es müssen klare Kriterien aufgestellt werden, welche KI-Systeme legal sind und welche nicht. Die Kriterien müssen sich auf den Nachweis stützen, dass die KI-Systeme funktionieren und wirklich notwendig sind. Es muß obligatorische Folgenabschätzungen zu Auswirkungen auf die Grundrechte unter demokratischer Kontrolle geben. KI, die Grundrechte verletzt, wie biometrische/Gesichtserkennung, muß grundsätzlich verboten sein.";;;"Folgende Anwendungen sind völlig inakzeptabel, weil sie gegen unveräußerliche Grundrechte verstoßen:
- Die Verwendung von KI, wenn es darum geht, wesentliche öffentliche Dienstleistungen bereitzustellen,
- ""predictive policing"",
- autonome tödliche Waffen,
- Identifikation und Auswertung von Emotionen und Identitätsmerkmalen
- anlaßlose biometrische Überwachung";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;"Unsere Grundrechte auf Privatsphäre, Versammlungsfreiheit, freie Meinungsäußerung, Nicht-Diskriminierung, Datenschutz, Würde und das Recht auf ein faires Verfahren würden irreversibel einsgechränkt und jedermann unter Generalverdacht gestellt.
Selbst Nutzungen, die nicht direkt oder indirekt zur Massenüberwachung in öffentlichen Räumen beitragen, stellen immer noch eine erhebliche Bedrohung für Privatsphäre, Datenschutz, Nicht-Diskriminierung und Würde dar.";Rather not;"Selbstkennzeichnungssysteme können für die Menschen verwirrend sein und ein falsches Gefühl der Sicherheit vermitteln, da es dasselbe Unternehmen ist, das ein Produkt entwickelt, das sagt, dass es sicher ist. Freiwillige Verpflichtungen von Firmen waren bisher immer zum Nachteil der Bürger. Auch besteht die Gefahr, wie diejenigen, die durch "" Systeme mit geringem Risiko"" geschädigt werden, in einem solchen Szenario keine Schadenswiedergutmachung verlangen könnten.";Other enforcement system;Alle Systeme müssen einer obligatorischen Ex-ante-Folgenabschätzung durch ein externes, unabhängiges Gremium bzgl. der Folgen für unsere Menschenrechte unterzogen werden. Menschenrechte müssen absolute Priorität in der KI-Verordnung haben und es muß sichergestellt sein, daß es keine Schlupflöcher gibt, nur weil ein System in eine Kategorie mit geringem Risiko fällt.;"Die Überwachung auf Einhalten der ""Compliance"" muß extern erfolgen, damit der Schutz der Grundrechte gewährleistet ist; wie die Erfahrungen der Vergangenheit zeigen, ist auf Selbstregulierung kein Verlaß.";Mental health risks;Von KI-Systemen geht immer ein Risiko für Diskriminierung aus. Insbesondere KI in Online-Produkten und -Dienstleistungen sammelt und nutzt Daten, was zu Diskriminierung in vielen Bereichen im Zusammenhang mit gezielter Werbung führt. Dies birgt die Gefahr einer differenzierten Preisgestaltung, von Diskriminierung und finanziellen Nachteilen, die Gefahr der Bildung von Filterblasen, Beeinflussung von politischen Prozessen, die alle auf zweifelhaften Schlussfolgerungen oder Assoziationen beruhen.;Yes;KI bringt Komplexität in die Entwicklung von Produkten, oder ist sogar das Produkt selbst, deshalb ist es wahrscheinlich, daß Komplexität und Undurchsichtigkeit hinzukommen und die Verfahren zur Wiedergutmachung im Schadensfall müssen gestärkt werden. Auch müssen die Datenschutzbeauftragten unter der DSGVO einbezogen und um Rat gefragt werden.;Yes;Entwickler und Anwender von KI müssen für alle Schäden haftbar gemacht werden, die durch ihre Produkte verursacht werden. Produkte, die mit Hilfe von KI entwickelt werden, dürfen keine Ausnahmen von den EU-Gesetzen genießen, unabhängig davon, ob es sich um Diskriminierung, Datenschutz oder Produkthaftung handelt.;Yes, for all AI applications;;Die EU muß sicherstellen, daß Urheberrechts- und Datenbankschutzmaßnahmen eine ordnungsgemäße Überwachung von KI-Anwendungen nicht verhindern. Es muß Haftungsregeln geben, die Anreize für Whistleblower bieten, die Verstöße gegen unsere Grundrechte durch KI offenlegen.;
F529984;11-06-2020 18:36;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;5 - Very important;;5 - Very important;3 - Neutral;4 - Important;4 - Important;5 - Very important;5 - Very important;;4 - Important;4 - Important;4 - Important;;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;Current legislation may have some gaps;;No;;;;;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;No opinion;;No opinion;;;Mental health risks;;No opinion;;No opinion;;No opinion;;;
F529983;11-06-2020 18:36;German;;;;;;;;;The feedback can be published in an anonymous way;3 - Neutral;3 - Neutral;3 - Neutral;2 - Not important;1 - Not important at all;1 - Not important at all;Transparente, demokratische Entscheidungs- und Kontrollprozesse, sowie unabhängige wissenschaftliche Nachweise über die Eignung und Wirksamkeit des KI-Einsatzes, bes. in essentiellen Lebensbereichen der Bürger;4 - Important;3 - Neutral;1 - Not important at all;2 - Not important;5 - Very important;2 - Not important;Jede aufgebaute Expertise sollte den Blick auf den Schutz der Grund- und Menschenrechte und die demokratische Kontrolle über eingesetzte KI-Systeme einschließen. Einsatz von KI zur Vermeidung von Missbrauch der Möglichkeiten durch andere KI-Prozesse.;3 - Neutral;4 - Important;1 - Not important at all;Unterstützung der Forschung zu sozialen Auswirkungen des KI-Einsatzes wie Diskriminierung und Desinformation. Projekte grundsätzlich an ihrer Vereinbarkeit mit den europäischen Grundwerten und Menschenrechten ausrichten. Somit keine KI-gesteuerten Massenüberwachungssysteme implementieren.;1 - Not important at all;3 - Neutral;4 - Important;2 - Not important;1 - Not important at all;Besonderes Augenmerk sollte auf die Weiterbildung der SMEs über die Risiken der KI und ihre mit der Nutzung verbundene Verantwortung für Datenschutz und Datensicherheit sowie Wahrung der Persönlichkeitsrechte gelegt werden, deren Bedeutung durch die geringe Firmengröße nicht gemindert wird.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;Das Individuum steht den Entscheidungen undurchsichtiger Algorithmen über sein Leben weitgehend hilflos gegenüber, da den gesammtelten Daten und ihrer Interpretation mehr Gewicht und Glaubwürdigkeit zugemessen wird als der Person und ihrer Realität. Da die KI ihre Einschätzung nicht begründen kann, ist eine Gegenargumentation unmöglich. Die Priorität des Menschen über seinen digitalen Datenzwilling muss gewährleistet werden. Die grundrechtskonforme Funktionsweise der KI ist zentrale Pflicht.;Other;Die bisherigen Gesetze berücksichtigen weder die Fähigkeiten noch die potentielle Macht der KI, das Leben der Menschen zu bestimmen und sich dabei in den Bereichen außerhalb der akt. Gesetzgebung zu bewegen. Zur Profilerstellung einer Person reichen genug unpersönliche (nicht geschützte) Daten. Kollektive, systemimmanente Diskriminierung ist juristisch kaum abzuwehren. Der lückenlose Schutz des Bürgers in seinen Grundrechten muss konkret gesetzlich abgesichert werden gegen die Gefahrenpotentiale;No;;;;Autonome Entscheidung über grundrechtsrelevante Aspekte wie (auch präventive) polizeiliche Maßnahmen, tödliche Gewalt, Gewährung / Verweigerung essentieller Services, Verwendung biometrischer Daten zur anlasslosen Massenüberwachung sowie die Erstellung umfassender Profile zur Verwendung im Rahmen derartiger Zwecke sollten komplett ausgeschlossen werden. KI-Systeme, deren Nutzung Diskriminierung zur Folge hätte, sollten unzulässig sein. ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;"Biometrische Überwachung und Auswertung öffentlicher Orte stellt eine tiefgreifende Verletzung der Grund- und Persönlichkeitsrechte dar und unterminiert den gesamten Geist einer freiheitlichen Gesellschaft, da die Wahrnehmung dieser Rechte selbst in ihrer harmlosesten Form mit einem subjektiven Unbehagen verbunden wird. Beinhaltet eine System-Definition von ""normal"" und ""abweichend"". Besonders Menschen, die vom ""Üblichen"" abweichen, z. B. aus medizinischen Gründen, wären ständig der Frage ausgesetzt ""Sehe ich 'schuldig' aus?"". Generalverdacht / Diskriminierung / Rückzug als erwartbare Folgen. Da Menschen verschieden sind wird die Anzahl der falschpositiven Markierungen auch bei optimaler Systemgestaltung immer die relevanter Ereignisse übersteigen.Die Auswirkungen auf die grundlos Betroffenen wären potentiell verheerend.";Rather not;Es gibt genug Beispiele für freiwillige Selbstverpflichtungen, seltsame Qualitätslabel und andere Mechanismen, die das Vertraue in Produkte stärken sollen und oft nicht mal die Farbe für ihren Druck wert sind. Dies wäre umso wahrscheinlicher der Fall für Produkte, die viel komplexer und undurchsichtiger sind als z. B. Lebensmittel. Selbst mit Vorschriften, (viel zu seltenen) Kontrollen und Strafandrohung ist die Zahl der Verstöße groß. Geschädigte könnten nur schwerlich Ansprüche durchsetzen.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Eine unabhängige Stelle sollte verpflichtend vor der Markteinführung einer KI deren vollständige Wahrung der europäischen Werte und Grundrechte verifizieren. Auch spätere Kontrollen sowie die Möglichkeit, bei unvorhergesehenen Verstößen Schadensersatzansprüche geltend zu machen sind erforderlich. ;Mental health risks;Risiken, die durch KI-Nutzung nicht dem Benutzer, sondern anderweitig Betroffenen entstehen. Risiken für Selbstbestimmung, Gleichbehandlung, Chancengleichheit, Zugänglichkeit. Risiken für die Unschuldsvermutung (Analyseergebnisse mysteriöser Algorithmen besitzen keine Beweiskraft und sind daher keine ausreichende Grundlage für bspw. die Verweigerung von Leistungen);Yes;Da die potentiellen Risiken sehr vielfältig und komplex sind, sollten an der Erarbeitung entsprechender Abläufe frühzeitig Experten aus den betreffenden Bereichen involviert werden (Datenschutz, Menschenrechte, Teilhabe...);Yes;HerKI-Systeme bzw. Produkte mit KI-Funktionalität sollten im Hinblick auf Verantwortlichkeiten, Haftbarkeit für durch diese Produkte verursachte Schäden und Sorgfaltspflichten genauso konsequent allen EU-Regularien (Datenschutz, Diskriminierung, Sicherheit, Zuverlässigkeit etc.) unterliegen wie andere Produkte auch. ;Yes, for all AI applications;;Die exakte Zurechenbarkeit von Schadensursachen gestaltet sich schwierig, da KI-Systeme häufig ihre interne Funktionalität verbergen und selbst, wenn dies nicht der Fall ist, einer selbst antrainierten statistikbasierten Wahrscheinlichkeitslogik folgen, deren einzelne Entscheidungen nicht begründbar nachvollzogen werden können. Daher sollten alle Systeme für ihre Schäden einstehen, auch wenn es für offene Systeme gewisse Erleichterungen geben könnte, um dieses Konzept zu fördern.;
F529982;11-06-2020 18:31;English;Other;Constant;KOHLER;;CEN and CENELEC, the European Standardization Organizations;;Large (250 or more);Belgium;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;CEN-CLC_AI_FG_White_Paper_Response_Final_Version_June_2020.pdf
F529981;11-06-2020 18:17;German;EU Citizen;hans;lauterwald;;;;;Germany;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;No opinion;3 - Neutral;4 - Important;;5 - Very important;5 - Very important;4 - Important;4 - Important;;;;5 - Very important;4 - Important;2 - Not important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;;There is a need for a new legislation;;No;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Not at all;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F529980;11-06-2020 18:16;English;Company/Business organisation;AURELIE;DOUTRIAUX;;ORANGE;76704342721-41;Large (250 or more);France;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;"- Creation of EU-funded lighthouse-projects to facilitate private development and implementation of AI. 
- Promote AI in  public procurement.
- Clear roadmap on funding instruments and priority sectors (AI application domains), including commitments by th";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"The EC should also consider the implementation and prioritize areas where it is most realistic to make impact, and achieve greatest value for money. 
A clearer division of roles and responsibilities between the EC and Member States should be explained.
Better funding and policy instruments for enabling startup ecosystems. 
More focus on AI scale-ups, and ways to enable their growth.
Intellectual Property for AI
European Open source environments
Fostering pilots / experimentations";5 - Very important;5 - Very important;5 - Very important;"- Intellectual Property: protecting Intellectual Property Rights in the field of AI seems difficult in practice
- European Open source environments (could be one important dissemination means for the lighthouse research centre)
- Develop communities aroun";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Develop public, open source datasets;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;Other;AI should not be subject to a new specific framework since existing laws already apply to AI systems. However existing regulations and frameworks should be adapted where needed.;Yes;;Other;The EC White Papers provides a pragmatic approach to AI focusing on the evolution of the notion of risk and proposing a framework for high risk AI-systems. The proposed definition for high risk AI is based on a double criterion and is realistic. The proposed list of high risk sectors is correct, however the periodical review suggested by the Commission may lead to legal uncertainty. It should be based on complementary criterion targeting the eventual impact on affected parties.;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;"The White Paper supports a soft regulatory approach; it suggests guidelines and a voluntary self-assessed “trustworthy AI” label for low risk AI systems. 
To preserve innovation, such a label should remain indeed on voluntary basis. However It is important not to blur the lines between high-risk and low risk applications, the voluntary labelling for non-high-risk AI applications should not become a de facto standard. ";A combination of ex-ante compliance and ex-post enforcement mechanisms;;The combination of such reviewed ex-ante and ex-post mechanism will ensure trustworthiness granting the technological neutrality of the proposed obligations.;Personal security risks;The connectivity feature is not directly linked to artificial intelligence and should not be a special focus in the directive.;Yes;Only for high risks AI;No;The current PLD remains a valid text, the scope of the PLD should be interpreted in a way  to cover integrated software when critical for the running of a product;Yes, for specific AI applications;Only for high risk AI systems.;New liabilities should be assigned to AI developers because they would be at the root of possible issues that would impact clients and users down the AI value chain. AI developers are in a better position to detect and to fix problems than AI distributors who are not in the details of the AI functioning.;Orange_position_paper_EC_consultation_AI_White_Paper.pdf
F529979;11-06-2020 18:05;French;Other;Rita;Kessler;;La revue Prescrire;982539711698-79;Medium (< 250 employees);France;The feedback can be published with your personal information;No opinion;No opinion;No opinion;No opinion;1 - Not important at all;No opinion;;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;;No opinion;No opinion;1 - Not important at all;;No opinion;No opinion;No opinion;1 - Not important at all;No opinion;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Les technologies de santé basées sur l’IA devraient reposer sur un encadrement spécifique, la transparence des données et de l’algorithme et une évaluation scientifique rigoureuse et indépendante des bénéfices/risques. Le manque d’informations sur la qualité des données, des évaluations cliniques et l’opacité de l'IA exposent les utilisateurs à des risques. La transparence des données et leur qualité est fondamentale pour permettre une évaluation rigoureuse et une surveillance adéquate.;There is a need for a new legislation;;No;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;;No opinion;Pour le secteur de la santé, quel serait l’objectif de santé publique et d’intérêt général d’un label non obligatoire ? Nous pensons que cela donnera lieu à des confusions, contribuera à des pratiques divergentes et ne sera pas rassurant pour les consommateurs et patients.  ;A combination of ex-ante compliance and ex-post enforcement mechanisms;;L’autorisation et la surveillance des technologies de santé basées sur l’IA devraient être calquées sur le système des médicaments. Nous sommes défavorables à la reproduction du système de certification des dispositifs médicaux à haut risque dont les faiblesses sont connues que ce soit pour l’évaluation des performances, de leur sécurité ou encore en matière de transparence et l’accès aux données. Le recours aux organismes notifiés engendre des conflits d’intérêt potentiels.;Personal security risks;;Yes;;Yes;La Directive sur la responsabilité du fait des produits ne convient pas pour les médicaments et dispositifs médicaux. Ce dispositif ne conviendra pas non plus pour les technologies de santé basées sur l’IA et les applications.;Yes, for specific AI applications;Pour les applications dans le secteur de la santé.;;FR_final_11_06__Prescrire_AI_Consultation_sur_le_Livre_blanc_.docx
F529978;11-06-2020 17:49;English;Other;Alex;Hubbard;;UK Information Commissioner's Office;;Large (250 or more);United Kingdom;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;20200610_-__ICO_Response_to_EU_Commission_White_Paper_on_AI_-_V_1.0.pdf
F529977;11-06-2020 17:37;English;Academic/Research Institution;Susan;Perry;;American University of Paris + Centre d’Etudes en Droits Humains et Technologies Numériques;;Medium (< 250 employees);France;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;Comments_on_EC_White_Paper_on_AI.pdf
F529976;11-06-2020 17:35;English;NGO (Non-governmental organisation);Pascal;GAREL;;European Hospital and Healthcare Federation;73872883198-91;Micro (< 10 employees);Belgium;The feedback can be published with your personal information;5 - Very important;4 - Important;5 - Very important;3 - Neutral;4 - Important;4 - Important;;5 - Very important;4 - Important;4 - Important;3 - Neutral;5 - Very important;4 - Important;;5 - Very important;5 - Very important;3 - Neutral;;3 - Neutral;4 - Important;3 - Neutral;3 - Neutral;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;Current legislation may have some gaps;;No;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;;Rather not;;Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F529975;11-06-2020 17:34;Spanish;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;4 - Important;4 - Important;4 - Important;4 - Important;3 - Neutral;3 - Neutral;;Current legislation may have some gaps;;Yes;;No opinion;;;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;Yes;;Yes, for all AI applications;;;
F529974;11-06-2020 17:26;English;Company/Business organisation;Andrew;Georgiou;;E.ON SE;72760517350-57;Large (250 or more);Germany;The feedback can be published with your personal information;4 - Important;5 - Very important;4 - Important;No opinion;4 - Important;4 - Important;;4 - Important;3 - Neutral;4 - Important;4 - Important;4 - Important;3 - Neutral;Enable a setup to allow data alliances and cooperation between companies;4 - Important;4 - Important;4 - Important;;;;;;;;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;4 - Important;;Other;Current legislation may have some gaps: If there are any gaps in the existing legislation these need to be adjusted, whereby over- or parallel regulation must be avoided.;Other;We welcome regulation of high-risk technologies more than harmless technologies. It is important to ensure that there is no over-regulation for those identified as high-risk. It must be clear and understandable for everyone when the classification is high-risk. The individual technologies must be understood and not assessed across the board. If a technology is seen as high risk in an application, regulation must be secure for all companies regardless of their size (i.e. municipal utilites). ;;;We are aware of our responsibility and will continue to use new technologies in critical areas (network control) only with extensive risk assessment. A blanket regulation risks sensible applications not being implemented and the possibility that AI in energy will not be exploited. If energy is seen as high-risk it is crucial that the criteria which relates to the application is clear and understandable. Only a clear definition can give necessary legal certainty over whether and when rules apply.;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;No opinion;;Rather not;In general voluntary labelling can be a way to build more trust in the technology. It is important, that requirements for labeling are clear and do not cause any additional costs. It must be ensured how the labeling process works and that every operator can volnuntary participate.;Other enforcement system;An enforcement system must offer clear demarcations. The focus should be on the applications themselves and the resprective use cases in the different sectors. Over- or double regulation with existing (national) guidelines must be avoided, responsibilities between the operators must be defined. ;The E.ON Code of Conduct is a prime example that summarizes the standards and principles everyone should adhere to and helps to ensure that technologies such as AI are handled trustworthy and secure.;Risks related to the loss of connectivity;;Yes;As algorithms age, the quality of the insights might decay, which requires periodic risk assessments ;Yes;Based on the white paper, it is difficult to assess exactly what should be regulated. An excessive product liability law must be avoided, but it is crucial that the respective responsibilities of the operators are clearly defined.;Yes, for specific AI applications;;;
F529973;11-06-2020 17:08;English;Company/Business organisation;Krisztina;BARACSI;;Telenor;74126393166-46;Large (250 or more);Norway;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"All the above are important factors to accelerate the development and uptake of AI in Europe. Distinguishing one particular factor will be suboptimal. What is critical is to consider those factors in tandem and invest accordingly, as the investment into one area (e.g., SMEs) without adequate investment into another domain (e.g., skills) will not accelerate growth of AI in Europe. 
See additional input in the uploaded document.";5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;3 - Neutral;5 - Very important;Investments into AI research and business capacity may be subject to Member State strategic AI focus (e.g., prioritized growth sectors), where coordination and joint investments at the EU level may be suboptimal. For example, in some Member States the volume of startups maybe relatively low, and the importance of large industries in leveraging innovations may be more important. The strategic importance of economic sectors where AI can accelerate growth also vary. More in the uploaded document.;3 - Neutral;4 - Important;5 - Very important;See our input in the uploaded document.;5 - Very important;No opinion;5 - Very important;5 - Very important;No opinion;See further input in the uploaded document.;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;We recognize that AI may pose concerns related to performance and trustworthiness. However, the extent of the concern depends on context – the use case, technology used, development and deployment protocols etc. Additionally, some concerns may already be address through existing laws and regulation. Therefore it is not possible to distinguish the importance of any one concern set out above. Our response above also does not distinguish between the relative importance of the concerns raised.   ;Other;See further input in the uploaded document.;Yes;;Other;See further input in the uploaded document.;;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;No opinion;Use of biometric identification systems is sufficiently regulated through GDPR. Guidance is needed on how to apply the regulation in this area . ;Rather not;"In our view the suggested possibility for AI solutions that are not ""high-risk"" to be subject to voluntary labelling is not helpful and risks becoming a de facto standard for market access. ";A combination of ex-ante compliance and ex-post enforcement mechanisms;;We believe some balance between ex ante and ex post enforcement mechanisms will be required. Where the exact demarcation lies must be subject to more thorough (gap) analysis as stated above and a more thorough definition of risk. ;Personal security risks;See further input in the uploaded document. ;Yes;See further input in the uploaded document. ;No;See further input in the uploaded document. ;No opinion;;See further input in the uploaded document.;AI_White_Paper_public_consultation_Telenor.pdf
F529972;11-06-2020 16:58;English;Other;Nathanael;Ackerman;;AI4Belgium;;Micro (< 10 employees);Belgium;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AI4Belgium_WhitePaper_Feedback_final.pdf
F529971;11-06-2020 16:56;English;Company/Business organisation;Winona;Bolislis;;Sanofi;61291462764-77;Large (250 or more);France;The feedback can be published with your personal information;4 - Important;4 - Important;5 - Very important;4 - Important;4 - Important;3 - Neutral;"identifying and removing obstacles to AI (e.g. existing regulation, overly burdensome approval or qualification processes) at least in low risk areas; expand actions to establish new enablers ";5 - Very important;4 - Important;2 - Not important;5 - Very important;5 - Very important;5 - Very important;Legal and regulatory harmonisation ;4 - Important;5 - Very important;4 - Important;"Fund pan-EU research projects with a view to also strengthen these networks and collaboration; De-regulate areas of low risk to enable experiments and foster entrepreneurship";4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;;4 - Important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;4 - Important;;Current legislation may have some gaps;;Yes;;Yes;;Autonomous decision making in applications where human intervention is not possible and errors would have potentially severe negative impact on humans (e.g. transport - accident avoidance, health - automated interventions, automated drug application, life support equipment, mass surveillance);5 - Very important;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;Guidelines and codes of conducts to give security and certainty both users and producers, about safety and good practice behaviour. In addition, rather than labelling the developer or deployer of AI, the label could be a linked to a product/standalone AI software. Otherwise, the label would risk to impose bureaucracy on processes where only part of the products may need to be produced to the label standards.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;Producers should not be allowed to waive away key risks in their terms of use. Adequate consumer protection should be ensured if an AI product is demonstrated to not perform or has not performed to the product claim made.;Yes;The burden of risk management and accountability should be put on the producer with adequate surveillance and sanction for misconduct in the market, which is sufficiently disuasive to deter malpractice. ;Yes;To ensure a better consumer protection and legal certainty for businesses, changes should be adapted to cover all AI impacts in real life, and that the interconnected applications or devices - developed or applied - should be considered.;Yes, for all AI applications;;Taking into account the multiple providers participating in the AI end-to-end processes, laws are not covering most damages and liabilities that AI can produce, or if any, there are challenges in relation to accessing the burden of proof. It is important that companies know their liability risks throughout the value chain in order to reduce or prevent them and insure themselves effectively against these risks, under mandatory compliance rules.;Sanofi_comments_to_EC_Whitepaper_on_AI_-_June_11.docx
F529970;11-06-2020 16:54;English;NGO (Non-governmental organisation);Fanny;Hidvegi;;Access Now Europe;241832823598-19;Micro (< 10 employees);Belgium;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;1 - Not important at all;1 - Not important at all;"Ecosystem of excellence must include trust. The development and deployment of AI must respect human rights. EU funded research must follow HLEG Ethics Guidelines
Encourage AI uptake in public sector only where evidence of benefit exists and safeguards prevent added risks
We call for additional safeguards for fundamental rights in the lifecycle of public procurement processes
Ensure democratic oversight, include civil society and impacted communities in meaningful consultation and decisions";4 - Important;No opinion;1 - Not important at all;2 - Not important;4 - Important;2 - Not important;Promotion of AI uptake is not a value in itself. The coordinated plan should include common scientific and policy criteria to determine the allocation of resources for the above listed purposes, rather than assume benefits in areas such as health and transport.The coordinated plan and MS strategies should include a section on human rights, societal impacts of AI and automation, inclusion and democratic oversight. The build-up of a European data space must comply with protections of personal data;3 - Neutral;4 - Important;1 - Not important at all;Public interest should set the priorities of research centers & research partnerships.Research priorities should include human rights & societal implications of the development & use of AI, fairness design, and discrimination risks & transparency.Receiving public funding for research should require fulfilling a set of criteria following the EU Ethics Guidelines. Review Horizon2020 program to ensure fundamental rights and public interest in both the funding process and funded projects;1 - Not important at all;2 - Not important;3 - Neutral;3 - Neutral;No opinion;"DIHs and other innovation incentives for startups and SMEs must not allow for exceptions from fundamental rights. The Clearview AI example shows that SMEs can also cause harms & violations. 
There should be no blanket exemptions in sandboxing for innovation. The EU should develop a scheme where public funding of AI applications returns to the public, e.g. by enhancing transparency, limiting tech-sector-funded research, making outcomes publicly available & publishing under Free Software licenses";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;Yes. The White Paper (and the consultation) portray these as potential risks while it takes the benefits for granted without evidence. It ignores that these “concerns” disproportionately impact disenfranchised people and communities. It puts the burden of proof on those who suffer the violation or negative impact. Finally, it assigns the harms being caused to the technology and not the people, institutions and policies around them. We focus on specific issues in our additional submission.;Other;"Strengthen enforcement of human rights & GDPR. New legislation complement broad interpretation of GDPR incl. affinity profiling, sensitive inferences & collective impact of data processing
Legislation must enforce transparency requirements for public-private partnerships, address impact of processing non-personal data, issues for meaningful consent, objection, data minimisation, purpose limitation, explanation
Public tenders for AI systems must evaluate performance against non-AI approaches";Other;"We call for rights-based AI
High-low risk distinction misses cumulative & distributive harms of low risk systems & unpredictable harms may arise after deployment. 
The EU should proactively stop or ban applications in areas where mitigating potential risk or violations is not enough and no remedy or other safeguarding mechanism could fix the problem
Mandatory human rights impacts assessments 
EU legislation to mandate that member states establish public registers of AI systems";;;"Some applications violate rights to the extent that they must be banned, including but not limited to AI systems that:
result in mass surveillance eg live biometric recognition systems in public spaces or on wearable devices
make behavioral predictions with significant effect on people based on past behavior, group membership, or other characteristics
are based on flawed scientific premises, eg. inferring emotion from facial analysis
determine delivery of essential public services";5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;Biometric identification systems should never be allowed in publicly accessible spaces;Biometric identification systems should never be deployed in publicly accessible spaces, whether by police, private companies or by individuals using personal devices or wearable tech like AR glasses. Regardless of the aim of such deployments or their technical specificities, they result in indiscriminate mass surveillance and thus violate fundamental rights to privacy, freedom of assembly, expression, non-discrimination, data protection, fair trials, democracy and the presumption of innocence.;Rather not;Voluntary self assessment is an inadequate mechanism for any form of legal compliance regardless of its high or low risk nature. These approaches to AI governance, such as ethics guidelines, can easily become mere box-ticking exercises & have no power to mitigate harms. Instead, human rights impact assessments must be performed to help developers and deployers understand potential risks & harms. ;Other enforcement system;To ensure that AI in the EU is trustworthy, the EU must draw clear red lines and ban certain use cases which inherently violate fundamental rights, such as biometric recognition that enables mass surveillance, & enforce high scientific standards for all applications. Regulation must be strictly & consistently enforced by well-equipped authorities. Risk assessments and prior & ex-ante human rights impact assessments must be mandated & made publicly accessible & contestable for all applications; If enforcement is not consistent, citizens will not see European AI as trustworthy.;Mental health risks;The integration of AI-based biometric analysis software into consumer devices, such as smartphones, and wearable tech, such as augmented reality glasses, poses significant risk to both users and non-users of such devices. There are also heightened risks of discrimination, in particular with reference to online products and services using data for targeted advertising both in the commercial and political context.;Yes;Risk assessments of AI systems should be accessible to the public after completion and procedures should be put in place to allow contestation of the assessments. They require independent audit and oversight and involvement of those affected, human rights experts and civil society. Self-assessment is insufficient to ensure human rights.;Yes;Liability should be aligned with failures around accountability measures such as transparency and explainability requirements, human rights impact assessments etc. ;Yes, for all AI applications;;"Legislation should incentive risk reduction by those actors who stand to benefit from the deployment of AI systems. Moreover, it should facilitate contestation by those affected by such systems by providing adequate information about the existence, operation and ex-ante assessment of such systems.
";EU_AI_white_paper_consultation_AccessNow_June2020_final.pdf
F529969;11-06-2020 16:41;English;EU Citizen;Bernd;Brincken;;;;;Germany;The feedback can be published with your personal information;2 - Not important;4 - Important;3 - Neutral;2 - Not important;2 - Not important;1 - Not important at all;"The EU should identify a business model for strong AI.
Weak AI (signal processing) works fine as a technical tool and does not need explicit subsidies from politics.";4 - Important;2 - Not important;1 - Not important at all;2 - Not important;3 - Neutral;1 - Not important at all;The EU should identify technological alternatives to AI, with more potential to become a new business, or play a role in the societies.;2 - Not important;2 - Not important;2 - Not important;A lighthouse can only attract the best minds for a technology that is capable to deliver on its promises. AI has failed in this regard several times already since the Dartmouth conference. ;2 - Not important;3 - Neutral;2 - Not important;3 - Neutral;2 - Not important;;2 - Not important;3 - Neutral;3 - Neutral;4 - Important;2 - Not important;3 - Neutral;"Strong AI will not endager anything because it will not work.
Signal processing with neural networks - weak AI - works fine and has to be used responsibly, like any tool, for example a knife. ";Current legislation is fully sufficient;;No opinion;;;;Strong AI does not work, the only real political risk is that 'AI' is used as a placeholder word for 'Big data', the indiscriminate collection of personal data, not only in violation of laws but also without long-term business models. ;3 - Neutral;4 - Important;4 - Important;3 - Neutral;3 - Neutral;4 - Important;Biometric identification systems should never be allowed in publicly accessible spaces;Biometric identification is not neccessary for the EU societies, other than in the current applications, like border control. ;No opinion;The concept of voluntary labeling is basically valid and may be applied to various fields, like consumer protection or nutrition. Strong AI does not work, so it is not needed there. The collection of data is already well regulated under data protection law.;No opinion;;;;;No opinion;;No;Strong AI is still purely speculative, so any question about legislation may be postponed until it really appears. Weak AI is typically a (software) technique as part of a technical product - current legislation should suffice. ;No;;;KD_Fly_2b.pdf
F529968;11-06-2020 16:13;English;Other;Nathanael;Ackerman;;AI4Belgium;;Micro (< 10 employees);Belgium;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AI4Belgium_WhitePaper_Feedback_final.pdf
F529967;11-06-2020 16:10;English;NGO (Non-governmental organisation);Hendrik;IKE;;GÉANT;435792917444-12;Medium (< 250 employees);Netherlands;The feedback can be published with your personal information;5 - Very important;4 - Important;5 - Very important;3 - Neutral;4 - Important;4 - Important;Partnerships with the private sector are important for the research and education community also. The focus should not be constrained to SMEs.  ;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;;2 - Not important;5 - Very important;3 - Neutral;Challenge becomes that these centres in effect act as commercial companies in that industrial sector and drive out economic growth and innovation. These in effect become expensive playgrounds for the latest technology. The focus should be on ensuring that these facilities act as non-commercial, not for profit entities. There should not be a super site for all the industries, but rather a specialisation centre for a particular industry based around existing clusters. ;3 - Neutral;4 - Important;4 - Important;5 - Very important;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;4 - Important;The classification or publication of the metadata on the AI data and documentation, should be published by the AI developers which could form part of the accreditation service. This would allow for automatic classification and verification of the AI systems which have been developed by AI developers and would allow greater transparency.  i.e automating the process.;Current legislation may have some gaps;;Yes;;Yes;;AI applications around the theme of Security or Defence-specific domains. AI applications regarding health and personally identifiable information. AI applications where human life and human rights are at risk.  ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);There should be a classification system for the training data with people authorised to classify and accredit the training data. This would also open a new market for AI services. The continuous manner or ongoing manner should be the driving force behind this, not the distance or public space argument. These factors serve to distract from the primary focus of the differentiation between the two types of technologies being talked about. For example biometric facial recognition can used to identify multiple people, in a public place, at a distance. For example, unlocking a phone (ok for one person), providing access control for multiple people to a locked room (preventing a door being opened if there are un-identified people in the group).;No opinion;If the AI system is not high-risk then there can misinterpretation of how credible the system is. ;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Offering the ability of human override so they can operate the system without AI. The mandatory logging of the switches from AI to human control.;Mental health risks;Please refer to the 'high-risk' AI application areas we have outlined above. ;Yes;Pre-emptively, it is advisable to build an AI system to test the and validate the datasets. Then, a checksum or block chain certificate can be used on the dataset. When the dataset changes, the blockchain cert becomes invalid (as it would be published through metadata) and the testing algorithm can be re-run automatically and issue a new cert. (Mitigation measure). ;Yes;;No opinion;;;
F529966;11-06-2020 16:05;English;Trade Union;Franca;SALIS MADINIER;;Union confédérale des cadres et des ingénieurs is a French trade union organisation which has 90000 members among professional and managers in all industrial sectors and public administration.;423075137009-82;Small (< 50 employees);France;The feedback can be published with your personal information;4 - Important;4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;AI and robotics significantly impact the labour market and the way of working, not only because older jobs and tasks transform or disappear, and new ones emerge but also because of change on the nature of human work in relation to AI systems. We need a deeper involvement of employees at workplaces especially those who design, planify, develop, purchase and use AI systems. If workers are to accept AI systems these systems are to be conform to ethical and social guidelines.;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;For data collection and management, we need clear rules and governance mechanisms. Fundamental rights, must be respected by the development and use of AI systems and as such design and implementation should first of all respect the privacy rights of employees. Social partner negotiations regarding data collection are key for implementing AI at the workplace.Concerning the use of any personal data European regulation should require informed consent and greater protections.  ;5 - Very important;5 - Very important;4 - Important;A key aspect to be included in the R&I, is the involvement of European and national social partners and sectoral trade unions, as they bring expertise and experience of situations of real workplace exposure.The lighthouse structure for innovation needs to have a space for trade unions.The role of EU could encourage the development of European science reviews which are mostly currently run by USA. EU could support the creation of an open source platform to facilitate companies' digitization. ;4 - Important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;"Digital Innovation Hubs need to be equipped to (a) give support to carry out risk assessment and managing data protection to the different SMEs across Europe. It is key to upgrade their capacity in these two issues that are impactful for work and employment; (b) to allocate trade unions equal access and participation to shape and monitor AI technologies at work and to take part to related employment discussions with the related national authorities.";5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;As the process of AI and apps is strongly dynamic, an open and evolving list of AI applications or use considered as intrinsic high-risk, should be drawn up.  In order to avoid  discrimination, exclusion, inequality, high risk applications at workplace should not generally be developed and their use should be submitted to a process of social dialogue with workers union representatives.;There is a need for a new legislation;;Other;"An open and evolving list of AI applications or use considered as intrinsic high-risk, should be drawn up.  

";;;HIRE VUE - used by some 600 multinational companies for recruiting on the basis of video interviews analyzing candidates’ facial expressions,  CallMiner – an AI application used for managing contact centres, ISAAK used to monitor workers in real time and to dismiss automatically low productive workers. They can create distrust, fear, stress and low productivity. Such surveillance systems can be introduced only after negotiation with the workforce union representatives.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;"The White Paper on AI should have had a reference to a ban on facial recognition in public spaces, at least ""for up to five years until safeguards to mitigate the technology's risks are in place"". The lack of such a ban is inacceptable in the face of the rash development of AI with little to no public control and no legally binding rules on ethics and references to human and fundamental rights instruments. Such ban should be also extended and applied to workplaces. The moratorium should be reconsidered, as facial recognition and other remote identification systems are intrusive technologies that can be used in multiple harmful and disruptive ways. The impacts of AI technology and its possible risks should be assessed. GDPR states that processing biometric data for the cause of identifying individuals is prohibited, except for specific circumstances. We should identify and limit these circumstances and the use of this technology must be pertinent and proportionate to the finality. Yet, there are still unsolved dilemmas about their implementation in policing and enforcement. One of the most probable risks for society is that facial recognition creates mass surveillance across the world, incompatible with human rights and democratic principles. It will raise inequalities and discriminations exponentially and exacerbate biases. Facial recognition should remain exceptional and reduced to clearly specific circumstances fixed in law. Any aspect of AI collection and processing of personal data should be based on sound, public and democratic rules, taken in cooperation with legitimate social partners and national democratic bodies. ";Rather not;"Voluntary labelling systems are problematic as they are granted by private organisations/companies with little to no public control, and becomes a profitable business that does not provide for independence, quality and trust. They also rely on voluntary will for implementation and compliance; are driven by marketing instead of by safety and quality; and such systems lack official and public evaluation and verification schemes.";A combination of ex-ante compliance and ex-post enforcement mechanisms;;"At the workplace the ex-ante assessment should be done through a well informed social dialogue by which full transparency concerning the use, the deployment and the impact on workers conditions of the AI system is assured.
Tech workers should have the right to know what they are building and to contest unethical or harmful uses of their work.Over the last two years, organized tech workers and whistleblowers have emerged as a powerful force for AI accountability, exposing secretive contracts... ";Mental health risks;The AI related risks are still highly unknown and they can emerge in many circumstances and be completely new. More legal certainty is needed to address new risks like the “deepfakes”, risks related to self-learning applications, bias and discrimination. A clear legal framework that specifies the responsibility of natural or legal person who developed and used AI (software publishers and developers, employer using AI, managers and workers using AI) is needed.Companies should remain responsible  ;Yes;The process of AI and its use are dynamic, so continous evaluation should take place. This evaluation would allow to verify that the new data from the workers and the modification of prediction algorithm does not lead to discriminatory or unfair biases for them. Assessments should include risks related to human decision-making, social discrimination, and impact on working conditions and any infringement and violation of human fundamental rights.;Yes;Priority must be given to defining clear rules attributing liability to natural or legal persons, in the event of failure to comply with these rules.  A business/employer that uses a technology with a certain degree of autonomy, should remain fully liable for any harm that results from using the technology. Manufacturers should make sure that the AI application works safely before it is applied. Amending the EU liability framework, trade unions need to be properly consulted and involved.;Yes, for all AI applications;;National legal regimes might require adaptation as they provide different liability considerations to the supply of services and to the supply of products. It is necessary to establish clear European rules attributing liability to natural or legal persons, in the event of failure to comply with EU ethical rules and guidelines. The scope of potential liability of designers, hardware manufacturers, operators, network service providers should be established. ;CONSULTATION_LIVRE_BLANC_EUROPEEN_29_Mai_consolid__avec_groupe_IA_CFDT_Cadres.docx
F529965;11-06-2020 16:01;German;NGO (Non-governmental organisation);Isolde;Fastner;;Deutsche Sozialversicherung Europavertretung;917.393.784-31;Micro (< 10 employees);Germany;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;2020-06-11_DSV_Position_Weissbuch_Endfassung.pdf
F529964;11-06-2020 15:56;English;Company/Business organisation;Jana;WALTER;;Bertelsmann SE & Co KGaA;26103486608-47;Large (250 or more);Germany;The feedback can be published with your personal information;4 - Important;4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;In view of the fact that global digital platforms are, by far, the biggest collectors and processors of data in the digital economy, we need a more balanced regulatory framework that recognizes the contribution of independent players and media companies to the data ecosystem, ensures fair access to data and opens opportunities to use AI and develop AI applications. In addition, better access to publicly available data and API-connectivity of such data would foster innovation.;5 - Very important;3 - Neutral;5 - Very important;2 - Not important;5 - Very important;5 - Very important;AI and ML require an agile, evolutionary approach and low business-entry barriers for scaling up. A prerequisite for creative and innovative business models based on AI is access to data and data quality.;4 - Important;5 - Very important;4 - Important;;4 - Important;3 - Neutral;3 - Neutral;4 - Important;3 - Neutral;;No opinion;5 - Very important;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;Copyright protected media and journalistic content is increasingly used – often for unauthorized downstream re-use/re-purposing – by services based on AI. EU rules should ensure that intellectual property rights are enforced. Strong editorial responsibility must also be maintained to ensure high trust in media.;Current legislation may have some gaps;;Yes;;Other;We agree with the approach in general but think that the “impact on the affected parties” should not be the only criteria which has to be taken into account in the second level assessment. ;"Most concerning from our perspective is, when AI applications:
•	use proprietary data (such as media or journalistic content) to create services or works without a suitable compensation 
•	help setting up illegal services, e.g. allowing widespread dissemination of disturbing or pirated content 
•	automatically generate opinion building content without human supervision
•	are used to disinform or mislead (e.g. with deep fakes) the general public.";2 - Not important;4 - Important;4 - Important;3 - Neutral;4 - Important;3 - Neutral;No opinion;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;Risks resulting from disinformation and fake news.;Yes;Carrying out a new risk assessment should only be required when there has been a significant change to the product which is likely to materially change the original risk assessment e.g. in case of introducing significant new risks to affected persons.;No opinion;;No;;The current German liability rules are sufficient but we would welcome a harmonized framework for liability rules for the EU.;
F529963;11-06-2020 15:49;English;NGO (Non-governmental organisation);Manon;Tabaczynsky;;Allied for Startups - a global network of startup associations;634665118544-37;Micro (< 10 employees);Belgium;The feedback can be published with your personal information;5 - Very important;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;The European Commission should create an open dialogue with startup ecosystems, which are the first innovators in AI applications. When assessing legislation around AI, the EU should design understandable and implementable laws for startup entrepreneurs.;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;"The European Union should build close bridges with EU startups. 
The EU can support startups by providing the right regulatory framework for AI development and deployment. An emphasis on skills/talent and world-class testing facilities is addressing two key bottlenecks for startups.
An idea would be to create an innovation ecosystems dialogue in which the Commission and startups can discuss the opportunities and challenges of developing AI in the EU. ";4 - Important;4 - Important;3 - Neutral;The European Commission should proceed to an AI refit of all its existing legislations in relation to new technologies. Past and new legislation should not hamper the development and deployment of AI in Europe. ;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;In the case of new digital legislations, sandboxes should be considered as a useful tool to develop innovation, where entrepreneurs and regulators can work hand in hand. Working directly with entrepreneurs will help the European Commission to collect important insights on AI before deciding if regulatory action is needed. It will also help to strike the right balance between facilitating AI development and deployment and mitigating the potential risks of AI. ;3 - Neutral;4 - Important;3 - Neutral;3 - Neutral;3 - Neutral;2 - Not important; As any new technology, AI poses new challenges, but it also has a tremendous potential to uplift EU citizens' lives. AI applications are already benefiting European citizens (e.g. Corti.ai), their development and deployment should be incentivised. If an AI application harms people, there should be a clear liability regime.;Current legislation may have some gaps;;Other;There should be a clear distinction between high and low risks applications. High risks should only be for AI applications that can harm physically or mentally users and if there is no human oversight on these applications. ;;;;2 - Not important;2 - Not important;4 - Important;3 - Neutral;4 - Important;5 - Very important;No opinion;;Much;It should remain voluntary and not mandatory. It should be done at a later stage, once the product has already entered a market, and there should be sufficient flexibility to update this (as startup founders frequently iterate their product). Otherwise, it will be complicated for entrepreneurs to innovate.;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;Ex-post measures generally give more flexibility to design and launch a product. A startup entrepreneur does not have the means nor the time to do an ex-ante assessment with every product iteration or to wait for weeks before its product/service is approved before putting it in a test market. This approach could be coupled with sharing best-practices and creating incentives to self-assess compliance ex-ante. ;Cyber risks;;No opinion;This also pertains to civil liability aspects of AI which is currently under discussion in our network.;No opinion;;No opinion;;Startup founders are looking for legal clarity, ideally based on one set of rules with one clear jurisdiction. ;AFS_contribution_AI_consultation.pdf
F529962;11-06-2020 15:19;English;Other;endy;mion;;Humankind;;Large (250 or more);Netherlands;The feedback can be published with your personal information;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;"If governments use AI for public functions (like healthcare, housing, education, social welfare,
transport), there must be:

1. clear, published reasons to justify the use of AI
2. scientific evidence that the technology works, and,
3. particularly where the technology will play an important role in determining people’s
access to vital services or to enjoy their fundamental rights and freedoms, people
should have a say in whether or not AI can be acceptably used in a democratic society.";1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;We propose that the coordinated plan on AI is updated to include criteria (scientific and policy) about how the EU will allocate its resources of AI. The plan should include a section on human rights, societal impacts of AI and automation, and how to ensure democratic oversight for the application of AI systems.;3 - Neutral;4 - Important;1 - Not important at all;"The Commission highlights research and innovation as a priority of its strategy, however there
is little mention of research into human rights and the societal impact of AI. We have major
concerns about potential discrimination, disinformation, a lack of transparency – all of these
issues require further research, and to consider which uses of AI are impermissible? Where
should we draw the lines?";1 - Not important at all;5 - Very important;5 - Very important;1 - Not important at all;3 - Neutral;SME's should not enjoy any exemptions to protecting human rights. Some of the biggest AI-related scandals have involved small companies, and so the EU should ensure that when small businesses to take up AI, whatever the circumstances, they should respect data protection, privacy and other fundamental rights. There should be no blanket exemptions to fundamental rights standards.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;Deployment AI in sensitive areas.. Increasing use of opaque, privately-developed technology in the public sphere.. Conscious avoidance of liability for harms produced by AI technology.. AI posing collective or societal-level harms which don’t have remedies in anti-discrimination of data protection frameworks.. Companies/governments using excuse of ‘innovation’ to justify trials without safeguards. The characteristics of machine learning can lead to unauthorized use/ purpose and function creep.;Other;AI regulation should not provide loop-holes to data protection legislation, or other frameworks. Current law does not address use of non-personal data for AI, types of data which do not fall under the GDPR AI can have huge collective impacts, such as furthering overpolicing, surveillance, inequalities – all not addressed in existing legal frameworks but are still major issues AI can lead to discrimination on financial status and other grounds which are usually not protected in discrimination.;Other;"1. New rules should clearly outline criteria to determine which AI systems are legal
and which are not. Such criteria should be based on proving that they work and are
needed, conducting mandatory fundamental rights impact assessment for all applications, and ensuring democratic oversight.
2. Uses of AI which breach fundamental rights - like biometrics/ facial recognition for
mass surveillance - should be banned outright.";;;"These uses of AI are incompatible with human rights and should be banned:
- The use of AI to determine delivery of essential public services,
- predictive policing
- autonomous lethal weapons
- identification/ analysis of emotion and identity traits,
- and indiscriminate biometric surveillance.

Determining ‘risk’ should be rights and outcomes focused, not according to the sector. ";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;"Biometric identification is unlawful and incompatible with human rights law.
- Their use in public spaces will lead to mass surveillance;
- This will irreversibly limit our fundamental rights to privacy, freedom of assembly, expression, non-discrimination, data protection, dignity and the right to a fair trial, creating societies of suspicion; and,
- Even uses which do not contribute directly or indirectly to mass surveillance in public spaces still pose significant threats to privacy, data protection, non-discrimination, and dignity.";Rather not;"Self-labelling systems can be confusing for people and may give a false sense of security since it is the same company that develops a product the one saying that it is safe.

We believe that the high/low risk distinction is overly simplistic and could very well
allow for loop-holes for systems with potentially very significant impacts on peoples’
safety and rights.

In addition, there is a concern about how those harmed by these low-risk systems
might seek redress in such a scenario.";Other enforcement system;All systems should undergo a mandatory ex ante human rights impact assessment from an external body. Make human rights a priority in the AI regulation, and ensure that there are no loop-holes just because a system falls into a low-risk category.;"Compliance should be external. We need this to guarantee fundamental rights are protected, we cannot rely on self-regulation for this.
";Mental health risks;Just to make it really clear, we highlight here again the potential risks of discrimination posed by AI systems. In particular, the use of AI in online products and services requires collection and use of data lending toward discrimination in many fields related to targeted advertising. This poses risks of differentiated pricing, discrimination and financial detriments, the risk of creating filter bubbles, interfences in the political process, all based on sensitive inferences or associations.;Yes;"Internal supervisors, such as Data Protection Officers under GDPR should be
included and asked for advice.
";Yes;We think that AI developers and deployers should be accountable for harm generated by their products, and that products developed using AI should not enjoy exceptions to any EU laws, whether it be discrimination, data protection, or product liability.;Yes, for all AI applications;;"EU should address copyright (which is an absurd notion) and database protections which prevent proper oversight of AI applications. Liability rules should provide incentives
for openness.";
F529961;11-06-2020 14:59;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;1 - Not important at all;5 - Very important;;1 - Not important at all;1 - Not important at all;The Commission should include stringent human rights safeguards, democratic oversight of AI in the public sector and consultations with civil society and affected communities. There must be clear published reasons to justify the use of AI, scientific evidence that AI works and where AI will play an important role in determining people’s access to vital services or to enjoy their fundamental rights, people should have a say in whether or not AI can be acceptably used in a democratic society.;5 - Very important;4 - Important;1 - Not important at all;2 - Not important;5 - Very important;1 - Not important at all;The coordinated plan on AI should include a section on human rights, societal impacts of AI and automation, and how to ensure democratic oversight for the application of AI systems (planing, development, implementation including the learning of the AI algorithms, testing, deployment, use and use of acquired data). The plan should be updated to include scientific and policy criteria about how the EU will allocate its resources of AI. ;2 - Not important;4 - Important;1 - Not important at all;Funding for EU projects on AI should be conditional on meeting the EU’s own ethical standards for AI (discrimination, disinformation, transparency) and fundamental rights laws. EU funds, e.g. Horizon2020, should comply and immediately stop funding for projects which pose a risk to fundamental rights, are not substantiated by scientific evidence or significantly infringes upon human dignity. Projects which could facilitate mass surveillance should be ceased.;1 - Not important at all;4 - Important;4 - Important;1 - Not important at all;1 - Not important at all;SME should not enjoy any exemptions to protecting human rights. Some of the biggest AI-related scandals have involved small companies e.g. Clearview AI, and so the EU should ensure that when small businesses to take up AI, whatever the circumstances, they should respect data protection, privacy and other fundamental rights. There should be no blanket exemptions to fundamental rights standards.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"The characteristics of machine learning can lead to unauthorised use and function creep. AI leads to increasing use of opaque, privately-developed technology in the public sphere , which do not meet transparency requirements (when they exist). The conscious avoidance of liability for harms produced by AI technology. AI posing collective or societal-level harms which don’t have remedies in anti-discrimination of data protection frameworks often because these frameworks focus on the
individual.";Other;AI regulation should not provide loop-holes to data protection legislation, or other frameworks, like discrimination law. Current law does not address use of non-personal data for AI, data types covered by the GDPR. AI can have huge collective impacts, such as furthering overpolicing, surveillance, inequalities, all not addressed in existing legal frameworks but are major issues. AI can lead to discrimination on financial status and other grounds which are not protected in discrimination law.;Other;This approach is over-simplified and narrow. It also skips the important step defining the red lines and the legal boundaries within which AI should operate. New rules should clearly outline criteria to determine which AI systems are legal and which are not. Such criteria should be based on proving that they work and needed, conducting mandatory fundamental rights impact assessment, and ensuring democratic oversight. Uses of AI which breach fundamental rights should be banned outright.;;;The use of AI to determine delivery of essential public services, for predictive policing, for autonomous lethal weapons, for identification and analysis of emotion and identity traits and for indiscriminate surveillance are incompatible with human rights and should be banned. Systems which impact fair trial, in migration control and policing and which may perpetuate inequalities in hiring are a major concern. Determining ‘risk’ should be rights and outcomes focused, not according to the sector;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;"The use of biometric identification systems in public spaces will lead to mass surveillance. This will irreversibly limit our fundamental rights to privacy, freedom of assembly, expression, non-discrimination, data protection, dignity and the right to a fair trial, creating societies of suspicion; and even uses which do not contribute directly or indirectly to mass surveillance in public spaces still pose significant threats to privacy, data protection, non-discrimination, and dignity.";Not at all;Self-labelling systems may give a false sense of security because  the company that develops a product is saying that it is safe. The high/low risk distinction could allow for loop-holes for systems with potentially very significant impacts on peoples safety and rights. This is especially so if ‘low risk’ systems are only voluntarily controlled, essentially letting big tech companies regulate themselves. There is also a concern about how those harmed by these low-risk systems might seek redress.;Other enforcement system;Mandatory ex ante human rights impact assessment from an external independent body initiated by the EU commission and paid for from a pool managed by the EU commission and  funded by all companies offering AI.;The assessment of compliance should be external to guarantee fundamental rights are protected, self-regulation is not sufficient for this. The assessment should be initiated by the EU commission and paid for from a pool managed by the EU commission and  funded by all companies offering AI.;Mental health risks;The use of AI in online products and services requires collection and use of data lending toward discrimination in many fields related to targeted advertising. This poses risks of differentiated pricing, discrimination and financial detriments, the risk of creating filter bubbles, interference in the political process, all based on sensitive inferences or associations. In certain cases AI may impact on accessibility and other rights of persons with disabilities.;Yes;Involvement of internal supervisors, such as Data Protection Officers under GDPR should be mandatory in any risk assessment procedure.;Yes;AI developers and deployers should be accountable for harm generated by their products. Products developed using AI should not enjoy exceptions to any EU laws, whether it be discrimination, data protection, or product liability.;Yes, for all AI applications;;The European Commission should address copyright and database protection laws which prevent proper oversight of AI applications. Liability rules should provide incentives for openness.;
F529960;11-06-2020 14:59;English;NGO (Non-governmental organisation);Krzysztof;IZDEBSKI;;ePa?stwo Foundation;020222326905-75;Small (< 50 employees);Poland;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;2 - Not important;;5 - Very important;3 - Neutral;4 - Important;3 - Neutral;3 - Neutral;3 - Neutral;;No opinion;No opinion;4 - Important;;No opinion;No opinion;No opinion;5 - Very important;No opinion;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Introducing AI into public sphere (Automatic Decision Making in citizen-governments relations) means less accountability and transparency connected with eliminating/limitating the presense of human beings from the process. Turning into ADM processes instead of human intervention can bring dispersal of responsibility and lack of accountability.;There is a need for a new legislation;;No;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Rather not;"In our opinion there is no possibility to implement such scheme in effective manner. The actual control over ""voluntary labelling"" would be, in the first place, based on self assessment of companies and possibilities to control the ""trustworthness"" would be limited. ";A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Risks related to the loss of connectivity;;No opinion;;No opinion;;Yes, for specific AI applications;The system of remedies, proper compensation for damage and fair allocation of liability should be implemented to cover consequences of malfunction on any Automated Decision Making processes within public sector. ;;
F529959;11-06-2020 14:36;German;Business Association;Fabian;FECHNER;;Handelsverband Deutschland (HDE);31200871765-41;Small (< 50 employees);Germany;The feedback can be published with your personal information;4 - Important;4 - Important;5 - Very important;5 - Very important;;3 - Neutral;Um KI zu nutzen und weiterzuentwickeln, muss Datenökonomie in der EU gelebt werden. Datenschutz ist ein hohes Gut, was es für Verbraucher und Händler der EU zu bewahren gilt. Dieser EU-Datenschutz muss in sich größtmöglich kohärent sein und eine Verarbeitung auf der Basis eines berechtigten Interesses zulassen. Bereits heute beruht große Marktmacht auf Datenmacht, denn Daten sind der Rohstoff digitaler Geschäftsmodelle. Im globalen Wettbewerb darf Datenschutz nicht zum Wettbewerbsnachteil werden;4 - Important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;Während wir den Aufbau eines EU-Datenraumes als Voraussetzung für das Gelingen der KI-Politik ausdrücklich begrüßen (siehe obige Antwort), möchten wir anmerken: Eine grundsätzliche Verpflichtung, Daten zu teilen, umfasst auch aus eigenen Datenbeständen neu generierte Daten. Dieses Know-How sollte geschützt und nicht offengelegt werden, um Innovation in der EU zu fördern. Wir sehen daher primär einen Bedarf zum Austausch von Daten, die von öffentlichen Einrichtungen zur Verfügung gestellt werden.;4 - Important;5 - Very important;4 - Important;;5 - Very important;4 - Important;5 - Very important;4 - Important;4 - Important;Es muss gewährleistet sein, dass KMU in der Breite extern generierte „KI as a service“ auch praktisch nutzen können, da sie vielfach nicht in der Lage sein werden, KI-Systeme selbst zu entwickeln. ;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;"KI ist keine weitere technische Entwicklung, die einer Sonderregulierung bedarf. Es gibt bereits Rechtsvorschriften, die diesen Bedenken entgegenwirken und Verbraucher hinreichend schützt, wie z.B. die DSGVO, das UWG oder AGG. Existierenden Vorschriften sollten überprüft und bei nachgewiesenem Bedarf gezielt an die von KI-Systemen ausgelöste Entwicklung angepasst werden. Sollte man sich dennoch für gesonderte Regulierung entscheiden, so muss diese anwendungsbezogen und risikobasiert sein.   
";Current legislation is fully sufficient;;Other;"Etwaige Regulierung sollte anwendungsbasiert und nicht technikbasiert sein. Sollte man sich für eine risikoorientierte Regulierung entscheiden, ist es absolut notwendig, dass „KI-Anwendungen mit hohem Risiko“ klar, zukunftsfest und rechtssicher definiert und abgegrenzt werden. 

Grundsätzlich halten wir den Ansatz, sich bei der Risikobewertung vor allem auf den Sektor zu konzentrieren für richtig, allerdings wirft die vorgeschlagene Risikobewertung Fragen und Probleme bei der Abgrenzung auf.";;;Verarbeitung von sensiblen Gesundheits- und Versicherungsdaten, Gefährdung von Leib und Leben ;2 - Not important;3 - Neutral;2 - Not important;4 - Important;2 - Not important;3 - Neutral;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);Bestimmte biometrische Identifikationssysteme tragen zu maßgeblichen Innovationen im Handel bei, wie z.B. das Bezahlen per Fingerabdruck. Diese sollten im Sinne der Innovationsförderung sowie der Erleichterung von Prozessen für den Kunden und Anbieter unter bestimmten Bedingungen weiterhin möglich sein. ;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;Es muss klar und präzise definiert sein, was unter einer „erheblichen Änderung“ zu verstehen ist. ;No;Sollte für bestimmte KI-Produkte mit hohem Risiko eine Gefährdungshaftung (in Kombination mit einer Pflichtversicherung) eingeführt werden sollte a) sichergestellt werden, dass es solche Versicherungen auch gibt und b) klar und zukunftsfest abgegrenzt werden für welche Produkte ein solches, verstärktes Haftungsregime gilt. ;No;;;F_nf_Punkte_f_r_die_Zukunft_des_Handels_mit_KI.pdf
F529958;11-06-2020 14:35;English;NGO (Non-governmental organisation);Austeja;Trinkunaite;;Council of European Professional Informatics Societies (CEPIS);372608916662-62;Micro (< 10 employees);Belgium;The feedback can be published with your personal information;5 - Very important;4 - Important;5 - Very important;3 - Neutral;4 - Important;5 - Very important;;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;4 - Important;;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;;3 - Neutral;3 - Neutral;5 - Very important;4 - Important;4 - Important;4 - Important;;There is a need for a new legislation;;Yes;;Yes;;;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Rather not;;Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;;Cyber risks;;Yes;;Yes;;Yes, for specific AI applications;;;CEPIS-Comments-on-White-Paper-on-AI.pdf
F529957;11-06-2020 14:24;English;Other;Dee;MASTERS;;AI Law Consultancy which operates from www.ailawhub.com;;Micro (< 10 employees);United Kingdom;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AI_Law_Consultancy_response_to_the_European_Commission_White_Paper_final.pdf
F529956;11-06-2020 14:19;English;Company/Business organisation;axel;minaire;;Samsung Electronics;40471017282-57;Large (250 or more);South Korea;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;"The efforts should also be oriented towards lowering entry barrier of technology accessibility for SMEs and developers.
Data collection and storage account for a large portion of initial capital investment for many industry players. EU strategy should aim to help reduce such capital outlay and support development and operation cost by facilitating access to common database (anonymized form) and research AI centers. 
";5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;"The policy should aim to facilitate emergence and access to common European data space. 
As mentioned in 1.2, data access requires the heaviest portion of initial capital investment for any industry player and can be a substantial obstacle to AI players aiming to engage development strategy across Europe. Such policy would act as a catalyst for developers and researchers at foundation level
";4 - Important;5 - Very important;5 - Very important;First action needed is to establish an efficient environment for the better coordination among already existing AI centers by founding a clear and mutual vision and arranging task priorities for investment and R&D.Second action is to encourage more engagement in PPP for industrial research. Samsung has been striving to promote AI R&D via a coordination with various academia & research institutes. By managing seven AI centers around the globe Samsung aims to continue engaging in PPP in the future;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Another factor to consider is the growing chasm (polarization) of the data access/capability inequality between the haves/have nots of industry players. Most SME players have limited capital to afford such investment in research assets. Therefore Innovation hub policy’s goal should be aligned to reduce the inequality gap to encourage SME growth;5 - Very important;5 - Very important;5 - Very important;4 - Important;2 - Not important;2 - Not important;;Other;"The AI industry is still at nascent stage and it should be allowed to evolve by observing the results/trends of current AI implementations upon which future amendments to legislation can be considered in later phase.  
A stringent and rigid regulation could have the unintended effect of stifling innovation in AI industry by putting unforeseen constraints on AI players. There is therefore a need to proceed with flexible and scalable regulatory framework
";Yes;;No opinion;;;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);"Biometric authentication has gradually become mainstream in key service segment (eg: Fintech), and therefore the regulation should be oriented based on key learnings from such deployment.
Biometric identification should be allowed in public spaces only under the 2 inclusive conditions.
1)	The biometric identification should be performed at trusted device and only in user initiated transaction where user has shown implicit/explicit consent to go through biometric authentication
2)	The biometric identification should only be used as authentication in addition to entry of user credential, and not as a substitute of entire identification (or user credential) by itself

These conditions are aimed to prevent usage of actual personal data to be exposed in public space and to further ensure data privacy of individuals.
";Rather not;"Voluntary labelling system could be a system that companies are unwilling to actively participate unless there is a proper environment with enough encouragement by government.Therefore there should be encouraging measures provided by regulators for companies to take advantage of the voluntary labelling system such as a 'merit' system. 
Also, if a voluntary labelling  has to be implemented, it should be dynamic enough to keep up with rapid technological growth and emergence of new cyber threats";A combination of ex-ante compliance and ex-post enforcement mechanisms;;An effective compliance assessment could be beneficial if designed properly. An excessive compliance assessment could potentially discourage future participants, especially SMEs and start-ups, to enter the market. GDPR regulation already brings a burden on some AI players, and any incremental assessment could bring a further negative impact. Rather, an environment that companies can internally and pro-actively combine ex-ante compliance and ex-post enforcement seems sufficient.;Cyber risks;Legal certainty on liability between IT device manufacturer and AI platform company (eg. when AI app is installed (outsourced) in a certain device such as mobile) should be made more clear especially in fields like data management. ;No opinion;;No opinion;;No opinion;;The rules should be consistent with global industry practice trend and regulations from other regions.;
F529955;11-06-2020 14:17;English;Other;margot;DOR;;ETSI;474710916419-15;Medium (< 250 employees);France;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;The program should add an action to develop and improve standardisation concerning AI. Standardisation is not about specific solutions, but how they can be modelled, invoked, managed, tested, interoperate. In many other technical areas (e.g. communication, energy) standardisation has proven to be a key asset for competition, interoperability, knowledge sharing, risk reduction, and forming partnerships between different stakeholders (public, private, academia, industry).;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;The listed policy areas are all important or very important and interdependent. The European Data Space activity requires much increased interoperability for data, recognizing GDPR constraints, so that an open and trusted system is built. That would also help SME activities. Standardisation is fundamental for this.;3 - Neutral;3 - Neutral;4 - Important;Research and innovation would be strengthened by use of PPP, which have proved to be useful tools for promoting cross-border development of technology. Interoperability through standardisation should be emphasized, for transparency and inclusion. Examples are the Lighthouse Projects, 5GPPP and others.;4 - Important;4 - Important;4 - Important;5 - Very important;No opinion;European Digital Innovation Hubs should have an additional task to strengthen cooperation with SDOs and foster adoption of standards, to facilitate broad deployment. SME-financing is out of scope of ETSI expertise, but it should be noted that risks for startups is are reduced by access to standardised solutions.;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;"The concerns about usage of AI are all important, but the fundamental underlying concern is perceived danger from lack of verification. Even if full ""explainability"" is not achieved, standardisation can facilitate product interchangeability, product replacement and product competition, ultimately improving detection of undesired outcomes";Current legislation may have some gaps;;Yes;;Other;ETSI considers that the approach be best developed and refined with broad stakeholder participation, which might be done with high transparency within a standardisation process (feasibility/requirements study).;"ETSI understands that risk assessment depends on ""value at risk"" and ""probability of loss(es)"", whereby ETSI can help construct a framework for determining probabilities, but it is the responsibility of society and politics to prioritize the values/ethics. 
Quality control for data (also referred to in the following question) is essential for reliable AI, and ETSI can develop metrics. 
";5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;Defining regulations is out of ETSI scope, however standards can characterize technical parameters which can be cited as complying with essential requirements in regulations. ;Much;"ETSI considers that voluntary labelling for AI systems is might be useful, but it absolutely requires reference to some publicly available specifications/guidelines, which should be derived (and validated) according to open standardisation processes.
To that extent, ETSI considers that open specifications and validated testing are essential";A combination of ex-ante compliance and ex-post enforcement mechanisms;;ETSI proposes to follow the NLF approach as appropriate;;;;ETSI is well-aware that changes are needed in testing and assessment/certification and monitoring of systems which use adaptive methods, including AI. Standards for these procedures are under development. Legislative frameworks can cite them and a dialogue for prioritizing the work would be very useful. ;;;;;;
F529954;11-06-2020 14:11;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;"Yes: Europe should focus AI investment on strengthening Europe’s strengths, i.e. on industrial AI. Compared to consumer-centric AI, Industrial AI has different (mostly stronger) requirements; it needs to be safe, secure (against cyberattacks) and in many cases able to self-assess or even explain itself (“responsible AI”). European R&D in AI should focus on these capabilities, also in collaboration with cyber security expert players. ";5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;"Activities to build European data spaces should take sector specific industrial use cases as a starting point and should also be well aligned with national activities. Data ecosystems, based on contracts between companies of how to access and use data, will be a vital part of a future European Industrial and Services Ecosystem, enabling a European Cloud Service and Data Economy and supporting the mass adoption of AI. 
SEE UPLOAD DOC.";5 - Very important;4 - Important;5 - Very important;Europe needs to make focused investments in industrial AI, based on a combination of technologies such as machine learning, semantics, NLP, vision, combined with domain know-how, in domains where Europe plays a leading role. Building on the idea of “Lighthouse Research Centres”, industry-led AI R&I super clusters should be established that can generate global leading innovations, enabling European AI talents and stakeholders to bundle forces for fast innovation. SEE UPLOAD DOC.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;The specialised DIHs where SME's are enabled to test their use cases, should focus on domains where Europe plays a leading role: combining hardware solutions, automation, semantics, edge computing, (data) analytics, explainable and data scarce AI, with as goal to take the efficiency of industrial infrastructures (factories, power, transportation, etc.) to the next level, such as through the Digital Europe Funding Program. SEE UPLOAD DOC.;4 - Important;2 - Not important;3 - Neutral;4 - Important;2 - Not important;3 - Neutral;"""not always accurate"" is not inherent to AI but depends strongly on the application where AI is used, on the used data set and on the input order of data. Explainability will always be difficult (""black box"") for complex, deep learning algorithms, but it should be possible to at least have AI algorithms advertise their level of confidence in their recommendations. AI development should be by design intertwined with cybersecurity. SEE UPLOAD DOC.";Other;Most industrial applications (80-90%) do not need new regulation, because already sufficiently covered by existing one (GDPR, Mach. Directive). No “one-size-fits all” approach for AI regulation possible: Europe must address each vertical market separately. Use sandboxing to test new concepts (e.g. aut. driving) within a regulation-flexible & innovation-friendly space. Free flow of data and support for contract-based solutions for sharing B2B (private) data as a general principle. SEE UPLOAD DOC.;Yes;;Other;"While some sectors may have higher numbers of high-risk applications, it cannot be excluded that some high-risk applications also appear in other sectors, especially as beside safety also privacy and fairness / bias risks are covered by the frameworks. In this context it is also important to clearly define the criteria for a high-risk rating of an application and who will do the rating. See also UPLOAD DOC P.2 – Para: ""High Risk Sectors / Applications""";"AI applications used in critical infrastructures (transport, energy, water supply, electricity grids, hospitals,...) and AI in IIoT applications where personal privacy or life & limbs are at stake must be critically evaluated (case-by-case approach). The risk level must be objectively determined by the criticality of the application itself. See also UPLOAD DOC P.2 – Para: ""High Risk Sectors / Applications""";4 - Important;4 - Important;5 - Very important;5 - Very important;3 - Neutral;4 - Important;No further guidelines or regulations are needed;"Existing policy framework and legislation (like GDPR) is sufficiently covering the use of biometric ID systems. Permanent human oversight of all AI activities is not needed, it depends on the criticality of the application; a clear definition of the objectives of the AI application by humans must be sufficient ";Rather not;Voluntary labelling of an industrial AI application (in the B2B area) is not bringing any extra information and will only provoke an extra admin burden for companies (especially for SME's and startups). A B2B relationship is by definition based on trust between partners (supplier - user) and based on mutual contractual agreements (specifications) which should always be respected.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Extensive exchanges with all stakeholders across the AI chain will be necessary to discuss further the appropriate compliance and enforcement mechanisms. In our view, a combination of ex-ante assessment & ex-post surveillance and enforcement mechanisms would be purposeful, in particular in light of the positive experiences with the EU product safety legislation following the so-called “New Legislative Framework / NLF” (Decision 768/2008/EU, Regulation 765/2008/EU). SEE UPLOAD DOC.;;In our view, the current EU product safety legislation (e.g. Low-voltage Directive, Machinery Directive, General Product Safety Directive,) is so-called “total safety” legislation, and covers all risks that arise or can arise from covered products and related technology. There is no need to expand on further risks in the text of the Directives. Clarification of aspects, such as relating to risk coverage or safety concepts, should be done through guidelines, where necessary. SEE UPLOAD DOC.;No;The risk assessment process and the principles of safety integration for risk mitigation according to the CE Directives have proven their effectiveness and been successfully implemented. Almost all industrial sectors carry out risk assessment and implement risk reduction measures according to processes that are required by the safety legislation. The iterative process of risk assessment and risk reduction measures, as currently defined, does not need further considerations for AI. SEE UPLOAD DOC;No;No;Yes, for specific AI applications;Owner’s strict liability should be introduced for highly automated / autonomous systems (whereas “owner” means the person using or having used the system for its purpose), where life & limbs are at stake (and personal or material damages could occur, eg caused by an autonomous robot in a factory).;The injured party should be able to file a claim for bodily injuries or property damages applying strict liability standards against the owner of the autonomous system. The owner may claim compensation from the manufacturer/insurer. The owner of the autonomous system may obtain insurance coverage for such liability. (Like for a car owner, it could be made mandatory to take an adequate insurance).;2020_06_11_Consultation_White_Paper_on_AI_Siemens_Upload_Document.pdf
F529953;11-06-2020 13:56;English;;;;;;;;;The feedback can be published in an anonymous way;;;;;;;;;;;;;;;;;;;;;;;;;5 - Very important;5 - Very important;No opinion;5 - Very important;No opinion;5 - Very important;Important to ensure journalistic sources and source material stay completely safe and not risk being exposed. When balancing fundamental rights, freedom of speech for journalism must  be assured. Journalists must not be hindered in carrying out their work. ;;;Other;New rules should be applied to high-risk applications and high-risk algorithmic behaviour in addition to, but not necessarily tied to, high risk, sectors.;;;"It is important to make sure monitoring and crime prevention does not endanger journalistic sources. source material and journalistic research. Fundamental rights need strong protection (not just be respected). 
Access to data and AI used to prevent crime must not interfere with or hamper investigative journalism or news reporting.
Safeguarding sources is exceptionally important when discussing use of biometric data.";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;Biometric identification pose major risks to journalistic sources.;;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Any kind of publishing, television, radio, the press, social media and similar, shall allways be monitored ex-post. It is very important to find a balance between national monitoring and EU monitoring. Any freedom of expression issues should always be monitored at national level. It is also of utmost importance to protect a wide freedom for journalism, both at national level but also when travelling cross border.;Personal security risks;Make sure there is technology safe to use for journalists needing safe channels for being in contact with sources.;Yes;;;;No opinion;;"Concerning high risk getting ex ante control and no risk getting ex post control, the specifics of media publishing needs to be adressed. Any kind of media publishing must be regulated via ex post control when it comes to utterances. However when it
comes to protection of data, especially in relation to journalistic sources, publishers deal with high risk information and system safety used for this purpose needs ex-ante control.";
F529952;11-06-2020 13:55;English;Business Association;Philipp;Goedecker;;Zentralverband Elektrotechnik- und Elektronikindustrie e.V. (ZVEI);94770746469-09;Medium (< 250 employees);Germany;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;Industrial AI must be brought into the context of cybersecurity. This is intended for trustworthy AI, whose critical protection level meets the product requirements. To achieve this, Algorithms must be protected. We need combined cybersecurity expert players and AI developing and/or using companies in EU funded projects under a strong e.g. Horizon Europe program.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;"Yes, there should be activities to build European data spaces under the consideration of use cases. Further they should keep an eye on national activities to strengthen the EU activities. 
Moreover, Data ecosystems must be based on contracts between companies (access or the use of data). This supports a competitive European Industrial Ecosystem, guiding a European Cloud Service and Data Economy. 
";4 - Important;4 - Important;5 - Very important;Europe must bring up a strong investment plan on industrial AI with a focus on the EU leading domains (Machine Learning, semantics, NLP, etc.). Further we need combined structures to bring skills and stakeholders together (in cluster structures), to succeed huge progresses in research and finally innovation on industrial AI.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;Digital Innovation Hubs should have a strong focus on EC agenda, to have a strong focus on AI and further leading domains (edge computing, data analytics etc.) to strengthen the European industry through further development and innovation for efficiency.;4 - Important;4 - Important;4 - Important;3 - Neutral;4 - Important;No opinion;"AI algorithms for deep learning should be based on design and should include cybersecurity with a high priority to safeguard these algorithms according to the level of (cyber) security determination by the criticality of the application.

@ ""not always accurate"" is not inherent to AI. It strongly depends on the application itself where AI is used.
";Other;A totally new legislation of AI algorithms is obsolete. There are several legislations (80-90%) which fit with AI, like the MD or GDPR. Further a separation between the vertical markets must be guaranteed, because no “one-size-fits all” approaches would benefit anybody and would massively harm the European industries. The use of sandboxes to test new concepts (e.g. autonomous. driving) within a delimited, regulation-flexible and innovation-friendly space supports these developments. Further the ;Yes;;Yes;;"AI (in IIoT) applications where personal privacy or where life & limbs are at stake are critically evaluated in a case-by-case approach. Hereby should the risk level be objectively determined in a short range for the beginning by the criticality of the application itself. Further there must be clear criteria to define exactly what is meant by a ""high risk"". In addition, it must be clarified who the assessment sovereignty is.";4 - Important;3 - Neutral;5 - Very important;5 - Very important;3 - Neutral;4 - Important;No further guidelines or regulations are needed;For example, biometric identification systems are regulated by the GDPR and further legislative frameworks. Further they are based on the critical point of the application. Moreover, a definition of the objectives of the AI application by humans have to be abundant. ;Rather not;Industrial AI application Voluntary labelling in B2B has no relevant effects compare to the bureaucratic burden a (Start-Up, SME or Midcap) company would face. In the B2B sector this is based on trust between partners and must base on contractual agreements (specifications).;A combination of ex-ante compliance and ex-post enforcement mechanisms;;A trustworthy AI should be ensured by a combination of ex-ante compliance and ex-post enforcement with existing mechanisms of conformity assessments.;Personal security risks;The current EU legislation through the product safety legislation is fine and valuable with no need to expand it on risk which maybe could possibly happen. Clarifications of aspects should be done by guidelines.;No;The current standards and principles regarding a risk assessment and risk reduction measures does not need further measures for AI.;No;Before a revision of the Product liability Directive there should be a check if specific AI may lead to liability situation which are not covered.;Yes, for specific AI applications;"The liability should be introduced for an autonomous system and its owner. The owner is here the one who use the system, where life & limbs are at stake or personal/material damages could occur.
But usually the exiting frameworks are useful, like the Product Liability Directive, General Product Safety Directive, Machinery Directive etc.
";There should be the possibility that bodily injuries or property damages should have strict liability standards on the owner of the autonomous system. The owner on the system otherwise could probably have a claim on the manufacturer or the insurer of the manufacturer. These implies insurance coverages for these kinds of harm/liabilities. ;
F529951;11-06-2020 13:45;English;Company/Business organisation;giacomo;robustelli;;TIM S.p.A.;078655018381-34;Large (250 or more);Italy;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;"The sharing of information, skills and best practices is necessary to ensure cooperation among Member States, public and private organizations, companies and citizens.
A more detailed roadmap on how to achieve an ecosystem of excellence (and related funding) would be welcome.
";4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;The development of pilot projects could be encouraged.;4 - Important;4 - Important;5 - Very important;"Every action that aims to improve funds in research and innovation should be 
encouraged.";4 - Important;5 - Very important;4 - Important;5 - Very important;4 - Important;Improve the digital maturity of each company and collaboration amongst the different players, with the view of accelerating the digital transformation at large.;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;Most of the concerns raised by AI can be addressed and mitigated increasing transparency and ensuring all current regulation on privacy, safety and liability are correctly applied.;Other;Most concerns can be addressed and mitigated with current legislation. In some cases, guidance could prove useful. Any additional regulation/legal adjustment should be necessary and proportionate. Horizontal rules should be applied in a technology-agnostic way as much as possible. ;Yes;;Other;We agree in principle, but further specifications are needed to avoid legal uncertainty. Likewise, the review timeframe of the sector list should be wide enough to ensure legal certainty. ;AI applied to health (AI medical instruments) or those using biometric identification systems.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Very much;Awareness of customers on AI applications is key to enhance trust. Applicable provisions falling under the label should be less burdensome than those required for high risk Ai aplications.;Other enforcement system;"To minimise risks, particularly high-risk products should fall under “ex ante” products safety rules. 
However, a balanced combination of ex ante and ex post safeguards is also reasonable due to the fact that an impact analysis – the potential risk resulting from a specific AI-application – is not always fully predictable upfront. For the sake of legal certainty, clear rules on “who” decides about whether a product imposes a high or low risk are required based on clear legal provisions.";;Cyber risks;"Connectivity is the basis of the digital market and the quality of connectivity is already subject to specific sectoral rules, providing for redress in case of significant connectivity interruptions; therefore, connectivity providers should be not held responsible for “falsely attributed” liabilities of increasingly complex connected devices that stem from potential defects in a supplier’s products and services. ";Yes;Reforms should primarily focus on ex ante mechanisms based on the product safety directive (higher requirements for accessing to the EU market). This will address those more upstream parts of the value chain that are often responsible for unsafe products. Minimum safety safeguards should be guaranteed by the producer throughout the entire life-cycle of a product embedding AI. Before updating software components of a product, it is necessary to verify if the updates affect its correct functioning;No;The current liability framework already provides sufficient safeguards. However, if liability rules are tightened, the deployer of AI systems (who is often also the trader) should not be held liable for problems created or neglected by other players in the data value chain. Correct attribution of responsibility is key along the value chain.;No;;;
F529950;11-06-2020 13:40;Italian;EU Citizen;ela;njeçi;;;;;Albania;The feedback can be published with your personal information;3 - Neutral;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;;5 - Very important;4 - Important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;Yes;;Yes;;;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F529949;11-06-2020 13:39;English;Academic/Research Institution;Nathalie;Laneret;;Centre for Information Policy Leadership (CIPL);980643334936-37;Micro (< 10 employees);Belgium;The feedback can be published with your personal information;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;"Rely on impact assessments based on context, impact, risks and benefits of AI 
Provide for outcomes for organisations to achieve through concrete and verifiable risk-based accountability measures 
Encourage co-regulatory tools (self-assessments, voluntary labels, certification, codes of conduct) 
Do not duplicate/ conflict with the GDPR
Rely on current regulators and regulatory hubs 
Promote innovative oversight and constructive engagement through data review boards and regulatory sandboxes";5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"Promote organisational accountability through Digital Innovation Hubs
Offer accountable organisations the opportunity to play a proactive advisory role for SMEs seeking to implement accountable AI practices (e.g. through sharing of competence through Digital Innovation Hubs);";4 - Important;4 - Important;4 - Important;4 - Important;3 - Neutral;4 - Important;By looking at AI risks and concerns only, individuals and society may lose the benefits of a particular AI use. In any AI assessment it is important to also consider the consequences for individuals and society of not using an AI system because of its potential risks;Other;Any AI legislation must not be drafted too prescriptively to enable flexible and future-proof regulation. It should provide for outcomes that organisations should seek to achieve, leaving it to them to define concrete, demonstrable and verifiable measures (based on their own assessment, industry practices, regulatory guidance, or external technical standards).This would allow for risk based and contextual adaptation as well as continuous improvement of such measures.  ;Other;The application of the requirements as well as their adaptation to specific AI applications should be based on impact assessments performed by organisations that would take into account the context, impact, risks, benefits and necessary trade-offs between different requirements.   ;;;Use of AI in the police or justice contexts;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;No opinion;;Very much;"Make voluntary labelling available for all AI
Arrange for a publicly available repository of AI labels, certifications and codes of conduct 
Define standards, and testing methodologies uniformly at the EU level with EU validity  
Work on a co-regulatory basis with input from industry
Focus labels on controls and safeguards rather than products  
Enable interoperability with non-EU schemes 
Ensure issuance of label in a reasonable timeframe and in a pragmatic manner";Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;"Harmonisation and consistency in enforcement 
One Stop Shop mechanism with a single regulator as interlocutor for cross border/cross sectoral matters 
Efficient decision making process that is time effective 
Constructive engagement between regulators and industry";;;;;;;;;;CIPL_Response_to_EU_Consultation_on_AI_White_Paper.FINAL_DOCX.DOCX
F529948;11-06-2020 13:25;English;Academic/Research Institution;Klaus;HEINE;;Erasmus School of Law, Erasmus University Rotterdam;Erasmus University Rotterdam;Large (250 or more);Netherlands;The feedback can be published with your personal information;4 - Important;4 - Important;5 - Very important;3 - Neutral;5 - Very important;No opinion;;4 - Important;4 - Important;4 - Important;No opinion;4 - Important;5 - Very important;;5 - Very important;5 - Very important;4 - Important;;3 - Neutral;3 - Neutral;3 - Neutral;4 - Important;2 - Not important;;4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;3 - Neutral;;There is a need for a new legislation;;No;;;;;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;No opinion;;Yes;;Yes, for all AI applications;;;1106_Reply_to_Consultation_White_Paper.pdf
F529947;11-06-2020 13:18;English;Business Association;Arkadiusz;Rzepka;;Die Deutsche Kreditwirtschaft;52646912360-95;Large (250 or more);Germany;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"The creation of a data pool for testing & training AI applications under clear regulatory conditions.This would enable providers to respond more quickly to market developments and customer needs and to gain new insights on a broader data basis. This could also improve their innovation and competitiveness in international comparison. 
In addition, the development of AI applications should take place within the framework of the EU-value system. ";4 - Important;4 - Important;5 - Very important;4 - Important;4 - Important;5 - Very important;"AI-relevant basic education in colleges and universities should be promoted.
However, the strengthening of AI-specific competencies must not be at the expense of training in basic disciplines (MINT). In practice, it often turns out that there is a much higher demand and a supply deficit for specialists with a sound mathematical/statistical education than for highly specialised AI experts. 
";4 - Important;4 - Important;5 - Very important;European companies and institutions must be supported not only in research, but also in the use and operation of applications in order to reduce the outflow of know-how and promote implementation in Europe;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;"The focus and the core competences of SMEs are regularly not the development of AI applications. European competence centres should develop solutions in partnership with companies and also offer their development competence to companies as a service.

In common European research projects, the challenges and practical solutions between regulation and technology could be examined more intensively. ";4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;"In order to resolve concerns about the use of AI applications, it must be ensured that discrimination and errors are avoided. Systems that interfere with people's lives or personal rights must be tested and monitored more closely than systems that merely automate repetitive work steps. 
The need to adapt regulation to respond to specific risks caused by AI should strive towards a technology-neutral approach. ";Current legislation may have some gaps;;Other;For AI-applications with high risks, higher requirements must be set than for low risk AI-applications. However, minimum standards should also be formulated for low-risk AI-applications. ;;;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;The nature and scope of monitoring must be discussed in society. In particular, it must be demonstrated which successes are actually achieved as a result of monitoring. In addition, only systems that achieve an error rate close to 0% should be used.;Not at all;;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;;;;No opinion;;No opinion;;No opinion;;;
F529946;11-06-2020 13:04;English;NGO (Non-governmental organisation);Giacomo;Cremonesi;;Human Rights International Corner;;Micro (< 10 employees);Italy;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;4 - Important;4 - Important;5 - Very important;No opinion;No opinion;No opinion;;4 - Important;4 - Important;4 - Important;;4 - Important;4 - Important;5 - Very important;5 - Very important;No opinion;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;See our position paper attached;There is a need for a new legislation;;No;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;See our reference to HRDD in the position paper as described in the UNGP;;;Yes;;Yes;;Yes, for all AI applications;;;HRIC_POSITION_PAPER_AI___UNGPs.pdf
F529945;11-06-2020 13:01;Swedish;Academic/Research Institution;Pär-Erik;MARTINSSON;;Luleå tekniska universitet;558221123297-13;Large (250 or more);Sweden;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;A multidisciplinary approach that connects knowledge from humanities, social sciences with technology is lackin in the description given in sector 4 of the document.;5 - Very important;4 - Important;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;;3 - Neutral;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;Kontinuitet är viktig för att långsiktigt etablera innovationspunkterna som självklar aktör som SME vänder sig till, det finns en uppenbar risk att dessa inte blir framgångsrika om projektperioderna är för korta. Enter-Exit strategi för nätverken på europeisk nivå säkerställer att det inte skapas statiska strukturer utan alla tänkbara och relevanta aktörer kan vara med.;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Koppling mot GDPR är viktig samt hur länge persondata får lagras för användning i AI-system;Current legislation may have some gaps;;No;;;;Lagstiftning måste vara generisk och inte kopplas till subjektiva bedömningar om vad som är hög eller låg risk. ;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;No opinion;;No opinion;;;
F529944;11-06-2020 12:33;English;Other;Luca;Pardo;;ONTIER;930124938162-46;Small (< 50 employees);Italy;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important; ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;Yes;;Yes;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;;No;;;Ontier_s_comment_on_the_EU_Commission_s_WP_on_AI__complete_.pdf
F529943;11-06-2020 11:47;English;NGO (Non-governmental organisation);Duncan;JARVIS;;Euramet eV;83842168796-93;Micro (< 10 employees);Germany;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;When measured data is used to train AI or AI is used to improve measurement modalities metrological principles ensure the quality standard of data from the physical world used to train artificial intelligent systems and assess the reliability of measurement systems using AI.  Therefore, the area of metrology should be considered by “Build up a European Metrology infrastructure supporting AI” or “Develop a European Metrology for AI”. ;4 - Important;5 - Very important;4 - Important;;4 - Important;5 - Very important;4 - Important;4 - Important;3 - Neutral;We consider the following tasks to be important for Digital Innovation Hubs: The development of methods and standards for data quality with respect to AI, traceability to standard references and procedures, conformity, interoperability of measurement results, providing open-source AI reference algorithms and core libraries, the inclusion of regulatory requirements in the application of algorithms (regular-compliance-by-design) and the uncertainty assessment for training data.     ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;;Current legislation may have some gaps;;Yes;;Yes;;;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;For the interplay between measurement data and decisions made by AI, standardised procedures must be developed. These procedures should indicate not only the decision on the measurement value but also associated uncertainties allowing for a quantification of the level of confidence and a risk assessment. The following questions may arise: Is the training data appropriate and reliable for the learning task? Is the training data sufficiently representative and have biases been quantified?;No opinion;;Yes, for all AI applications;;;MATHMET_Statement_AI_final_1.pdf
F529942;11-06-2020 11:45;English;NGO (Non-governmental organisation);Brigitte;Vézina;;Creative Commons;;Small (< 50 employees);United States;The feedback can be published with your personal information;;;;;;;Creative Commons is grateful for the opportunity to provide comments as part of this consultation. The Commission’s AI White Paper would benefit from a more thorough and extensive analysis of the copyright law implications of AI, as part of a coordinated approach to developing policy in the field of AI. For details, please refer to the attached supporting document, which provides our provisional views on the interrelation between AI and copyright law. ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
F529941;11-06-2020 11:44;German;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;3 - Neutral;1 - Not important at all;1 - Not important at all;2 - Not important;protection of personal health care data against abuse by thrids (recruiters,employers, insurance companies etc.);3 - Neutral;4 - Important;1 - Not important at all;3 - Neutral;3 - Neutral;4 - Important;If there should be a sector that developpes an tests AI tools, it should be exclusively the public sector. Parliament and Commission of the EU should be in fully control of AI developpement and be at any moment able to ensure that AI developpement  is conform to human rights, and EU-citizen's rights of protection of personal sphere and liberties (also including personal health, fisal and other data relative to public administration and including the right of free expression of opinion).;4 - Important;4 - Important;1 - Not important at all;Corresponding to my statements before: Any lighthouse reseach centre or AI reseach network should be composed by public reseach entities (universities etc.), in control of public democratic institutions, such as parliaments or university parlaments which give regularily openly published preports, that can be consulted  and discussed by any citzen or media. Transparency!!!;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;The developpement of AI is - again - not to be exposed to uncontrolable liberal market mechanisms! From an purlely economic point of view: AI should encourage employment, not cancel jobs. Manpower, human skills an humainly acting SME's, well ancered in their local and regional economic landscape are to be considered as first interest for any EU's economic policy - before AI!!!!;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"AI cannot replace or compensate human political or social responsibility or make the human decision-making-process in parliaments of all kinds of political levels, in human ressources ressorts or agencies, insurance agencies, public institutions like public health care etc. ""more correct or efficent"" !! Human, democracy-based desisions must be the base of all choices of categories with which i.e algorythma work. And they must be transparent to any public!!";There is a need for a new legislation;;No;;;;"No categories as ""High-risk"" should be employed!!!!!";3 - Neutral;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;Where there ist no Biometric identifacation system, there ist no mass-surveillance possible! We cannot know whether EU or any oft ist member states turn into dictatorship (well, there are already signs), we cannot know up to which degree big enteprises can take control over people employed there - let's not give life to bearely controlable mass-surveillance facilities!! Let's establish and implement EU human rights and democracy values first and make them reliable also in the sphere of AI !!!!!!;Not at all;"As I explained before: The problem ist the categorization in ""high-risk""....";A combination of ex-ante compliance and ex-post enforcement mechanisms;;As I stated before: AI technology should be developped and tested by reseach and developping entities that are public and therefore controlable by democratically legimitized authorities (parliaments or entities reliable to them!!;Personal security risks;"As stated before: AI technoligy should only be develloped by public research entities; there must be a legally binding charta developped, that binds any technology to human and citizen's rights. And that's it.";Yes;"EU should focus on longtime-effects that lead to discrimination, social segregation, criminalization of people just because they seem to fit in any algorythm-based categories that qualifie them as ""negative, unuseful or dangerous"" persons, without that human beings could steer or intervene these algorythm-based categorizations!!!! In France we already have porblems with de fabulous ""fichier S"". We do not need algorhythms that do this doubtfoul work without us!!!!!!!";Yes;;Yes, for all AI applications;;;
F529940;11-06-2020 11:42;German;Company/Business organisation;David;Schimanko;;TÜV NORD AG;41604864983-63;Large (250 or more);Germany;The feedback can be published with your personal information;5 - Very important;4 - Important;4 - Important;3 - Neutral;5 - Very important;3 - Neutral;Die Kooperation zwischen staatlichen Institutionen, Forschungseinrichtungen und privaten Unternehmen stellt einen wichtigen Pfeiler da, um Innovationen aus dem Forschungsumfeld zeitnah in der Wirtschaft und letztendlich in alltäglichen Prozessen zu realisieren.;5 - Very important;5 - Very important;3 - Neutral;4 - Important;5 - Very important;4 - Important;Der Aufbau weltweit anerkannter Testeinrichtungen ist elementar für die gesellschaftliche Akzeptanz von KI Anwendungen. Ohne unabhängige Prüfungen kann kein Vertrauen in diese relativ neue Technologie geschaffen werden. Starke und innovative Prüfkonzepte können auch ein Alleinstellungsmerkmal europäischer KI-Kompetenzen darstellen und eine bisher wenig beachtete, aber wichtige Nische in der weltweiten KI-Entwicklung ausfüllen.;4 - Important;4 - Important;5 - Very important;;3 - Neutral;5 - Very important;4 - Important;5 - Very important;No opinion;;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;Yes;;Yes;;Hochriskant sind alle Anwendungen, die einen unmittelbaren Einfluss auf Leib und Leben und die Grundrechte haben. So kann z.b. die „Durchleuchtung“ und Überwachung von Menschen durch das Zusammenbringen einer Vielzahl von verschiedenen Datenquellen durch KI hochgradig automatisiert und skaliert werden. Gleichzeitig verringert sich der personelle Aufwand durch diese Technologie enorm und erleichtert dadurch auch den Missbrauch.;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;3 - Neutral;4 - Important;Other special requirements in addition to those mentioned in the question above should be imposed (please specify);KI bietet das Potential, enorme Datenmengen aus unterschiedlichsten Bereichen auszuwerten und Muster darin zu erkennen. So können schnell Finanz-, Bewegungs- oder Konsumprofile von vielen Menschen erstellt werden. Die erforderlichen Ressourcen sind dabei relativ klein. Dadurch kann ein möglicher Missbrauch dieser Daten sehr schnell skaliert werden. Dies sollte in etwaigen Anforderungen reflektiert werden.;Much;Sollte ein freiwilliges Kennzeichnungssystem für KI-Systeme mit geringem Risiko eingeführt werden, sollte die Vergabe des Kennzeichens durch unabhängige Prüforganisationen erfolgen. Ein freiwilliges Kennzeichnungssystem, bei dem für die Vergabe des Kennzeichens eine Herstellerselbsterklärung ausreicht, sollte vermieden werden.;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Cyber risks;Ein weiterer Aspekt betrifft die Datenspeicherung innerhalb von künstlichen neuronalen Netzen. Im Gegensatz zur klassischen Datenspeicherung, bei der Speicherort und Dateninhalt genau bestimmt werden können, sind die Daten in einem neuronalen Netz in den Gewichten codiert und assoziativ verknüpft. Das heißt Daten können weder exakt lokalisiert werden noch kann angegeben werden, welche Art von Daten vorliegen.;Yes;"Analog zur ""klassischen Welt"" muss nach jedem Update, welches das Verhalten des ML-Algorithmus beeinflusst, eine neue Bewertung bzw. Überprüfung hinsichtlich der vorher definierten Kriterien erfolgen. Besondere Beachtung gilt weiterlernenden Systemen. Auch wenn diese Klasse von Algorithmen derzeit kaum eine Rolle in sicherheits- oder gesellschaftlich relevanten Bereichen spielt, erfordert die kontinuierliche Veränderung des KI-Systems auch besondere Beachtung und Kriterien in Regelwerken.";Yes;;Yes, for all AI applications;;;
F529939;11-06-2020 11:37;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;;Current legislation may have some gaps;;Yes;;Other;We find the definition to vague to comment on at this stage. Further clarifications on what constitutes a sector 'where, given the characteristics of the activities typically undertaken, significant risks can be expected to occur' are needed. Are non-physical risks considered? Democratic risks etc.?;;5 - Very important;3 - Neutral;4 - Important;4 - Important;5 - Very important;4 - Important;No opinion;;Much;The idea is interesting, but given the uncertainty of which sectors and applications that shall be considered high risk it is difficult to further comment on its practical implementation. The fundamental question, which we consider unclear, remains: who gets to apply for the labelling?;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;No opinion;;No opinion;;;Schibsted__EU_Whitepaper_on_AI.pdf
F529938;11-06-2020 11:30;English;Company/Business organisation;Kalliopi;Spyridaki;;"SAS Institute 
http://www.sas.com";175323916168-03;Large (250 or more);United States;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;"With offices in all Member States, a strong engagement with the public sector, and a longstanding commitment to R&D and education, SAS believes the six actions are well identified. Along with the recent expansion of our R&D Centre in Scotland, SAS’s major investment in AI comprises three main areas: R&D innovation; education initiatives; and expert skills to optimize return on AI projects. International cooperation will be paramount to ensuring global uptake of AI that has no borders. ";5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;World class infrastructure, volume and quality of available data as well as the development of strong AI skills will be core elements of a successful AI strategy. The development of a seamless, EU-wide data space will be fundamental and can give Europe a considerable advantage. ;4 - Important;5 - Very important;5 - Very important;There is considerable existing investment from the public and private sectors in Europe on AI research. With a clear vision, renewed priority focus along with adequate financial commitment existing and new investment in AI research centers will contribute significantly to Europe’s global leadership on AI development and uptake.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Given their role as the backbone of the European economies, SMEs & start-ups play a key role in the AI ecosystem. They need to be adequately supported with tangible services and a clear simple regulatory framework. ;5 - Very important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;4 - Important;In general, concerns differ in type and gravity depending on the AI systems or applications in question as well as the specific use or context. Concerns must also be weighed against the potential AI benefits and possibilities AI presents to address the concerns. For example, AI may not always be accurate, but data/model quality issues can be detected and addressed which may not be the case for decisions that are not based on AI. ;Current legislation may have some gaps;;Yes;;Other;In principle, we agree with a risk-based approach. We wonder if the first criterion of listing specific sectors is useful given high-risk applications can be envisaged in many sectors, e.g. education or retail. ‘High’ or ‘significant’ risk should be clearly & reasonably qualified and quantified. It seems pragmatic to further frame “high-risk” with the introduction of a meaningful definition of ‘AI’ applications to which the rules will apply. ‘Exceptional instances’ must be carefully defined.;AI applications involving physical safety or impacting democracy and fundamental rights.  In addition, the ability for AI to disrupt the stability of the economy through market manipulation deserves close oversight.;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;No opinion;Businesses, consumers and the society as a whole should leverage the benefits from the use of biometric identification systems. The risks should be mitigated with clear rules. ;No opinion;The development of a genuinely voluntary system would need to overcome important challenges in order to be widely accepted across Europe and beyond.  It would also need to be a consumer-friendly system that is adequate for the enormous breadth of AI systems and their uses. Experience with e.g. privacy labelling does not show that labelling is always purposeful.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;The suggestion to remedy shortcomings by re-training the system in the EU is concerning. If the intention is to re-train by relying on European algorithms and data sets, this approach would not encourage a global outlook that is needed for AI to deliver its potential benefits to business, the economy as a whole and citizens. This suggestion could be focused instead on re-training the system “in accordance with EU requirements” and not “in the EU”. ;Cyber risks;It will be important to define the terms ‘use’ and ‘artificial intelligence’ so as to assess risks in a more concrete and effective way. ;No opinion;What is considered a “change”, which changes are “important” and what is considered a “product” within the AI ecosystem will need to be debated and defined before discussing requirements around new risk assessment procedures during the lifetime of a given product.;No opinion;A one-size-fits-all approach in relation to AI and liability seems inadequate. Liability rules must be clear but also flexible to enable operators to put in place the most adequate arrangements for specific types and levels of risk as well as for specific AI applications or uses or systems as appropriate. ;No opinion;;Our considerations in response to 3.6 also apply here. Furthermore, we believe that harmonizing liability rules across Europe will contribute considerably to AI development by start-ups, to consumer trust and protection and to legal clarity, thus to the overall uptake of AI in Europe. ;
F529937;11-06-2020 11:20;English;NGO (Non-governmental organisation);Simone;CUOMO;;Council of Bars and Law Societies of Europe (CCBE);4760969620-65;Small (< 50 employees);Belgium;The feedback can be published with your personal information;No opinion;No opinion;5 - Very important;5 - Very important;No opinion;3 - Neutral;"The CCBE fully supports the idea that EU-level funding should be made available for sectoral regulators (including bars) as they are best positioned to address the training needs for their respective sectors (such as lawyers) particularly as regards how AI can be used in a way which is compatible with their ethical codes. 
Interaction amongst all sectors, private and public, is crucial to ensuring that the ethical values are designed into the AI systems themselves. 
See attached CCBE response.";No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;;No opinion;No opinion;No opinion;;5 - Very important;5 - Very important;4 - Important;No opinion;No opinion;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"AI may lead to unfair legally binding outcomes
AI may hamper access to justice
AI may undermine fair trial rights
In general, the use of AI in automated decision-making processes may reshape the interaction between citizens and public/private decision-makers. This may undermine citizens’ ability to seek advise, contest or reverse decisions. Therefore, robust redress mechanisms need to be ensured as well as a close involvement of actors protecting citizens’ rights (eg lawyers and judges)";Other;For some areas there might be a need for new legislation, whereas for others not, or only further clarifications are needed as to how existing rules apply to new circumstances resulting from the use of AI. See the attached CCBE response for further details.;No;;;;"- The use of AI tools by courts in different phases, i.e. pre-trial, trial, deliberation/decision-making, and post sentencing.
- The use of AI tools for law enforcement purposes.
For more explanation, reference is made to the separate CCBE paper attached ";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;"Biometric identification systems technologies tend to have serious flaws that endanger civil rights. For example, facial recognition technology has been proven in multiple studies to be inaccurate at identifying people of different races. Also, there are grave concerns that the trigger words which are used by national security agencies are not sufficiently refined and thus the phone conversations and email correspondence of millions of people are monitored without a legal basis. 

Further, the widespread use of facial recognition may pose severe risks for an open and pluralistic society if not used proportionately with a proportionate intended aim such as ensuring public safety. In many situations, anonymity is the most important safeguard of freedom, and facial recognition techniques that cover major areas in the public space endanger this freedom. The more accurate they are and the more widespread their use, the more dangerous they become. 
";No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Instead of adhering to a very generic and abstract compliance framework, the appropriate compliance measures must be considered and tailored to the needs in specific sectors and circumstances. ;Risks related to the loss of connectivity;Yes, there are further risks that need to be considered as regards the use of AI in justice. In particular, AI tools must be properly adapted to the justice environment, taking into account the principles and procedural architecture underpinning judicial proceedings. In this regards, reference is made to the CCBE separate response.;Yes;;Yes;The current legislative framework should be amended considering the fundamental differences that exist between traditional products and AI when it comes to the notions of product, fault and defect. Questions of to whom liability might be extended, the burden of proof and defences must also be reconsidered. The CCBE would, however, opt for a separate instrument on AI liability issues rather than amending the Product Liability Directive, recognised as effective in respect of traditional products.;No opinion;;The CCBE is not able to comment on how the current national liability rules regarding compensation for damage and allocation of liability should possibly be adapted. However, we do believe that these aspects, including rules on the burden of proof, should be regulated at EU level and included in the reviewed Product Liability Directive (or in a separate instrument). Another approach might lead to a situation where the adapted national rules would differ significantly between the Member States. ;FR_20200605_CCBE-Response-to-the-consultation-regarding-the-European-Commission-s-White-Paper-on-AI.pdf
F529936;11-06-2020 11:13;English;Academic/Research Institution;Dirk;DE CRAEMER;;Ghent University;006043210924-49;Large (250 or more);Belgium;The feedback can be published with your personal information;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;;3 - Neutral;5 - Very important;4 - Important;;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;Other;Irrespective of whether there should be new rules, there is a lack of expertise on current EU rules applicable to AI, which are particularly diverse. ;Other;Current guidelines (privacy and data governance, non-discrimination, training data, robustness…) are also good practices for non-high-risk applications. Compliance with these rules could be part of an ethics check when receiving funding by the EU (e.g. for Horizon Europe projects) instead of introducing compulsory requirements.;;;AI applications which impact on democratic institutions (i.e. election process, targeted political advertising, etc.) and fundamental rights.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;;No opinion;"Certification procedure should differ from CE-marking which relies on the manufacturer’s declaration that products meet EU standards. 
There might be a role here for external consultants/certifiers. Certifications provide a clear checklist and might quicken the research funding process (a commitment that research results will be certified ensures that research is in line with ethical/regulatory obligations).
Different grades of certifications could be useful, in light of the later use or risk.";No opinion;;;Mental health risks;;No opinion;;No opinion;;No opinion;;;
F529935;11-06-2020 11:09;English;Academic/Research Institution;Belen;MARTIN-BARRAGAN;;The University of Edinburgh;293375634866-01;Large (250 or more);Spain;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;4 - Important;2 - Not important;5 - Very important;;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;5 - Very important;4 - Important;;4 - Important;5 - Very important;4 - Important;;5 - Very important;4 - Important;5 - Very important;5 - Very important;3 - Neutral;;2 - Not important;4 - Important;5 - Very important;5 - Very important;No opinion;1 - Not important at all;Research and mathematical modelling can help overcome the issues raised by these concern. This is were the focus should be: research and innovation that overcomes those issues. Legislation alone create barriers and should be an enabler.;No opinion;;No;;;;;4 - Important;No opinion;4 - Important;5 - Very important;5 - Very important;No opinion;No opinion;;No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Risks related to the loss of connectivity;;No opinion;;No opinion;;Yes, for all AI applications;;;
F529934;11-06-2020 10:48;English;Academic/Research Institution;Svetlana;Tikhonenko;;Informatics Europe;256512130951-89;Micro (< 10 employees);Switzerland;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;4 - Important;A regeneration of the knowledge transfer process is needed, promoting entrepreneurship in the scientific community and creating companies led by experts, with deep knowledge of the technology and its implications. Currently, few business leaders are trained to lead transformation processes involving these technologies. Attention needs to be paid to both technology development and the understanding of the impact of AI on European society. Early deployment is not essential.;5 - Very important;4 - Important;2 - Not important;2 - Not important;5 - Very important;4 - Important;Promoting early uptake of technology before the implications of this technology are understood is dangerous. A need exists for a balance and broad analysis of the impact of AUI by a broad collection of experts that include members from societal, legal and ethical communities in addition to technologists and business leaders.;2 - Not important;4 - Important;4 - Important;"To specialize the training at any education level, based on teaching staff with digital native profiles and with real experience, especially at university level where exists less adoption of technology. That will permit to evolve to multidisciplinary research teams crucial for the future.
Give more freedom to research of innovation actions that are now depending on the topics of specific calls.
Efforts should be made to strengthen existing centres, rather than creating new lighthouse centres.";3 - Neutral;4 - Important;4 - Important;3 - Neutral;3 - Neutral;"DIH must be created once the ecosystem is established and they should represent a source of innovation and not an elitist environment difficult to reach by SMEs. Therefore, DIH must be created to address the challenges proposed by the SME. That will enable their connection from the beginning.
In general, it is the task of the European community to help citizens and enterprises to understand the impact of deploying AI technology, rather than promoting the technology itself.";5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;4 - Important;"We should not focus on problems of AI itself, but problems on training data. If data is safe, also AI will. It's important to know how to use AI. We should not limit AI, but to make sure that its potential users exploit it with competence.
There will be temptations to deploy AI technology to solve short-term societal problems, but these ‘solutions’ may have unwanted long-term consequences. We should not rush to adopt technological solutions that compromise existing social and legal protections";There is a need for a new legislation;;Other;"Low/mid-risk applications can result in side effects which can end up generating profound negative social changes such as inequality or discrimination.
A short-term focus on high-risk needs to be broad enough to encompass various forms of risk that may also include non-high-risk areas.
The definition of high-risk application may change over time, then it is important to reason on requirements case by case. It is not possible to distinguish between high-risk and not high-risk applications.";;;One of the most concerning area is the economic one, especially in sectors of access to resources, financing and other benefits that may be affected by hidden demographic values that AI can detect and amplify, generating a greater gap in sectors that are already vulnerable and that the society tries to protect by positively biasing the decisions that affect them.;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;Without being completely against the use of this technology, the identification of citizens should never be carried out autonomously, but must be done with a prior and justified request for such recognition, with the explicit consent of the citizen and regulated in the legislation. Otherwise a large number of fundamental citizen rights can be violated. ;Rather not;Voluntary labelling has led to consumer misinformation rather than any testable, verifiable labelling of approaches. Benefits should be encouraged and citizens should be educated in a way that it would be positive and desirable that all AI systems were registered. The certification of these systems must be transparent and consistent with their purpose, but without flooding of bureaucracy and barriers to innovation in order to prevent the standard from becoming a design imposition or requirement.;Other enforcement system;High-risk systems must be validated before starting their development, that is, the use of data for this purpose must be identified and authorized if it has not been previously authorized.;It is essential than pan-European compliance rules be established and monitored, with the possibility of Citizen/enterprise feedback. A watchdog agency that monitors compliance and which can enforce legislation is needed to build confidence and enable the resolution of compliance conflicts.;Mental health risks;Machine learning may lead to the identification of false positive and negatives. This implies that there is a risk associated to any automated choice. Opacity of AI shouldn't be addressed a-posteriori as described at page 9 of the document. Instead, it should be addressed in such a way that the risks associated to this opacity are eliminated or at least kept under control. In general, the level of required risk protection mechanisms should depend on the way a certain piece of technology is used.;Yes;Risk assessment procedures should run continuously during the lifetime of critical systems as it happens for any technological product subject to deterioration. While, apparently, software does not deteriorate, aspects concerning architectural drift and erosion apply also to traditional software and should be considered as very critical factors that may determine a reduction in the safety of the product. This issue is not only related to AI software but to any piece of long-living software.;Yes;"Citizens must be educated to understand the basic operation of these stochastic systems to be aware that malfunction can be developed.
Software can be assembled or developed by third parties. This supply chain is critical as it can determine a change in the software while this is running and outside the control of both the software owner and its producer. For this reason, new liability mechanisms should be considered to identify the parties that can change dynamically during software operation.";Yes, for specific AI applications;"Specially applications that may put people’s health and safety at risk.
Not only AI applications but any software suffers from the dynamic changes in the liable parties. Changes may also involve hardware components and the interplay between hardware and software.";The attributions of liability must be clear from the beginning and always fall on a natural or legal person.;
F529933;11-06-2020 10:33;English;Trade Union;Oliver;Suchy;;German Trade Union Confederation (DGB);07595112423-87;Large (250 or more);Germany;The feedback can be published with your personal information;4 - Important;;5 - Very important;4 - Important;4 - Important;4 - Important;;5 - Very important;4 - Important;4 - Important;3 - Neutral;5 - Very important;4 - Important;;4 - Important;4 - Important;4 - Important;;4 - Important;4 - Important;5 - Very important;5 - Very important;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;Other;"It depends on the definition of ""high Risk"". A European framework should be developed in cooperation with European social partners.";;;For a detailed opinion, please refer to the attached document (Oponion DGB);5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;200519_STN_White_Paper_EU-COM_AI_final_EN.pdf
F529932;11-06-2020 10:26;German;EU Citizen;Birgit;Lohmeyer;;;;;Germany;The feedback can be published with your personal information;5 - Very important;5 - Very important;4 - Important;3 - Neutral;4 - Important;2 - Not important;;5 - Very important;5 - Very important;2 - Not important;2 - Not important;4 - Important;No opinion;;5 - Very important;5 - Very important;3 - Neutral;;2 - Not important;2 - Not important;2 - Not important;3 - Neutral;No opinion;Information der Öffentlichkeit über Entwicklungen;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;No opinion;4 - Important;;There is a need for a new legislation;;No;;;;Medizin, Social Media, Sicherheitsbehörden;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;No opinion;;Yes, for all AI applications;;;
F529931;11-06-2020 10:26;German;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;4 - Important;;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;3 - Neutral;;4 - Important;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;No;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Not at all;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Risks related to the loss of connectivity;;Yes;;Yes;;Yes, for all AI applications;;;
F529930;11-06-2020 10:20;German;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;4 - Important;4 - Important;Schwerpunkt Datensicherheit und Datenschutz;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;;5 - Very important;4 - Important;3 - Neutral;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;4 - Important;;There is a need for a new legislation;;No;;;;Militär;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;;Biometric identification systems should never be allowed in publicly accessible spaces;;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;Yes;;Yes, for all AI applications;;;
F529929;11-06-2020 09:49;English;Business Association;Sanna-Maria;Bertell;;Confederation of Finnish Industries EK;1274604847-34;Medium (< 250 employees);Finland;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;While we need state-of-the-art facilities and research, we need to keep in mind that majority of companies (especially SMEs) will remain users, not innovators of AI. This means that great execution matters, and we should support SMEs adopting and investing early in all technologies improving competitiveness, incl. AI. ;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;Context is everything and therefore the answers are not straightforward - for example, accuracy is heavily dependent on the use case (chat bot vs. automated traffic). Also this is a question about how these are used, and what methods are used / available to mitigate or even negate risks. ;Current legislation may have some gaps;;Other;We remain very critical to plans to risk management via pre-market assessment and authority approvals. Any ex ante regulation could severely impede and stifle innovation and competitiveness in critical sectors. Creating a new governance model market approvals would require deep expertise from different subject matter experts in various fields and generally deep knowledge of different algorithm, data set and AI technologies. ;;;;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;No further guidelines or regulations are needed;Question should be addressed under the GDPR ;Much;Primary focus should be on supporting sandbox-initiatives for certain more sensitive sectors and uses to promote understanding for needed regulations together with developers and users. While voluntary labelling can remain an option,  we need to ensure that any program, even if voluntary, does not create undue cost or administrative burden, especially to SMEs. ;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;Learn from the GDPR´s data protection impact assessment experiences. ;Personal security risks;To note, context is everything. Also, the safety/liability regime should not be expanded to cover novel concepts for AI (mental health) via legislation. Other policy measures should be more suitable for these. ;Yes;;No opinion;Subject to further assessment and evidence based approach. ;Yes, for specific AI applications;Autonomous vehicles;;AI_White_Paper_position_paper_EK_FINAL.pdf
F529928;11-06-2020 09:43;English;Academic/Research Institution;Emilio;Lopez Cano;;Universidad de Castilla-La Mancha;;Large (250 or more);Spain;The feedback can be published with your personal information;No opinion;4 - Important;5 - Very important;4 - Important;5 - Very important;4 - Important;"I would consider two important topics in the ecosystems excellence. Not sure about new ones, but maybe crossing ones: (1) attention to underrepresented groups, specially women; and (2) sustainability in general, covering from energy use to pollution, and of course environment conservation. Both are somehow mentioned, (1) in action A, (2) in action C, but I think they could be more stressed.";5 - Very important;4 - Important;2 - Not important;4 - Important;5 - Very important;5 - Very important;Promote multidisciplinarity across organizations (business, academia, and public sector). We should avoid silos of discipline-closed experts with a short sight. AI cannot be developed just by Computer Scientists, nor just by Statisticians or Mathematicians. AI needs a cooperative approach between them, and also with the aid of Philosophers, lawyers, creative designers, and subject-matter experts. e.g., in public biddings and research calls, the assessment criteria could consider this fact.;3 - Neutral;5 - Very important;5 - Very important;My previous comment applies also here, focused in the Academia. But I keep it there as it is as well very important in companies and public bodies.;4 - Important;4 - Important;5 - Very important;5 - Very important;No opinion;Provide continuous learning to SMEs employees in a way both employer and employee can perceive a real advantage or benefit. For example by encouraging follow-up projects with some mentoring/tutoring by the teaching institution (academic or non-academic). Again, multidisciplinary (trade-off between Computer Science and Statistics/Maths);4 - Important;4 - Important;5 - Very important;2 - Not important;No opinion;2 - Not important;AI may hide (intentionally or not) information, or influence opinions or actions in a given direction. AI can also discourage change if only shows what the majority (or the leaders) are willing to see.;Current legislation may have some gaps;;Yes;;No opinion;;;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;4 - Important;4 - Important;No further guidelines or regulations are needed;;Very much;Individuals should have full control on their data: backup, deletion, modification, by their own means (not just as in GDPR as a right out of their control). Also a standardized and accessible format should be forced, e.g. via a CEN standard.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Standardised formats, if implemented, should be tested.;Risks related to the loss of connectivity;;No opinion;;No opinion;;No opinion;;;
F529927;11-06-2020 09:34;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;4 - Important;;3 - Neutral;1 - Not important at all;1 - Not important at all;"If  governments  use  AI  for  public  functions  (essential services like healthcare, housing, education, social welfare, transport), there must be:
clear, published reasons to justify the use of AI
- scientific evidence that the technology works, and,
- where technology will play a role in determining people’s access to vital services  or to enjoy their fundamental rights and freedoms,  people should have a say in whether or not AI can be acceptably used in a democratic society.";4 - Important;4 - Important;1 - Not important at all;2 - Not important;4 - Important;2 - Not important;The plan should include a section on human rights, societal impacts of AI and automation, and how to ensure democratic oversight for the application of AI systems.;3 - Neutral;4 - Important;1 - Not important at all;"Funding for EU projects on AI should be conditional on meeting EU ethical standards for AI and fundamental rights laws.
-EU funds should immediately stop funding projects posing risks to fundamental rights, such as iBorderCtrl - which aims to use facial and emotion recognition  to detect lies in visa applications, but is not substantiated by scientific evidence and significantly infringes upon human dignity.
- Any projects facilitating mass surveillance should be ceased.";1 - Not important at all;4 - Important;3 - Neutral;3 - Neutral;3 - Neutral;"- Some of the biggest AI-related scandals have involved small companies, and so the EU should ensure that when small businesses to take up AI, whatever the circumstances, they should respect data protection, privacy and other fundamental rights. 
- There ";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;"- Deployment of AI in public services without democratic oversight, transparency or sufficient evidence to justify the need/ purpose.
- Use of opaque, privately-developed technology in the public sphere, which do not meet transparency requirements.
- Cons";Other;"- Current law does not address use of non-personal data for AI, types of data which do not fall under the GDPR
- AI can have huge collective impacts, such as furthering overpolicing, surveillance, in-equalities – all not addressed in existing legal framew";Other;"New rules should clearly outline criteria to determine which AI systems are legal and which are not. Such criteria should be based on proving that they work and are needed, conducting mandatory fundamental rights impact assessment for all applications, and ensuring democratic oversight.
- Uses of AI which breach fundamental rights  - like biometrics/facial recognition for mass surveillance - should be banned outright.";;;Biometric identification systems should never be allowed in publicly accessible spaces.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;"- Their use in public spaces will lead to mass surveillance;
- This will irreversibly limit our fundamental rights to privacy, freedom of assembly, ex-pression, non-discrimination, data protection, dignity and the right to a fair trial, creating societies";Rather not;"Self-labelling systems can be confusing for people and may give a false sense of secu-rity since it is the same company that develops a product saying that it is safe.
- The high/low risk distinction is overly simplistic and could very well allow for loop-holes for systems with potentially significant impacts on peoples’ safety and rights. This is especially so if ‘low risk’ systems are only voluntarily controlled, which is essentially letting big tech companies regulate themselves.";Other enforcement system;All systems should undergo a mandatory ex ante human rights impact assessment from an external body. This safeguards the need to make human rights a priority in the AI regulation, and ensure that there are no loop-holes  just because a system falls into a low-risk category.;Compliance should be external. This guarantees that fundamental rights are protected.  The EU cannot rely on self-regulation for this matter.  ;Mental health risks;AI systems pose inherent potential risks of discrimination. The use of AI in online products and services requires collection and use of data lending toward discrimination in many  fields related to targeted  advertising. This poses risks of differentiated pricing, discrimination and  financial detriments, the risk of creating filter bubbles, interfences in the political process, all based on sensitive inferences or associations. Also, AI may  impact on rights of persons with disabilities.;Yes;Internal  supervisors,  such  as  Data  Protection  Officers  under  GDPR  should  included and and asked for advice.;Yes;AI developers and deployers should be accountable for harm generated by their products,  and  that  products  developed  using  AI  should  not  enjoy  exceptions  to  any  EU  laws,  whether it be discrimination, data protection, or product liability.;Yes, for all AI applications;;The EU should address copyright and database protections which prevent proper oversight of AI applications. Liability rules should provide incentives for openness.;
F529926;11-06-2020 04:31;English;Academic/Research Institution;Antonio;HYDER;;Hackers and Founders Research;;Micro (< 10 employees);Spain;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;;;4 - Important;5 - Very important;4 - Important;4 - Important;3 - Neutral;;3 - Neutral;5 - Very important;4 - Important;3 - Neutral;4 - Important;3 - Neutral;;There is a need for a new legislation;;No opinion;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;4 - Important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);Security Issues for society and prevention of crime;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;;Yes, for specific AI applications;Security, Job Loss, Inequality / wealth distribution;;
F529925;11-06-2020 03:10;English;EU Citizen;Hauke;JENSEN;;;;;Germany;The feedback can be published with your personal information;5 - Very important;4 - Important;5 - Very important;3 - Neutral;4 - Important;2 - Not important;;5 - Very important;5 - Very important;3 - Neutral;2 - Not important;3 - Neutral;2 - Not important;;2 - Not important;5 - Very important;4 - Important;;3 - Neutral;4 - Important;3 - Neutral;4 - Important;3 - Neutral;;2 - Not important;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;4 - Important;;There is a need for a new legislation;;Yes;;No;;"Social media applications don't have a ""high-risk"" in most cases. But badly trained AI might lead to a wide spread discrimination that could undermine the social cohesion of our society.
So I would like to see AI, that work with personal data, added to the ""high-risk"" group.";4 - Important;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;Like mentioned in the white paper, the risk of discrimination due to biased data, is a big risk, that might be hard to handle due to the opacity of the system.;Yes;Since most changes might be small, after some time the scope of the AI might have changed considerably. So in my opinion a (superficial) regular assessments to see if the original scope still fits the actual product is in order.;No opinion;;No opinion;;;
F529924;11-06-2020 01:28;English;Academic/Research Institution;Marc;Idelson;;"Africa Business School
Université Mohammed VI Polytechnique";;Large (250 or more);France;The feedback can be published with your personal information;3 - Neutral;4 - Important;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;Excellence also encompasses ensuring through AI governance that algorithmics doesn't deepen biases already embedded in society but works instead towards narrowing gaps .;4 - Important;5 - Very important;3 - Neutral;4 - Important;5 - Very important;5 - Very important;x;3 - Neutral;5 - Very important;2 - Not important;;4 - Important;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;Ensuring the digital innovation hub adheres to EU principles (notably diversity) in all its processes, including hiring, sourcing and funding .;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;"traceability may be the greatest challenge ; full transparency in training data sets to prevent/remedy biases may be explored";No opinion;;No;;;;;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);All uses of remote or not biometric identification systems should be posted in real time on a publicly available website with a responsive search engine . Each entry should include date, time, place, sponsor, full description of all technologies employed and their IP holders, as well as all direct and indirect recipients of identification data produced .;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;Yes;;Yes, for all AI applications;;;
F529923;11-06-2020 00:41;English;NGO (Non-governmental organisation);Marta;Mizak;;Business & Science Poland;548212735276-89;Small (< 50 employees);Poland;The feedback can be published with your personal information;3 - Neutral;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;Creating easy and safe access to data for companies and researchers;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;Explaining AI mechanism to citizens and business and therefore creating trust in technology and encourage entities to implement new technologies to daily life. Improvement of workforce digital skills.;1 - Not important at all;5 - Very important;5 - Very important;Commercialization of research results should be required for all  projects which are financed by European funds;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;DIHs should understand the industry, use business language, and operate in a smart and agile way. To achieve it DIHs ought to be created by people who have practical attitudes, business backgrounds, and are goal-oriented.;3 - Neutral;4 - Important;4 - Important;2 - Not important;4 - Important;3 - Neutral;We should also be aware that unnecessary blocking of the development of artificial intelligence also creates the risk of delaying the development of technology, economy, and society.;Current legislation may have some gaps;;Yes;;Yes;;Applications that process large amounts of sensitive data, especially in the matter of political views, health conditions, surveillance. Algorithms that control the weapon of mass destruction.;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);Biometric identification systems should be allowed in public spaces only for reasons of substantial public interest;Much;The labelling system should be created with participation of companies whose create AI systems and have practical experience in that matter. We believe that also high-risk applications should be the subject of voluntary labelling.;No opinion;;Ex-ante analysis should check if the product create the significant risk for users. However, the procedure must be clear, simple and do not create unnecessary burdens for producers. We believe that low-risk applications should be excluded from the ex-ante analysis. SMEs should have assistance form regulatory bodies in order to go through the assessment process smoothly. ;Mental health risks;Risks emerging from possessing too much sensitive data by one entity, which is not trustworthy. ;No;;No;;Yes, for specific AI applications;;;BSP_contribution_AI.pdf
F529922;11-06-2020 00:25;German;;;;;;;;;The feedback can be published in an anonymous way;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;1 - Not important at all;;3 - Neutral;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;;2 - Not important;2 - Not important;2 - Not important;;No opinion;No opinion;No opinion;No opinion;No opinion;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;Current legislation may have some gaps;;Yes;;Yes;;;No opinion;3 - Neutral;No opinion;No opinion;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Rather not;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F529921;11-06-2020 00:19;German;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;2 - Not important;;4 - Important;5 - Very important;2 - Not important;1 - Not important at all;4 - Important;4 - Important;;3 - Neutral;4 - Important;3 - Neutral;;3 - Neutral;5 - Very important;5 - Very important;3 - Neutral;2 - Not important;;4 - Important;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;4 - Important;;Current legislation may have some gaps;;No;;;;;3 - Neutral;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;;;Yes;;Yes;;No opinion;;;
F529920;11-06-2020 00:04;German;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;4 - Important;5 - Very important;4 - Important;1 - Not important at all;1 - Not important at all;KI darf nicht zum Selbstzweck geförtert werden. Einem Einsatz von KI muss stets eine ausführliche Begründung sowie eine Abwägung von alternativen Möglichkeiten vorausgegangen sein. Auch muss nachgewiesen werden, dass die eingesetzte KI ihren Zweck erfüllt und eine ausreichend große Genauigkeit erzielt. Zusätzlich muss untersucht werden, ob der Einsatz der KI möglicherweise Personen benachteiligen könnte, um Diskriminierung entgegenzuwirken.;5 - Very important;4 - Important;1 - Not important at all;1 - Not important at all;4 - Important;1 - Not important at all;"Viel wichtiger als die oben genannten Bereiche in einem koordinierten Plan wären beispielsweise Kriterien, nach denen die EU ihre Ressourcen für KI aufteilt (welche Projekte werden unterstützt, welche nicht) und Anforderungen an unterstützte Projekte. Außerdem sollte festgehalten werden, wie mit moralischen, ethischen, sozialen und juristischen Implikationen von KI umgegangen werden soll, insb. auch die Frage nach der ""Haftbarkeit"" im Falle von durch eine KI verursachten Fehlern.";4 - Important;4 - Important;1 - Not important at all;KI-Forschung beschränkt sich längst nicht mehr nur auf die (Weiter-)entwicklung von KI-Methoden und -Implementierungen. Es fehlt an Untersuchungen zum gesellschaftlichen, sozialen, moralischen und ethischen Einfluss von KI. KI birgt große Risiken für Diskriminierung, Desinformation, Tranzparenzmangel uvm. Es entsteht ein Eindruck, die Politik und Gesellschaft wird von der rasanten Entwicklung überrumpelt und ist nicht in der Lage, diese zu regulieren um Risiken zu vermeiden.;1 - Not important at all;4 - Important;4 - Important;1 - Not important at all;2 - Not important;Die Verbündung mit dem privaten Sektor ist stets mit Vorsicht zu genießen. Private Institutionen haben eigene Interessen am Einsatz von KI. Beispielsweise lässt sich mit durch KI gesammelten, verarbeiteten und aufbereiteten Daten oder mit durch KI zielgerichtet ausgeschütteter Werbung viel Geld verdienen. Es muss verhindert werden, dass der Einsatz von KI im privaten Sektor Rechte wie die Privatsphäre oder informationelle Selbstbestimmung der Bürger beeinträchtigt werden.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;KI verletzt das Recht auf informationelle Selbstbestimmung. Eine Person muss sich für oder gegen den Einfluss von KI auf das eigene Leben entscheiden können. Sobald KI allerdings an wichtige Bereiche des alltäglichen Lebens gekoppelt wird, z.B. Bildung (insb. Hochschulbildung), Sozialversicherung, Gesundheitsversorgung etc., und Entscheidungen über Individuen auf Basis von KI gefällt werden, ist es nicht mehr möglich, sich dem Einfluss von KI auf das eigene Leben zu entziehen.;Other;Es besteht ein großer Bedarf an Rechtsvorschriften zur Regulierung von KI. Die Haftbarkeit, die Beurteilung von Schadensersatzansprüchen, der Datenschutz insb. bei Verwendung von nicht-personenbezogenen Daten (großen Datensätzen) oder der Umgang mit Überwachung ist unzureichend geklärt. Außerdem entstehen durch KI neue Arten der Diskriminierung auf Basis des Finanzstatus und anderen Grundlagen, die nicht durch Anti-Diskriminierungsgesetze gedeckt werden.;Other;"Natürlich nicht. Auflagen für den Einsatz von KI sind unabhängig von einer möglichen Risikobewertung einzuführen. Es ist nicht eindeutig definierbar, welche Awendungen ein hohes und welche ein niedriges Risiko bergen. Beispielsweise ist der Medizinsektor für mich ein klares Beispiel eines Hochrisikobereiches entgegen der Einschätzung der EU, da hier über Menschenleben entschieden wird. Der legale Einsatz von KI muss klar definiert sein, ohne Ausnahmen für Anwendungen mit ""niedrigem Risiko"". ";;;"Zu ""hochriskanten"" Einsatzgebieten von KI gehören beispielsweise ""Predictive Policing"", autonome Waffensteuerung, Zugang zu oder Verteilung von öffentlichen Ressourcen wie Bildung, Sozialhilfe, Gesundheitsversorgung etc. oder die Massenüberwachung, insb. Verarbeitung von Biometriedaten. All diese Einsatzzwecke sind unvereinbar mit EU-Grundrechten und müssen daher verboten oder strengstens reguliert werden. Sie bergen höchste Risiken, da sie fatalen Einfluss auf das Leben einzelner haben können.";4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;Der Einsatz von biometrischen Identifikationssystemen in öffentlichen Räumen impliziert eine Massenüberwachung, welche Grundrechte, Rechte auf Freiheit, Diskriminierungsfreiheit, Datenschutz, informationelle Selbstbestimmung und die freie Entwicklung zu verletzen droht und einen Generalverdacht in der Gesellschaft fördert.;Rather not;"Freiwillige Kennzeichnungssysteme bergen das Risiko von falscher Sicherheit, da es an unabhängigen Kontrollen fehlt. Schließlich ist die Partei, die die Sicherheit eines Systems attestiert dieselbe Partei, die dieses entwickelt und an dessen Einsatz interessiert ist. Auch ist die Unterscheidung zwischen ""Hoch- und Niedrigrisikosystemen"" zu vereinfachend, insbesondere Spätfolgen können schwer abgeschätzt werden.";Other enforcement system;"Alle in der EU eingesetzten KI-Systeme müssen einer verpflichtenden Untersuchung hinsichtlich der Konformität mit Grundrechten und Menschenrechten unterzogen werden, insbesondere hinsichtlich Diskriminierung und Datenschutz. Es darf keine Schlupflöcher basierend auf einer Bewertung als ""niedriges Risiko"" geben.";Eine Überprüfung muss stets durch eine externe Drittpartei erfolgen. Eine unabhängige, unvoreingenommene Bewertung ist durch Selbstregulierung nicht zu erreichen, da hier stets die eigenen Interessen der Institutionen involviert sind und keine neutrale Bewertung erfolgt.;Mental health risks;KI ist häufig mit der Erhebung, Konsolidierung und Verarbeitung von großen Datenmengen verbunden. Überall dort, wo große Datenmengen anfallen, gibt es auch ein erhebliches Datenschutzrisiko - einmal durch den planmäßigen Einsatz der Systeme, aber auch durch ungeplante Zwischenfälle wie Sicherheitsvorfälle. KI birgt also auch Sicherheitsrisiken. Ebenfalls müssen Diskriminierungsrisiken analysiert, klar definiert und reguliert werden.;Yes;Der Einsatz von KI führt neue Komplexität und Intransparenz im Vergleich zu bestehenden technologischen Systemen ein. Risikobewertungsverfahren und Kennzeichnungssysteme werden durch den Einsatz von KI noch viel wichtiger als bisher. Die Ergebnisse eines Risikobewertungsverfahrens müssen sowohl vollständig öffentlich publiziert werden, als auch in einfacher Sprache zusammengefasst werden, um die Implikationen durch den Einsatz von KI auch für Laien verständlich zu machen.;Yes;Es ist wichtig, dass Instanzen, die KI-Systeme entwickeln und einsetzen, für deren Fehler haftbar gemacht werden können. Nur so kann sichergestellt werden, dass die Herausgeber von KI-Systemen ein eigenes Interesse an deren Sicherheit, Korrektheit und Konformität mit geltenden Bestimmungen haben. Auch Spätfolgen müssen berücksichtigt werden, es ist bei Entwicklung oder Inbetriebnahme eines KI-Systems nicht absehbar, wie lange es in dieser Form eingesetzt werden wird.;Yes, for all AI applications;;;
F529919;10-06-2020 23:59;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;4 - Important;3 - Neutral;1 - Not important at all;1 - Not important at all;1 - Not important at all;If governments  use  AI  for  public  functions  (determining people’s access to vital services  or to enjoy their fundamental rights and freedoms), there must be clear, published reasons to justify use of AI, scientific evidence that the technology works, and people should have a say in whether or not AI can be acceptably used in a democratic society.;4 - Important;4 - Important;1 - Not important at all;1 - Not important at all;4 - Important;2 - Not important;The coordinated plan on AI is updated to include criteria (scientific and policy) about how the EU will allocate its resources of AI. The plan should include a section on human rights, societal impacts of AI and automation, and how to ensure democratic oversight for the application of AI systems.;3 - Neutral;4 - Important;1 - Not important at all;"- Funding for projects on AI should be conditional on meeting EU ethical standards for AI and fundamental rights laws.
- EU funds should comply and immediately stop funding projects which pose a risk to fundamental rights, such as iBorderCtrl - which aims";1 - Not important at all;4 - Important;3 - Neutral;3 - Neutral;1 - Not important at all;The EU should ensure that when small businesses take up AI, whatever the circumstances, they should respect data protection, privacy and other fundamental rights. There should be no blanket exemptions to fundamental rights standards.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;"Many concerns, examples include:
- The deployment of AI in sensitive areas (public services) without democratic oversight, transparency or sufficient evidence to justify the need/ purpose.
- The conscious avoidance of liability for harms produced by AI technology.
- AI use in policing, recruitment, healthcare conflicts with rights of minority groups
- AI use in surveillance, targeting and profiling people conflicts with basic human and privacy rights.";Other;"It is important that GDPR must be reinforced, not undermined by any AI regulation. Therefore, AI regulation should assure not to provide loop-holes to data protection legislation, or discrimination law.
- AI can have huge collective impacts, such as furthering overpolicing, surveillance, in-equalities – all not addressed in existing legal frameworks but are still major issues
- AI can lead to discrimination on financial status and other grounds usually not protected in discrimination law";Other;- New rules should clearly outline criteria to determine which AI systems are legal and which are not. Such criteria should be based on proving that they work and are needed, conducting mandatory fundamental rights impact assessment for all applications, ;;;"I consider the following uses of AI incompatible with human rights, which therefore should be banned:
- The use of AI to determine delivery of essential public services,
- predictive policing
- autonomous lethal weapons
- identification/ analysis of emotion and identity traits,
- indiscriminate biometric surveillance.";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;"- Their use in public spaces will lead to mass surveillance;
- This will irreversibly limit our fundamental rights to privacy, freedom of assembly, ex-pression, non-discrimination, data protection, dignity and the right to a fair trial, creat-ing societie";Rather not;"Self-labelling systems can be confusing for people and may give a false sense of security since the same company that develops a product declares that it is safe.
- The high/low risk distinction is overly simplistic and could very well allow for loop-holes for systems with potentially very significant impacts on peoples’ safety and rights. This is especially so if ‘low risk’ systems are only voluntarily controlled, this is essentially asking us to let big tech companies regulate themselves.";Other enforcement system;"All systems should undergo a mandatory ex ante human rights impact assessment from an external body.
This way the EU can make human rights a priority in the AI regulation, and ensure that there are no loop-holes  just because a system falls into a low-risk category.";Compliance assessment should be external. Only this can guarantee fundamental rights are protected, since one cannot rely on self-regulation of actors with commercial interests for this. ;Mental health risks;"There are inherent, potential risks of discrimination posed by AI systems. 
In particular, the use of AI in online products and services requires collection and  use  of  data  lending  toward  discrimination  in  many  fields  related  to  targeted  advertising.  This poses risks of differentiated pricing, discrimination and  financial detriments, the risk of creating filter bubbles, interfences in the political process, all based on sensitive inferences or associations.";Yes;Internal  supervisors,  such  as  Data  Protection  Officers  under  GDPR  should  be included and asked for advice.;Yes;AI developers and deployers should be accountable for harm generated by their products,  and  that  products  developed  using  AI  should  not  enjoy  exceptions  to  any  EU  laws,  whether it be discrimination, data protection, or product liability.;Yes, for all AI applications;;The EU should address copyright and database protections which prevent proper oversight of AI applications. Liability rules should provide incentives for openness.;
F529918;10-06-2020 22:42;German;;;;;;;;;The feedback can be published in an anonymous way;3 - Neutral;2 - Not important;4 - Important;2 - Not important;1 - Not important at all;1 - Not important at all;;3 - Neutral;3 - Neutral;1 - Not important at all;2 - Not important;4 - Important;1 - Not important at all;;2 - Not important;No opinion;1 - Not important at all;;1 - Not important at all;No opinion;No opinion;2 - Not important;1 - Not important at all;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;"Anwendungen der KI wie Gesichtserkennung kann es Regierungen erlauben einen Überwachungsstaat zu errichten
Durch den Einsatz von KI in Polizeibehörden können leicht Unschuldige ins Visier geraten";There is a need for a new legislation;;No;;;;"Gesichtserkennung
Einsatz in Sozialämter und Arbeitsämter
Einsatz in Polizeibehörden
Zuhilfenahme von KI in Strafprozessen
Jegliche Bewertung von Personen mit KI";5 - Very important;No opinion;5 - Very important;4 - Important;5 - Very important;4 - Important;Biometric identification systems should never be allowed in publicly accessible spaces;;No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Transparenz ? Veröffentlichungspflicht von Algorithmen, Trainingsdaten, Testergebnissen, Gutachten, usw.;Mental health risks;"Möglicher Druck/Zwang sich der Bewertung von KI (z. B. bei Kreditscoring) zu unterwerfen
Mangelndes Wissen von Kunden über den Einsatz von KI in bestimmten Produkten";Yes;;No opinion;;No opinion;;;
F529917;10-06-2020 22:39;German;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;4 - Important;5 - Very important;1 - Not important at all;3 - Neutral;5 - Very important;Förderung von Open Source (Bibliotheken, Entwicklern);5 - Very important;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;Förderung von Open Source Software;5 - Very important;;1 - Not important at all;Förderung von Open Source Projekten wie Biblliotheken für Machine Learning und Deep Learning;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Fokus auf Open Source;1 - Not important at all;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Bias (Benachteiligung von Geschlecht, Herkunft und bestimmten Minderheiten);There is a need for a new legislation;;Yes;;Yes;;Finanzen und Gesundheit;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;4 - Important;Biometric identification systems should never be allowed in publicly accessible spaces;Biometrische Identifikationssysteme sind nicht zuverlässig (false positive) und anfällig gegenüber Adversarial Attacks.;Very much;Richtlinienkatalog von unabhängigen Organisationen, wie Beispielse die Corana-App-Richtlinien des Chaos Computer Clubs (10 Prüfsteine für die Beurteilung von „Contact Tracing“-Apps);Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;Hinzuziehen von unabhängigen Organisationen (z,B. CCC, Digitale Gesellschaft, Gesellschaft für Informatik);Personal security risks;;Yes;;Yes;;Yes, for all AI applications;;;
F529916;10-06-2020 21:38;Spanish;;;;;;;;;The feedback can be published in an anonymous way;3 - Neutral;5 - Very important;;5 - Very important;3 - Neutral;5 - Very important;los gobiernos regionales deberían de ser tenidos en cuenta en las acciones. Por otra parte, el elemento lingüístico es muy importante al trabajar y no solo el de las lenguas oficiales de la Unión, las otras lenguas europeas deben de tenerse en cuenta.;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;;4 - Important;5 - Very important;3 - Neutral;;4 - Important;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;;4 - Important;5 - Very important;5 - Very important;No opinion;;4 - Important;;Current legislation may have some gaps;;No;;;;;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;No opinion;;No opinion;;No opinion;;;
F529915;10-06-2020 21:19;German;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;4 - Important;5 - Very important;No opinion;3 - Neutral;1 - Not important at all;;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;;1 - Not important at all;1 - Not important at all;1 - Not important at all;;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;No;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;Je mehr IT, desto weniger ist unsere Privatsphäre sicher.;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F529914;10-06-2020 21:14;English;EU Citizen;Fieke;Jansen;;;;;Netherlands;The feedback can be published with your personal information;4 - Important;4 - Important;4 - Important;2 - Not important;2 - Not important;1 - Not important at all;Dedicate EU research budget to understanding the negative impact of AI on climate change and human rights and support the development of privacy-, human rights-, and environment- by design AI. Rapid AI uptake by the public sector is dangerous, research shows that data-driven systems in the public sector disproportionately harm the most vulnerable in society. Time, caution, deliberation and oversight is needed before deciding where, if at all, AI should be applied to the public sector.;4 - Important;No opinion;2 - Not important;1 - Not important at all;4 - Important;4 - Important;The GDPR has shown that enforcement is the Achilles heel of the EU. Stricter enforcement and coordination between existing regulatory authorities is needed to ensure the safeguarding of fundamental right, and upholding specific regulation, directives and policies on anti-discrimination, GDPR, the law enforcement directive and the climate goals. Enforcement should apply to the entire AI supply chain, which includes EU R&D funding mechanisms. See our document attached for more detail. ;2 - Not important;4 - Important;3 - Neutral;One new lighthouse research centre will increase inter EU competition for scarce knowledge, instead, the EU should invest in existing public research centres across disciplines, allowing long term research and sustainable career trajectories. Inclusion is more than skills and women. The tech industry ‘bro culture’ has created a hostile environment for women and racialized communities, which has resulted in an exodus of them in the tech labour force. See our document attached for more detail ;;;;;;;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;The context which drives the use of AI should also be considered a risk. Risks do not only relate to the system in ‘isolation’, AI optimized for ‘fairness’ can be punitive and discriminate by its focus and its use. There is no mention of the environmental harm of the AI supply chain. Nor is there a distinction between the risk to the individual and to communities, nor a proposition on how to safeguards the later. See our document attached for more detail;Other;To answer this question the EU first needs to clearly define what is new with AI and where old legislation falls short. Secondly, the problem is effective enforcement of existing legislation on AI systems, this requires new skills, knowledge, and coordination between enforcement bodies across topics and jurisdictions. Also, there is a need to increase avenues for justice for all. See our document attached for more detail.;No;;;;The dichotomy of high risk and low risk is a false one. The two variables offered by the EU are 1) not sufficient for this determination and 2) too limited in scope and depth. It offers no avenue for saying certain AI systems are too harmful and should be banned. Nor provides an understanding that AI systems that are classified as low risk can be high risk to certain minorities, or in specific use cases, or are vulnerable to exploitation. See our document attached for more detail;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;The use of biometric technologies for untargeted mass processing of personal data in public space and communication infrastructures is a violation of our fundamental human rights. We, therefore, support the call of the European digital rights movements to ban the use of biometric technologies in public and semi-public spaces, until the impact and harms are fully understood and there are clear legal frameworks that guide them. See our document attached for more detail.;No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;All 1) ex-ante impact assessment and ex-post evaluation on harm to human rights and environment 2) high scrutiny of AI systems implemented by actors on which the public depends, i.e. public sector and specific private actors 3) mandatory publishing of datasheets and statistics. See our document attached for more detail. ;;Environment impact AI systems through out the supply chain, from R&D funding, research, testing to deployment;No opinion;;No opinion;;No opinion;;;Submission_to_AI_WP_Fieke_Jansen.pdf
F529913;10-06-2020 21:11;German;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;2 - Not important;;5 - Very important;4 - Important;3 - Neutral;5 - Very important;4 - Important;4 - Important;;4 - Important;5 - Very important;3 - Neutral;;4 - Important;4 - Important;5 - Very important;3 - Neutral;4 - Important;;3 - Neutral;2 - Not important;4 - Important;5 - Very important;2 - Not important;4 - Important;;Current legislation may have some gaps;;Yes;;Yes;;;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Much;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Risks related to the loss of connectivity;;Yes;;Yes;;Yes, for all AI applications;;;
F529912;10-06-2020 20:51;English;EU Citizen;Thomas P.;Buehner;;;;;Germany;The feedback can be published with your personal information;3 - Neutral;4 - Important;4 - Important;4 - Important;3 - Neutral;3 - Neutral;;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;4 - Important;3 - Neutral;;4 - Important;4 - Important;4 - Important;;3 - Neutral;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;;2 - Not important;5 - Very important;4 - Important;4 - Important;3 - Neutral;2 - Not important;;Current legislation may have some gaps;;No;;;;;4 - Important;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Personal security risks;;Yes;;Yes;;Yes, for all AI applications;;;
F529911;10-06-2020 20:26;English;;;;;;;;;The feedback can be published in an anonymous way;;1 - Not important at all;;5 - Very important;1 - Not important at all;1 - Not important at all;"There must be:
1. clear, published reasons to justify the use of AI
2. scientific evidence that the technology works, and,
3. particularly where the technology will play an important role in determining people’s access to vital services  or to enjoy their fundamental rights and freedoms,  people should have a say in whether or not AI can be acceptably used in a democratic society.";1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;I propose that the coordinated plan on AI is updated to include criteria (scientific and policy) about how the EU will allocate its resources of AI. The plan should include a section on human rights, societal impacts of AI and automation, and how to ensure democratic oversight for the application of AI systems;2 - Not important;2 - Not important;1 - Not important at all;"Funding for EU projects on AI should be conditional on meeting the EU’s own ethical standards for AI and fundamental rights laws.
EU funds, i.e. Horizon2020 should comply and immediately stop funding for projects which pose a risk to fundamental rights, such as iBorderCtrl - which aims to use facial and emotion recognition technology to supposedly detect lies in the course of visa applications, but is not substantiated by scientific evidence and significantly infringes upon human dignity.";1 - Not important at all;;;1 - Not important at all;1 - Not important at all;Small businesses should not enjoy any exemptions to protecting human rights. Some of the biggest AI-related scandals have involved small companies, and so the EU should ensure that when small businesses to take up AI, whatever the circumstances, they should respect data protection, privacy and other fundamental rights. There should be no blanket exemptions to fundamental rights standards.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"The deployment of AI in sensitive areas (public services) without democratic oversight, transparency or sufficient evidence to justify the need/ purpose.
The conscious avoidance of liability for harms produced by AI technology.
Companies and governments using excuse of ‘innovation’  to justify trials without safe-guards.";Other;"AI can have huge collective impacts, such as furthering overpolicing, surveillance, in-equalities.
Current law does not address use of non-personal data for AI, types of data which do not fall under the GDPR.";Other;"Uses of AI which breach fundamental rights  - like biometrics/ facial recognition for mass surveillance or ""crime prediction"" - should be banned outright.";;;"Particularly public services, such as healthcare, policing, social welfare should all be considered very high risks. Also, large companies that offer AI services should be considered high-risk too. Identification/analysis of emotion and identity traits should be banned outright.
Determining ‘risk’ should be rights and outcomes focused, not according to the sector. ";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;"Use in public spaces will lead to mass surveillance (see ""1984""), will irreversibly limit fundamental rights to privacy, freedom of assembly, expression, data protection, dignity and the right to a fair trial, creat-ing societies of suspicion; and, even uses which do not contribute directly or indirectly to mass surveillance in public spaces still pose significant threats to privacy, data protection, non-discrimination, and dignity.";Rather not;"Self-labelling systems can be confusing for people and may give a false sense of security since it is the same company that develops a product the one saying that it is safe.
In addition, there is a concern about how those harmed by these low-risk systems might seek redress in such a scenario.";Other enforcement system;All systems should undergo a mandatory ex ante human rights impact assessment from an external body.;Compliance should be external. We need this to guarantee fundamental rights are protected, we cannot rely on self-regulation for this.;Mental health risks;Targeted media outlet articles to fit a narrative should be prohibited.;Yes;Internal supervisors, such as Data Protection Officers under GDPR should be included and and asked for advice;Yes;AI developers and deployers should be accountable for harm generated by their products,  and  that  products  developed  using  AI  should  not  enjoy  exceptions  to  any  EU  laws,  whether it be discrimination, data protection, or product liability.;Yes, for all AI applications;;The EU should address copyright and database protections which prevent proper oversight of AI applications. Liability rules should provide incentives for openness.;
F529910;10-06-2020 20:07;English;Public authority;roberto;fraile;Local;Alcobendas City Hall;;Large (250 or more);Spain;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;;4 - Important;3 - Neutral;5 - Very important;;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;5 - Very important;3 - Neutral;4 - Important;2 - Not important;2 - Not important;5 - Very important;;Current legislation may have some gaps;;Yes;;Yes;;critical aplications where security problems could have impact in public services like traffic lights, or cause victims due to invalid decisions (AI system taking inaccurate decisions);5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;Yes;;Yes, for specific AI applications;;;
F529909;10-06-2020 19:29;French;Business Association;Nicolas;GAUBERT;;Fédération nationale des Travaux Publics (FNTP);30032231266-82;Medium (< 250 employees);France;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;La mise en place d'un écosystème de confiance est importante. Les entreprises doivent s'assurer que leurs données sont garanties sans limitation de durée et qu'elles en ont la propriété. Dans l'hypothèse où elles sont utilisées par des tiers, elles doivent être anonymes. ;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;"Comme le souligne le Livre blanc sur l'intelligence artificielle (Point 4.D ""Focus on SMEs""), il est particulièrement important que les PME puissent accéder et utiliser l'intelligence artificielle. Une révision du plan coordonné dans le domaine de l'IA doit tenir compte de cet aspect important.";4 - Important;5 - Very important;5 - Very important;Le renforcement des structures public-privé dans l'Intelligence artificielle est essentiel pour renforcer la communauté de la recherche et de l’innovation.;5 - Very important;4 - Important;5 - Very important;5 - Very important;4 - Important;;No opinion;5 - Very important;5 - Very important;4 - Important;4 - Important;No opinion;"Les formulations ""L'IA peut compromettre la sécurité"" et ""L'IA n'est pas toujours exacte"" sont maladroites. L'IA n'est qu'un outil.";Current legislation may have some gaps;;No opinion;;;;Contrôle humain.;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;No opinion;;No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;Les procédures d'évaluation des risques pourraient comprendre un mélange de certification et de règlementation, à mener avec souplesse et rigidité.;No opinion;;No opinion;;;
F529908;10-06-2020 18:48;English;Business Association;Anne-Sarah;Skrebers;;Union Européenne de Radio-Télévision (UER) / European Broadcasting Union (EBU);;Large (250 or more);Switzerland;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;4 - Important;5 - Very important;"The use of AI technology offers new opportunities for the media sector. Public service media (PSM) plays an important role in developing AI tools to reach their audience and to ensure that they fulfil their remit. PSM must ensure that their activities are not affected by AI technologies used by other players they interact with.
PSM can play a crucial role in Artificial Intelligence literacy. They must take part in EU research and development initiatives and have access to EU funding programmes.";5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;EBU Members have identified a need to establish shared European infrastructures that would support streamlining currently scattered efforts across the continent. These would include for instance European data centres and cloud computing infrastructures. In parallel to the “Coordinated Plan on AI”, an ambitious EU data strategy should reduce the dependency of European media organizations on AI technologies developed by increasingly powerful and mostly non-European third parties.;4 - Important;4 - Important;4 - Important;"Partnerships between public and private sectors could further the research and the development of trustworthy and ethical AI solutions in Europe. These should be subject to conditions that allow a balanced cooperation between all actors with regard to the protection and sharing of AI outcome and ingested data.
Data pooling initiatives would benefit from regulatory measures compelling platforms to grant PSM access to audience data regarding their content and services offered on such platforms.";4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;;4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;AI technologies rely on data to build efficient and innovative applications. It is crucial that PSM have access to data related to the use of their content and services when offered on third-party platforms. This is essential notably to better meet audience expectations and fulfil their remit of universality and diversity, and to promote competition in data-driven markets. With this in mind, PSM must also have the means to hire, train and retain AI-skilled people for their related activities.;Current legislation may have some gaps;;Yes;;Yes;;AI applications that violate or undermine European fundamental rights standards, in particular the right to freedom of expression, the right to information and the right to privacy and protection of personal data, are of great concern. Any usage of AI that would result in a violation of these rights would be particularly detrimental if it affected the media sector, given the essential role media play in opinion-forming and in freedom of expression, both of which are essential in democracies.;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;No opinion;;Much;Since not all ‘no-high risk’ applications share the same ethical and trustworthiness values, the development of voluntary labelling would ensure that these applications would be subject to mandatory requirements that overpass the algorithmic level. Standardised labels would enlighten the ethical and trustworthy aspects of AI systems and ensure that they are under human oversight.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;;;;;;;;;;
F529907;10-06-2020 18:45;German;Company/Business organisation;Daniel;ROGOWSKI;;Daniel Rogowski: Software, Coaching, Training;;Micro (< 10 employees);Germany;The feedback can be published with your personal information;3 - Neutral;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;;2 - Not important;5 - Very important;4 - Important;;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;5 - Very important;2 - Not important;1 - Not important at all;;There is a need for a new legislation;;No;;;;Militär, Geheimdienste, Polizeien: Entscheidungen über den Einsatz von Waffen, Überwachung;1 - Not important at all;5 - Very important;5 - Very important;2 - Not important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F529906;10-06-2020 18:41;English;Company/Business organisation;Margareta;Chesaru;;UiPath;360376737899-76;Large (250 or more);Romania;The feedback can be published with your personal information;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;A harmonized global approach to AI regulation will benefit trade and the economy. In the future of work, upskilling and re-skilling schemes should be prioritized accordingly. The “Adopt AI programme” should include key digital technologies (e.g. big data analytics, predictive analytics, Robotic Process Automation), as such technologies accelerate and enable AI adoption. It is also important to improve how services are delivered within organisations, along with the adoption of new technologies.;4 - Important;3 - Neutral;5 - Very important;3 - Neutral;5 - Very important;4 - Important;We recommend broadening the scope of the European AI Alliance, to act as a platform that can help promote the uptake of AI by businesses and the public sector. We also encourage a close cooperation between the EC and the MS on their national AI strategies to achieve a harmonized approach to AI. Existing funding EU streams (e.g. ERDF, Cohesion Fund) should also be leveraged to support the uptake of AI by the public sector. The Tallinn Ministerial Declaration should be reinforced and updated.;5 - Very important;4 - Important;4 - Important;Partnerships between academic institutions and the private sector are very important. Establishing a lighthouse research centre would create a common pool of expertise and can help the EU drive more ambitious projects. The centre should have a robust organization and governance structure - e.g. Mila research institute (Montreal, Quebec). The EU also needs to leverage its already existing network of existing AI research excellence centres.  ;4 - Important;4 - Important;5 - Very important;4 - Important;3 - Neutral;In order to help spread digital innovation, foster AI skills development and promote the uptake of AI within SMEs, the networks that are already established under Enterprise Europe Networks (EEN) should be in close connection with DIHs. DIHs should work closely with venture capital firms to identify future tech talent and secure funding for start-ups. ;3 - Neutral;3 - Neutral;4 - Important;4 - Important;3 - Neutral;3 - Neutral;The EC needs to provide a clear and concise definition of AI. A broad definition of AI may lead to considering numerous technologies as AI, which creates uncertainty. To avoid over-regulation, EC should propose a narrow definition of AI systems. AI alone is unlikely to cause harm or damage if trained and designed in accordance with applicable legislation and fundamental rights. On “accuracy”, it is important to highlight that AI does not pretend to be accurate, as it gives confidence intervals.;Current legislation may have some gaps;;Yes;;Other;We recommend a prudent and gradual approach to AI regulation. Risk assessment should be based on the probability and severity of an adverse outcome in an AI use case. References to “immaterial damages” should be removed, as this is a vague term and creates legal uncertainty. By leveraging the expertise of an EU AI Agency, the EU could work with the industry and academia on developing a risk grading system for high-risk AI based on quantifiable and predictable risks. ;;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);Biometric identification systems should be allowed in certain, exceptional cases, but more guidance is needed to ensure safety and privacy. ;Rather not;Any possible voluntary labelling mechanism should be implemented without creating room or giving rise to discrimination among products that are placed on the market and without, directly or indirectly, setting barriers to trade (including in public procurement bids) or hindering innovation. A harmonized approach across the EU with regards to potential voluntary labelling systems would be extremely important for maintaining a strong single market and ensuring a healthy free movement of goods.;Other enforcement system;We recommend taking a prudent and gradual approach with regards to regulation and encourage more pilot projects and testing programmes. The proposed EU Agency for AI could work closely with developers and deployers, roll out early adopter programmes for high-risk AI applications, develop guidelines and codes of conduct to test the efficiency of possible compliance frameworks.;Due to the characteristics of AI systems (including its ability to improve and self-learn), we believe that prior conformity assessments can have a negative impact on innovation and can block a timely uptake of AI technologies. AI technologies are very different to other products where prior conformity assessments and ex-post market surveillance tools are being used so that any replication of the legal framework should be done mutatis mutandis while keeping in mind the uniqueness of AI.;;The current broad concept of product safety is sufficient, as (i) the existing product safety framework is technology neutral and rationale should not be amended and (ii) impact of new technologies on specific products are already subject to subsequent legislative frameworks, such as: medical devices or cars. ;No;AI is embedded in various products and services. Each of these products and services is already operating today on well-established safety frameworks. Legislating AI specifically will be elusive, too difficult, and a potential barrier to innovation. We do not see the need to amend the safety legislation at the source, but rather at the usage and output levels.;No;The current concept of liability in the Product Liability Directive is adequate and remains fit for its purpose. We do not support the idea of broadening the definition of “product” in the Product Liability Directive to include software. It is hard to imagine a standalone software creating damage in the form of death, personal injuries, or property damage. Such results are usually the result of human misconduct or faulty hardware.;No opinion;;AI algorithms are oftentimes industry agnostic. In such cases, any potential risks, and the underlying liability for potential harms, depend strongly on the AI use case. It is therefore impossible to define a general and holistic liability regime applicable to AI technologies. Given the complexity and characteristics of AI technologies, we believe that contractual liability should be maintained in the B2B space.  ;
F529905;10-06-2020 18:27;German;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;4 - Important;5 - Very important;3 - Neutral;5 - Very important;1 - Not important at all;Prüfung von Technologien, die alternativ oder sinnvoller eingesetzt werden können. Priorität!;2 - Not important;5 - Very important;1 - Not important at all;1 - Not important at all;2 - Not important;;;3 - Neutral;2 - Not important;1 - Not important at all;Überwachung der entsprechenden Gemeinschaften um potenziellen Missbrauch zu verhindern und um potentielle Nachteile der Technologie zu erkennen.;3 - Neutral;4 - Important;3 - Neutral;1 - Not important at all;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Bei KI, im besondere bei  selbstlernenden Systemen, kann nach kürzester Zeit nicht mehr nachvollzogen werden, wie welche Entscheidungen getroffen werden. Insbesondere können auf Programmierfehlern beruhende Entscheidungen schweren Schaden anrichten.;There is a need for a new legislation;;No;;;;"Da alle KI-Systeme von Personen originär programmiert werden müssen und ein Softwarecode prinzipiell nicht abschliessend auf Fehlerfreiheit überprüft und getestet werden kann, sind alle KI-Systeme ""hochriskant"", weil die Auswirkung der auf Fehlern beruhenden Entscheidungen alle Bereich massiv beeinträchtigen können.";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;Jede Entscheidung, die durch KI getroffen wird und die eine Auswirkung auf die gesellschaft im Ganzen oder eine juristische oder natürliche Person hat, muss mit ihrem Zustandekommen offengelegt und bekanntgegeben werden.;
F529904;10-06-2020 18:23;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;We should avoid excluding contributing groups not meeting some formal requirements. This world is so huge that it would be counter-productive using rigid limitations. At the same time research excellence should be there.;4 - Important;4 - Important;5 - Very important;4 - Important;5 - Very important;3 - Neutral;;4 - Important;5 - Very important;No opinion;Measuring excellence scares me a bit leaving apart some centers for not having some formal feature;4 - Important;5 - Very important;5 - Very important;5 - Very important;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;It is important a deep training for developers in order to avoid those risks;There is a need for a new legislation;;Yes;;No opinion;;;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;5 - Very important;No opinion;;No opinion;;No opinion;;;Mental health risks;;Yes;;Yes;;No opinion;;;
F529903;10-06-2020 18:03;German;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;4 - Important;5 - Very important;5 - Very important;2 - Not important;5 - Very important;;4 - Important;4 - Important;4 - Important;3 - Neutral;4 - Important;4 - Important;;4 - Important;;2 - Not important;;4 - Important;4 - Important;3 - Neutral;4 - Important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;No;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Very much;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Personal security risks;;Yes;;No;;Yes, for all AI applications;;;
F529902;10-06-2020 18:00;English;NGO (Non-governmental organisation);Karolina;Iwa?ska;;Panoptykon Foundation;26332554844-60;Micro (< 10 employees);Poland;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;1 - Not important at all;1 - Not important at all;Please refer to the attached policy paper.;4 - Important;No opinion;1 - Not important at all;2 - Not important;4 - Important;2 - Not important;Please refer to the attached policy paper.;3 - Neutral;4 - Important;1 - Not important at all;;1 - Not important at all;2 - Not important;3 - Neutral;3 - Neutral;No opinion;Please refer to the attached policy paper.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;Please refer to the attached policy paper.;Other;Please refer to the attached policy paper.;Other;Please refer to the attached policy paper.;;;Please refer to the attached policy paper.;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;Biometric identification systems should never be allowed in publicly accessible spaces;Please refer to the attached policy paper.;Rather not;;Other enforcement system;Please refer to the attached policy paper.;Please refer to the attached policy paper.;Mental health risks;Please refer to the attached policy paper.;Yes;Please refer to the attached policy paper.;Yes;Please refer to the attached policy paper.;Yes, for all AI applications;;;Panoptykon_AI_whitepaper_submission_10.06.2010_final.pdf
F529901;10-06-2020 17:58;German;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;No opinion;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;Current legislation may have some gaps;;No;;;;Scoring;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Very much;sollte Pflicht sein: keine KI ohne Kennzeichnung;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F529900;10-06-2020 17:49;German;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;3 - Neutral;5 - Very important;4 - Important;2 - Not important;2 - Not important;;5 - Very important;3 - Neutral;2 - Not important;2 - Not important;4 - Important;5 - Very important;;1 - Not important at all;5 - Very important;4 - Important;;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;No;;;;Je mehr Datenquellen einer KI zur Verfügung gestellt werden oder würde sogar mehrere KI vernetzt, desto riskanter sind die Resultate zu bewerten.;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Not at all;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Es sollte weder freiwillig, noch (aussschließlich) privat sein.;Personal security risks;Risiken für die Umwelt;Yes;;Yes;;Yes, for all AI applications;;;
F529899;10-06-2020 17:47;German;EU Citizen;Erich;Krichbaum;;;;;Germany;The feedback can be published with your personal information;5 - Very important;3 - Neutral;;;1 - Not important at all;2 - Not important;;3 - Neutral;3 - Neutral;2 - Not important;1 - Not important at all;3 - Neutral;4 - Important;;1 - Not important at all;2 - Not important;1 - Not important at all;;3 - Neutral;4 - Important;4 - Important;1 - Not important at all;1 - Not important at all;;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;;There is a need for a new legislation;;No;;;;;3 - Neutral;3 - Neutral;4 - Important;4 - Important;5 - Very important;4 - Important;Biometric identification systems should never be allowed in publicly accessible spaces;;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F529898;10-06-2020 16:47;English;Business Association;Filip;GEERTS;;CECIMO - European Association of Machine Tool Industries and related Manufacturing Technologies;79464041975-17;Micro (< 10 employees);Belgium;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;The development of AI capabilities in Europe requires a clear and flexible business and investment framework. That is why CECIMO stands for specific rules as to the adoption of AI tools and applications in European manufacturing, ensuring that these new means comply with the European Single Market principles and with free and fair trade conditions. ;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;Define clear rules concerning the ownership of data in relation to the European data space.;3 - Neutral;5 - Very important;5 - Very important;"Set up a right policy that enables and fosters progress in research while at the same time guaranteeing innovation on industrial AI. 
 A comprehensive European investment plan should be laid out by EU policymakers, focusing on the development of machine learning, AI applications and other digital areas in which European companies are global leaders. Furthermore, this investment plan should boost the adaption of AI and digital tools in manufacturing.";5 - Very important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;;3 - Neutral;3 - Neutral;3 - Neutral;2 - Not important;4 - Important;3 - Neutral;"From European machine tool manufacturers, AI systems are not a product; AI is a broader technology that is integrated into their industrial products. Therefore, rules and responsibilities concerning the use of AI systems in manufacturing must be clearly specified. ";Current legislation may have some gaps;;Yes;;No;;;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;No further guidelines or regulations are needed;The biometric identification system is already covered under the EU GDPR (General Data Protection Regulation) and it is considered a “special category of personal data” that requires both a special legal basis for processing and an accompanying data protection impact assessment.;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;CE-marking self-certification;Risks related to the loss of connectivity;;No;;No;;No;;;Position_paper_-_AI_-_submitted_.pdf
F529897;10-06-2020 15:55;English;Business Association;EDiMA;EDIMA;;EDiMA;53905947933-43;Micro (< 10 employees);Belgium;The feedback can be published with your personal information;No opinion;4 - Important;5 - Very important;5 - Very important;No opinion;No opinion;;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;3 - Neutral;;No opinion;No opinion;No opinion;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;;4 - Important;4 - Important;3 - Neutral;3 - Neutral;1 - Not important at all;3 - Neutral;Please refer to the annex submitted together with this questionnaire.;Other;Please refer to the annex submitted together with this questionnaire.;Other;Please refer to the annex submitted together with this questionnaire.;;;N/A;4 - Important;3 - Neutral;4 - Important;3 - Neutral;3 - Neutral;3 - Neutral;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);Please refer to the annex submitted together with this questionnaire.;Rather not;Please refer to the annex submitted together with this questionnaire.;Other enforcement system;Please refer to the annex submitted together with this questionnaire.;;;Please refer to the annex submitted together with this questionnaire.;No opinion;Please refer to the annex submitted together with this questionnaire.;No;;No;;Please refer to the annex submitted together with this questionnaire.;
F529896;10-06-2020 15:07;English;NGO (Non-governmental organisation);Asha;Allen;;European Women's Lobby;85686156700-13;Small (< 50 employees);Belgium;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;1 - Not important at all;It will be essential that each of the proposed actions show clear implementation and consistent monitoring of gender mainstreaming mechanisms, including, mandatory gender impact assessments, gender budgeting and gender specific indicators, as well as the promotion of gender equality training and awareness raising across the sector to ensure institutional transformation and a more inclusive private/research sector. Member States must also proceed, abiding by these parameters.;5 - Very important;5 - Very important;1 - Not important at all;4 - Important;5 - Very important;3 - Neutral;Specific measures, within an extensive monitoring framework, as well as dedicated budget lines must be guaranteed to ensure the gender gap in skills, employment research are closed and support to women-led AI SME’s are guaranteed. With this, it is essential these measures are aligned with the EC Gender Equality Strategy 2019-2024, Digital Education Action Plan, Updated Skills Agenda and recommendations of the Women in Digital Scoreboard into each of the proposed actions;4 - Important;4 - Important;4 - Important;"The representation of women within existing research & innovation structures for AI and STEM related fields is critically low. This is due to the high levels of sexist discrimination & harassment they face within these sectors. Actions to strengthen this community must include the development of specific initiatives to support and promote the inclusion of women and girls, particularly in decision making and leadership roles; concretely in recognition & alignment with the Work Life Balance";3 - Neutral;4 - Important;4 - Important;5 - Very important;5 - Very important;Each proposal for the specialised Digital Innovation Hubs must include measureable gender mainstreaming objectives that can be regularly reviewed and monitored by specific experts. More broadly within the Digital Europe Programme, Horizon Europe and Structural & Investment funds, budget must be allocated to support women-led SME’s that statistically perform 63% better, however have received significantly less funding from existing EC programmes;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;The application of AI has already proven to be discriminatory to women in Europe in software developed for recruitment and symptomatic detection in healthcare, thereby contravening EU Directives on equal treatment between women and men in employment and occupation. AI has also been used in the perpetration of violence against women and therefore must be expressly referenced as a potential breach of fundamental rights in the proposed risk assessment. See attached opinion.;There is a need for a new legislation;;No;;;;"The iEC's ""risk"" identifier is misleading, all sectors maintain high risks. Women and girls experience significant discrimination within the health sector presently, especially for women from ethnic minorities & women with disabilities. Rapid deployment in this sector prior to the establishment of clear guidelines and oversight committee which includes gender equality & health experts significantly increases the risk to life and lack of access to adequate healthcare for all women and girls.";5 - Very important;2 - Not important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;The use of any AI based technology which existing analysis suggests, significantly increases the risk of discrimination, particularly for persons from an ethnic minority, should not be used within the EU. Taking into consideration that data that informs such technologically has systematically ignored the inclusion of women and girls in testing phases, there is no indication that this technology would be safe for use or from being abused within increasingly democratically challenged environments.;Very much;The labelling system should not be voluntary but mandatory. Though some systems clearly have higher risks than others (health, public sector, recruitment) all applications of AI have the potential to become high risk. This mandatory labelling system must ensure providers have conducted adequate testing in order to acquire the necessary labelling and use within the EU such as with consumer protection and to ensure products don’t contravene any aspect of the Charter of Fundamental Rights.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;"AI technologies should be subject to rigorous assessment with specific tests to ensure nondiscrimination and must continue to be subject to regular monitoring as the technology
adapts to maintain compliance with these provisions. The human oversight committee and
supporting entities at member state level must be given the mandate and support to establish such a mechanism without interference.";Mental health risks;Personal security and Mental Health risks should be elaborated to include risks of violence & discrimination against women and girls, expressly within the legislative framework. It should be clear that use of artificial intelligence and associated risks do not just apply to the end user but also the use of AI in the perpetration of acts that violates another’s personal safety and health.Therefore, these need to reference the Victim’s Rights Directive and existing relevant EU legislation.;Yes;"Risk assessment procedures must be at the heart of the safety legislative framework to ensure that all AI related uses are consistently monitored. These procedures must be operated transparently to ensure maintenance of European Fundamental Rights. This assessment should include specifications on ensuring the safety of all women and girls and be subject to regular review; see attached document";Yes;;Yes, for all AI applications;;;European_Womens_Lobby__Recommendations_on_European_Commission_Public_Consultation_on_AI.pdf
F529894;10-06-2020 14:10;German;;;;National;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;4 - Important;4 - Important;;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;4 - Important;4 - Important;;5 - Very important;4 - Important;4 - Important;;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;Current legislation may have some gaps;;No;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;Yes;;Yes, for all AI applications;;;
F529893;10-06-2020 13:28;Polish;NGO (Non-governmental organisation);El?bieta;Dziuba;;Konfederacja Lewiatan;;Large (250 or more);Poland;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;Komisja Europejska w sposób prawid?owy zidentyfikowa?a dzia?ania, które zapewni? prawid?owy rozwój ekosystemu doskona?o?ci. Z zadowoleniem przyjmujemy dzia?ania Komisji, które maj? si? koncentrowa? na absorpcji i odwa?niejszym wdra?aniu rozwi?za? sztucznej inteligencji (AI) przez europejskich obywateli, przedsi?biorstwa, naukowców i sektor publiczny. Te dzia?ania s? jeszcze bardziej kluczowe w kontek?cie obecnej sytuacji wywo?anej COVID-19, bowiem technologie cyfrowe b?d? uznane za kluczowe.;4 - Important;3 - Neutral;5 - Very important;4 - Important;5 - Very important;3 - Neutral;Wydaje si?, ?e dla odpowiedzialnego i efektywnego wykorzystania AI kluczowe znaczenie ma harmonijna wspó?praca sektora publicznego oraz prywatnego i wsparcie transformacji cyfrowej we wdra?aniu zmian cyfrowych w ma?ych i ?rednich przedsi?biorstwach (M?P). Szczególnie istotne jest, aby równolegle prowadzi? szerokie dzia?ania edukacyjne, które pozwol? stworzy? ekosystem, który b?dzie odgrywa? kluczow? rol? we wdra?aniu AI.;5 - Very important;4 - Important;3 - Neutral;Obecnie mo?emy zaobserwowa? brak wystarczaj?cej liczby centrów badawczych AI w Europie. Zwi?kszenie ich liczby, a przy tym usprawnienie i wzmocnienie koordynacji mi?dzy tego typu centrami, zwi?kszy?oby synergi? i mo?liwo?ci partnerstwa. Propozycja KE dotycz?ca stworzenia specjalnego centrum bada? nad AI w Europie to doskona?a inicjatywa, która mog?aby pomóc w tym zakresie. Po??danym modelem by?by jeden tego g?ówny instytut, posiadaj?cy oddzia?y rozproszone w kilku miejscach w Europie.;5 - Very important;3 - Neutral;5 - Very important;4 - Important;3 - Neutral;;4 - Important;4 - Important;5 - Very important;3 - Neutral;2 - Not important;3 - Neutral;;Current legislation may have some gaps;;Other;Istnieje ju? wiele przepisów prawnych, które maj? neutralny technologicznie charakter, a zatem s? wystarczaj?co szerokie, aby mo?na je by?o zastosowa? do regulowania AI. Warto przy tym jednak ka?dorazowo pochyla? si? równie? nad konkretnymi problemami i lukami prawnymi dotycz?cymi uregulowa? AI. Wszelkie stwierdzone luki nale?y usun?? za pomoc? rozwi?za? opartych na istniej?cym prawodawstwie. Pozwoli to na unikni?cie tworzenia nadmiernie skomplikowanego ustawodawstwa.;;;;3 - Neutral;3 - Neutral;5 - Very important;3 - Neutral;4 - Important;3 - Neutral;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);"Warto mie? na uwadze, ?e ostatecznie to rz?dy decyduj? o konkretnych podej?ciach do dalej id?cych regulacji dotycz?cych tych technologii. Jednak niektóre wa?ne czynniki, które rz?dy powinny wzi?? pod uwag? obejmuj? m.in.: kwesti? czy te rozwi?zania s? wymagane dla bezpiecze?stwa publicznego; czy by?y wst?pnie zatwierdzone jako racjonalne; a tak?e czy istnieje praktyczny sposób na osi?gni?cie tych samych celów bez wykorzystania tak wra?liwych danych. Te kwestie wydaj? si? kluczowe dla zastosowania tych regulacji przez rz?dy pa?stw cz?onkowskich. ";Rather not;M?P b?d? mia?y du?e trudno?ci z dostosowaniem si? do systemu etykietowania. Taka sytuacja sprzyja?aby tylko i wy??cznie du?ym podmiotom. Podej?cie to zapewni konsumentom jedynie minimaln? korzy??. Aby taki system móg? przynie?? jak?kolwiek korzy?? dla konsumentów, konieczne jest szerokie porozumienie w sprawie okre?lenia standardów tego typu rozwi?zania. Ka?dy system musia?by by? bardziej elastyczny. Przy konstrukcji przepisów nale?y równie? mie? na uwadze samoregulacyjne podej?cie firm.;Other enforcement system;Uwa?amy, ?e najlepsze podej?cie do aplikacji AI wysokiego ryzyka to ocena rozwi?za? ex-ante po??czona z mechanizmami egzekwowania ex-post, w przypadku rozwi?za?, w których spodziewane s? problemy. Jednocze?nie jednak nale?y zadba? o to, aby proces samocertyfikacji nie by? zbyt uci??liwy, szczególnie w zakresie wymogów dotycz?cych dokumentacji. Inaczej istnieje ryzyko, ?e z uwagi na nadmierne wymogi prawne, proces ten mo?e zniech?ci? M?P do podejmowania dzia?a? na rzecz wprowadzania innowacji.  ;Dokonuj?c wyboru systemu oceny ryzyka, istotne jest, aby zapewni?, ?e nie b?dzie on nadmiernie uci??liwy dla dostawców aplikacji. Jednocze?nie powinien by? on dostosowany do poziomu wiedzy wyznaczonych organów kontroli. Wa?ne jest równie?, aby pami?ta?, ?e nie ma jednego „uniwersalnego” podej?cia do zgodno?ci i nale?y zastanowi? si? nad kontekstem, w którym dzia?a technologia AI.;Mental health risks;Uwa?amy, ?e g?ówne wysi?ki powinny by? skoncentrowane na obszarach, w których unikalne w?a?ciwo?ci AI, tzw. Internetu Rzeczy (IoT)  lub robotyki zwi?kszaj? ryzyko dla fizycznej i psychicznej integralno?ci konsumentów. Nale?y równie? pami?ta?, ?e wybór ryzyka (zwi?zany z cyberbezpiecze?stwem, ??czno?ci?, zdrowiem psychicznym) nie powoduje, i? potrzebne s? dodatkowe przepisy w reguluj?ce te kwestie. Raz jeszcze wskazujemy, ?e istniej?ce przepisy s? wystarczaj?ce.;Yes;Stoimy na stanowisku, ?e przeprowadzanie procedury oceny ryzyka powinno by? wymagane tylko wtedy, gdy nast?pi?a znacz?ca zmiana funkcjonalno?ci produktu, która mo?e w sposób istotny zmieni? jego dzia?anie w testach lub w ujawnionych informacjach dotycz?cych bezpiecze?stwa. Ogólne aktualizacje tzw. over-the-air (OTA), takie jak poprawa bezpiecze?stwa, naprawa b??dów lub proste ulepszenia po wprowadzeniu produktu do obrotu, nie powinny uruchamia? procedury ponownej oceny ryzyka.;No;Globalnie ramy ?cis?ej odpowiedzialno?ci za produkt s? zastrze?one dla wyj?tkowo niebezpiecznych sytuacji. Koncepcja ?cis?ej odpowiedzialno?ci pomija wszelkie przes?anki do wy??czenia odpowiedzialno?ci, cho?by te zwi?zane z zaniedbaniem. Rozszerzenie zakresu dyrektywy w sprawie odpowiedzialno?ci za produkt na oprogramowanie i wszystkie aplikacje AI oznacza?oby, ?e ka?dy zaanga?owany w tworzenie systemu AI móg?by zosta? poci?gni?ty do odpowiedzialno?ci za problem poza jego kontrol?.;No;;Istniej?ce ramy odpowiedzialno?ci s? solidne i neutralne technologicznie. Dzi?ki temu s? wystarczaj?co elastyczne, aby sprosta? wyzwaniom pojawiaj?cym si? w zwi?zku z powstaj?cymi technologiami, w tym rozwojem AI. Ka?da zmiana tak fundamentalnych ram prawnych i spo?ecznych powinna by? dokonana tylko w odpowiedzi na znacz?ce braki regulacyjne w obecnych ramach prawnych. Warto przy tym pami?ta?, ?e rygorystyczny system odpowiedzialno?ci za dzia?anie oprogramowania zahamowa?by innowacje w Europie.;KL_AI_Consultation.docx
F530520;10-06-2020 12:57;English;;;;;;;;;The feedback can be published in an anonymous way;3 - Neutral;5 - Very important;3 - Neutral;4 - Important;3 - Neutral;5 - Very important;;4 - Important;3 - Neutral;5 - Very important;4 - Important;3 - Neutral;No opinion;;3 - Neutral;3 - Neutral;3 - Neutral;;2 - Not important;2 - Not important;4 - Important;4 - Important;4 - Important;;No opinion;No opinion;3 - Neutral;2 - Not important;No opinion;No opinion;;No opinion;;No opinion;;;;;3 - Neutral;3 - Neutral;5 - Very important;3 - Neutral;3 - Neutral;4 - Important;No opinion;;No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;;;No opinion;;No opinion;;No opinion;;;
F530519;10-06-2020 12:47;Spanish;Academic/Research Institution;Javier;CALLEJO;;Universidad Nacional de Educación a Distancia;;Large (250 or more);Spain;The feedback can be published with your personal information;4 - Important;5 - Very important;4 - Important;3 - Neutral;4 - Important;5 - Very important;;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;;5 - Very important;4 - Important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;;There is a need for a new legislation;;No;;;;;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;Yes;;Yes, for all AI applications;;;
F530518;10-06-2020 12:36;German;Public authority;Eva;Lux;National;"Arbeitsgruppe ""Digitaler Neustart"" der Konferenz der Justizministerinnen und Justizminister der Länder";;Small (< 50 employees);Germany;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;2020-06-08_Stellungnahme_Konsultation_Wei_buch_KI_final.pdf
F530517;10-06-2020 11:13;English;Company/Business organisation;Jan Wolfgang;Doser;;Deutsche Börse Group;20884001341-42;Large (250 or more);Germany;The feedback can be published with your personal information;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;The cooperation between authorities and market participants can bring valuable outcomes, not only during the actual cooperation. It also widens the mutual understanding of benefits and risks associated with a technology and lays the ground for a wider ecosystem. These ecosystems should be promoted and supported. Together with the Hessian Ministries, Deutsche Börse Group and others, have such a cooperation within the so called “Financial Big Data Cluster”(FBDC). ;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;"A balanced approach is most beneficial, as it can support the cooperation of small/big public/private actors which are necessary for successful AI ecosystems within/beyond one industry sector. Regarding the EU data spaces, we see the political interest, however one “single common EU data space” would not lead to innovation; a competitive approach would be preferable. Also, we would prefer to start with “meaningful content clusters” and develop standards for those data for interoperability.
";4 - Important;4 - Important;5 - Very important;;4 - Important;5 - Very important;4 - Important;5 - Very important;4 - Important;;1 - Not important at all;4 - Important;4 - Important;3 - Neutral;No opinion;4 - Important;"From our perspective as a financial market infrastructure, we see no safety concerns with AI regarding the physically well-being.
We think it is necessary to focus on the tasks performed and the potential impact by AI applications in order to evaluate whether the rationale of a decision must be explainable mandatory. If AI applications would impact/interact with consumers directly this is more important, than if an AI performs business-internal “technical” tasks.";Other;From our point of view as a financial market infrastructure, most activities/services performed by AI applications in the financial sector would be regulated by already existing rules and legislation. Examples are the MiFID II/MiFIR framework or the GDPR, which would have to be respected anyway. To ensure speed-to-market, we would prefer to adjust/finetune existing regulation, if necessary. New regulation should only cover new issues related to AI. ;Other;As stated above, and in order to support the data driven EU economy, as well as the usage of AI applications, we think that the adjustment existing regulations is preferable to new rules. Therefore, we think that the introduction of new compulsory requirements should be limited to high-risk applications. Nevertheless, while we agree with the approach to determine risk in general, we think there is still a need for clear criteria to define “high-risk”. ;;;"In general, any AI application must have clear and well-designed rules/objectives to minimize the associated risks. 
Further, it is important to differentiate between AI applications operating in “open systems” (e.g. road traffic) or “closed systems” (e.g. playing chess). In “open systems”, the AI will never be able to cover all eventualities, as the training data is always limited. Here humans must make the final decision. This is also true for high-risk AI applications in “closed systems”. ";5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;No opinion;;Very much;See position paper. ;Other enforcement system;See position paper. ;"While designing the compliance assessment process, it is important to keep start-up companies in mind. Therefore, the process should be as efficient as possible, depending on the AI application in question. 
In some cases, it might be necessary to focus more on ex-post assessments (e.g. via increased monitoring), e.g. if ex-ante assessments are only of limited meaningfulness, as the data to train an application will differ from “live” data. ";;Not relevant for our business, as we do not offer products, which endanger retail customers.;Yes;As an ex-ante risk assessment is not fully possible for every AI application, a distinction between “self-learning” and “release-based” might be useful. In case of “self-learning” AI applications, again, a focus on ex-post control mechanisms seems beneficial. Therefore, regular reviews and potential re-training “check-points “might be established in the process, this is especially necessary for “self-learning” AI applications (see the controversy around the chatter bots in 2016). ;;In general, it might be useful to ask whether a completely “new”, and therefore unregulated, task is performed by an AI application in contrast to an already “known”, and therefore regulated task. In the latter case, adjustments to the existing framework might be sufficient. For example, if a company can prove that it fulfilled all requirements, it should not be held liable because of “negligence”. Notwithstanding a human or an AI application caused the accident. ;;;Please see position paper;20200610_EC_AI_whitepaper_DBG_response.pdf
F530516;10-06-2020 10:52;English;Business Association;Nina;Elzer;;European Association of Communications Agencies (EACA);397482431021-09;Micro (< 10 employees);Belgium;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;No opinion;5 - Very important;5 - Very important;3 - Neutral;No opinion;3 - Neutral;"Our industry POV: Concerns with AI are an extension of concerns with data use overall bc of opaque decisioning/use/responsibility. But these risks can be mitigated, e.g. through compliance with regulations, adherence to data processing std’s for robustness, accuracy & security; ethics guidelines & policies, bias detection software and record keeping for/on data sets, goals & algorithms; testing outcomes with different input data; explicit statements of expectations & responsibility for accuracy.";Current legislation is fully sufficient;;Yes;;Yes;;"The challenge is the definition of high-risk sectors/industries. The purpose of the AI model use is key, not the industry. It is impossible to predict the long term impact of behaviour, therefore it is highly difficult to assume that we can know today which industries are/will be high-risk in the future. 
We agree with the statement (White Paper). But, a proper definition of high risk needs to be agreed. Alternatively, there could be no separating out by risk at all in order to avoid loopholes.";4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;4 - Important;;;;We are not opposing a voluntary labelling system, however, we believe that is unlikely to work Any of the requirements suggested in the White Paper and in this consultation will add cost, effort and paperwork without providing a competitive advantage – thereby lacking an incentive. ;;;;;;;;;;;;;
F530515;10-06-2020 10:40;English;Company/Business organisation;Alexander;Frank;;Confederation of European Security Services (CoESS);61991787780-18;Micro (< 10 employees);Belgium;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;AI skills strategies are required along the entire value chain (developers, users, testers) based on sectoral needs, taking account of STEM and non-STEM skills, to ensure a human-centric AI approach. For high-risk applications, curricula and a licensing framework based on formal qualifications should be considered, if necessary incl. vetting. Close cooperation with Social Partners is crucial. With regard to Action 6, CoESS emphasizes that quality criteria should always prevail the cost.;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;;4 - Important;4 - Important;5 - Very important;;4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;;4 - Important;5 - Very important;4 - Important;5 - Very important;4 - Important;3 - Neutral;"AI can be used for malicious purposes. Not only technical robustness is important, but also physical protection. AI must be looked at from three security and safety dimensions: positive impact of AI on public security; needs for physical and technical protection of AI technologies; and risks of malicious abuse. Security and fallback plans, constant human agency and oversight by qualified, if needed licensed and vetted staff, are key for the operation of high-risk AI solutions. ";Other;To build trust in AI, risk-based legislation is needed based on the 7 requirements identified in the Ethical Guidelines for Trustworthy AI. CoESS believes that particular attention needs to be given to human review, governance mechanisms and trained staff. AI should not be used to make fully automated decisions that may result in civil rights violations. AI systems should rather support human decision-making. An ongoing ethics debate led by the High Level Expert Group is required. ;Yes;;Yes;;Any AI application must be considered as concerning, if there is a lack of human oversight and governance – particularly in high-risk applications such as access control and monitoring of Critical Infrastructure and public spaces. Lack of human oversight and possible intervention enhances violation of civil rights and risks of malicious use. Human oversight must be required in varying degrees to support other security measures, depending on the AI system’s application area and risk assessment. ;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);"CoESS supports the 7 key requirements identified in the Ethics Guidelines for Trustworthy AI by the European Commission. We also believe that new regulatory requirements as listed in the White Paper can be suitable to build trust in high-risk AI applications, including Facial Recognition. It is important to have on the one hand effective oversight, transparency, and accountability of all AI technologies, and on the other qualified staff among developers, users, and testers, to effectively protect fundamental and consumer rights in a human-centric approach. Based on these requirements and following a thorough impact and risk assessment for each use-case, CoESS agrees that the use of remote biometric identification systems, such as Facial Recognition, should be allowed in public spaces if they can bring considerable added value to public safety and security, and respond to risks to public security.

CoESS strongly objects to a moratorium on these tools based on concerns on their possible misuse only. A moratorium would negatively impact investment to improve accuracy, effectiveness, efficiency, and testing of remote biometric identification tools, and leave the market to other countries instead of establishing a European, human-centric leadership in these technologies.

Further, a moratorium would have negative consequences for the security of citizens, as it would pre-emptively deprive law enforcement of technologies that would bring considerable added value in fighting crime when used alongside human oversight and intelligence in specific cases. Facial recognition can be critical to enhance capabilities of solutions like video surveillance, access control, and identity management systems for the protection of people – especially at Critical Infrastructure. When deploying remote biometric identification technologies only in specific cases, it is not about nationwide surveillance of citizens, but a targeted search for criminals at particularly vulnerable locations. The automated comparison of video images with police databases, in which photos of criminals are stored, is not comparable with the use of systems for which millions of photos of uncontested citizens are stored. Also, the alternatives a ban would leave would be far slower and also prone to errors – for example when officers have to manually go through large quantities of videos and images of police databases. 

CoESS would also like to stress that there are different use-cases of Facial Recognition, which do not all come with a higher threat to fundamental rights than other applications that the Commission may consider being “high-risk applications”. For example, when it is used for verification purposes (e.g. in banking, electronic device unlocking, access control), individuals have consented to, or are required to prove their identities without a negative impact on privacy, if the requirements are fulfilled as lined-out in the White Paper. It is therefore important to conduct impact and risk assessments before using remote biometric identification systems in public spaces:

First, remote biometric identification tools are not by default a necessity when there are other means to achieve the underlying purpose. When using AI technologies, their added value and expected impact must be clear. Depending on the mission and location, AI technologies, physical intervention, and the blending of the two through ”augmented security” must be considered in a risk and impact assessment. On-site law enforcement personnel or private security guards can often deliver better added value, e.g. by means of behavioral detection techniques or by being able to react and, if necessary, intervene directly on the spot. The deployment of surveillance technologies and/or physical guarding must always fulfill the objectives of a mission under careful consideration of data protection, privacy, and fundamental rights. Further, there must be legitimate interest to use remote biometric identification tools in public spaces based on a risk assessment process taking into account threats to public security, and an evaluation of physical and technological solutions that properly respond to the risk-level. 

In addition to this impact- and risk-based approach, CoESS stresses the relevance of human oversight over remote biometric identification tools. Human review and, if needed, intervention is crucial to ensure that any decision made by AI tools does not violate civil rights. Technologies like facial recognition should not be used to make fully automated, final decisions. Human review of facial recognition results should be used to ensure rights are not violated. To that end, those conducting human oversight need to have adequate training, skills and qualifications. Users should be enabled to reasonably self-assess or challenge the system. The Ethics Guidelines for Trustworthy AI provide an excellent overview of the different approaches of human oversight. 
";Much;To ensure uptake, any labelling system must respond to societal and industry needs. In the light of technological development, the standardization process must be efficient and flexible. Participation of all relevant stakeholders is crucial. Investments must be made in the promotion of standards among industry and lawmakers - if of added value also for the uptake in laws, procurement, and contracts. Labelling should be complemented by accountability as well as review and redress mechanisms.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Compliance assessment must have harmonized standards across countries to not leave gaps. Investments must be made in testing infrastructure. Support structures are needed for SMEs. To ensure AI uptake, financial and administrative burdens to users need to be as low as possible. Predefined conformity assessments or standard scenarios for the use of AI should be considered. AI tools could include a self-reporting function (“ethics switch”) as addressed in the Ethics Guidelines on Trustworthy AI.;Risks related to the loss of connectivity;;Yes;CoESS believes that it could be assessed whether a mandatory feature of “high-risk” AI solutions could be a built-in automatic signaling or reminder to users after a certain number of software updates – in line with technical methods explained in the Ethics Guidelines on Trustworthy AI.;Yes;Legal certainty on victim rights and liability is key. Developers carry a particular responsibility, as they largely define AI behavior and learning. Producers must ensure that all products put on the market are safe throughout their lifecycle. Users should only be held liable, if they are best suited to respond and entitled to autonomously intervene in AI decisions. Again, human oversight, traceability, skills, and, if necessary, licensing, are key at all stages of the value chain.;No opinion;;;pp_2020_06_10_CoESS_Position_Paper_White_Paper_on_Artificial_Intelligence.pdf
F530514;10-06-2020 10:19;English;Business Association;Zoi;Sagia;;European Tyre & Rubber Manufacturers Association;6025320863-10;Micro (< 10 employees);Belgium;The feedback can be published with your personal information;4 - Important;5 - Very important;4 - Important;3 - Neutral;5 - Very important;4 - Important;;5 - Very important;3 - Neutral;5 - Very important;3 - Neutral;4 - Important;5 - Very important;;2 - Not important;5 - Very important;5 - Very important;"Allocation of more funding for AI research. 
Not all legal requirements that apply to the operation of AI products should  necessarily apply to the development of these products. This should give room to innovation.";No opinion;5 - Very important;4 - Important;4 - Important;3 - Neutral;;4 - Important;4 - Important;4 - Important;2 - Not important;3 - Neutral;4 - Important;Shared liability has to be defined in the legal AI framework.;Current legislation may have some gaps;;Other;The idea of harm should be well defined and probability should be well balanced with the severity. ;;;;No opinion;3 - Neutral;4 - Important;4 - Important;4 - Important;5 - Very important;No opinion;;No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;No opinion;Liability is very important to AI based solutions/products. In this respect, the assessment of the regulatory options should not be limited to PLD but also include other pieces of legislation which address digital services and software. ;Yes, for all AI applications;;;
F530513;10-06-2020 09:53;German;NGO (Non-governmental organisation);Carla;Hustedt;;"Bertelsmann Stiftung, ""Ethics of Algorithms"" Project";13571025706-27;Large (250 or more);Germany;The feedback can be published with your personal information;No opinion;5 - Very important;5 - Very important;No opinion;No opinion;5 - Very important;Instead of the creation of a quasi-monopolistic digital market with few dominating players, the EU should aim for the creation of an “ecosystem of plurality”, where a wide variety of stakeholders works on the development of digital tools. Next to stakeholders form research and SMEs, civil society is key for the development of AI for social good and should thus receive active support and funding. ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Research funding should not only focus on technical fields but also on research areas working on the societal effects of AI and the embedding of the technology in society. Interdisciplinary work should be specifically promoted.;No opinion;5 - Very important;No opinion;An infrastructure that allows knowledge exchange between stakeholders working on AI innovation for social good needs to be set up. The establishment of an organizational entity commissioned with the task of competence building and knowledge exchange among these different stakeholder should be considered.  ;No opinion;No opinion;No opinion;No opinion;No opinion;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"Instead of aiming for a 100% accuracy, which is mostly impossible, we should make sure, that AI systems perform significantly better than the alternative human decision maker and that oversight measures are in place to detect and take measures against wrong decisions based on AI. An additional concern: AI might be used to avoide ethical decisions and push away responsibilities. 
Not all AI systems pose the same harm. There is a need for differentiation based on risk. ";There is a need for a new legislation;;Yes;;No;;A risk-based regulation approach is the right way to go! For the identification of cases that need regulation the following criteria should be applied: 1.The intensity of potential harm (The potential negative impact on fundamental rights, equality or social justice as well as on society as a whole, the number of people affected) and 2. the dependency of the potentially affected parties on the AI system (Is there a human in the loop? Is there the possibility to change the AI system for another?);5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Very much;"We recommend the development of a label that includes both, requirements for the technical system and requirements for the implementation process of the system. We recommend a nuanced labeling approach, similar to the energy efficiency label, where different levels of implementation of requirements are made visible. See: shorturl.at/cMTU2
It should be evaluated whether there might be a need for different types of labels, depending on whether we are dealing with B2B and B2G or with B2C products.";A combination of ex-ante compliance and ex-post enforcement mechanisms;;To increase the likelihood of compliance, the identified requirements should also become part of public sector procurement standards. To ensure appropriate ex-post enforcement, civil society watchdog organizations need to be strengthened. There is also a need for competence building for existing oversight bodies. The establishment of an agency to coordinate competence pooling between oversight bodies is recommended.;;;;;;;;;;The_Urgent_Need_for_Robust_Trust_Input_Paper_Bertelsmann_Stiftung.pdf
F530512;10-06-2020 09:34;French;;;;National;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"Explorer la question de la monétisation des données nécessaires dans le cadre de l'IA en construisant un modèle économique soutenable de l'accès aux données, confortant un partage équitable de la valeur et tenant compte de la position déjà structurante de certains acteurs qui pourraient être les plus à même de bénéficier de l'harmonisation du cadre européen.
Proposer une législation plus permissive notamment dans les contextes expérimentaux.";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Favoriser des conditions de level playing field grâce aux leviers de la politique commerciale, des sanctions en cas d'atteintes aux droits des individus, et de la coopération internationale par le biais des organisations internationales (GPAI, OCDE).;2 - Not important;5 - Very important;5 - Very important;"L’impératif est qu’un réseau étroitement fédéré et très sélectif d’organisations existantes de recherche européennes de réputation internationale s’engage sur une feuille de route stratégique conjointe pour positionner l’Europe au premier rang mondial.
La possibilité de mise en commun des capacités de calcul (ex. CEA GENCI) et de partage sécurisé de la donnée métier (mise en relation algorithmes / data effectuée par exemple par un tiers de confiance) à des fins de R&D est également cruciale.";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Connecter l'offre de solutions IA européennes avec la demande ;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;;Current legislation is fully sufficient;;Yes;;Other;Oui mais;Les applications à haut risque doivent également être évaluées en tenant compte de l'impact des solutions algorithmiques sur les droits individuels et de leur tendance à produire des biais sur la base des données individuelles et/ou personnelles. Ainsi l'impact des décisions algorithmiques de certaines applications B2C (assurances, services financiers, mais également médias, commerce électronique et réseaux sociaux) devra être évalué afin de préciser le champ d'application de la réglementation.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);"La reconnaissance faciale pourrait être utile là où les enjeux de sécurité intérieure sont importants, et sous des conditions juridiques, d'information et d'accès aux données strictes.
";Much;"Nous souscrivons à l'objectif de permettre aux entreprises de s'auto-labelliser ""IA de qualité"". Néanmoins, il conviendra de veiller à ce que cette mesure ne défavorise pas les start-ups et PME européennes au profit de grandes entreprises extra-européennes. La mise en conformité avec les obligations de cette auto-labellisation devra être à la portée de l'écosystème européen. Il ne s'agit pas de mettre en place un référentiel dont la mise en place soit lente et coûteuse pour les PME européennes.";Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;Le mécanisme d'évaluation d'IA doit se faire avec une combinaison de mécanismes a priori et a posteriori. Notamment, les performances devraient être suivies au fur à mesure pour ajuster l'algorithme (cf. travail de l'agence EU-LISA dans le cadre des tests du futur Biometric Matching System de l'EES).;Cyber risks;S'agissant de la sécurité des produits, les autorités françaises se réservent le droit d'apporter tout complément nécessaire à la consultation publique à l'avenir.;;;Yes;"Les autorités françaises se sont déjà prononcées, dans une NAF du 3 juillet 2017, en faveur d'une révision de la directive ""produits défectueux"". 
Il convient toutefois de réitérer les réserves déjà exprimées relatives à la difficulté à appréhender des technologies extrêmement hétérogènes et en plein développement, qui n’ont à ce stade pas généré à notre connaissance de contentieux spécifique.  ";No;;Les autorités françaises s'interrogent quant au fondement juridique d'une intervention législative de l'UE en matière de responsabilité civile, et quant au fait que l’action envisagée serait effectivement plus efficace que ne le serait celle des Etats membres en la matière (principe de subsidiarité).Les règles de droit interne (droit de la preuve et responsabilité civile), ont déjà fait preuve de leur capacité à s’adapter à de nouveaux enjeux tels que ceux posés par l'IA.;
F530511;10-06-2020 08:56;German;Business Association;Marc;Lemanczyk;;Deutscher Steuerberaterverband e.V.  (dstv.de);845551111047-04;Small (< 50 employees);Germany;The feedback can be published with your personal information;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;"Freiberufliche und Mikrounternehmen sind eine tragende Säule der europäischen Wirtschaft. Damit ihr Zugang zu Förderungen, zu Know-How und zu adäquaten Datenströmen sichergestellt wird, sollte unabhängig von einer weiteren Entwicklung der Definition ""KMU"" sichergestellt sein, dass diese vom Anwendungsbereich umfasst sind. ";4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;Der Erfolg der KI-Strategie für die Wirtschaft hängt von einer umfassenden Nutzung durch Unternehmen ab. Daher wird die nachhaltige Vermittlung von KI-Kompetenzen für Unternehmer & Personal eine Schlüsselrolle einnehmen. Das bedeutet, dass eine ausreichende Anzahl von Fachleuten in Universitäten & Bildungseinrichtungen zur weiteren Kompetenzvermittlung ausgebildet werden muss. Zudem müssen KMU einen einfachen, unbürokratischen Zugang zu Fördermitteln & Angeboten der Kompetenzzentren erhalten.  ;3 - Neutral;4 - Important;5 - Very important;Der DStV unterstützt die Spezialisierung von Innovationszentren und regt darüber hinaus deren nachhaltigen Ausbau sowie den der gemäß den Förderprogrammen bestehenden bzw. vorgesehen Hubs an, damit neben der Fokussierung auch eine Sektorisierung der Forschung und Kompetenzvermittlung nach den Bedürfnissen der Unternehmen in den verschiedenen Wirtschaftsbereichen erfolgen kann.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"a)	Beitrag zur Erarbeitung von konkreten KI-Strategien und Einsatzmöglichkeiten für KMU
b)	Bereitstellung oder Unterstützung bei Zugang und Nutzung von Datenströmen
c)	Aufbau von Datenqualitätsmanagement 
d)	Sicherung von personenbezogenen und vertraulichen Daten (etwa Kundendaten)
";5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;3 - Neutral;Der Einsatz von KI darf neben den genannten Grundrechten im Interesse der Gewährleistung eines effektiven Verbraucherschutzes auch nicht die berufsrechtlich geschützte Geheimhaltung von Informationen, etwa von Geistlichen, Ärzten, Rechtsanwälten oder Steuerberatern einschränken. Zur Sicherstellung der Datenanonymisierung, die durch den Einsatz von KI zukünftig exponentiell genutzt werden, sollten deshalb technische Mindestanforderungen an hohe Standards zur Datenanonymisierung formuliert werden.;Current legislation may have some gaps;;No;;;;;4 - Important;5 - Very important;4 - Important;4 - Important;5 - Very important;3 - Neutral;No opinion;;Very much;Als freiw. Kennzeichnungssystem könnte ein Gütesiegel für europäische Standards eingeführt werden. Der öffentliche Sektor sollte lediglich KI-Produkte mit einem solchen Gütesiegel beschaffen und dies in öffentlichen Ausschreibungen kenntlich machen.Für KMU sollte geprüft werden, ob der Einsatz zertifizierter KI förderungsfähig ist und ggf. weitere Anreize geschaffen werden könnten, indem ihr Einsatz sich z.B. günstig auf zu zahlende Versicherungsprämien, etwa bei der Haftpflicht auswirken kann.;Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;Als Cyberrisiko sollte auch die gezielte Qualitätsminderung oder die Manipulation von Daten zählen, die beim Einsatz von KI verwendet werden, soweit diese Qualitätsminderung oder Manipulation dazu geeignet ist, dass der Einsatz dieser KI  fehlerhaften Ergebnisse führt. ;Cyber risks;Als Cyberrisiko sollte auch die gezielte Qualitätsminderung oder die Manipulation von Daten zählen, die beim Einsatz von KI verwendet werden, soweit diese Qualitätsminderung oder Manipulation dazu geeignet ist, dass der Einsatz dieser KI  fehlerhaften Ergebnisse führt. ;No opinion;;Yes;;Yes, for all AI applications;;;
F530510;10-06-2020 08:32;Slovenian;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;4 - Important;4 - Important;3 - Neutral;4 - Important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;;4 - Important;4 - Important;4 - Important;;4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;;2 - Not important;3 - Neutral;3 - Neutral;2 - Not important;2 - Not important;2 - Not important;;There is a need for a new legislation;;Other;;;;;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Not at all;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Mental health risks;Tveganja za zasebnost pri pomanjkljivi ali ne dovolj kakovostni rabi podatkov v AI.;Yes;;Yes;Aplikacije AI v genetiki imajo u?inek na ve? generacij uporabnikov (sorodnikov in drugih).;Yes, for specific AI applications;;;EU_posvetovanje.pdf
F530509;10-06-2020 04:22;English;Business Association;Sub-committee;Digital Economy;;Keidanren;267303827175-27;Medium (< 250 employees);Japan;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;Keidanren_Comments.pdf
F530508;10-06-2020 02:32;English;Company/Business organisation;Megan;Haas;;Kinnected.org;;Small (< 50 employees);United States;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Creating co-efficient systems of ethical purpose-built technology through which open exchange can occur. Giving gravity to early stage disruptive technology and welcoming ideas from disparate sectors and across facilities and countries. ;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;Creating a new, or more visible public international body of oversight regarding ethical and best practice use of AI. ;3 - Neutral;4 - Important;4 - Important;5 - Very important;3 - Neutral; Promote a strong, healthy culture within each Digital Innovation Hub that encourages an open and transparent exchange of ideas.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;;Current legislation may have some gaps;;No;;;;Using AI to incite divisiveness, competition and misinformation on global social network platforms to increase traffic and revenue. ;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;I am not familiar enough with EU legislation and regulations to make a comprehensive statement, however, I do understand that there is incredible complexity to biometric identification, which needs to be addressed, and in all cases there is a risk of error. ;Not at all;Labelling should apply to all applications regardless of risk. ;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Setting up a regulatory framework that would both encourage innovation and enforce mandatory compliance through ex-ante and ex-post mechanisms. ;Mental health risks;Risk and opportunities to contribute to civil unrest through the spread of misinformation and propaganda.;Yes;The necessity for a dynamic framework that holds companies to a standard throughout the entirety of the company lifecycle. This could be demonstrated by an annual reassessment, showing a company is meeting expectations and ethical requirements.;Yes;;No opinion;;I am not familiar en;Kinnected_LLC___Consultation_response_to_EU_White_Paper_on_Artificial_Intelligence_.docx
F530507;09-06-2020 21:21;English;Trade Union;Brigitte;CASTELL;;Federation of Craft Businesses in the automotive sector and in mobility services (FNA);705440625408-01;Small (< 50 employees);France;The feedback can be published with your personal information;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;"Securing access to data is fundamental. Promoting responsible data management practices and compliance of data with the following FAIR principles, ( i.e. data need to be Findable, Accessible, Interoperable and Reusable),will contribute to build trust and ensure re-usability of data. We call on the European Commission to support a level playing field amongst market stakeholders by ensuring full independent and direct access to data and motorists.

";4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;The Coordinated Plan could also address to help SMEs reimagining business models and business processes to make full use of the power of analytics. Leaders will need to reimagine how new business models could be created based on these capabilities. For example, the possibility of seamless remote interaction with the vehicle and motorists will allow new innovative services, i.e. remote diagnostics and prognostics, service appointments from the dashboard, software update over the air, etc.;4 - Important;4 - Important;5 - Very important;More R&D is needed to develop AI architectures that incorporate ethical, legal, and societal concerns through  technical  mechanisms  such  as  transparency  and  interoperability. A more intensive  collaboration  among  technical  experts  as  well  as  stakeholders  and  specialists is required to impede market imbalances in relation to access to and use of data.;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;"Safeguarding effective competition is critical. Vehicle manufacturers' Digital Innovation Hub is seen as  a ""center of excellence with the expertise, tools and knowledge to revolutionize the automotive industry"", but there is a high risk of competition being distorted by the unbalanced market power of gatekeepers increasing their control over relations with end consumers. A public monitored ""Open Telematics Platform"" Hub is to be set up to ensure a level playing field amongst stakeholders.";5 - Very important;4 - Important;5 - Very important;4 - Important;4 - Important;5 - Very important;"The European Parliament resolution of 15 January 2019 on autonomous driving hightlights ""the need to explore legislative actions to ensure fair, secure, real-time and technology-neutral access to in-vehicle data for some third party entities; takes the view that such access should enable end users and third parties to benefit from digitalisation and promote a level playing field and security with regard to storage of in-vehicle data."" AI should not be used to foreclosing independent repairers. ";There is a need for a new legislation;;Other;Vehicle data were previously accessed via a physical connection and a diagnostic tool in a repair shop while now they are more and more accessible remotely. Therefore competition starts in vehicle data, their availability and free access.Independent and unmonitored access to in-vehicle data is critical to maintain fair competition in the market for automotive services.;;;Consumers must be truly free to choose their service provider.This implies that independent market stakeholders must have competitive access to in-vehicle data. Consumers should be put in full control to decide which service providers can access their data, without interference and steering from vehicle manufacturers. ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Other special requirements in addition to those mentioned in the question above should be imposed (please specify);"In order to address possible societal concerns relating to the use of AI for such purposes in public places, and to avoid fragmentation in the internal market, we welcome that the Commission representatives ""launch a broad European debate on the specific circumstances, if any, which might justify such use, and on common safeguards"", as stated in the White Paper on Artificial Intelligence.";Much;Voluntary labelling system must only be used for AI applications that do not qualify as ‘high-risk’. We consider connected vehicle and autonomous driving as high-risk AI applications. We welcome the White Paper emphasizing that transport is included in the high-risk AI applications. We call on the European Commission to state that connected vehicle and autonomous driving AI applications are subject to competition rules and the new regulatory framework for AI should be very clear on this issue.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;"We support the White Paper opinion: ""the applicable legal requirements need to be complied with in practice and be effectively enforced both by competent national and European authorities and by affected parties. Competent authorities should be in a position to investigate individual cases, but also to assess the impact on society."" Adequate human and financial resources should be provided to enhance the monitoring of competition in the market.  ";Personal security risks;"We are in accord with the White Paper: ""in an AI based system such as autonomous cars, it may be difficult to prove that there is a defect in the product, the damage that has occurred and the causal link between the two. In addition, there is some uncertainty about how and to what extent the Product Liability Directive applies in the case of certain types of defects, for example if these result from weaknesses in the cybersecurity of the product.""
";Yes;Automation and digitalization, in the form of Connected and Autonomous Vehicle will lead to a reduction in the total number of accidents and fatalities, thereby offering significant societal benefits to the end consumer. This implies that the  responsibility and thus the civil liability will shift from the driver to the vehicle manufacturer, including all relevant market stakeholders involved in the automotive supply chain.;Yes;"We support the White Paper: ""EU product liability legislation provides for liability of producers and leaves national liability rules to govern liability of others in the supply chain."" Therefore there is a need to fully harmonise product liability laws throughout Europe.

";Yes, for all AI applications;;"We suggest to think about adapting to connected and autonomous vehicles Article 12 of United Nations Convention on the Use of Electronic Communications, which states that ""a person on whose behalf a computer was programmed should ultimately be responsible for any message generated by the machine. Such an interpretation complies with a general rule that the principal of a tool is responsible for the results obtained by the use of that tool since the tool has no independent volition of its own."" ";FNA_Paper_on_access_to_automotive_data_-_03.06.20.docx
F530506;09-06-2020 21:04;English;NGO (Non-governmental organisation);Mher;Hakobyan;;European Disability Forum (EDF);57868523887-16;Small (< 50 employees);Belgium;The feedback can be published with your personal information;4 - Important;4 - Important;5 - Very important;No opinion;5 - Very important;5 - Very important;"• Coordinated Plan should ensure societal wellbeing, broadest possible range of users, all members of society equally benefit from AI.
• Persons with disabilities (PwD) must be able to participate in Research & Innovation (R&I), AI development and public oversight.
• Skill-building and awareness must be accessible for PwD.
• Public procurement of AI must comply with EU public procurement, anti-discrimination, and accessibility laws and standards.
(see attached position for more details)";4 - Important;5 - Very important;2 - Not important;4 - Important;4 - Important;4 - Important;"• R&I, update of AI, skill development must ensure accessibility and collaboration with organisations of persons with disabilities (DPOs).
• Participation of stakeholders will ensure that no one is left behind during societal progress, and EU's unique diversity landscape contributes to social justice, substantive equality and reaching sustainability goals in the EU.
• AI human rights and societal impact assessment, democratic oversight must be ensured. (see attached for more details). ";5 - Very important;4 - Important;4 - Important;"• Participation of DPOs in R&I must be ensured, also through funding.
• Inclusive, participatory principle is crucial to ensure AI does not aggravate discrimination and inequality, and to make use of PwD’s valuable contribution to reaching substantive equality through AI-based solutions.
• Ex ante rules for funding with reference to accessibility, fundamental rights assurance, participation of PwD in R&I are crucial.
(See attached document for more details). ";No opinion;No opinion;No opinion;No opinion;No opinion;"• DIHs must also ensure that fundamental rights, accessibility and inclusive participation principles and laws are upheld during development and application of AI by SME and potential partners.
• DPOs must be able to participate in partnerships at equal level around AI projects.
• Ex ante rules for funding of AI projects by SME should also apply in reference to accessibility, fundamental rights involvement of DPOs. 
(see attached document)";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;AI can increase existing inequalities for PwD and lead to their discrimination and harm, if development and deployment of AI applications do not consider the requirements, interests and concerns of PwD. There have already been several worrying practices. So, there is great need for a strong regulatory framework (safeguards, oversight, redress mechanisms) against discrimination and increase of inequalities, and criteria for accessibility based on design for all approach. (attached document). ;Other;It is important to consider that current EU equal treatment and accessibility-related legislation is not comprehensive. So, specific rules guaranteeing accessibility and protection of fundamental rights for AI systems are needed.;Other;Non-transparency of algorithms, ‘mutation’ capabilities of AI-based products may result in unforeseen risks and expansion of our understanding of safety. So, limiting requirements to a list of ‘high-risk’ AI use is not future-proof. Risks may evolve over time and vary considering human diversity. At first, the focus should be on setting boundaries to the domains in which AI can be deployed. Even if there is small risk of harm, AI should not be used in a certain domain. (See attached document).;;;AI datasets referring to people risk overseeing minority groups (e.g. PwD) even in “low-risk” qualifying AI use. Domain by domain assessment must be done, considering all potential implications for the widest range of people. EU must set the limits as well as ex ante and regular ex post evaluations of allowed AI systems. Applications affecting citizens must be considered with the presumption of high-risk. Long-term consequences of PwDs' personal data use for AI must be assessed. (see attached).;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;Biometric identification systems should not be allowed in publicly accessible spaces, as they risk being highly discriminatory, often inaccessible for persons with disabilities, and deployed without the consent of affected persons. Sensitive data about an individuals’ chronic illness or disability can be gathered without their consent, and later be used to discriminate against the person. ;Rather not;It will be very difficult to establish risk levels, as AI use poses considerable transparency concerns and can change functionality during their lifespan. Since experience with AI is still concerning for many citizens, strong regulatory frameworks with safeguarding mechanisms, including clear governance structures and robust enforcement are vital. This needs to be applied to all AI usage for now, with the possibility to review and soften compliance, if proven unduly demanding. (see attached);Other enforcement system;Ex ante by means of external conformity assessment procedures, as well as an enforcement mechanism, to include a redress mechanism for users, must also be put in place. We also must ensure that there is high awareness among the population on AI and how it works. Public authorities must safeguard citizens against potential risks associated with AI, as many people who currently use AI-powered ‘solutions’ do not understand how their data is used by the technology industry. ;It should be clearly stated that external conformity assessment procedures should be carried out by an independent public organisation to avoid private entities from unduly certifying clients for commercial reasons.;Mental health risks;Automated discrimination is likely to be far more difficult to detect as people do not have access to the algorithms that underpin them. This create significant legal uncertainty and will make it difficult to determine liability for AI-powered ‘solutions’. There are other risks to consider, including risks impacted personal security, mental health, discrimination, inequality, and lack of accessibility and data privacy.;Yes;Any assessment of risk to fundamental rights, non-discrimination, equality, accessibility, privacy, personal security and mental health should be based on an intersectional approach, taking into account full diversity of affected persons/groups in society. DPOs should be involved in such assessment procedures.;No opinion;The current EU legislative framework should be strengthened to take new risks into account. Any revision of current EU legislative framework on liability should ensure mechanisms for the users to claim their rights. For example, if a city or company procures an AI-based solution, which discriminates against persons with disabilities, there should be an accessible non-judicial mechanism prior to Court action that should be able to remedy the situation.;Yes, for all AI applications;;Persons affected by AI applications must be able to redress issues that have been observed. Copyright, database rights protection, or other forms of business confidentiality principles should not be used to prevent affected persons from seeking redress for harm caused, especially (but not limited to), cases of discrimination and breach of fundamental rights.  ;EDF_Position_on_EC_AI_White_Paper_06.2020.pdf
F530505;09-06-2020 19:34;Portuguese;NGO (Non-governmental organisation);APDSI;Secretariado;;Associação para a Promoção e Desenvolvimento da Sociedade da Informação;435520331024-09;Micro (< 10 employees);Portugal;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Partilha de boas práticas. Partilha de soluções consideradas de excelência. ;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Assegurar que todos os Estados-Membros participam e colaboram neste desenvolvimento em condições equivalentes. Incentivar / desafiar todos os Estados-Membros a atingirem os níveis mais elevados de desenvolvimento em IA.;5 - Very important;5 - Very important;5 - Very important;Incentivar as comunidades de investigação e inovação dos diferentes Estados-Membros a unirem esforços na procura, experimentação e aceleração de soluções que possam ser transpostos para a atividade privada, pública e social.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Construir, lançar e apoiar momentos e espaços de difusão, discussão e avaliação dos resultados e outcomes dos programas e projetos de desenvolvimento de soluções de IA nas PME.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"É uma preocupação a não existência de sistemas de IA auditáveis sobre os próprios raciocínios e algoritmos utilizados pela IA. 
Ter em atenção as diferentes opções de transposição que a legislação da UE pode assumir quando transposta para os Estados-Membros.
As novas regras têm de contribuir para a criação de um ecossistema de confiança pública na IA.
É desejado que estas regras sejam feitas de forma proporcional, equilibrando danos com benefícios.";Current legislation may have some gaps;;No;;;;Quando as aplicações de IA assumem decisões que podem pôr em risco o equilíbrio humano da sociedade.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;Os sistemas de identificação biométrico devem ser utilizados com o equilíbrio que permita a manutenção da privacidade do individuo.;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Desenvolver soluções automáticas certificadas para participarem nestes mecanismos ex ante e ex post.;Mental health risks;Ineficiência e incapacidade do poder judicial, das suas infraestruturas e das competências dos profissionais afetos a esta área de conseguirem proporcionar uma maior segurança jurídica num contexto de IA.;Yes;Não.;Yes;No geral, não há necessidade de revisão da Diretiva de Responsabilidade pelos Produtos, exceto quando existirem alterações de funcionalidade de um produto que tenha um impacto nos testes de segurança, o que obrigará a uma nova avaliação de risco.;Yes, for specific AI applications;As aplicações de IA que possam contrariar e ser nefastas ao bem-estar da humanidade.;Não deve ser introduzida uma responsabilidade rígida para sistema de IA (como aconteceria de estes fossem incluídos na Diretiva de Responsabilidade pelo Produto), porque isso significaria que qualquer pessoa envolvida num sistema de IA podia ser responsabilizada por problemas sobre os quais não teria conhecimento ou influencia. um tal regime teria um efetivo nefasto sobre a inovação e dificultaria a adoção da IA pelas empresas europeias.;Consulta_P_blica_IA_-_APDSI_comentario_aberto_ao_livro_branco.pdf
F530504;09-06-2020 18:25;Spanish;Academic/Research Institution;Bárbara;Urban;;Universitat Jaume I;;Large (250 or more);Spain;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;No opinion;Cualquiera de las actuaciones que se proponen han de ir acompañadas del marco ético que requiere la implementación de la IA en general, y particularmente en la economía. ;5 - Very important;5 - Very important;No opinion;4 - Important;4 - Important;4 - Important;;5 - Very important;5 - Very important;4 - Important;Cualquier investigación, al margen de su ámbito, de su presupuesto o envergadura, ha de contar de principio a fin con un código ético y con investigadores comprometidos que contribuyan a la construcción de una IA sólida acompañada de valores y orientada al bien común en todo momento. ;No opinion;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Es deseable potenciar la figura del eticista en los centros de innovación y las PYMES. Concienciar a los profesionales técnicos de la IA de que ésta es una disciplina transversal implica la inserción de perfiles vinculados a la filosofía en el tejido empresarial. ;5 - Very important;5 - Very important;5 - Very important;1 - Not important at all;1 - Not important at all;5 - Very important;De la IA se esperan resultados muy por encima de lo éticamente deseable y de lo realmente alcanzable. La opinión pública no es insensible a la expectativas que se generan principalmente desde los medios, y en este aspecto sería conveniente reclamar un ejercicio de transparencia y especialización en la comunicación científica. ;There is a need for a new legislation;;No;;;;"La IA aplicada a ámbitos que afecten directamente a derechos fundamentales como la salud o la educación; aquellas aplicaciones que puedan implicar cambios en el transcurso de la evolución de la especie humana; consecuencias discriminatorias...";5 - Very important;No opinion;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;Los sistemas de identificación biométrica, aunque tenga lugar en espacios públicos y abiertos, al margen de su legalidad, no son éticamente aceptables desde el punto de vista de la IA, pues toman más datos de los necesarios para el fin que la propia acción persigue. ;Rather not;A nivel estadístico no sería representativo. ;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;Los riesgos para la infancia;Yes;;Yes;Se ha implantado la IA en aparatos de consumo sin haber valorado en profundidad los riesgos de un uso indebido por parte del consumidor. ;Yes, for all AI applications;;;
F530503;09-06-2020 17:58;German;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;No opinion;4 - Important;4 - Important;;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;;Current legislation may have some gaps;;No;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F530502;09-06-2020 17:39;German;NGO (Non-governmental organisation);Maria;Wersig;;Deutscher Juristinnenbund e.V.;9304391101-08;Large (250 or more);Germany;The feedback can be published with your personal information;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;Bürger*innen und Verbraucher*innen dürfen nicht aus dem Fokus geraten. Diese liefern die Daten und konsumieren die KI-unterstützten Produkte und Dienstleistungen. Ohne entsprechende Kenntnisse werden Bürger*innen und Verbraucher*innen kein Vertrauen in neue Technologien entwickeln. Daher empfehlen wir, in Aufklärungskampagnen – bspw. durch schulische Bildung oder Maßnahmen der Verbraucherzentrale zu investieren.;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;"Neben den genannten Bereichen sollte außerdem ein interdisziplinärer Austausch in Erwägung gezogen werden. Zu den Disziplinen gehören neben Data and Computer Science nach Ansicht des djb auch die Rechtswissenschaften, Philosophie und Sozialwissenschaften. Ein besonderes Augenmerk bei der Entwicklung einer europäischen KI-Strategie sollte auf der Gewährleistung der Gleichberechtigung aller
Menschen liegen.";3 - Neutral;4 - Important;3 - Neutral;Der djb begrüßt, dass insbes. auch Frauen* Kompetenzen im Bereich KI entwickeln sollen u.dies entsprechend zu fördern ist. Er sieht die Gefahr, dass wegen des Mangels an Frauen* in Führungs- und Entscheidungspositionen der Beitrag u.die Interessen v. Frauen* unterberücksichtigt bleiben. Er empfiehlt, dass insbesondere WissenschaftlerInnen bei Forschungsprogrammen beteiligt, Unternehmerinnen angehört u. Startups von Frauen besonders gefördert werden. Gender-u. Diversitätskonzepte sind notwendig.;4 - Important;4 - Important;4 - Important;3 - Neutral;4 - Important;Der djb empfiehlt insbesondere Unternehmerinnen bei der Investitionsvergabe zu berücksichtigen. Es sollten solche Projekte gefördert werden, die bestehenden strukturellen Benachteiligungen entgegenwirken und Gleichberechtigung gewährleisten.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Der djb teilt die Auffassung der KOM, dass KI sowohl Chancen als auch Risiken birgt. Insbesondere begrüßt der djb, dass die KOM das Diskriminierungsrisiko gegenüber Frauen* durch KI erkennt und dieses auch explizit nennt. Eine Verbesserung der Gleichberechtigung kann nicht gelingen, wenn KI bestehende Ungleichheiten fortschreibt. Andererseits wohnt der KI das Potenzial inne, bestehende Ungleichbehandlungen aufzudecken, sie von vorneherein zu verhindern oder sie zu korrigieren.;Other;"Der djb teilt die Ansicht der KOM, dass Regelungen grundsätzlich technologieneutral formuliert sein sollten, um Innovation und Fortschritt zu ermöglichen. Gleichzeitig bestehen Lücken in den bestehenden Regelungsregimen. KI-Anwendungen
bergen ein erhebliches Diskriminierungsrisiko. Der djb empfiehlt den situativen Anwendungsbereich der Europäischen Antidiskriminierungsrichtlinien und von § 2 AGG auf KI-Anwendungen zu erweitern und Art. 22 DSGVO entsprechend zu
ergänzen.";Other;Der djb schließt s. Datenethikkommission (DEK) (B2,S.41) an, nach der nur KI-Anwendungen m. ganz geringem Risiko nicht verbindlich reguliert werden sollten. Zu Recht hat DEK dies m.d. verfassungsrechtl. Wesentlichkeitsgrundsatz u. bisherigen Erfahrungen m. Selbstregulierung begründet. Es ist in jedem Zeitpunkt v. allen KI-Systemen Nichtdiskriminierung einzuhalten. Damit vermeintl. risikolose KI-Anwendungen nicht doch diskriminieren, sollte Kategorisierung aller KI-Anwendgen regelm. evaluiert w.;;;"Beispiele für besonders risikobehaftete KI-Anwendungen finden sich z.B. im Bereich der
Leistungsverwaltung und bei der Begründung, Durchführung und Beendigung des
Arbeitsverhältnisses.";5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;Erst wenn die Effektivität v. biometrischen Fernidentifikationssystemen als zuverlässiges Mittel der Gefahrenabwehr u. Strafverfolgung festgestellt ist, sollten diese nur unter strengen Auflagen einsetzbar sein (z.B. Kennzeichnung d. überwachten Bereiche u. Verwertung nur für abschließend im Gesetz genannte Straftaten). Auch müssen die Trainingsdatensätze so repräsentativ zusammengesetzt sein, dass d. Gefahr v. Falschidentifikationen - etwa v. Frauen bestimmter Ethnien - maximal minimiert wird.;Much;Die Einführung eines freiwilligen Kennzeichnungssystems könnte für KI-Anwendungen mit ganz geringem Risiko i.S. der Definition der Datenethikkommission sinnvoll sein. Dieses Kennzeichnungssystem sollte u.a. prüfen, ob eine KI-Anwendung geschlechterrelevant ist.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Für ein freiwilliges Überprüfungssystem könnte die Schaffung eines Gütesiegels sinnvoll sein, das u.a. die Überprüfung des Diskriminierungspotenzials von Frauen* beinhaltet.;Mental health risks;"Der djb vertritt die Ansicht, dass KI-Anwendungen das Qualitätsmerkmal
‚Nichtdiskriminierung‘ erfüllen müssen. Die Regelungssysteme von Art. 22 DS-GVO, § 2
AGG und Produkthaftung sind insofern aufeinander abzustimmen, als Geschädigte ihre
Rechte effektiv geltend machen können. Der djb schließt sich außerdem den Empfehlungen der DEK (Teil E, Ziff. 3,6) „Datenschutz durch Technikgestaltung“ an.";Yes;Insbesondere KI-Anwendungen, die maschinell lernen, sollten einer regelmäßigen Kontrolle unterliegen, um ihr Diskriminierungsrisiko so gering wie möglich zu halten.;Yes;Der djb ist der Ansicht, dass die Produkthaftungs-RL den Herausforderungen von KI-Anwendungen nicht ausreichend begegnet. Deswegen sollte der europäische Gesetzgeber nachjustieren, damit Verbraucher*innen EU-weit auch im digitalen Zeitalter effektiv ihre Rechte durchsetzen können.;Yes, for all AI applications;;;st20-20_Wei_buchKI_EU.pdf
F530501;09-06-2020 16:51;English;Business Association;Anna;Bosch;;ACT | The App Association;72029513877-54;Small (< 50 employees);Belgium;The feedback can be published with your personal information;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;The cultural, workforce training and education, data access, and technology-related changes associated with AI will require strong guidance and coordination. An EU-wide AI strategy incorporating guidance on issues including research, quality assurance and oversight, thoughtful design, access and affordability, ethics, privacy and security, interoperability, biases, and education will be vital to achieving the promise that AI offers to consumers and European economies.;4 - Important;5 - Very important;5 - Very important;;No opinion;No opinion;No opinion;No opinion;No opinion;They should provide practical information on EU funding opportunities and how to accessing it. Guides and resources on best practices for complying with regulation are important especially for small businesses. ;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;This question is framed negatively and should be revisited by the Commission. For example, of course AI could endanger safety, but it could also hugely benefit safety. There is no question that these concerns are important, but it is important to keep in mind that AI is capable on having significant positive effects overall.;Current legislation may have some gaps;;Other;This depends on the definition of high-risk applications. A risk-based approach, while comprehensive, must continue to allow for development and innovation, especially from SMEs. Certain applications may require stricter regulation. Many others may not. It’s currently unclear by whom and how the determination of what is a high-risk application will be made. Currently the high-risk assessment lacks nuance, and details will need to be carefully examined to avoid unintended consequences.;;;"""High-risk"" depends to a large extend on what is done with the information AI creates, rather than on the application itself. The risk level is determined by how an application is used and in what context. It is not the AI-produced information per se that is 'high' or 'low' risk, but how we (decision makers) use that information that creates the level of risk. ";5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Not at all;Any AI labelling would be premature. AI applications are changing so rapidly that any labelling system would be outdated by the time it enters into force. Generally, voluntary labelling may be useful in cases where there is no legislation. If done in addition to existing legislation, voluntary labelling is not useful and creates an unfair difference between those who can afford to comply with the ‘voluntary labeling scheme’ on top of regulation and those who can’t. ;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;;Risks related to the loss of connectivity;;Yes;Policy frameworks that use risk-based approaches should ensure that the use of AI aligns with the recognized standards of safety, efficacy, and equity. Providers, technology developers and vendors, and other stakeholders all benefit from understanding the distribution of risk and liability in building, testing, and using AI tools. It is also important to understand how and by whom these assessments will be conducted.;Yes;Any update of the product liability directive needs to be principle-based rather than technology-based, so it is future proof and can remain relevant, especially considering the speed of which AI applications are changing and being developed.;No;;Rather than updating current national liability rules, it would be more beneficial to hamonise frameworks and to establish a European-level policy framework. This would generate legal certainty for businesses all over Europe. ;AI-policy-principles-ACT.pdf
F530500;09-06-2020 16:49;Spanish;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;3 - Neutral;No opinion;4 - Important;4 - Important;4 - Important;;;;;;;;;;;;;;;;;;;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;;There is a need for a new legislation;;No;;;;Spreading of fake information affecting the values of democracy;3 - Neutral;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;Biometric identification will limit human and social rights;Much;The problem is to what extent you can ensure that it will be voluntary without limiting the services to the people that don't want to be labelled.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;;;Yes;;Yes;;Yes, for all AI applications;;;
F530499;09-06-2020 16:22;English;NGO (Non-governmental organisation);EDRi;Policy;;European Digital Rights (EDRi);16311905144-06;Small (< 50 employees);Belgium;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;1 - Not important at all;1 - Not important at all;Ecosystem of excellence must include trust. Development and deployment of AI systems must respect human rights. EU funded research must follow HLEG Guidelines • We oppose AI uptake in public sector in absence of substantial benefits and heightened safeguards against risks to human rights • We call for additional fundamental rights safeguards in the lifecycle of public procurement processes • Ensure democratic oversight, include civil society and impacted communities in meaningful consultation;4 - Important;No opinion;1 - Not important at all;2 - Not important;4 - Important;2 - Not important;"Promotion of AI is not a value in itself. The coordinated plan should include common scientific/ policy criteria to determine the allocation of resources for the
above listed purposes, rather than assume benefits in areas such as health and
transport
The coordinated plan and member state strategies should include a section on
human rights, societal impacts of AI/ automation, and how to ensure democratic
oversight
The build-up of European data space must comply with the GDPR";3 - Neutral;4 - Important;1 - Not important at all;"Public interest should set priorities of research centers and research partnerships
Research priorities should include the human rights and societal implications of
the development and use of AI, fairness design, discrimination risks and transparency
Receiving public funding for research should require fulfilling a set of criteria
following the EU Ethics Guidelines
The Horizon2020 program must be reviewed to ensure fundamental rights both in the funding process and in funded projects";1 - Not important at all;2 - Not important;3 - Neutral;3 - Neutral;No opinion;"DIHs and other innovation incentives for SMEs must not provide exceptions from fundamental rights. The Clearview AI example shows that small companies can cause harms, too. There should be no blanket exemptions of sandboxing for innovation solely based on the size of the entity

The EU should develop a scheme where the public funding on AI applications is reverted to the public, for example by making outcomes publicly available and applications licensed under Free Software licenses";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;"AI used in sensitive areas (public services) without democratic oversight, transparency
or sufficient evidence to justify the purpose
Increasing use of opaque, privately owned tech
Conscious obsfucation of accountabilityAI poses collective harms which cannot be addressed in anti-discrimination or data
protection frameworks
‘Innovation’ invoked to justify trials without safeguards
Characteristics of machine learning can lead to unauthorised/ secondary use and
function creep";Other;"Legislation must strengthen, not replace, GDPR. AI presents issues for meaningful consent, objection, data minimisation, purpose limitation, explanation
AI law must complement a broad GDPR interpretation, including affinity profiling + sensitive inferences
Current law does not address use of non-personal data, and collective impact of AI, such
as overpolicing, surveillance, inequality
Current law does not cover AI discrimination on non-protected grounds, eg financial
status";Other;"New rules are necessary to determine the criteria for when it should be legal to develop
and deploy AI
- standards for scientific and policy evidence
- burden of proof is on the developer/deployer and not on impacted groups
- mandatory democratic oversight before deployment of AI in public sphere
Mandatory fundamental rights impact assessments for all uses
The EU should proactively ban AI applications in areas where the fundamental rights and
societal implications are too great to risk";;;"Use of AI to determine delivery of essential public services, predictive policing, autonomous lethal weapons, identification/ analysis of emotion and identity traits, and indiscriminate biometric surveillance, are incompatible with fundamental rights and should be banned by default
Determining ‘risk’ should be rights and outcomes focused, not sectoral. Major concern with
systems which impact fair trial, in migration control and policing, and systems which may perpetuate inequalities in hiring";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;"The use of biometrics for remote identification in publicly-accessible spaces significantly contributes to unlawful mass surveillance so should never be deployed.
Such uses will transform public spaces into sites of continuous watching and irreversibly compromise fundamental rights to privacy, freedom of assembly, expression, non-discrimination,
data protection, fair trials, democracy and the presumption of innocence. The EU must harmonise and enforce existing legislation to protect these rights.";Not at all;"We should exercise caution deciding that some applications are inherently low risk and therefore do not require oversight to guarantee fundamental rights.
We strongly caution against incorporating voluntary, self-regulatory and ethics based approaches in AI regulations. Such approaches provide scope to cirucmvent accountability and soften fundamental rights obligations. Further, they reduce certainty and impede access to
justice for those harmed.";Other enforcement system;"In addition to regulatory delineation of scope of lawful AI, including bans, there should be oversight of all applications to guarantee fundamental rights
We call for mandatory, ex ante human rights impact assessments (include an assessment of collective and social harms posed by applications, with review at the stages of design, development, testing and deployment. Clearly enforced consequences should applications fail to meet certain standards, including the potential to halt deployments";Insofar as possible compliance with regulatory measures should be guaranteed by external and independent entities, avoiding self regulation and ensuring there are no loop holes to fundamental rights protection.;Mental health risks;Heightened risks of discrimination, in particular with reference to online products and services using data for targeted advertising. This poses risks of differentiated pricing, discrimination financial detriments, the risk of creating filter bubbles, interference in the democratic process, based on sensitive inferences or associations. In addition, there are concerns related to accessibility or harms specifically to be experienced by people with disabilities. ;Yes;Internal supervisors, such as Data Protection Officers under GDPR should be included and and asked for advice.;Yes;"Liability should be centered around accountability and the extent to which faults can be redressed by users of technology.
There should be liability for producers of AI that do not disclose source code (including their algorithmic models/ datasets) and do not provide fixes for issues brought to their attention or otherwise hinder fixes from being applied, for example by not allowing third-party fixes based on any disclosed source code. The issue of repurposing of AI systems should be addressed";Yes, for all AI applications;;"AI applications are covered both by copyright and database rights protections, which prevent users from assessing their quality and limit their ability to redress issues that have been observed. Moreover, current software liability is usually exonerated through license agreements.

Liability rules should limit the ability to exonerate such liabilities by providing incentives for
openness.";Paper-Ban-Biometric-Mass-Surveillance.pdf
F530498;09-06-2020 15:57;Bulgarian;Public authority;Nina;Dimova;National;?????????? ???????? ?? ????????????;;Medium (< 250 employees);Bulgaria;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;4 - Important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;4 - Important;;4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;;There is a need for a new legislation;;No;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Risks related to the loss of connectivity;;Yes;;Yes;;Yes, for specific AI applications;;;Perspectives_NIJ.docx
F530497;09-06-2020 15:52;English;Company/Business organisation;Jens;BJÖÖRN;;Fortum;03501997362-71;Large (250 or more);Finland;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;3 - Neutral;4 - Important;3 - Neutral;;5 - Very important;3 - Neutral;4 - Important;3 - Neutral;4 - Important;3 - Neutral;;3 - Neutral;3 - Neutral;4 - Important;;4 - Important;4 - Important;4 - Important;5 - Very important;3 - Neutral;;3 - Neutral;5 - Very important;4 - Important;4 - Important;No opinion;No opinion;;Current legislation may have some gaps;;Yes;;No;;Decisions that could compromize basic rights;2 - Not important;3 - Neutral;5 - Very important;3 - Neutral;1 - Not important at all;5 - Very important;No further guidelines or regulations are needed;This is covered with the GDPR but further guidelines could be appriciated;Rather not;If it's a European wide labeling standard it could be useful especially if its an alternative to regulation.;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;Accountability should be the key element when creating an AI regulatory framework instead of creating exhaustive lists of sectors and critical use with demands of prior conformity assessments and approval. Emphasis should be on self assessement.;Personal security risks;;No opinion;;No opinion;;No opinion;;;Fortum_AI_position.docx
F530496;09-06-2020 15:44;English;Business Association;Constantin;Greim;;Gesamtverband der Deutschen Versicherungswirtschaft e.V.;6437280268-55;Medium (< 250 employees);Germany;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;;;;5 - Very important;;4 - Important;4 - Important;4 - Important;5 - Very important;3 - Neutral;;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;"This point is difficult to assess because the questionnaire treats different kinds of AI in a uniform way, but in fact large range of possible applications, each with a specific hazard potential.
Existing legislation already applies to AI (data protection, anti-discrimination, etc.). When factoring in protective measures established by existing regulation, the assessment shifts towards ""not important"".

";Other;Existing schemes should be reviewed for their effectiveness. Many regulations already affect AI. It should be examined not only whether further regulation is necessary, but also which existing regulations need to be adjusted for facilitating the use of AI (data protection).;Yes;;Other;The limitation to high-risk AI applications is reasonable. However, it must be precisely determinable which applications are considered high-risk. The approach of possibly deviating from the cumulative requirement of sector and concrete hazardousness of the application creates considerable legal uncertainty and should be reconsidered. If exceptions are created, they must be strictly limited. In addition, the requirements for the exceptions should be clearly defined.;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;No opinion;;Very much;"We believe that the system should be unbureaucratic. Furthermore we do not believe there would be a need for frequent re-certification.
Existing certification options should be adapted if necessary instead of creating new / unnecessary ones.
";A combination of ex-ante compliance and ex-post enforcement mechanisms;;"A differentiated approach is required here. First and foremost, we believe that an ex-ante self-assessment of the companies is sufficient. For high-risk applications, a conformity check may be considered; a limitation to high-risk AI applications is urgently required.  
Moreover, countermeasures should be taken if a risk subsequently arises.
";;;No opinion;;No;PLD is fit for purpose in the context of AI applications. Our position is more fully explained in the attached document.;No;;Our position on operator’s liability for AI applications is more fully explained in the attached document;GDV_Position_Paper_AI_White_paper.pdf
F530495;09-06-2020 14:52;English;;;;National;;;;;The feedback can be published in an anonymous way;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;;4 - Important;5 - Very important;4 - Important;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;;Current legislation may have some gaps;;Yes;;Yes;;Decision-making AI applications;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;Data training etc. must be performed periodically;No opinion;;Yes, for all AI applications;;;
F530494;09-06-2020 14:26;English;Consumer Organisation;Chiara;GIOVANNINI;;ANEC;507800799-30;Small (< 50 employees);Belgium;The feedback can be published with your personal information;3 - Neutral;4 - Important;4 - Important;3 - Neutral;3 - Neutral;3 - Neutral;Research activities on interoperability of AI systems.;4 - Important;4 - Important;3 - Neutral;3 - Neutral;4 - Important;4 - Important;"the AI Coordinated Plan should also address societal and environmental well-being priorities, including consumer protection.
When dealing with skills, it is important not to forget that not all consumers are digitally savvy and therefore measures have to be put in place to avoid that digital/AI exclusion. Consumers might need new skills to chose and use AI products/services and consumers organisations might need new skills to advise consumers and test AI products/services/standards.";4 - Important;4 - Important;3 - Neutral;;No opinion;No opinion;No opinion;No opinion;No opinion;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"Humans interacting with AI systems must be able to keep full and effective selfdetermination/autonomy over themselves, and be able to partake in the democratic process. AI systems must not create asymmetries of power or information, such as between employers and employees, businesses and consumers or governments and citizens. 
AI systems must not endanger the environment. 
New AI definition: EC HLEG on AI in the ethical guidelines.";Other;"There are gaps in present legislation and new AI related aspects such as explicability require new legal provisions, especially for enforcement purposes.
We also believe that new consumers rights should be enshrined, for all AI systems, and not only high-risk applications, as follows:
-Right to Transparency, Explanation, and Objection
-Right to Accountability and Control
-Right to Fairness
-Right to Safety and Security
-Right Access to Justice
-Right to Reliability and Robustness";Other;"New rules need to cover risks posed by AI systems in a proportionate manner, with more stringent rules for high-risk application. 
The EU regulatory approach on safety should be based and explicitly refer to the precautionary principle.";;;Legislation should be adopted to make the appropriate risk assessment of all AI systems which takes account of the nature of the hazard and the likelihood of its occurrence. Based on the assessment results, different rules can be applied in a proportionate manner. Once the risk is identified, mitigating measures have to be adopted (by industry, public authorities).;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;Legislation is needed to determine how and by whom the technology can be used and the guarantees for citizens and consumers.  the EU must develop a strong, privacy-protective approach for biometrics systems before they are largely used  in public spaces.;Rather not;Consumer information is always useful in order to ensure transparency. However labels are as good as the requirements and enforcement systems they are based on. Once clear legal rules and enforcement mechanisms will be in place, the role of a trustworthy label could be considered. However this also depends on the high-risk and low-risk definition. But the inherent information asymmetry in AI evolving/machine learning system, makes the role of a label different than like about food (eg: ecolabel;A combination of ex-ante compliance and ex-post enforcement mechanisms;;any assessment, audit, certification, market surveillance activities have to cover the evolving nature of the AI system. For this, access to the AI system algorithms, codes and data sets must be ensured to understand and assess the risks.;Mental health risks;risks to the environment;Yes;"continuous aspect of conformity assessment.
it could also be useful to consider requirements for a built-in and automatically starting self-diagnostic mechanism of the AI system.";Yes;"We believe that updated product liability rules should serve as a clear safety net for consumers when they face problems with their digital goods. 
What truly matters for consumers is a fair compensation for the damage suffered, regardless of what type of risk ultimately materialised. ";Yes, for all AI applications;;;
F530493;09-06-2020 14:01;Finnish;;;;;;;;;The feedback can be published in an anonymous way;3 - Neutral;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;;3 - Neutral;3 - Neutral;5 - Very important;4 - Important;4 - Important;No opinion;;3 - Neutral;4 - Important;5 - Very important;;5 - Very important;4 - Important;4 - Important;5 - Very important;No opinion;;3 - Neutral;5 - Very important;5 - Very important;4 - Important;4 - Important;3 - Neutral;;There is a need for a new legislation;;Yes;;Yes;;;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Risks related to the loss of connectivity;;Yes;;Yes;;No opinion;;;
F530492;09-06-2020 13:34;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;3 - Neutral;3 - Neutral;4 - Important;5 - Very important;4 - Important;In addition to working with member states, the European Commission should consider closer alignment, legal reciprocity outside the Member states. ;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;Testing facilities should open up to labelling and certification to increase confidence and proactive compliant by design AI applications.;4 - Important;4 - Important;5 - Very important;;4 - Important;5 - Very important;4 - Important;4 - Important;3 - Neutral;We believe that specialized innovation hubs should focus on specific industries to maximize the opportunities to foster innovation and development in specific application of AI.;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;;Current legislation may have some gaps;;Yes;;Other;We support a risk based approach to determine the level of oversight, with certain sectors being inherently more risky than others, and certain activities within those sectors being the highest risk. However, the approach to determine “high risk” AI applications is unclear, and clarification (including specific factors to be taken into consideration) would be welcome. We agree that the introduction of new compulsory requirements should be limited to high risks applications;;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);Biometric identification systems should be used in case of security and law enforcement as long as privacy rights are respected. ;Much;"We believe that a new voluntary labelling system would be useful for AI systems that are not considered high risk. Standardization of what such labels should contain would provide greater ease of use by individuals.
More clarification on how this labelling system would work in practice (e.g. how it would be monitored, where the labels would be placed) would be welcome";Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;If Europe wants to be competitive while  remaining trustworthy it needs to create self assessment by specialized third parties following a standard assessment procedure ;Personal security risks;;No;;Yes;"We believe that the current EU liability legislative framework should be adapted to address new AI risks and cover the specific characteristics of new technologies. 
Similarly, the new rules should however be built on existing legislations and focus on existing gaps to avoid uncertainty. 
";No opinion;;;American_Express_comments_on_the_White_Paper_on_Artificial_Intelligence.pdf
F530491;09-06-2020 13:12;English;Business Association;Laurent;Louette;;European Confederation of Pharmaceutical Entrepreneurs - EUCOPE;87600691525-93;Micro (< 10 employees);Belgium;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;The MHRA’s Clinical Practice Research Datalink is a best practice model for data sharing for research today, data quality, and country participation. In that line, we would recommend the creation of a EU common data model and improved access to all EU Member States data for research. The Commission could consider partnership with the WHO, the International Council for Harmonisation, the International Medical Device Regulators Forum, and the Institute of Electrical and Electronics Engineers.;4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;4 - Important;;5 - Very important;4 - Important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Small and medium companies are a central player in the deployment of AI services in Europe. They form the backbone of the EU economy and their inherent flexibility and innovation focus will prove to be substantial advantages. We need to develop centers of Excellence. The Israeli program StartUp Nation Central can be seen as best practice as a lighthouse, guiding academics, entreprises, government agencies and entrepreneurs all to work towards one direction.;3 - Neutral;3 - Neutral;4 - Important;3 - Neutral;3 - Neutral;3 - Neutral;;Current legislation may have some gaps;;Other;We support defining risk categories for AI systems that correlate with the anticipated level of regulatory oversight and requirements, similar to the IMDRF SaMD risk classification guidance, with additional external stakeholder input.Changes and/or additions to the EU legal framework may be necessary to accommodate AI and machine learning, including ‘unlocked’ algorithms that change over time. We encourage policies that place an initial focus on sponsor governance and accountability;;;A lack of transparency can lead to a more fraud-prone system;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Very much;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Cyber risks;;No opinion;;No opinion;;No opinion;;;AIConsult2020_EUCOPE_080620.pdf.docx
F530490;09-06-2020 13:12;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"1) Implement de gender perspective in all aspects of the sector (from use of inclusive language, to parity in working teams, leadership and academic enrollment from womens)
2) Implement actions to evaluate the ecological footprint of AI at least in terms of energy required to keep the datacenters to store all data and in terms of CPU time required to process it.  We need to understand where BigData is really needed and gives added value and keep with right (and may be small) data when suitable";5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;Strength the financing of RESEARCH in AI;4 - Important;5 - Very important;4 - Important;"There is an important need to establish actions to bridge the gap between the places where knowledge is created (research and academy) and where it is used (industrial sector) by strengthing the technology transfer mechanisms university/research centers-industry/bussiness. This is complementary to strength innovation in companies or technological centers

Also, we miss actions to strength the conservation of local talent";4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;;4 - Important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;4 - Important;"Missunderstandings: The total transparency of the AI extent assumptions, accuracy, tests done to validate the system, privacy rules, security procedures etc etc is required to make an appropriate use of it whitout implicit assumptions that something has been checked and guarantees robust use in a particular future scenario. ""Extrapolation"" of performance to non tested scenarios is dangerous and there is a need to identify when the red line is crossed";There is a need for a new legislation;;Other;"We are for the implementation of some ETHICAL AI seal, such that the AI systems can certify which ethical properties they accomplish and add the seal to their products. We are not thinking about a binary procedure ""this system has the seal or not"" but something with graduation ""ETHICAL AI"" level 1, 2, 3, .... whatever, and transparent list of ethical properties associated to each level";;;"This is an additional concern not sufficiently clear in the previous question: 
We observe that ""de facto"" it is assumed that personal data is ""name, id, address, mail, phone"" and it is enough to eliminate this data from data bases to get anonymized data that can be served to third parties for whatever use (bussiness, research....). Minorities and rare patterns of specific individuals reveal their identity even without their names if anonymization is not aggregating/encrpypting individual data";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Other special requirements in addition to those mentioned in the question above should be imposed (please specify);"If we are talking about remote identification, the person should always be aware of it and CONSENT

If we are talking about authentication based on biometric systems, there would be never mandatory to access the system, but alternative to other systems that the person should use  if he do not wants to share its personal image to the authentication system

Also AI system must be very transparent on the assumptions made and ethical properties  before users decide to use them or not";Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;"Total transparency on the assumptions, accuracy, tests done to validate the system, privacy rules, security procedures etc etc and periodic monitoring of how the adaptive AI systems might degenerate in performance along time can help to make proper use of the system and decrease risk 

Also, explanatory & argumentative layers are clue to understand the behaviour of AI systems and properly interact with them in real life (not specific for robots, but also recommenders, IDSS...

";Risks related to the loss of connectivity;;Yes;;Yes;"We wonder about the capacity of member states to enact new laws that allow them to close intelligent systems that can fullfill all ethical and safety and accuracy legal requests in favour of the ""general interest"" just for policital interests and that this precludes the right of information or other fundamental rights to citizens. New AI european legislation must be such that european values are preserved and guaranteed in all countries.
";Yes, for all AI applications;;;
F530489;09-06-2020 13:10;English;NGO (Non-governmental organisation);Adam;Bartha;;European Policy Information Center - EPICENTER;080362721653-38;Micro (< 10 employees);Belgium;The feedback can be published with your personal information;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;;4 - Important;4 - Important;4 - Important;3 - Neutral;3 - Neutral;2 - Not important;;No opinion;No opinion;No opinion;;3 - Neutral;;3 - Neutral;2 - Not important;3 - Neutral;;2 - Not important;4 - Important;2 - Not important;No opinion;No opinion;2 - Not important;;Current legislation may have some gaps;;Yes;;No opinion;;;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;No further guidelines or regulations are needed;;Much;;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;;;;No;;Yes;;Yes, for specific AI applications;;;The_Future_of_Artificial_Intelligence.pdf
F530488;09-06-2020 12:15;German;EU Citizen;Tom;Grille;;;;;Germany;The feedback can be published with your personal information;3 - Neutral;5 - Very important;5 - Very important;2 - Not important;4 - Important;2 - Not important;;4 - Important;5 - Very important;2 - Not important;3 - Neutral;5 - Very important;3 - Neutral;;5 - Very important;5 - Very important;4 - Important;;2 - Not important;5 - Very important;3 - Neutral;3 - Neutral;3 - Neutral;;2 - Not important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;;There is a need for a new legislation;;Other;Ein einheitlicher Regelsatz (z. B. bzgl. Nachvollziehbarkeit) verhindert das Etablieren von Vermeidungsstrategien und langwierigen Feststellungsverfahren.;;;Öffentlicher Sektor, persönliche Daten aller Art, Entscheidungen die das Leben von Menschen relevant betreffen (eigentlich alles außer Gaming);5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;Biometrische Eigenschaften sind unveränderbar. Einmal verlorene Daten können nicht wiedergewonnen werden. Damit ist jeder Fehler mit diesen Daten eine Schädigung auf Lebenszeit aller Betroffener;Much;Voluntary labeling ist sinnvoll, wenn ein solides grundlegendes Regelsystem besteht. Leider birgt es immer die Gefahr, notwendige Regeln ins Freiwillige auszulagern.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Meines Wissens nach sind Techniken der AI-Auditierung (sowohl ex-post als auch ex-ante) immer noch Forschungsthemen;;Nachvollziehbarkeit, Verantwortlichkeit, Ansprechpartner, WIderspruchsmöglichkeiten;Yes;;Yes;;Yes, for all AI applications;;Ich denke eine allgemeine Regelung, die durch spezifische Ausnahmen z. B. für Gaming ergänzt wird, ist sicherer;
F530487;09-06-2020 12:14;English;Academic/Research Institution;Sandra;Wachter;;University of Oxford;;Large (250 or more);Austria;The feedback can be published with your personal information;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;Accountable AI and protecting Human Rights ;No opinion;No opinion;No opinion;No opinion;No opinion;;Accountable AI and protecting Human Rights ;No opinion;No opinion;No opinion;take steps to ensure fair, explainable and privacy perserving AI ;No opinion;No opinion;No opinion;No opinion;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"















";There is a need for a new legislation;;No;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;"The current laws are not sufficient to govern AI. 3 areas of critical concern include a) XAI b) privacy and data protection  and c) bias and discrimination 

a)	On how there is no right to have AI based decisions explained “Wachter et al Why a Right to Explanation of Automated Decision-Making Does Not Exist in the General Data Protection Regulation” https://academic.oup.com/idpl/article/7/2/76/3860948 and how Counterfactual Explanations can help to make black box AI explainable “Wachter et al Counterfactual Explanations without opening the black box https://jolt.law.harvard.edu/assets/articlePDFs/v31/Counterfactual-Explanations-without-Opening-the-Black-Box-Sandra-Wachter-et-al.pdf

b)	On how privacy laws are not sufficient to guard against the pervasive nature of inferential analytics (e.g. inferring health status, sexual orientation, gender, ethnicity) and on how these laws offer too little protection against inferred data and trade secrets and ideas to move forward see “Wachter and Mittelstadt A Right to Reasonable Inferences: Re-Thinking Data Protection Law in the Age of Big Data and AI” https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3248829 and here “Wachter Data Protection in the Age of Big Data” https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3355444 

c)	On how non-discrimination laws are not good enough to protect against biased and discriminatory AI, plus ideas on how to fix this problem “Wachter et al Why Fairness Cannot Be Automated: Bridging the Gap Between EU Non-Discrimination Law and AI https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3547922  and in the context of online targeting and online ads see also “Wachter Affinity Profiling and Discrimination by Association in Online Behavioural Advertising  https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3388639 

";No opinion;;No opinion;;;;;No opinion;;Yes;;Yes, for specific AI applications;;;Wachter_et_al_Why_Fairness_Cannot_Be_Automated_Bridging_the_Gap_Between_EU_Non-Discrimination_Law_and_AI.pdf
F530486;09-06-2020 12:13;Spanish;Business Association;Asociación Nacional de;Establecimientos Financieros de Crédito;;Asociación Nacional de Establecimientos Financieros de Crédito;11218815591-29;Large (250 or more);Spain;The feedback can be published with your personal information;5 - Very important;4 - Important;5 - Very important;4 - Important;4 - Important;3 - Neutral;-;4 - Important;4 - Important;3 - Neutral;4 - Important;4 - Important;4 - Important;-;3 - Neutral;;2 - Not important;-;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;-;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;-;Current legislation may have some gaps;;No;;;;-;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;4 - Important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Rather not;-;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Cyber risks;-;Yes;-;Yes;-;Yes, for all AI applications;;-;
F530485;09-06-2020 11:27;German;Public authority;Damian;Patting;International;Bevollmächtigter des Rates der Evangelischen Kirche in Deutschland (EKD) - Dienststelle Brüssel;61973396926-78;Small (< 50 employees);Germany;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;Aus EKD-Sicht setzt die Etablierung eines Exzellenz- und Vertrauenssystems eine umfassende gesamtgesellschaftliche Debatte über Chancen und Risiken von KI-Systemen sowie über das Verhältnis der KI zum Menschen und der Gesellschaft im Allgemeinen voraus. Die hat bisher nur in Ansätzen stattgefunden. KI könnte unsere Gesellschaft und Prinzipien wie Menschenwürde und Grundrechte verändern. Dafür braucht es die Einbindung von Expertise im Bereich Ethik, um einen ganzheitlichen Ansatz zu garantieren.;5 - Very important;5 - Very important;5 - Very important;No opinion;4 - Important;5 - Very important;Als Teil eines integralen Konzeptes könnte angesichts der Dynamik im Bereich KI und aufbauend auf den Arbeitsergebnissen der High-Level Working Group eine begleitende Dialogplattform etabliert werden, in der Akteure der Zivilgesellschaft, Ethiker und Experten verschiedener Disziplinen über die politischen Maßnahmen in adäquater Art und Weise  informiert werden und im Rahmen des politischen Entscheidungsprozesses zu berücksichtigende Empfehlungen abgeben können.;5 - Very important;4 - Important;3 - Neutral;Sofern ein solches „Leuchtturm-Forschungszentrum“ tatsächlich entstehen sollte, müsste man konsequenterweise nicht nur technische Expertise, sondern auch Ethiker und Sozialwissenschaftler dort ansiedeln. In Kooperation mit den Praktikern könnten die entsprechenden Experten u.a. die bestehenden Leitlinien für neue KI-Technologien testen bzw. weiterentwickeln. Im Rahmen des Horizon Europe Programms (Säule III) sollten Forschungsprojekte zur Stärkung eines ethischen Ansatzes gefördert werden.;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Das Weißbuch stellt stark auf Algorithmen-spezifische Perspektiven ab. Es sind aber auch bei der Sammlung von Daten ethische Grundsätze und Regeln erforderlich, um Wohlfahrt durch Nutzen und Teilen von Daten, Datenqualität und Transparenz und Rechenschaftspflichten zu garantieren. Zudem sollte die angestrebte Kohärenz mit der Verwirklichung der Ziele zur nachhaltigen Entwicklung 2030 und zum Europäischen Green Deal sich auch konkret in den Regulierungsmaßnahmen abbilden.;There is a need for a new legislation;;Other;Eine bloße Regulierung von „high-risk“ Anwendungen reicht aus Sicht der EKD nicht aus. Stattdessen sollte ein abgestufter Ansatz eingeführt werden, der auch schwächere KI-Systeme einschließt, z.B. in Anlehnung an das Gutachten der Datenethikkommission der Bundesregierung der BRD.;;;Der risikobasierte Kommissionsansatz ist zu befürworten, die zwei-gliedrige Herangehensweise ist jedoch nicht ausreichend. Es können nicht allein der Sektor und der KI-Einsatz bewertet werden, vielmehr müssten u.a. Schädigungspotential, Wahrscheinlichkeit eines Schadenseintritts und befürchtete Schadensschwere im Rahmen einer wertenden Gesamtbetrachtung kombiniert werden. Zudem sollten Anwendungen mit „hohem Risiko“ nicht nur reguliert werden, sondern auch Verboten als ultima ratio unterliegen.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;Der Einsatz von Fernerkennungstechnologien muss unter allen Umständen verhältnismäßig, zeitlich begrenzt und unter Achtung der Grundrechte erfolgen. Der jetzige rechtliche Rahmen ist in keinem Fall als ausreichend zu bezeichnen, die Datenschutzgrundverordnung sollte entsprechend weiterentwickelt werden.;Much;Wenn man freiwillige Zertifizierungen auf EU-Ebene einführt, muss gewährleistet sein, dass unabhängige Zertifizierungsstellen Entwicklern, Betreibern und Nutzern das Zertifikat ausstellen. Die Einrichtung einer neuen Europäischen Agentur für künstliche Intelligenz könnte diesem Vorhaben am besten nachkommen und eine EU-weit einheitliche Regelung garantieren, so dass damit auch das Vertrauen innerhalb der europäischen Bevölkerung in die neu zugelassenen KI-Anwendungen gestärkt würde.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Die zuständigen nationalen Behörden sollten primär für die Bewertung und Überwachung verantwortlich sein, während die EU eine koordinierende Rolle in Form einer neuen Agentur für KI übernehmen könnte. Grundsätzlich sollte der Vorrang menschlichen Handels und menschlicher Aufsicht uneingeschränkt gelten. Zudem ist die Erklärbarkeit und Rückverfolgbarkeit von Algorithmen von zentraler Bedeutung.;Mental health risks;Bei den Risiken sollte man stets zwischen individuellen und gesellschaftlichen Risiken unterscheiden, sodass entsprechend auch ein anderer Regulierungsrahmen notwendig sein kann. Zudem gibt es erhebliche Bedenken bezüglich der Identifizierung und Ortung von Personen mithilfe von KI sowie beim Einsatz tödlicher autonomer Waffensysteme.;Yes;;No;Haftungsfragen sind auch ethische Fragen. Aktuell ist der bestehende Rechtsrahmen ausreichend, um den bestehenden Risiken zu begegnen. Sollten autonome KI-Systeme marktreif werden, müsste der europäische Rechtrahmen angepasst und weiterentwickelt werden, etwa indem Zurechnungsfragen wie jene der Gehilfenzurechnung (in D. in § 278 BGB) europaweit soweit  harmonisiert, dass sie auch etwaige Rechtsfiguren wie den „elektronischen Gehilfen“ erfassen, um (ethisch) sachgerechte Ergebnisse zu erzielen.;No opinion;;;Evangelische_Kirche_in_Deutschland.pdf
F530484;09-06-2020 11:08;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"Ensure dversity in resarch and education; provide low barrier entry to the topic; data test sets need to consider diversity (people of color, women, age)";5 - Very important;5 - Very important;5 - Very important;;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;No opinion;;Other;This is a broad statement. It really depends on what is considered harmful.;;;Decisions leading to death, incarceration;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Cyber risks;;No opinion;;No opinion;;No opinion;;;
F530483;09-06-2020 09:32;English;Other;Susanne;KRAEMER;;Council of the Notariats of the European Union (CNUE);98885666486-72;Large (250 or more);Belgium;The feedback can be published with your personal information;5 - Very important;4 - Important;4 - Important;3 - Neutral;4 - Important;3 - Neutral;;5 - Very important;4 - Important;3 - Neutral;5 - Very important;4 - Important;5 - Very important;;4 - Important;5 - Very important;4 - Important;;3 - Neutral;4 - Important;3 - Neutral;4 - Important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;Other;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;No opinion;;No opinion;;;Contribution-Consultation-AI-CNUE-Accompagnement-09-06-20.doc
F530482;09-06-2020 09:11;German;;;;;;;;;The feedback can be published in an anonymous way;No opinion;5 - Very important;5 - Very important;5 - Very important;2 - Not important;2 - Not important;;5 - Very important;5 - Very important;1 - Not important at all;3 - Neutral;5 - Very important;1 - Not important at all;;5 - Very important;5 - Very important;2 - Not important;;No opinion;5 - Very important;No opinion;4 - Important;1 - Not important at all;;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;3 - Neutral;;There is a need for a new legislation;;No;;;;Automatische Gesichtserkennung;4 - Important;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;Ich kann meine biometrischen Merkmale nicht ändern. Wenn sie jemand vortäuscht, kann dieser Jemand Zugriff erlangen, den er nicht erlangen sollte.;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;Yes;;Yes, for specific AI applications;;;
F530481;09-06-2020 08:49;English;Company/Business organisation;Asuncion;LERA;;DNV GL;414057515145-62;Large (250 or more);Norway;The feedback can be published with your personal information;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Member states must attend in national strategies to the risks that may emerge from the deployment of AI . Access to data is key. R&I organisations must interact more with private sector actors and the R&I system, specially Horizon Europe should ensure  evaluations of proposals  do value partnering with private sector.The section on partnerships portrays the private sector as only co financing. This is a mistake. Private actors should have access to R&I funds. ;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;The alignment proposed lacks reference to the need to pay specific attention to risks that may emerge from AI deployment. A critical issue is not to see AI outside domain knowledge. In sectors like health or autonomous vehicles aliment and coordination is required between specialized AI and domain knowledge. Strengthening research should be done in joint work with private actors to ensure saliency and uptake.;4 - Important;5 - Very important;5 - Very important;Europe aims for a normative deployment of AI, but the texts do not consider the critical importance of setting up independent third part quality assurance and risk management guidelines for deployment in industrial contexts. The risks and potential of AI need to be seeing hand in hand with sector specific risks. Research excellence without third party experience offering this hybrid knowledge cannot produce urgently needed assurance methods and tools. ;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;These hubs ought to be seen as cornerstones for public-private partnerships. to create a direct flow of information an enable flexible change on the direction of research to enhance Europe's competitiveness. Evaluating the trustworthiness of AI applications ought to be a priority. The hubs ought to promote and support the work of independent third party providers. National innovation hubs ought to have capabilities to provide support on the social and ethical consequences of AI.  ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;No opinion;The  White paper is very focused on building trust between business and consumers but does not take into account the complexities added by AI in existing complex systems within Industry.  The deployment of AI in industrial sectors (for example in energy, transport, food products supply chains, embedded in IoT, or in general in B2B contexts seems to be missing. Yet these are of fundamental importance. however, we think breach of rights is of utmost importance. Accuracy requires context.;There is a need for a new legislation;;Other;We think the question is not a yes or no answer. In the maritime and power sectors some legislation can be adapted but new is also needed. Instead of excessive regulation, cooperative public and private regulation, with domain sectorial knowledge can establish a system of standards for independent third party verification and assurance that can determine AI deployed in a particular context is indeed trustworthy.;;;There are different types of high risk. A health related application is high risk but the deployment of AI in engineering ( airplanes, energy assets or other) are also high risk. It is important not to loose sight to the fact that AI is often added to existing complex systems adding more layers complexity and creating emergent properties. Digital risks are often the result of faliures in the system not only one component. Critical risks to society, like failures in the power system are also key.;3 - Neutral;3 - Neutral;5 - Very important;4 - Important;4 - Important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;The most important thing is to find the right balance between excessive oversight and preventing innovation. Clear guidelines and standards and independent systems of verification and assurance will be fundamental pillars in finding that balance. It is also important not to replicate existing legislation, such as GDPR and to think more along the lines of a sandbox of measures with appropriate independent evaluation systems in place. ;Very much;Voluntary labeling on the level of trustworthiness of an AI system for non high risk applications, such as for example products and elements in a supply chain, is fundamental, as they can spill over as critical risks, but also undermine consumer confidence.  However, voluntary labeling would only work if hand in hand with a system of independent assurance that can establish the trustworthiness of the system.This is key for Europe's competitiveness.  ;A combination of ex-ante compliance and ex-post enforcement mechanisms;;We need both assurance that systems are properly developed and verification that they work as intended in their context. AI is also often integrated with other technologies. The application defines the risks, and as such it is not possible to establish trustworthiness only ex-ante. Most important, what is needed and not considered in the paper is  continuous assurance, the possibility to provide ongoing verification of compliance and intelligent systems change over time.;Personal security risks;"Yes, there are further risks, those that emerge from deploying AI in already complex systems, Integrated with other technologies and other digital technologies. It will be important to attend to those, which could be called ""digital risks"". It is important to note that risks emerge in the context of an application. Those owning and using the algorithms must provide legal certainty. ";Yes;Yes. It is of utmost importance to revise the limitations of existing ways  to risk assess risks to account for the integration of intelligent technologies. While existing assurance methods can go a long way we also need innovation in new approaches.  ;Yes;We believe an appropriate involvement of private actors in regulatory processes is important. Public-private partnerships in governance have produced important results that have impacted beneficially the trustworthiness and competitiveness of European companies. For Example, in the maritime sector, IMO issues requirements, but independent third party actors carry on technical assurance. This is an excellent model to regulate AI. ;Yes, for all AI applications;;While we think current liabilities rules need to be modified to ensure a fair allocation of liability, we think this must be done in a manner that strengthens European markets and competitiveness.;DNV_GL_Supplementary_information_consultation_AI_White_Paper.pdf
F530480;09-06-2020 08:40;English;NGO (Non-governmental organisation);Mislav;Malenica;;Croatian Artificial Intelligence Association - CroAI;832067837735-94;Micro (< 10 employees);Croatia;The feedback can be published with your personal information;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;;3 - Neutral;2 - Not important;4 - Important;5 - Very important;4 - Important;4 - Important;;1 - Not important at all;3 - Neutral;4 - Important;;4 - Important;5 - Very important;3 - Neutral;5 - Very important;4 - Important;;3 - Neutral;4 - Important;4 - Important;2 - Not important;3 - Neutral;1 - Not important at all;;There is a need for a new legislation;;No;;;;Applications for influencing public opinion.;1 - Not important at all;1 - Not important at all;4 - Important;3 - Neutral;3 - Neutral;4 - Important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;;Cyber risks;;No;;No opinion;;Yes, for all AI applications;;;The_Croatian_AI_Association_-_white_paper_position_.pdf
F530479;09-06-2020 02:40;Latvian;EU Citizen;Lauris;Frein?ts;;;;;Latvia;The feedback can be published with your personal information;5 - Very important;5 - Very important;4 - Important;3 - Neutral;4 - Important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;No opinion;No opinion;5 - Very important;;5 - Very important;5 - Very important;4 - Important;"Cilv?ku un MI siner?ijas centrs
";3 - Neutral;No opinion;4 - Important;5 - Very important;No opinion;;No opinion;No opinion;3 - Neutral;4 - Important;No opinion;5 - Very important;;There is a need for a new legislation;;No opinion;;;;Nezin?mais;4 - Important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;No further guidelines or regulations are needed;;Much;Dariet to, ja tas nevar kait?t personai, bet var pal?dz?t daudz?s jom?s;Other enforcement system;;;Cyber risks;;No opinion;;Yes;;No opinion;;;
F530478;08-06-2020 22:50;French;;;;;;;;;The feedback can be published in an anonymous way;2 - Not important;;3 - Neutral;2 - Not important;1 - Not important at all;1 - Not important at all;;4 - Important;4 - Important;1 - Not important at all;1 - Not important at all;3 - Neutral;1 - Not important at all;;4 - Important;;1 - Not important at all;;3 - Neutral;3 - Neutral;2 - Not important;2 - Not important;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;No;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;Yes;;Yes, for all AI applications;;;
F530477;08-06-2020 19:14;German;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;3 - Neutral;5 - Very important;1 - Not important at all;2 - Not important;1 - Not important at all;;3 - Neutral;4 - Important;3 - Neutral;2 - Not important;5 - Very important;4 - Important;;2 - Not important;2 - Not important;4 - Important;Datenschutz und Persönlichkeitsrechte;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;2 - Not important;1 - Not important at all;3 - Neutral;;There is a need for a new legislation;;No;;;;Biometrische Systeme / Überwachung;5 - Very important;5 - Very important;4 - Important;1 - Not important at all;2 - Not important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;"Es schränkt die Freiheitsrechte ein und dies kann ggf. missbraucht werden. 

Anonymität ist wichtig. Für bekannte wie weniger bekannte Personen. Sie erhält die Privatsphäre und ermöglicht frei zu wählen, ob man erkennt werden möchte, oder nicht. Dies wird ohnehin immer schwieriger.

Zudem kann eine riesige Sammlung biometrischer Daten missbraucht werden. Es gibt keine 100%ig sicheren IT-Systeme und es gibt leider auch immerwieder Mitarbeiter die solche Informationsquellen für ihre eigenen Zwecke missbrauchen.";Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;No opinion;;Yes;;Yes, for all AI applications;;;
F530476;08-06-2020 18:04;English;Academic/Research Institution;Marie-Hélène;PAUTRAT;;Inria;373620420777-24;Large (250 or more);France;The feedback can be published with your personal information;5 - Very important;No opinion;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"Citizen engagement initiatives need to be strengthened so that citizens can help shape AI uses and ensure their widespread adoption.
Partnership with the private sector is important as a strategic platform for exchange between the different communities involved. This partnership will only be effective if the relevant stakeholders (industry, academia, politics) are represented and active.";5 - Very important;3 - Neutral;4 - Important;5 - Very important;4 - Important;5 - Very important;"The revised coordinated plan should include elements to respond to changing (policy) priorities (e.g.Green Deal, COVID-19).
With regard to the uptake of AI, support should be given to technological development in Europe (such as the ML Scikit learn library).
The uptake of AI in the public sector should be promoted, taking into account diversity:
appropriate use of AI will necessarily be sectorial.";No opinion;5 - Very important;5 - Very important;Recent investments in ICT-48 should contribute to foster collaboration between communities and different approaches of AI.   A lighthouse  centre,  if   consisting   of   a  tightly   federated  and  highly selective network of a small number of major existing organizations, with international  visibility, committed to a joint strategic roadmap, would be able to position Europe as a world leader in AI R&I.  The priority should be given to support existing organizations and tools.;4 - Important;5 - Very important;4 - Important;5 - Very important;No opinion;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;"The concept of risk must be clarified according to whether it concerns citizens, companies or public authorities. Regulations and technical tools will be different. More research is needed to qualify and quantify the risks, and propose methods for auditing algorithms.
AI-based systems can be used to circumvent existing economic and competition regulations and consequently require a specific regulation.";No opinion;;No opinion;;;;Regalian matters such as defense, security, law enforcement, etc. should fall within the scope of the highrisk applications.;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;No opinion;"European regulation must be accompanied by vigilance with regard to the conditions of employment in the field of AI, for example for learning and content moderation.
In addition to data themselves, a complete history of their origin and processing throughout the value chain will have to be kept for the purpose of later identification of liability.
Rules on liability may clarify the framework but there should be exceptions, so that academic research can generate and experiment disruptive ideas.
The use of biometric identification can have negative effects on privacy and individual freedom that should be carefully analyzed before providing the required authorizations.";Very much;"The voluntary labelling scheme must be based on transparency and rely on trusted toolboxes, otherwise the value of the label will be further questioned .
Ex-post mechanisms should be put in place to audit the label.";A combination of ex-ante compliance and ex-post enforcement mechanisms;;"Given the complexity, scale and volume of data involved, the assessment of the conformity of systems with self-labelling will have to be carried out using automated assessment tools (with human oversight) for the low-risk applications.
A toolkit for auditing AI systems will therefore need to be developed.
AI systems should also provide APIs to enable auditing.";Mental health risks;;Yes;Important changes may occur in the lifetime of a product due to software updates. Compliance should therefore be re-assessed;No opinion;;No opinion;;;Doc_2_-_20200528_Position_Paper_on_AI_VF.pdf
F530475;08-06-2020 17:59;Portuguese;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;;There is a need for a new legislation;;No;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F530474;08-06-2020 17:08;French;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;4 - Important;"Il nous paraît important 
- de clarifier la définition de l'IA afin de préciser le périmètre de ces actions
- de statuer les entreprises de l'IA qui apportent un positionnement stratégique de l'Europe sur des problématiques d'intelligence économique (défense & sécurité - énergie & climat - sécurité alimentaire)
- d'investir également dans la partie matérielle (puces notamment) pour plus d'indépendance";4 - Important;3 - Neutral;5 - Very important;4 - Important;5 - Very important;5 - Very important;Il est important de s'assurer que le champ de l'IA ne sera pas préempté par les US et Chine (GAFAM et entreprises chinoises). L'Europe doit donner les moyens à ses entreprises de rester dans la course. Cependant, le développement de l'IA ne doit pas se baser uniquement sur un système de subventions mais plutôt sur le développement libre des entreprises (notamment startups et PME) qui investissent dans les technologies autour de l'IA.;4 - Important;5 - Very important;5 - Very important;Renforcement de l'excellence à l'université et intégration dans l'enseignement (apprentissage des algorithmes, des bases du code...);5 - Very important;4 - Important;5 - Very important;5 - Very important;4 - Important;Mise à disposition par les grandes entreprises de jeux de données accessibles aux PME;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;;There is a need for a new legislation;;Yes;;Yes;;L'application de l'IA la plus préoccupante est lorsque la décision est prise de manière autonome par le système d'IA et qu'il a un impact systémique (surveillance de masse, justice, restriction des libertés individuelles, décisions financières), un impact sur la sécurité des personnes (armes, voitures autonomes, santé) ou un impact sur le droit des personnes (recrutement, octroi de crédit, discriminations, etc.). ;4 - Important;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;Ces systèmes devraient être encadrés très fermement dans leur usage (ex : restriction aux domaines du terrorisme) et dans leurs modalités d'utilisation (conservation des images, pas de généralisation, sécurité des données...);Very much;Ce label devrait permettre le développement d'une IA de confiance basée sur un certain nombre de bonnes pratiques pouvant être facilement auditées par un organisme externe (pour éviter une mauvaise utilisation, voire de fausses promesses sur des systèmes de pseudo IA ou mal implémentés et pour garantir la transparence sur l'utilisation qui en est faite);A combination of ex-ante compliance and ex-post enforcement mechanisms;;Définition d'un cadre d'analyse permettant une évaluation objective de l'IA;Personal security risks;Responsabilité civile et pénale;Yes;"Les risques les plus sensibles doivent faire l'objet d'un contrôle régulier et externe, l'autocontrôle ne pouvant suffire
Et notamment la variabilité des données, du paramétrage, des algorithmes";Yes;;;;La notion de fabricant du produit est large - qui entre bibliothèques API, fournisseurs data, développeur ?;
F530473;08-06-2020 16:39;English;Academic/Research Institution;Francisco Javier;DIEZ;;Universidad Nacional de Educación a Distancia (UNED);;Large (250 or more);Spain;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;Collaboration with developing countries;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;3 - Neutral;5 - Very important;5 - Very important;;4 - Important;3 - Neutral;5 - Very important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;4 - Important;4 - Important;3 - Neutral;4 - Important;The white paper does not mention how AI will affect doctors-patients relations, for better or worse. This is an important issue that will concern each one of us in the mid-term.;There is a need for a new legislation;;No;;;;One of them is the application of AI to health care;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);Only for security reasons;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;;Risks related to medical decision-support systems;Yes;;Yes;;Yes, for all AI applications;;;comments-about-White-Paper-AI-Europe.pdf
F530472;08-06-2020 16:27;English;Public authority;Alexandre;Barrat;National;Autorité des marchés financiers (AMF);;Large (250 or more);France;The feedback can be published with your personal information;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;As mentioned in the White Paper document on AI, we think this work should be done in close collaboration with the European data strategy aims. Since the AI developments need large volume of data, AI work should be done within the framework of the future genuine single market for data brought by the European initiative.;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;At this stage, the AMF does not notice other areas to be considered.;4 - Important;4 - Important;5 - Very important;;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;"The AMF supports the idea of developing a European digital strategy applied to financial services that would enable European players to innovate in a secure environment, based on the following axes:
- encouraging the experimentation of new projects to foster innovation (see our proposition of an “EU digital lab” in our publication of 6 March 2020);
- managing the risks identified in the relationships between financial institutions and cloud providers to take full advantage of the use of AI.";5 - Very important;5 - Very important;4 - Important;4 - Important;No opinion;No opinion;"From our regulatory point of view, it seems premature to regulate the use of AI in the financial sector, which remains at an early stage of development in the EU.
However, we have another concern about the risks associated to the increasing reliance on cloud services provided by a limited number of external suppliers, which frequently use lock-in practices that prevent their customers to easily recover their data.
Such a practice creates a dependency of companies on their cloud providers.";Other;"The AMF did not receive a large amount of AI projects. The large majority of projects met are robo advisor and NLP applied to document analysis. From a regulatory point of view, there is no need to change the regulation at this stage.
Nonetheless, we are closely following any updates that the regulation might need in the future. Finance is a regulated sector, which means that AI applied to financial services is already regulated depending on the applications in which it is employed.";Yes;;Yes;;The AMF agrees not to place financial services in the high-risk category because AI is at this stage little adopted in financial services, in France and in the European Union. It would be necessary to wait for a real emergence of AI in this sector before imposing new standards. Moreover, finance is a regulated sector, AI applied to financial services is de facto regulated depending on the applications in which it is employed.;5 - Very important;3 - Neutral;4 - Important;5 - Very important;4 - Important;5 - Very important;No opinion;This issue is outside the AMF’ remit.;Rather not;Although a voluntary label could allow the European AI sector to stand out from its competitors, helping the most virtuous companies to market effectively to sell their services, we think that it is premature for the AI applied to financial services. Furthermore, we think that labelling will be effective for the sector of AI if it helps to simplify the burdens on businesses, in particular by reducing the documentation to provide to the customers to demonstrate compliance with their obligations.;No opinion;;;;The product safety legislation is outside the AMF’s remit.;No opinion;The product safety legislation is outside the AMF’s remit.;No opinion;The Product Liability Directive is outside the AMF’s remit.;No opinion;;Ibid.;
F530471;08-06-2020 14:30;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;3 - Neutral;4 - Important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;2 - Not important;4 - Important;5 - Very important;3 - Neutral;No opinion;4 - Important;;There is a need for a new legislation;;Yes;;No;;;5 - Very important;5 - Very important;4 - Important;5 - Very important;3 - Neutral;4 - Important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Rather not;;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;;Mental health risks;;Yes;;Yes;;No opinion;;;
F530470;08-06-2020 14:07;French;Company/Business organisation;Stephane;REQUENA;;GENCI (Grand Equipement National de Calcul Intensif);;Small (< 50 employees);France;The feedback can be published with your personal information;5 - Very important;4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;focus also on the availability and the exchange of data across Europe using sovereign facilites (HPC, storage, networks);5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;"important also to address ethics in AI (as a difference with commercial US based AI and security AI in China) based on European values 
adress XAI (explainable AI) for being used and trusted by European industries and even citizens ";3 - Neutral;5 - Very important;4 - Important;rely on existing initiative at the national level (for example in France with the 3IA centers) and bridge a light coordination on top. Develop joint research programs across such network and involve industry as well ;5 - Very important;4 - Important;5 - Very important;5 - Very important;3 - Neutral;"we have in France an initiative called SiMSEO where we try to raise awareness of French SMEs to numerical simulation, HPC, AI and even quantum computing soon !
the most important part of this programme is the expertise brought by experts from academia who understand very well the business/challenges of the SME as well as the technology and help the SME to setup a concrete PoC using such new technologies. ";5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;Explanaible AI is the key of AI adoption in industry (certification) or for citizen (trusting and understanding the decision of AI based systems). I dont believe in strong AI in the future by the way but ethics is very crucial as well ;Current legislation is fully sufficient;;;;;;;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);information to people, limited used;Much;its already the case with the captcha systems we fill every day for training US and Chinese AI based systems !!!;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;No opinion;;No opinion;;No;;here again explainable AI in autonomous cars will allow to understand the rationale of the decisions taken by an AI system and understand when a problem occurs where the respoinsaublites lies;
F530469;08-06-2020 13:34;English;;;;Local;;;;;The feedback can be published in an anonymous way;4 - Important;3 - Neutral;5 - Very important;4 - Important;4 - Important;5 - Very important;;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;5 - Very important;5 - Very important;"Access to data is essential for the development of AI. If possible to create, a European data space would a useful source of data. In order to develop such a data space, policies need to be aligned and coordination between member states is very important. 

A lot of data will be needed in order to create a European data space. With no common language standard in Europe, will it be possible to develop a joint data space in the EU? The creation of national data spaces could be considered.";5 - Very important;5 - Very important;4 - Important;-;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;-;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;It is very important that the data that is being shared is correct and of high quality. Mechanisms to secure this must be in place before data is shared and AI is developed and implemented. ;There is a need for a new legislation;;Yes;;Yes;;-;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;-;Very much;A volutary labelling system could be very useful, giving support in for example procurement processes. However, the system needs to be created in a way that does not burden the public sector with extensive administration and costs.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;-;Risks related to the loss of connectivity;-;Yes;-;Yes;Note: the question above is unclear;Yes, for specific AI applications;It is difficult to specify AI application. It seems difficult to adapt liability rules to each specific AI application.;"The White Paper addresses opportunities and risks at a rudimentary level rather than the legal aspects in detail and how existing regulations need to be updated; only individual aspects are highlighted. A lot of guidance will be needed in order to facilitate the implementation of a future AI legislation in practice. An important issue that needs to be addressed is the connection between a future AI legislation and GDPR as the objectives of these might be interpreted as differing.";
F530468;08-06-2020 12:35;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;1 - Not important at all;1 - Not important at all;Diversity among AI developers is essential to achieve excellent. I would like to see further initiatives to make AI training and the entire development process more inclusive. ;5 - Very important;5 - Very important;1 - Not important at all;1 - Not important at all;5 - Very important;4 - Important;;1 - Not important at all;5 - Very important;1 - Not important at all;One of the main issues with the development of AI is the low participation of women which greatly influences the design of AI systems. Design of new infrastructures and organisations must account for this disparity and design structures that would address the issue.;1 - Not important at all;5 - Very important;4 - Important;3 - Neutral;5 - Very important;To date the design of digital innovation hubs has missed several new opportunities within the field of AI. The design of the infrastructure and the way they work tends to suit those who live in the centre of main cities and are in a position to take high risks and short term contracts. This has ended up fostering young white male talent to the exclusion of others. There is so much talent outside of this narrow pool of people. I would like to see a re-think of how digital hubs are designed. ;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;3 - Neutral;The training of AI systems needs to be reformed entirely. ;There is a need for a new legislation;;Yes;;No;;AI application that influence people's opinions are most influential. Recommender systems online for instance, are highly influential and have undermined democracies across the world. Personalisation of content based on inferred attributes and stereotypical classifications of people fragments social cohesion and can often work against fundamental EU values. ;5 - Very important;4 - Important;4 - Important;3 - Neutral;5 - Very important;4 - Important;Biometric identification systems should never be allowed in publicly accessible spaces;It is crucial that the fundamental EU value of freedom and autonomy is protected - biometric systems undermine that freedom, without sufficient gain. ;Very much;Labelling based on agreed ethical standards are crucial. Also- transparency labelling for  what kind of information is inferred based on personal data is important. ;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;The definition of high-risk applications is too narrow;Mental health risks;AI use in recommender systems, news sites and social media is been show to negatively affect mental health - transparency as to what kind of information is inferred based on online behaviour tracking would help protect people as they would be able to think critically and have more control over what is sent to them online. ;Yes;Risk assessment should include the risk of influencing and encouraging extreme opinion;Yes;;Yes, for all AI applications;;;
F530467;08-06-2020 11:46;English;NGO (Non-governmental organisation);Kirsten;HAWLITSCHEK;;European Organisation of Prison and Correctional Services (EuroPris);972634836901-11;Micro (< 10 employees);Netherlands;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;Current legislation may have some gaps;;Other;High risk would need to defined. Also, when is harm considered to be particularly high? Harm caused to whom?;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;Citizens have to be made aware of the use of their data. Legislation has to be very clear  on what one is allowed to do.;Rather not;If something is voluntary there is a risk that it will not be used.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;Yes;Question: Is AI a product liability or a service liability?;Yes, for all AI applications;;;
F530466;08-06-2020 10:41;German;Business Association;wolfgang;lindner;;Wirtschaftskammer Österreich;10405322962-08;Large (250 or more);Austria;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;3 - Neutral;3 - Neutral;4 - Important;2 - Not important;4 - Important;3 - Neutral;"KI kann zu diskriminierenden Ergebnissen führen, wenn der ethische Rahmen bei der Schaffung der KI nicht angemessen berücksichtigt wird

";Current legislation may have some gaps;;Yes;;Yes;;"Da sich die Materie laufend weiterentwickelt führt dies zur Notwendigkeit auch die Gesetze (laufend) zu prüfen und falls nötig anzupassen. Wichtig erscheint in diesem Zusammenhang die Vermeidung von ""Überregulierung"".
Es sollten möglichst exakte Kriterien für „..besonders großen Schaden..“ festgelegt werden.";5 - Very important;4 - Important;3 - Neutral;4 - Important;4 - Important;4 - Important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Rather not;Möglicherweise könnte dies analog zum Beispiel für freiwillige Kennzeichnung von Cloudprodukten eingeführt werden (in Österreich zB das Gütesiegel Austrian Cloud);A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;No;;No;Derzeit sind einige RIchtlinien nur auf Produkte und nicht auf Dienstleistungen anwendbar. Der Rechtsrahmen im Bereich der Produkthaftung sollte zunächst überprüft, und wenn nötig punktuell optimiert werden, sodass für die Unternehmen kein unverhältnismäßiger Aufwand bei der Einhaltung verursacht wird;No;;Zunächst sollte Überprüfung stattfinden, ob punktuelle Änderungen anhand neuer AI-Anwendungen nötig sind;EC_Consultation_AI_White_paper_Division_Bank_Insurance.pdf
F530465;08-06-2020 10:28;German;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;2 - Not important;5 - Very important;5 - Very important;Das Netzwerk sollte Vorrang vor einem Leitzentrum erhalten. Besser eine dezentrale Aufstellung wählen.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;Besonders die KMUs sollten durch Förderung und durch Sensibilisierung für KI die Verbreitung der Technologie voran bringen. Vorteile in der Produktivität, Qualität und in der Erfüllung der Kundenerwartung sollten herausgestellt werden. Die fachliche und akademische Zusammenarbeit zur Förderung von KMUs sollte einen größeren Stellenwert haben als die Bereitstellung von Beteiligungskapital.;2 - Not important;4 - Important;4 - Important;3 - Neutral;3 - Neutral;3 - Neutral;KI wird einen hohen Beitrag zur Erhöhung der allgemeinen Sicherheit leisten. Die Gefahren für einzelne Bürger der EU ohne KI sind erheblicher als sie mit sein werden.;Current legislation may have some gaps;;Yes;;Yes;;KI im automobilen Umfeld bietet die höchsten Chancen zur Erhöhung der Sicherheit, ist aber auch mit hohen Risiken behaftet. Die Technik scheint bereits berherrschbar und eine allgemein verbindliche Regelung zur Zertifzierung könnte helfen, das Risiko der Markteinführung zu minimieren.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No further guidelines or regulations are needed;Die bestehenden Regulierungen sind ausreichend. ;Very much;;Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;Beispiele sind in der Automobilindustrie bekannt. ISO 26262 oder IEC 61508;Risks related to the loss of connectivity;"Risiken aus der Konnektivität von KI Produkten sind besonders genau zu analysieren. Der Verlust der Konnektivität kann ein erhebliches Risiko darstellen. 
Was sind Cyberrisiken? Wie kann man diese präzisieren?  ";No;;Yes;Beispiel Automobil mit Einbeziehung der Eigentümerhaftung;Yes, for specific AI applications;Speziell beim automatisierten Fahren sind Anpassungen notwendig;;
F530464;08-06-2020 10:15;German;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;4 - Important;4 - Important;;4 - Important;3 - Neutral;4 - Important;4 - Important;3 - Neutral;2 - Not important;;2 - Not important;4 - Important;4 - Important;;4 - Important;3 - Neutral;5 - Very important;4 - Important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;No;;;;;5 - Very important;No opinion;4 - Important;4 - Important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;Yes;;No;;;
F530463;08-06-2020 09:42;German;;;;;;;;;The feedback can be published in an anonymous way;3 - Neutral;1 - Not important at all;5 - Very important;1 - Not important at all;2 - Not important;2 - Not important;;2 - Not important;5 - Very important;3 - Neutral;2 - Not important;5 - Very important;1 - Not important at all;;4 - Important;3 - Neutral;2 - Not important;;3 - Neutral;4 - Important;3 - Neutral;3 - Neutral;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;5 - Very important;;There is a need for a new legislation;;No;;;;Die üblichen hochriskanten Anwendungen, wie sie schon gelten (also SIL 2 / 3 / 4.), aber auch Funktionen, die Aussagen über Individuen treffen (z.B. für Strafverfolgungsbehörden das Gefahrenpotential von Menschen bewerten oder z.B. Versicherungsbeiträge ermitteln).;5 - Very important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Very much;;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;;Personal security risks;;No opinion;;No opinion;;No opinion;;;
F530462;08-06-2020 07:53;French;Consumer Organisation;Vanden Clooster;Marcel;;Fédération Internationale des Clubs de Motor-homes;Feder4217543766;Micro (< 10 employees);France;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;4 - Important;"Approfondir ce que l'on appelle l'IN (acronyme signifiant ""Intelligence Naturelle""). Le processus démarre in Utero et se poursuit tout au long de la vie. Les premiers artifices interviennent en milieu scolaire et sont cause de nombreuses distorsions ultérieures. L'école doit permettre de faire germer les concepts nouveaux et leur laisser le temps de grandir discrètement. Une stratégie IA en appoint à l'enseignant doit permettre de fonder le Lifelong Knowledge Stack de chaque petit Européen.";5 - Very important;1 - Not important at all;4 - Important;5 - Very important;5 - Very important;5 - Very important;"Améliorer la communication d'Homme à Homme, Brain à Brain (B2B), et ensuite l'inferface Homme-Machine. Passage obligé par l'école. Prendre le temps d'enseigner. ""A School is a seamless process for transfering & experimenting newly acquired knowledge"" ";5 - Very important;4 - Important;5 - Very important;"Mieux communiquer sur les conditions et implications personnelles des scientifiques: le fameux S1 décrit par  Gerald Holton ""Thematic Origins of Scientific Thought"". Infrastructure de soutien aux jeunes sur smartphone, avec service confidentialisé. Voir programme OLILOKSE IV. DG XIII - Education Multimedia Task Force (Dec 17th, 1996) ";4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;"Il faut recentrer l'IA sur l'humain: c'est lui qui est intéressant et qui doit apprendre à mieux se connaître. Promouvoir le partage des modèles mentaux entre concepteurs et utilisateur d'information. Voir article joint:  ""Ergonomie cognitive du logiciel"" présenté en 1993 à Georges Metakidès, Directeur du Programme ESPRIT.";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;L'IA est un outil, une technologie. Elle doit être utile et légitime. Elle n'a pas été inventée hier. La sémantique probabiliste induite par l'IA sur Big Data est normative. Pas applicable partout, notamment dans l'innovation. Au-delà de la logique, il y a l’analogique, le disruptif propre à la créativité, le malentendu permanent et la fécondité du doute. Einstein a remporté le prix Nobel pour UNE ligne de texte. Alfred Korzybski et Gerald Holton expliquent comment et pourquoi.;There is a need for a new legislation;;Yes;;;;Les plate-formes: les réseaux sociaux, achats en ligne. L'utilisation: protection des Données privées (problème légiféré depuis 45 ans déjà, et de plus en plus critique). Plateformes d'achat et de publicité. Transactions financières. Publicité politique.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;consignes d'orientation de niveau européen: car cela correspond à nos valeurs démocratiques. Nous ne devons pas céder ni à la tentation sécuritaire ni au lobby sécuritaire. Il faut un monitoring actif et éducatif.;Very much;Il faut une certification, via service en ligne sécurisé, stipulant les clauses d'agrément entre service et utilisateur.;No opinion;;Avant mise sur le marché: dossier fonctionnel et logique d'accès selon schéma « AI Online content moderation workflow », de Cambridge Consultants (2019). Mise en service via certification (registre UE en ligne), et suivi des mises à jour logiques du dispositif (n° de version des composants+dossier justificatif). Explicitation des critères utilisés et des décisions.;Mental health risks;l'utilisateur doit disposer de la liste des sociétés ou intermédiaires auxquelles ses données ont été communiquées, les ports utilisés et les charges de trafic soustraite à ses équipements.;Yes;tout site est responsable des fraudes occasionnées par détournement d'enseigne. publicitaire (phishing, entre autres). La charge de se retourner contre tout intermédiaire lui revient.;Yes;Il faut en arriver à certifier les produits logiciels (registre de niveau UE). Juridiction compétente: celle de l'acquéreur si le vendeur n'est pas UE.;Yes, for all AI applications;;L'IA est un concept flou. Toutes les applications classiques comprennent des éléments de décision automatisée qui ne sont pas toujours explicites. Même au niveau de l'Administration. En particulier, le détournement d'information à des fins de croisement avec d'autres sources est pratique courante et contraire à la règle n°1 de protection des données privées, à savoir que l'information donnée ne peut être utilisée que pour les fins du service offert. Tout autre usage doit être anonymisé.;
F530461;07-06-2020 23:27;Spanish;NGO (Non-governmental organisation);Arthur;Corral;;gffgfg;;Large (250 or more);United States;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Cyber Crime on my Account;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Cyber crime on my Accounts corrala2018@gmail.com.  ;5 - Very important;5 - Very important;5 - Very important;Cyber Crime on my Account ArturoCorral001@icloud.com/gmail.com/outlook.com;;;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Brenda Corral Araceli Ramirez Veronica Ramirez Jesus Ramirez Jr Pat Corral Jesus Garcia Chad Mark Gonzalez Robert Ontiveros Brenda Lisa Moran 3/13/81 Indianapolis Indiana;There is a need for a new legislation;;Yes;;Yes;;Platinum_F5/Platinum/F5_B4_V9.1_20200511/user/Sky-Devices/Platinum_F5/Platinum_F5:8.1.0/011019/1589162363:user/release-keys/SDK27/REL/k80hd_bsp_fwv_512m/install_session_id/oNebxbqvSDOcQ700GfJjxA;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;;Very much;com.android.vending/82041800/20.4.18-all[0][PR]314396108;Other enforcement system;ArturoCorral001@gmail.com Running AdSense Mob Analytical World Wide Federal Communications Tower In Sylmar California At the entrance of Cariso Park On Hubbard St. Off the 210 Freeway in the San Fernando Valley;Constitutional Rights broken , this is the only way I could contact any one please help I'm a slave ;Cyber risks;NA;Yes;NA;Yes;NA;Yes, for all AI applications;;NA;
F530460;07-06-2020 17:44;German;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;4 - Important;3 - Neutral;2 - Not important;3 - Neutral;;5 - Very important;4 - Important;3 - Neutral;4 - Important;3 - Neutral;5 - Very important;;5 - Very important;5 - Very important;2 - Not important;;4 - Important;3 - Neutral;5 - Very important;3 - Neutral;4 - Important;;4 - Important;5 - Very important;4 - Important;3 - Neutral;No opinion;3 - Neutral;;No opinion;;No opinion;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;No opinion;;No opinion;;No opinion;;;
F530459;07-06-2020 15:36;German;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;4 - Important;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;4 - Important;5 - Very important;;5 - Very important;5 - Very important;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;No;;;;"Gesichtserkennung; Zugang / Nutzung personenbezogener Gesundheitsdaten;Nutzung individueller Bewegungsgewohnheiten";5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;damit wäre eine dauernde Rasterfahndung ermöglicht;Very much;;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;;Mental health risks;keine einheitlichen Regularien in den EU-Staaten vorhanden;Yes;;Yes;;Yes, for all AI applications;;;
F530458;07-06-2020 13:54;Spanish;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;4 - Important;4 - Important;3 - Neutral;4 - Important;5 - Very important;;4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;;4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;;There is a need for a new legislation;;No;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;Yes;;Yes, for all AI applications;;;PROPUESTA_UE_CERTIFICACIONES.pdf
F530457;07-06-2020 10:18;German;;;;;;;;;The feedback can be published in an anonymous way;3 - Neutral;4 - Important;4 - Important;4 - Important;4 - Important;1 - Not important at all;;4 - Important;3 - Neutral;1 - Not important at all;4 - Important;3 - Neutral;3 - Neutral;;4 - Important;4 - Important;3 - Neutral;;2 - Not important;3 - Neutral;4 - Important;4 - Important;4 - Important;;3 - Neutral;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;3 - Neutral;;There is a need for a new legislation;;Yes;;No opinion;;Public face recognition, racism in application for employment sorting ;4 - Important;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F530456;07-06-2020 09:19;German;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;4 - Important;5 - Very important;3 - Neutral;1 - Not important at all;3 - Neutral;;5 - Very important;5 - Very important;2 - Not important;3 - Neutral;4 - Important;5 - Very important;;4 - Important;5 - Very important;1 - Not important at all;;4 - Important;4 - Important;4 - Important;3 - Neutral;3 - Neutral;;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;3 - Neutral;;Current legislation may have some gaps;;No;;;;;5 - Very important;5 - Very important;4 - Important;5 - Very important;3 - Neutral;4 - Important;Biometric identification systems should never be allowed in publicly accessible spaces;;Much;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Personal security risks;;Yes;;Yes;;Yes, for all AI applications;;;
F530455;07-06-2020 09:04;German;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;4 - Important;4 - Important;3 - Neutral;3 - Neutral;1 - Not important at all;;4 - Important;4 - Important;1 - Not important at all;2 - Not important;4 - Important;5 - Very important;;5 - Very important;5 - Very important;2 - Not important;;3 - Neutral;3 - Neutral;4 - Important;5 - Very important;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Strong AI mußte be discussed before! Programming it;There is a need for a new legislation;;Yes;;Yes;;AI is just the second Option after nuclear weapons for total extinction for mankind and should be absolutely handled with care. There might be no second try for strong AI;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;I think it is against human rights for Private living and if available IT will be used everywhere;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F530454;07-06-2020 08:42;English;Academic/Research Institution;Christoph;LUETGE;;TUM Institute for Ethics in Artificial Intelligence;;Small (< 50 employees);Germany;The feedback can be published with your personal information;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;AI has the potential to improve the lives of many people in the long term, be it in healthcare, autonomous driving, big data applications and much more. But it needs clear rules to develop this potential. These rules should not only be legal regulations, but especially ethical guidelines to address widespread fears. It is time for the EC to foster the adoption of ethical standards and put forward regulations for a coordinated European approach to the ethical implications of AI.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;Support for independent, interdisciplinary academic research, not only in AI development, but also in AI ethics and social implications is fundamental to addressing the variety of issues surrounding AI use. Academic institutions need to play a prominent role in designing and implementing ethical frameworks and guidelines for the development and the use of AI-based technologies.;3 - Neutral;5 - Very important;5 - Very important;The EC should not only promote and develop a network of research centers working on AI, but should bring together experts within and between these centers with different types of expertise. Scientists and speicalists from technical fields and experts in the humanities and social sciences should work together on AI development and ethical guidance. The EC should also ensure that a broad set of stakeholders are engaged in this process.;4 - Important;3 - Neutral;4 - Important;5 - Very important;3 - Neutral;AI is a powerful tool that can be employed in various contexts and has the ability to amplify current positive efforts, but also entrench and increase harms and discrimination if not used responsibly. Of ethical importance are the questions by whom, how and when the impacts of AI technology will be felt. Independent counseling can help SME's find ways to build trustworthy and responsible ??.;4 - Important;4 - Important;4 - Important;3 - Neutral;4 - Important;4 - Important;It is not responsible anymore not to use AI. However, ethical issues surrounding AI must be addressed. The question is no longer if AI will have an impact, but rather how it will happen and what the repercussions will be. R+D should focus not only on what is possible, but how do we ensure that as many people as possible benefit from its rewards. Developers and policymakers alike should consider the ethical implications of AI use related to the five AI ethics principles by AI4People and HLEG.;Current legislation may have some gaps;;Yes;;No;;At the moment, Contact and Proximity Tracing Applications pose a number of urgent ethical questions. The COVID crisis has underlined the potential for AI to help manage pandemics, but there is a pressing need to discuss the ethical considerations for the use of AI in emergencies and develop guidelines for their responsible use. This will not only help foster trust in the tools that lead to more effectiveness, but help to promote the consideration of human rights and ethical use of AI in crises.;4 - Important;4 - Important;4 - Important;5 - Very important;3 - Neutral;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);Yes, clear rules for data access, use and storage should be addressed.;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;The challenges related to the development and use of AI are not purely technological ones. Dealing with ethical and social challenges arising at the interface of technology and human values requires input and investigation from social science and legal experts. Therefore, compliance and enforcement assessment should also involve ethicists and legal scholars. ;Risks related to the loss of connectivity;;Yes;;Yes;AI applications may require new considerations in terms of product liability, but this should be done in a way that does not at the same time stifle innovation.;Yes, for specific AI applications;Autonomous driving, AI in Health care;;
F530453;06-06-2020 22:00;German;EU Citizen;Andreas;Söllner;;;;;Germany;The feedback can be published with your personal information;3 - Neutral;4 - Important;5 - Very important;4 - Important;4 - Important;2 - Not important;;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;4 - Important;;4 - Important;5 - Very important;3 - Neutral;;4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;;2 - Not important;4 - Important;5 - Very important;4 - Important;3 - Neutral;4 - Important;;There is a need for a new legislation;;Yes;;Yes;;;5 - Very important;3 - Neutral;4 - Important;4 - Important;3 - Neutral;4 - Important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;;Risks related to the loss of connectivity;;No opinion;;Yes;;No opinion;;;
F530452;06-06-2020 21:25;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;4 - Important;4 - Important;No opinion;3 - Neutral;No opinion;;4 - Important;4 - Important;2 - Not important;2 - Not important;4 - Important;4 - Important;;No opinion;5 - Very important;4 - Important;;3 - Neutral;4 - Important;4 - Important;3 - Neutral;2 - Not important;;3 - Neutral;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;Other;Considering other EU legislation regarding to digital innovation, regulation, products, rights etc. that either is non-existent or not suitable for the digital era and considering the tremendous failure of the EU with regards to the copyright law and upload filters in 2018, I strongly assume that current legislation is insufficient. However, I do not have a proper insight on that matter and for that do not want select a determining option. ;No;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;The downside of biometric identification outweighs the benefits that can be expected from this kind of surveillance. Existing law enforcement options continue to proof sufficient. The impact on and threat to personal and privacy rights of EU citizens, the risk of misuse and the threat of some countries using it against their citizens or e.g. political opposition is too great to be neglected and downplayed. We already have proof of that today within the EU with Hungary and Poland. There are examples of other countries around the world, China being the most prominent, how these systems can be (mis-)used and I am sure, that democracy and checks and balances do not provide a sufficient safety net for that. ;Rather not;Companies and capitalist organisations, despite some improvements in recent in some areas, usually do what is in their own interest. Voluntary measures have hardly proven successful in the past.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure + Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;Personal security risks;Not sure what is meant by cyber risk here. Cyber risks for me mean risks of products being hacked / taken over. ;Yes;"Risk assessment should include checks for 1) vulnerability of the AI (code) to be manipulated in any way and being manipulated without anybody realising it, 2) that there are sufficient fallback system to prevent an AI from ""acting out"" resp. to have sufficient options to shut the system down. ";No opinion;I can't judge as I don't know the details of the directive nor do I have a proper overview or future prospect of potential applications of AI. ;No opinion;;I can't judge as I don't know the details of the directive nor do I have a proper overview or future prospect of potential applications of AI. ;
F530451;06-06-2020 18:03;German;;;;;;;;;The feedback can be published in an anonymous way;3 - Neutral;4 - Important;3 - Neutral;No opinion;3 - Neutral;3 - Neutral;;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;;4 - Important;3 - Neutral;4 - Important;;No opinion;4 - Important;;No opinion;No opinion;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;No;;;;;4 - Important;4 - Important;5 - Very important;4 - Important;4 - Important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Much;;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;;Personal security risks;;Yes;;Yes;;No opinion;;;
F530450;06-06-2020 15:52;English;NGO (Non-governmental organisation);yvette;RAMOS;;WOMENVAI -  WOmen and Men in ENVironment and Artificial Intelligence;497914838344-36;Medium (< 250 employees);France;The feedback can be published with your personal information;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;3 - Neutral;NGOs registered in Europe in specific sectors (STEM, gender, diversity, ethics, education, youth) need also to be associated as we bring expertise and the civil society representation. Especially we need to be more inclusive when it comes to Young generations, especially if they study STEM (Science, Technology, Engineering, Mathematics).;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;All the above is important: however, these must consistent with similar actions on AI towards the Humanities /Social Science sector, be it at the University, public and/or private sector. For ex. at the Humanities Faculties, Ethics and Human Resources Development programme need to be AI-mainstreamed. Moreover, European Commission should promote the interaction between Humanities and STEM stakeholders at all levels, including at Government/State, University, Research Labs, NGOs, SMEs, etc.) ;1 - Not important at all;5 - Very important;5 - Very important;A lighthouse principle must not and can not be the unique entry point in Europe for investments and skills, as this could threaten a global opened and inclusive AI. A network of existing AI research excellence centres could be though guided by a peer based strategic approach, together with a sound public-private partnership, particularly targeting startups, SMEs academic institutions.;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;Supporting partnerships between SMEs, larger enterprises and academia around AI projects is key, and our group has promoted and engaged concrete initiatives towards this (example: our partner EPF School of Engineering in France kicked-off in 2018 a programme on AI and Ethics together with small SME, large companies renowned globally, and the civil society). We strongly believe that SMEs have a central strategic role to play.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Especially we consider that AI is still gender-biased. Initiatives to fight gender-biased AI have been promoting a more equality based AI, and we, at WOMENVAI and partners support these.;There is a need for a new legislation;;Other;While regulation is very important, there are many different ways to ensure the protection of citizens while deploying AI-based technologies, such as General guidance on AI, Ethical Guides, Gender mainstreaming principles, impact-focused strategies, etc. WOMENVAI and its partners work together to contribute to such different frameworks.;;;In general, Human-related impacts of AI are extremely high-risk, especially when the usage of AI might lead to a penal decision, a human loss, property loss, hunger, health problems, etc. Study of hi-risks should be based for example on the UN 2030 SDGs- and align to them. ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Rather not;;No opinion;;;Mental health risks;;No;;No opinion;;No opinion;;;
F530449;06-06-2020 14:48;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;5 - Very important;3 - Neutral;1 - Not important at all;1 - Not important at all;"The European Commission should include actions such as democratic oversight
of AI in the public sector, consultations with civil society, the general public and affected communities, as well as stringent human rights safeguards.";5 - Very important;3 - Neutral;1 - Not important at all;1 - Not important at all;5 - Very important;3 - Neutral;"The coordinated plan on AI should be updated to include criteria (scientific and policy)
about how the EU will allocate its resources of AI. The plan should include a section on human rights, societal impacts of AI and automation, and how to ensure democratic oversight for the application of AI systems.";3 - Neutral;4 - Important;1 - Not important at all;"Funding for EU projects on AI should be conditional on meeting the EU’s own ethical
standards for AI and fundamental rights laws. EU funds, such as the Horizon2020 fund, should comply and immediately stop funding for projects which pose a risk to fundamental rights, such as iBorderCtrl - which aims to use facial and emotion recognition technology to supposedly detect lies in the course of visa applications, but is not substantiated by scientific evidence.";1 - Not important at all;2 - Not important;2 - Not important;2 - Not important;1 - Not important at all;Small businesses should not enjoy any exemptions to protecting human rights. Some of the biggest AI-related scandals have involved small companies, and so the EU should ensure that when small businesses to take up AI, whatever the circumstances, they should respect data protection, privacy and other fundamental rights. There should be no blanket exemptions to fundamental rights standards.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;"The deployment of AI in sensitive areas (public services) without democratic oversight,
transparency or sufficient evidence to justify the need/ purpose. Increasing use of opaque, privately-developed technology in the public sphere , which do not meet transparency requirements (when they exist). The conscious avoidance of liability for harms produced by AI technology. AI posing collective or societal-level harms which don’t have remedies in anti-discrimination of data protection frameworks.";Other;AI regulation should not provide loop-holes to data protection legislation, or other frameworks, like discrimination law.;Other;"New rules should clearly outline criteria to determine which AI systems are legal
and which are not. Such criteria should be based on proving that they work and are
needed, conducting mandatory fundamental rights impact assessment for all appli-
cations, and ensuring democratic oversight. Uses of AI which breach fundamental rights - like biometrics/ facial recognition for mass surveillance - should be banned outright.";;;These uses of AI are incompatible with human rights and should be banned: The use of AI to determine delivery of essential public services, predictive policing, autonomous lethal weapons, identification/ analysis of emotion and identity traits and indiscriminate biometric surveillance.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;"Their use in public spaces will lead to mass surveillance; This will irreversibly limit our fundamental rights to privacy, freedom of assembly, expression, non-discrimination, data protection, dignity and the right to a fair trial, creating societies of suspicion; and even uses which do not contribute directly or indirectly to mass surveillance in pub-
lic spaces still pose significant threats to privacy, data protection, non-discrimina-
tion, and dignity.";Not at all;"Self-labelling systems can be confusing for people and may give a false sense of secu-
rity since it is the same company that develops a product the one saying that it is safe. I believe that the high/low risk distinction is overly simplistic and could very well
allow for loop-holes for systems with potentially very significant impacts on peoples’
safety and rights. This is especially so if ‘low risk’ systems are only voluntarily con-
trolled.";Other enforcement system;All systems should undergo a mandatory ex ante human rights impact assessment from an external body;"Compliance should be external. We need this to guarantee funda-
mental rights are protected, we cannot rely on self-regulation for this.";Mental health risks;"I'd like to highlight the potential risks of discrimination posed by AI systems. In particular, the use of AI in online products and services requires collection and use of data lending toward discrimination in many fields related to targeted advertising. This poses risks of differentiated pricing, discrimination and financial detriments, the risk of creating filter bubbles, interfences in the political process, all based on sensitive inferences or
associations.";Yes;"Internal supervisors, such as Data Protection Officers under GDPR should
included and and asked for advice.";Yes;"AI developers and deployers should be accountable for harm generated by their
products and products developed using AI should not enjoy exceptions to any EU laws,
whether it be discrimination, data protection, or product liability.";Yes, for all AI applications;;I highlight that the EU address copyright and database protections prevent proper oversight of AI applications. Liability rules should provide incentives for openness.;
F530448;06-06-2020 14:48;German;;;;;;;;;The feedback can be published in an anonymous way;3 - Neutral;2 - Not important;4 - Important;1 - Not important at all;5 - Very important;3 - Neutral;;4 - Important;3 - Neutral;4 - Important;1 - Not important at all;3 - Neutral;4 - Important;AI in energy economics;3 - Neutral;5 - Very important;5 - Very important;;2 - Not important;3 - Neutral;2 - Not important;5 - Very important;1 - Not important at all;;4 - Important;4 - Important;2 - Not important;4 - Important;3 - Neutral;No opinion;;There is a need for a new legislation;;Yes;;No opinion;;;5 - Very important;3 - Neutral;2 - Not important;5 - Very important;3 - Neutral;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);looking for people on the terrorist list;No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Cyber risks;energy economics ;Yes;;Yes;;No opinion;;;
F530447;06-06-2020 11:48;English;Business Association;Secretary;General;;Alliance for Internet of Things Innovation AISBL;380738729287-22;Micro (< 10 employees);Belgium;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Please see side document we enclosed.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Please see side document we enclosed.;4 - Important;4 - Important;5 - Very important;Please see side document we enclosed.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;Please see side document we enclosed.;5 - Very important;4 - Important;4 - Important;3 - Neutral;4 - Important;3 - Neutral;Please see side document we enclosed.;Other;Please see side document we enclosed.;Yes;;Yes;;Please see side document we enclosed.;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No further guidelines or regulations are needed;Please see side document we enclosed.;Rather not;Please see side document we enclosed.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Please see side document we enclosed.;Personal security risks;Please see side document we enclosed.;No;Please see side document we enclosed.;No;;Yes, for specific AI applications;Please see side document we enclosed.;Please see side document we enclosed.;AI_Consultation_Document_AIOTI_response_side_Paper.pdf
F530446;06-06-2020 09:38;English;Non-EU Citizen;Julia;Longtin;;;;;United States;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;5 - Very important;2 - Not important;4 - Important;data availability / standardization of softwares licensing;5 - Very important;4 - Important;4 - Important;2 - Not important;5 - Very important;3 - Neutral;develop ethical standards and trainings for the application of AI technology;4 - Important;5 - Very important;1 - Not important at all;Grant programs for practitioners working in the open with public data, in the public interest.;3 - Neutral;5 - Very important;5 - Very important;4 - Important;4 - Important;a focus on utilizing the technologies being developed in their local community (real world examples) should be fostered.;1 - Not important at all;5 - Very important;5 - Very important;4 - Important;3 - Neutral;4 - Important;uneven access to compute resources for training AI will further the divide between the haves and the have nots.;There is a need for a new legislation;;No;;;;The mining of data from social networks to establish the value / danger of a citizen to it's government, and the utilization of that value to limit the social options provided to that citizen. we cannot just cut off the undesirable among us, we must lift them up with us.;3 - Neutral;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;tracking citizens in public is a slippery slope, we should not let industry start down that path without a great social good in return.;Much;should be manditory.;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;there should be a publicly available report of the benefit and challenges of each application issued.;Mental health risks;access / repair risks. the lack of data training sets and compute resources may represent a risk to right to repair.;Yes;access to softwares and data sets will determine a large part of the social good of an AI product.;Yes;;Yes, for all AI applications;;;
F530445;05-06-2020 23:02;German;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;4 - Important;4 - Important;3 - Neutral;4 - Important;3 - Neutral;;4 - Important;5 - Very important;3 - Neutral;4 - Important;4 - Important;4 - Important;Privatsphäre;3 - Neutral;5 - Very important;2 - Not important;;4 - Important;4 - Important;3 - Neutral;4 - Important;3 - Neutral;;3 - Neutral;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;;Current legislation may have some gaps;;No;;;;;4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Very much;;No opinion;;;Mental health risks;Risiken durch Diskriminierung;Yes;;Yes;;Yes, for all AI applications;;;
F530444;05-06-2020 22:54;German;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;2 - Not important;1 - Not important at all;1 - Not important at all;;5 - Very important;3 - Neutral;1 - Not important at all;1 - Not important at all;1 - Not important at all;5 - Very important;;5 - Very important;5 - Very important;1 - Not important at all;;1 - Not important at all;5 - Very important;1 - Not important at all;1 - Not important at all;1 - Not important at all;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;Yes;;Yes;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Very much;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F530443;05-06-2020 22:53;German;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;No opinion;No opinion;No opinion;2 - Not important;5 - Very important;;No opinion;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;No opinion;No opinion;No opinion;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;;There is a need for a new legislation;;No opinion;;;;Überwachung, zB durch Gesichtserkennung;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;Biometric identification systems should never be allowed in publicly accessible spaces;;No opinion;;No opinion;;;Mental health risks;;Yes;;No opinion;;No opinion;;;
F530442;05-06-2020 22:36;German;EU Citizen;Julian;Mair;;;;;Italy;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;3 - Neutral;;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;;4 - Important;5 - Very important;2 - Not important;;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;;There is a need for a new legislation;;No;;;;;3 - Neutral;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;Biometrische Identifikationssysteme im öffentlichen Raum eliminieren jegliches Recht eines Indiviuums sich frei im öffentlichen Raum zu bewegen und aufzuhalten.;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;No opinion;;Yes;;No opinion;;;
F530441;05-06-2020 17:18;English;Company/Business organisation;Benedikt;Brecht;;Volkswagen AG;6504541970-40;Large (250 or more);Germany;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;4 - Important;5 - Very important;5 - Very important;"Focusing on a single lighthouse centre, as suggested, seems limiting. We deem an approach that would create multiple lighthouses to be worthwhile
The creation of a PPP on AI is already undertaken https://ec.europa.eu/digital-single-market/en/news/artificial-intelligence-public-private-partnerships-join-forces-boost-ai-progress-europe and is important to join forces in Europe. Following the footsteps of euRobotics, it could serve to create value for SMEs, large companies, and the public sector.";3 - Neutral;5 - Very important;4 - Important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;;Current legislation may have some gaps;;Yes;;Yes;;"We consider the focus on a sector (compare 5C) confusing and believe a sector-specific approach to classification is not the right way. As digitization increases, sectors overlap and are no longer distinct. Should banking services of a car manufacturer have higher scrutiny than of another bank?  Should we ignore face recognition risks of non-high-risk sectors?
Furthermore, this assessment is very dependent on the definition of AI.";4 - Important;4 - Important;3 - Neutral;5 - Very important;2 - Not important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);"The use of biometric data with machine learning is technically rather new and application-wise only marginally explored—there are certainly many upcoming possibilities. New developments will certainly pop up, which can also address privacy, discriminatory and other concerns. A careful analysis of merit vs costs must be done with existing and new methodologies. While GDPR gives a good framework, it does not address machine learning-based processing of data, differential-privacy approaches, etc. A careful hand-in-hand of regulatory and technical development is required. Using an ethical principle framework can serve as a preparatory approach for regulation.
We think that there are several approaches that could restrict remote biometric identification systems in an acceptable manner, e.g., by limiting the system in a physical way to a near field / the relevant lines of sight, limiting the time the system is active, defining high security standards for biometrical data in transit and rest or by filtering and removing irrelevant data close to the optical source. On the one hand that would allow, e.g., authentication and authorization applications in and around a vehicle, where the system is optically restricted to a near field of, e.g., two meters, and needs to be activated by the user, e.g., by touching the car. Biometrical data should preferentially be generated and stored locally. Storage should occur in secure elements and be separated from additional personal data. On the other hand it would restrict the potential of misuse of the system.";Much;;No opinion;;;Personal security risks;;No opinion;It is unclear what exactly the question is aiming at. The current legal framework requires to meet reasonable safety expectations. Whether “new risk assessment procedures” to determine these safety expectations will be implemented appears to be a factual question rather than judicial question. ;No;The current liability system is generally effective and should in principle be maintained. Specification of certain aspects could however be considered (e.g. is software a product and does a software update represent a new placing of a product). It is also unclear what AI understanding is assumed. Self-developing AI in homologated systems does not exist in our vehicles. We also assume we contribute to road safety with our vehicles.;No;;The current liability system is based on a fair distribution. It is important that liability continues to be adequately allocated and that national law is governed/influenced by European law.;
F530440;05-06-2020 17:04;English;NGO (Non-governmental organisation);Carola;STREUL;;European Visual Artists;121604011075-40;Micro (< 10 employees);Belgium;The feedback can be published with your personal information;2 - Not important;2 - Not important;4 - Important;5 - Very important;5 - Very important;3 - Neutral;;3 - Neutral;4 - Important;2 - Not important;2 - Not important;5 - Very important;4 - Important;;4 - Important;3 - Neutral;3 - Neutral;;3 - Neutral;4 - Important;4 - Important;4 - Important;3 - Neutral;;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;"Among the Fundamental Rights that AI may breach there are Intellectual Property rights.
Data used as input for machine learning of AI systems may be protected by copyright.
AI developers or deployers can ask authorisation to use copyrighted material. If not, its use is infringement.
";Current legislation may have some gaps;;No;;;;AI working with copyrighted material should be obliged, in line with current international copyright protection laws, to get authorisation to use those works even if the AI system itself is labelled as low risk. It may have a low risk for the society as a whole, but a HIGH RISK for artists who may otherwise never see any compensation for massive use of their work by AI, in whatever measure.;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;No opinion;;Much;;No opinion;;;Cyber risks;IP infringiments;No opinion;;No opinion;;Yes, for all AI applications;;None;EVA_reflection_paper_on_AI_fin.pdf
F530439;05-06-2020 16:38;German;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;4 - Important;3 - Neutral;4 - Important;5 - Very important;2 - Not important;Monopole verhindern;5 - Very important;3 - Neutral;2 - Not important;;4 - Important;5 - Very important;;5 - Very important;5 - Very important;1 - Not important at all;Public-Private-Partnership ist von Nachteil für die Zivilgesellchaft;4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;;Current legislation may have some gaps;;No;;;;Gesichtserkennung - insbesondere bei staatlichen Akteuren;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;4 - Important;Biometric identification systems should never be allowed in publicly accessible spaces;Zitat von Herrn Papier: wo es einen Trog gibt, kommen die Säue von alleine;Very much;Die Einstufung soll freiwillig sein, aber dass gekennzeichnet wird, muss Pflichst sein;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;Sicherheitsslücken in der Software. Nicht gewartete Software. Osbsoleszenz.;Yes;;Yes;Es soll eine Pflicht zur Software-Wartung eingeführt werden. ;Yes, for all AI applications;;;
F530438;05-06-2020 16:30;German;EU Citizen;Günter;Thiele;;;;;Germany;The feedback can be published with your personal information;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;4 - Important;Mittel bereit stellen für bessere Informations- und Qualifierungsangebote zu KI für alle Bürger, insbesondere durch NGOs;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;5 - Very important;4 - Important;;3 - Neutral;5 - Very important;2 - Not important;;5 - Very important;4 - Important;4 - Important;4 - Important;3 - Neutral;;5 - Very important;5 - Very important;4 - Important;3 - Neutral;4 - Important;5 - Very important;;There is a need for a new legislation;;Yes;;No;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Rather not;;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;;Personal security risks;;Yes;;Yes;;Yes, for all AI applications;;;
F530437;05-06-2020 15:26;English;Business Association;Jethro;Schiansky;;EUnited AISBL - European Engineering Industries Association;0289344948-82;Micro (< 10 employees);Belgium;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;#NAME?;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;4 - Important;5 - Very important;#NAME?;3 - Neutral;3 - Neutral;3 - Neutral;"- Above questions difficult to assess/rate in terms of importance without knowing what aspects of AI are to be the subject of research.
- R&I should focus on AI applications rather than AI per se. 
";4 - Important;5 - Very important;5 - Very important;3 - Neutral;4 - Important;#NAME?;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"- Despite importance of each issue, the question poorly framed. AI is a technology. Such concerns relate to specific applications & technology within which AI is used. 
- AI applications are embedded in products which are subject to existing regimes on sa";Other;What is an AI system? No definition provided therefore impossible to answer the question. In EUnited’s case AI is a technology embedded in a product. For products with embedded AI systems produced by EUnited’s members, the current regimes are sufficient .;Other;High-risk AI needs to be properly defined to provide legal certainly. Alternatively, an exhaustive set presumptive elements / indicators for the existence of high-risk AI must be developed. Otherwise the level of legal certainly required in the case of rules on high-risk AI will not be sufficient.;;;An application's risk level should be determined by the level of criticality of the application itself. Any application which impacts life or fundamental rights such as privacy should be considered on a case-by-case basis.;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Rather not;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;Any system for industrial machines should be aligned with NLF legislation regarding placing on the market and market surveillance. For High-Risk applications a 3rd party conformity assessment procedure is necessary.;Risks related to the loss of connectivity;;No;Unclear what is meaning of important change. If it constitutes substantial modification then risk assessment is needed e.g. re-manufacturing requiring a new CE marking. Depending on who is carrying out the important change is relevant as to who should carry out the risk assessment. Determination of intended use by the manufacturer is already part of the conformity assessment procedure under the machinery directive. But the above doesn't deviate from current rules so no new RA procedure needed.;Yes;The Directive is broadly fit for purpose. There may be a need to clarify the scope, to make clear that embedded software is considered to be a product in the scope of the directive.;No opinion;;EUnited will not comment on the effectiveness of national regimes. Rather, it believes that a harmonised European approach is most preferable to avoid market fragmentation.;
F530436;05-06-2020 14:08;German;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;;;2 - Not important;;;5 - Very important;;;;;;;4 - Important;4 - Important;2 - Not important;;2 - Not important;;3 - Neutral;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;No;;;;;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;;;;;;Mental health risks;;Yes;;Yes;;;;;
F528915;05-06-2020 11:54;English;NGO (Non-governmental organisation);Cianán;Russell;;ILGA-Europe;11977456675-84;Small (< 50 employees);Belgium;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;1 - Not important at all;1 - Not important at all;"The framing of the ""ecosystem of excellence"" must be reconsidered in terms of the assumption that increasing use of AI is always for the best. This assumption is dangerous in terms of the likelihood that it will overpower marginalised communities as they attempt to ensure protection of their fundamental rights, including LGBTI communities. 
We do not endorse use of AI by the public sector broadly, and encourage creating a very high bar for approval of this use, rather than exclusion criteria.";4 - Important;No opinion;1 - Not important at all;2 - Not important;4 - Important;2 - Not important;"The promotion of AI is not itself a value, but should be used only to serve the values of the European Union directly. We are concerned with the assumption that AI is always good, and object to this framework.
All plans ans strategies should include sections on human rights, with meaningful methods to monitor, document, and respond to human rights violations in the sector.";3 - Neutral;4 - Important;1 - Not important at all;"Research priorities must include research regarding human rights, societal implications, fairness, discrimination risks, transparency, and protection from misuse and abuse of AI.
Public funding must be linked to protection and promotion of fundamental rights.";1 - Not important at all;2 - Not important;3 - Neutral;3 - Neutral;No opinion;It is vital that SMEs are not exempt from fundamental rights laws and protections, as small companies can and do cause harms as well.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;"- AI use in public services and other sensitive areas without sufficient evidence, oversight and transparency
- Private ownership of technology
- Accountability measures for harm
- Collective and non-personal harm in the form in discrimination and data pr";Other;"Legislation must strengthen, rather than replace, GDPR. AI presents unique issues related to consent, objection, purpose
Current law does not address use of non-personal data, which can lead to inequality, discrimination, overpolicing
Current law does not prohibit AI discrimination on non-protected grounds";Other;"The EU should proactively ban use of AI in circumstances where fundamental rights and societal impacts are too great to risk
Fundamental rights impact assessments must be mandatory
New rules on criteria for use should place the burden of proof on the developer or deployer of these technologies, not on affected communities";;;"The use of AI to determine delivery of public services, such as healthcare, are incompatible with fundamental rights and should be banned completely
Determination of risk should be based on rights and outcomes, not on the sector, size of the developer/deployer, etc.";5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;Biometric identification systems should never be allowed in publicly accessible spaces;Public space is a fundamental component in the protection of the rights of freedom of assembly and freedom of expression. Use of biometrics in these spaces actively undermines these rights and should never be allowed.;Rather not;The low-risk/high-risk framework in itself is not robust enough, and leaves too much to chance when it comes to protection of vulnerable and marginalised communities. We are opposed to voluntary oversight measures and in favor of mandatory measures.;Other enforcement system;We believe a mandatory ex ante human rights assessment should be used, conducted by an external body.;Compliance assessment and monitoring must be external.;Mental health risks;There are heightened risks for discrimination, particularly in terms of online advertising.;Yes;A similar structure to the Data Protection Officers under GDPR should be mandated.;Yes;Liability should be framed as accountability and access to redress.;Yes, for all AI applications;;the EU should address copyright and database protections which prevent proper oversight of AI applications. Liability rules should provide incentives for openness.;
F528914;05-06-2020 11:14;German;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;4 - Important;3 - Neutral;4 - Important;4 - Important;;4 - Important;4 - Important;4 - Important;3 - Neutral;5 - Very important;3 - Neutral;;5 - Very important;4 - Important;2 - Not important;;5 - Very important;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;4 - Important;3 - Neutral;;There is a need for a new legislation;;Yes;;No opinion;;;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F528913;05-06-2020 10:59;Swedish;Public authority;Magnus;Garp;Regional;Region Kalmar län;;Large (250 or more);Sweden;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;No opinion;;No opinion;;;;;No opinion;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;;Very much;;Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;;Mental health risks;;No opinion;;No opinion;;No opinion;;;
F528912;05-06-2020 10:51;German;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;3 - Neutral;5 - Very important;2 - Not important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;;5 - Very important;3 - Neutral;3 - Neutral;;4 - Important;4 - Important;5 - Very important;4 - Important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;Current legislation may have some gaps;;No;;;;"Identifikation von Personen anhand biometrischer Daten im öffentlichen Raum, ""Scoring"" von Personen ähnlich wie in China, Diagnose Hilfen für Ärzte, Strafverfolgung und ""Pre-crime"" Dienste ähnlich zu denen der USA.";4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;Biometric identification systems should never be allowed in publicly accessible spaces;Solange KI zu unvorhersehbaren Ergebnissen kommt will ich Sie nicht mit biometrischen Daten operieren lassen.;Not at all;Es sollte verpflichtend sein.;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Mental health risks;;No opinion;;No opinion;;Yes, for all AI applications;;Als größte Schwierigkeit sehe ich die mangelnde Reproduzierbarkeit der Ergebnisse von AI und Bug Data Analysen. Das zweite ist die Missbrauchsanfälligkeit durch die nationalen Regierungen und der EU. Zukünftigen Regierungen darf es nicht gelingen eine flächendeckende Massenüberwachung und damit das Ende der Freiheitlichen Demokratie zu errichten.;
F528911;05-06-2020 10:40;Romanian;Academic/Research Institution;Gabriela;Dragan;;Facultatea Relatii Economice Internationale, ASE, Bucuresti;;Large (250 or more);Romania;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;Corelarea politicilor initiate la nivelul UE cu initiativele si politicile decise in plan global.;4 - Important;5 - Very important;3 - Neutral;;5 - Very important;5 - Very important;Reglementari comune, la nivelul Uniunii, privind protectia spatiului privat/personal ?;3 - Neutral;4 - Important;5 - Very important;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;Cum vor fi atrase ISD-urile ?;5 - Very important;5 - Very important;5 - Very important;No opinion;3 - Neutral;3 - Neutral;;There is a need for a new legislation;;No;;;;"Definirea ""riscului ridicat"" nu este simplu de facut. Ce inseamna ""risc ridicat"" ?";3 - Neutral;3 - Neutral;3 - Neutral;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);De stiut cum se vor folosi ulterior datele colectate.;Much;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F528910;05-06-2020 08:53;German;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;4 - Important;;5 - Very important;4 - Important;3 - Neutral;4 - Important;4 - Important;5 - Very important;;3 - Neutral;5 - Very important;4 - Important;;4 - Important;3 - Neutral;4 - Important;4 - Important;4 - Important;;3 - Neutral;5 - Very important;4 - Important;3 - Neutral;4 - Important;3 - Neutral;;There is a need for a new legislation;;No;;;;;4 - Important;3 - Neutral;5 - Very important;4 - Important;4 - Important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;Yes;;Yes, for all AI applications;;;
F528909;04-06-2020 20:58;German;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;4 - Important;5 - Very important;3 - Neutral;3 - Neutral;3 - Neutral;;5 - Very important;4 - Important;2 - Not important;;4 - Important;4 - Important;;4 - Important;3 - Neutral;3 - Neutral;;3 - Neutral;4 - Important;;5 - Very important;2 - Not important;;4 - Important;4 - Important;5 - Very important;;3 - Neutral;4 - Important;;Current legislation may have some gaps;;No;;;;;4 - Important;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;Biometric identification systems should never be allowed in publicly accessible spaces;;Very much;;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;;Personal security risks;;Yes;;No opinion;;No opinion;;;
F528908;04-06-2020 17:01;English;;;;Regional;;;;;The feedback can be published in an anonymous way;4 - Important;4 - Important;5 - Very important;4 - Important;4 - Important;5 - Very important;It would be important to work with Member states but also with regions. AI must be developed for non-European official languages too.;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;Non European official languages (mainly official languages in some of the states of the EU) should be taken into account in the Coordinated Plan on AI.;4 - Important;4 - Important;4 - Important;To give the opportunity to develop artificial intelligence in non-official European languages, especially in official languages in some of the EU states.;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;European regions and non-official European languages (such as languages that are official in one of the States of the Union) should be taken into account. AI can be an excellent opportunity for the human, social, economic and political development of regions that have a non-official European language, if AI were developed in those languages, obviously.;4 - Important;4 - Important;4 - Important;4 - Important;No opinion;3 - Neutral;A concern: AI to be a privilege or a power of the states and their hegemonic languages, forgetting the languages (and cultures) of the European Union, some of them being official in some EU states.;There is a need for a new legislation;;No;;;;Breach fundamental rights (such as human dignity, privacy, data protection, freedom of expression, workers' rights etc.) // Loss of languages and cultures in Europe that do not have access to artificial intelligence;4 - Important;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;No opinion;;Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;;Mental health risks;Changes and technological advances are very rapid, so new dangers may arise constantly.;Yes;;Yes;;Yes, for all AI applications;;;
F528907;04-06-2020 16:43;German;Other;Nathalie;SCHLENZKA;;Federal Antidiscriminiation Agency;;Small (< 50 employees);Germany;The feedback can be published with your personal information;5 - Very important;4 - Important;5 - Very important;3 - Neutral;3 - Neutral;4 - Important;Secure non-discrimination in the development of AI.;4 - Important;4 - Important;4 - Important;3 - Neutral;5 - Very important;5 - Very important;Focus on the development for ethical Guidelines on non-discrimination;4 - Important;4 - Important;3 - Neutral;Focus on Research activities identifiying discrimination risks related to AI;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;Risks of discrimination might be difficult to identify by users, discrimination might be difficult to notice by users and by companies and public Administration using AI systems;Other;EU directives on non-discrimination have to cover the protection of discriminiation by AI and have to be amaned accordingly ;Yes;;Yes;;HR Analytics in the Company and in recruitment procedures, profiling in Access to bank credits, insurances and similar Services, profiling in policing and legal proedures e.g. decisions on the Probation of Prisoners  ;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;Should be transparent for the users.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;;;No opinion;;Yes;;Yes, for specific AI applications;e.g. Inscurances, fiancial Services, bank credits;;
F528906;04-06-2020 16:09;German;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;;3 - Neutral;5 - Very important;4 - Important;;5 - Very important;4 - Important;4 - Important;3 - Neutral;5 - Very important;4 - Important;;5 - Very important;5 - Very important;4 - Important;;5 - Very important;3 - Neutral;5 - Very important;4 - Important;5 - Very important;;5 - Very important;5 - Very important;4 - Important;5 - Very important;3 - Neutral;4 - Important;;There is a need for a new legislation;;No;;;;;3 - Neutral;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;No opinion;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Mental health risks;;Yes;;Yes;;No opinion;;;
F528905;04-06-2020 15:33;English;Other;Sibylle;GABLER;;DIN;989808524267-58;Large (250 or more);Germany;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"Standardization is an important tool to achieve a harmonized European market for AI products and services and to foster innovation in the field. Moreover, standards will be the key for an EU-wide certification of AI products, services, and companies using or providing AI based technologies and are a necessary element of establishing a common level for trustworthiness and performance in Europe. 
DIN believes that the promotion of European standards is a highly important additional action. ";5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;;3 - Neutral;5 - Very important;5 - Very important;Similar to standardization, research on a European scale ought to be a participatory and collective effort uniting the expertise, priorities and viewpoints of a plurality of diverse stakeholders.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important; Provide information on existing standards, how to use standards to market SME’s AI products and applications and how to actively participate in the development of standards.;;;;;;;"The above table is open for a wide variety of interpretations. It is not a useful tool to gather the targeted input.
The angle and granularity of information are hardly suitable for an expedient discussion regarding the topic of potential risks posed by the introduction of AI components into practical applications. ";Other;Most industrial applications do not need new regulations as they are sufficiently covered by existing ones. Sector-specific legislation already exists in particular for those sectors regarded as “high-risk”. The legislator can references to standards (according to Regulation (EU) No 1025/2012) to address the concerns expressed in the whitepaper. However, guidance on how to apply sector-specific legislation would be helpful in order to have a common understanding.;Other;"Requirements should focus on the need to perform thorough risk analyses and assessments of the impact AI may have on individuals and society. 
Basing compulsory requirements on a general category of “high risk applications”, if it strongly emphasizes safety issues, is not appropriate. Relevant risks can also be related to privacy, fairness, erosion of democratic processes, etc. Make use of ISO/IEC 23894 Information technology-Artificial intelligence - Risk management";;;;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;No opinion;;Not at all;"Voluntary labelling in a B2B context does not offer added value and imposes an extra administrative burden for companies, especially for SMEs and startups. There is a risk that some providers of AI systems and services could use it rather as a marketing tool. 
Instead, better invest in the development of testing and validation methods, which would be applied based on a risk impact assessment. The legislator should instead promote a proper certification approach for AI systems.";A combination of ex-ante compliance and ex-post enforcement mechanisms;;Certification schemes should be based on international (ISO/IEC JTC 1 SC 42) or European standards, developed by European standardization organizations (Reg. 1025/2012). ;;;Yes;These risks assessments should be built upon European and international standards as is the case in other areas of safety legislation. ;Yes;;No opinion;;;
F528904;04-06-2020 15:32;German;NGO (Non-governmental organisation);Rudolf;REIBEL;;Bundesärztekammer / German Medical Association;89648243865-50;Medium (< 250 employees);Germany;The feedback can be published with your personal information;5 - Very important;4 - Important;5 - Very important;4 - Important;4 - Important;5 - Very important;An der Entwicklung, Prüfung und Umsetzung von KI-Anwendungen im Gesundheitsbereich sollten Organisationen von Ärzten und Patienten beteiligt werden. Im Interesse der Anwenderfreundlichkeit, der Sicherheit und der Verwertbarkeit der gewonnenen Daten sollte die Sichtweise der Gesundheitsberufe und Patienten als Anwender in jeder Etappe berücksichtigt werden.;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"Datensicherheit, die Schaffung eines Regelungsrahmens, die Frage der Inhaberschaft von eingespeisten Daten und Ergebnissen, Datenschutz, hohe Qualität und Repräsentativität der Daten zum ""Anlernen"" von KI-Anwendungen.
Die Entwicklung evidenzbasierter, vertrauenswürdiger, an Gerechtigkeit ausgerichteter und nichtdiskriminierender KI-Anwendungen sollte der Möglichkeit einer öffentlichen und industrieunabhängigen, nicht-öffentlichen Überprüfung unterliegen.";4 - Important;4 - Important;5 - Very important;;No opinion;No opinion;No opinion;No opinion;No opinion;;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;"- KI kann intransparent und unethisch genutzt werden.
- Die Haftung für Entwickler, Hersteller, Betreiber und Nutzer ist nicht immer klar.
- Eingespeiste unvollkommene Gesundheitsdaten können eine Verzerrung der Ergebnisse und künftigen Funktionsweise von";There is a need for a new legislation;;Yes;;No;;Alle KI-Anwendungen im Gesundheitssektor bergen ein hohes Risiko und sollten entsprechend behandelt werden. Falsche KI-Einschätzungen können gesundheits- oder lebensgefährdend sein (Empfehlung eines falschen Arzneimittels, Fehleinschätzung beim Krebsscreening, Verletzung bei einer Operation, fehlerhafte Priorisierung von Patienten etc.). Anhand fehlerhafter oder verzerrter Daten entwickelte KI kann diskriminierende Ergebnisse liefern. Eine strenge Einhaltung der DSGVO ist essenziell.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Bevor eine KI-Anwendung in die Versorgungspraxis übernommen wird, sollte eine unabhängige Bewertung ihres Nutzens und ihrer Sicherheit stattfinden. Bei der Bewertung medizinischer KI-Anwendungen sollten die zuständigen Stellen Fachwissen und Erfahrung von Ärzten und anderen betroffenen Berufsangehörigen und Nutzern einbeziehen.;Mental health risks;"- Datensicherheitsrisiken insb. aufgrund von Sekundärnutzung und der Fähigkeit von KI, Anonymisierung oder Pseudonymisierung rückgängig zu machen,
- Begrenzung von Haftungsrisiken für Personen, die KI-Anwendungen beruflich einsetzen (müssen), ohne KI-Fach";Yes;Da KI-Anwendungen Aktualisierungen unterliegen und/oder sich durch neu eingespeiste Daten oder Selbstoptimierung weiterentwickeln, ist ein Abstellen auf den Zeitpunkt des Inverkehrbringens nicht sachgerecht. Eine Risikobewertung muss über den gesamten Lebenszyklus einer KI-Anwendung erfolgen.;Yes;Die Produkthaftungsrichtlinie bedarf einer Überarbeitung. Die bestehende Regelung berücksichtigt nicht die neuartigen Risiken hinsichtlich Komplexität, Intransparenz, Autonomie und Offenheit von KI-Anwendungen. Unter anderem sollten die Definitionen von Produkt, Fehler, Beweislast und haftende Personen aktualisiert werden.;Yes, for all AI applications;;;
F528903;04-06-2020 14:05;English;Academic/Research Institution;Jurgen;Ganzevles;;Eindhoven University of Technology TU/e;026513529943-78;Large (250 or more);Netherlands;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"Prioritizing human centred design; democratization of AI: facilitating access and understanding by non-experts";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important; facilitating learning-by-doing;3 - Neutral;5 - Very important;5 - Very important;Facilitating a multidisciplinary approach;5 - Very important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;"Inclusion of human and ethical aspects; involvement of citizens; hubs as anchor points (no scattered/isolated initiatives)";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Learning-by-doing and monitoring (test) outcomes is important. Stay alert: legislation will always lag behind innovation.;Current legislation may have some gaps;;Yes;;Yes;;blackboxed decision making by AI. Design of AI should always be transparent, human centred, ethically sound. ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);"ethical design: prevention of discriminatory outcomes; sound balance between privacy and safety with respect to data storage";Much;should always be developed being complementary to legislation and certification;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Mental health risks;Privacy risks. Market monopoly risks. ;Yes;;;;No opinion;;;
F528902;04-06-2020 14:04;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;;3 - Neutral;3 - Neutral;5 - Very important;;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;4 - Important;;There is a need for a new legislation;;No;;;;Profiling;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;No opinion;;Much;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Personal security risks;;Yes;;Yes;;Yes, for all AI applications;;;
F528901;04-06-2020 13:35;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;The development of AI capabilities in Europe requires a clear and flexible business and investment framework. That is why the MTA stands for specific rules as to the adoption of AI tools and applications in European manufacturing, ensuring that these new means comply with the European Single Market principles and with free and fair trade conditions.;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;Define clear rules with regards to the ownership of data in relation to the European data space.;3 - Neutral;5 - Very important;5 - Very important;"Set up a right policy that enables and fosters progress in research while at the same time guaranteeing innovation on industrial AI.
A comprehensive European investment plan should be laid out by EU policymakers, focusing on the development of machine learning, AI applications and other digital areas in which European companies are global leaders. This investment plan should boost the adaption of AI and digital tools in manufacturing, as they can provide significant advantages to companies.";5 - Very important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;;3 - Neutral;3 - Neutral;3 - Neutral;2 - Not important;4 - Important;3 - Neutral;"From European machine tool manufacturers, AI systems are not a product; AI is a broader technology that is integrated into their industrial products. Therefore, rules and responsibilities concerning the use of AI systems in manufacturing must be clearly specified.";Current legislation may have some gaps;;Yes;;No;;;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;No further guidelines or regulations are needed;The biometric identification system is already covered under the EU GDPR (General Data Protection Regulation) and it is considered a “special category of personal data” that requires both a special legal basis for processing and an accompanying data protection impact assessment.;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;CE marking self-certification;Risks related to the loss of connectivity;;No;;No;;No;;;CECIMO_Position_paper_on_AI.pdf
F528900;04-06-2020 11:47;English;;;;;;;;;The feedback can be published in an anonymous way;No opinion;5 - Very important;;5 - Very important;5 - Very important;No opinion;;5 - Very important;3 - Neutral;4 - Important;4 - Important;5 - Very important;5 - Very important;"The focus should be to support and facilitate private initiative by developing market incentives and better way to scale ideas.

Furthermore, where we believe public policy can help in promoting an ecosystem of excellence is by working with the different players to reach data interoperability. Data operability is a key factor in allowing to reach greater economies of scale and supporting SMEs active in the field of AI.  
";3 - Neutral;No opinion;4 - Important;EU institutions should focus on supporting private initiatives as many cutting-edge R&D centers of excellence already exist across the EU. Additional efforts are needed to create more and bigger data pools that enable new business models. This would allow SMEs to pool data, for them to reach scale advantages similar to big corporates. EU institutions should consider providing direct grants to businesses developing AI techniques to help them reach a wider scale (e.g., MAS program) ;4 - Important;3 - Neutral;5 - Very important;5 - Very important;4 - Important;Ensure SMEs understand risks and chances of AI and are prepared to follow current and future legal and regulatory framework without limiting chances and opportunities.;3 - Neutral;5 - Very important;5 - Very important;5 - Very important;2 - Not important;3 - Neutral;;Current legislation may have some gaps;;Yes;;Other;Any classification / taxonomy must provide the sufficiently clear in order to provide legal certainty. See full answer in attached document. ;Use cases which have the potential to significantly impact a person or its personality (data). See full answer in attached document. ;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;No opinion;EU-wide minimum requirements will help in maintaining the people’s trust in such identification systems by ensuring the necessary safety.;Much;A labelling system could allow for comparison and increased transparency for consumers. However, if voluntary it may bear the risk that not all players including the big players strive for such a regional certification. Also, all the players would need to provide the necessary insight into their AI operations. Moreover, it would require significant resources. Finally, if customers base their decisions on such a label, who would bear the liability risk if a certified provider fails. ;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Given that AI solutions are highly dynamic and subject to change / modifications when operational it seems recommendable to combine an ex-ante with and ex-post enforcement mechanism. However, it should be noted that regarding an “ex-ante” approach, “robustness” can be partially tested, but never fully confirmed given the intrinsic nature of deep neural networks. ;Mental health risks;Suggest adding health risk in general instead of mental health risk.;No opinion;;No;See full answer in attached document. ;No;;See full answer in attached document. ;
F528899;04-06-2020 11:10;Italian;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;"Definizione di regole chiare sull'""European Data Space""";3 - Neutral;4 - Important;4 - Important;"Definire una giusta politica che consenta e promuova i progressi nella ricerca, ma allo stesso tempo, garantire l'innovazione sull'intelligenza artificiale industriale.
I responsabili delle politiche dell'UE dovrebbero elaborare un piano di investimenti europeo globale, concentrandosi sullo sviluppo dell'apprendimento automatico, delle applicazioni AI e di altre aree digitali in cui le aziende europee sono leader globali. Inoltre, questo piano di investimenti dovrebbe favorire l'adattamento";4 - Important;4 - Important;4 - Important;4 - Important;3 - Neutral;;3 - Neutral;3 - Neutral;3 - Neutral;2 - Not important;4 - Important;3 - Neutral;"Per i produttori europei di macchine utensili, i sistemi di intelligenza artificiale non sono un prodotto; AI è una tecnologia più ampia che è integrata nei loro prodotti industriali. Pertanto, le regole e le responsabilità, relative all'uso dei sistemi di intelligenza artificiale nei sistemi di produzione, devono essere chiaramente specificato.

";Current legislation may have some gaps;;Yes;;No;;;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;No further guidelines or regulations are needed;"Il sistema di identificazione biometrica è già coperto dal GDPR dell'UE  ed è considerata una ""categoria speciale di dati personali"" ";Not at all;Un sistema di etichettatura, volontario o meno, presuppone una base normativa consolidata, attualmente non disponibile per la IA (a partire da una norma che dia una definizione chiara e condivisa di IA);A combination of ex-ante compliance and ex-post enforcement mechanisms;;Marcatura CE in autocertificazione;Risks related to the loss of connectivity;;No;;No;;No;;;
F528898;04-06-2020 08:36;English;EU Citizen;Dirk;KLUG;;;;;Germany;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;The development of AI capabilities in Europe requires a clear and flexible business and investment framework. That is why CECIMO stands for specific rules as to the adoption of AI tools and applications in European manufacturing, ensuring that these new means comply with the European Single Market principles and with free and fair trade conditions.;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;Define clear rules concerning the ownership of data in relation to the European data space;2 - Not important;5 - Very important;5 - Very important;Set up a right policy that enables and fosters progress in research while at the same time guaranteeing innovation on industrial AI. A comprehensive European investment plan should be laid out by EU policymakers, focusing on the development of machine learning, AI applications and other digital areas in which European companies are global leaders. Furthermore, this investment plan should boost the adaption of AI and digital tools in manufacturing, as they can provide significant advantages.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;2 - Not important;;3 - Neutral;3 - Neutral;3 - Neutral;2 - Not important;4 - Important;4 - Important;"From European machine tool manufacturers, AI systems are not a product; AI is a broader technology that is integrated into their industrial products. Therefore, rules and responsibilities concerning the use of AI systems in manufacturing must be clearly specified.";Current legislation may have some gaps;;Yes;;No;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No further guidelines or regulations are needed;"The biometric identification system is already covered under the EU GDPR (General
Data Protection Regulation) and it is considered a “special category of personal data”
that requires both a special legal basis for processing and an accompanying data
protection impact assessment.";Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;CE-marking self-certification;Mental health risks;;No;;No;;No;;;
F528897;03-06-2020 21:15;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"Creating a distributed research center in which universities will have a key role in terms of strategies and decision making
Increase diversity, equity and inclusion of the AI field to ensure sufficient talent and diversity of viewpoints
Develop centers of expertise on responsible design of AI, including an understanding of AI suitability, risks and limitations
Focus on multidisciplinary AI skill development, including non-technical AI expertise and AI subfields beyond data-driven AI";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;To implement the testing facilities, it is necessary to establish how (and which metrics) AI should be assessed (a standard). This is not only about data, software and hardware. AI is NOT only a matter of reliability, trust and acceptance, but also (and mainly) of quality of interaction, the safety and the impact on people. We believe AI assessment process should be built upon well-consolidated approaches with appropriate adaptations i.e. the one of medtech (IEC62366,  ISO14971) ;2 - Not important;5 - Very important;5 - Very important;Action 2 “legal instrument”. A large consultation is needed for this. As AI is not only hardware and data, and AI can be used in intended and unintended ways with serious (inadvertent) risks for people, the testing centers should not only focus on technical aspects but also considering ethics, the human factors, and risk in use. We strongly believe in the opportunity to transfer knowledge from the field of medtech to build upon standards, legislations and practices already in place;5 - Very important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;To promote openness and transparency, and to foster the competitiveness of students and SME we believe that the centrality of the public universities and higher education and research institutes as representative of the scientific (national and international) interests should be prioritized over private institutions in the composition of the Innovation Hubs. As the commercial interests on AI should be led by the common interests of human and technological advancement and coexistence.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;There is a tendency to separate (in EU calls) technical and social aspects, however,  AI does not exist in isolation. Focusing vigorously on AI technical aspects may have drawbacks. The investments should actually require technical AI projects and consortium to always involve a component regarding impact on people from the interactive and the human factors point of view. This is important in order to keep the current spearheads sharp and to prevent the inflation of the AI concept.;Other;The scope should be carefully defined by the properties of the technology rather than a definition of AI. The current definition of AI as “data and algorithms” is too general. Moreover, regulation should be accompanied with appropriate design practices. AI that can be used in Europe should be certified and monitored (post-market surveillance). We are calling for a CE Marking like the one of medtech. The risks in use of AI cannot be solved by a simple compliance with a set of requirements;Other;The mechanism to identify high-risk AI seems to assume that risks can be divided in high or low. Risk is context dependent and it is a matter of potential hazard, level of exposure and vulnerability of people. Even low risk AI can seriously harm people. Developers should mitigate all the potential risks in any case by applying human factors and risk assessment. To generate a minimal set of evidence regarding safety of use (not only reliability) should be a requirement for all developers.;;;Determining high-risk AI by identifying risky domains is not future-proof as we cannot oversee which novel risks may arise in different domains with future AI developments. Moreover, UT believes that safety in the interaction should be guaranteed a priori, to reduce the risks of harms. Therefore, all the AI should follow a common UCD approach of evidence generation (e.g., IEC62366) to mitigate the risks in the interaction (including post-market surveillance). Again, a mechanism like the CE Marki;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;The EU should balance freedom and safety of citizens. UT agrees that access and usage of these types of invasive and invisible applications should be restricted to a few, and very limited and well-specified events concerning public security threats with a rigid protocol. UT could envisage the risks of such types of applications left in the hands of national states. The new regulation should therefore strongly limit the usage in the EU of such technologies at the national level.  ;Not at all;"UT does not believe that voluntarily labelling is an acceptable system, as all the risks involved in the usage of the applications “that do not qualify as high-risk"" will be unknown without a minimal mandatory process of evidence generation to support a check regarding the quality of interaction and the risk management. UT does believe that untested AI applications even for trivial tasks should not be allowed without at least mapping the risks in use and by mitigating these to an acceptable leve";A combination of ex-ante compliance and ex-post enforcement mechanisms;;UT believes that compliance to requirements is not enough to ensure safety and trust toward AI applications but a scalable process of evidence generation should be enforced for AI categorized at different levels of risks. Independently from the AI categorization the assessment of risks, human factors and interaction with people should be performed and used to mitigate potential issues;Mental health risks;;Yes;The simple dichotomy (high or low) is not appropriate. There is always a level of residual uncertainty when a powerful technology is inserted in a context of use. For some application this level is higher and this will require an extra effort to bring the AI to the market, nevertheless, independently from the risk level of the AI a framework of evidence generation to guide AI designers is necessary to ensure risk mitigation, usability and safety in the interaction exchange.;Yes;;Yes, for all AI applications;;;The_University_of_Twente_is_the_ultimate_people_first_university_of_technology.pdf
F528896;03-06-2020 19:43;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;;3 - Neutral;4 - Important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;;Current legislation may have some gaps;;Yes;;Other;Please see document attached;;4 - Important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;4 - Important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Very much;;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;;Cyber risks;;Yes;;Yes;;Yes, for specific AI applications;;;AI_response.pdf
F528895;03-06-2020 12:33;English;Academic/Research Institution;Paul;MERKUS;;Eindhoven University of Technology;026513529943-78;Large (250 or more);Netherlands;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Eindhoven University of Technology (TU/e) has established the Eindhoven AI Systems Institute (EAISI) as a regional hub for leveraging AI knowledge to the regional industrial and societal ecosystem, focusing on AI for the real world. With AI moving Data-only to Data-Human-Machine, we aim to use our traditional strengths to significantly leverage the huge potential of this next generation AI.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"Joint EU investment agenda is important. The EC should facilitate the establishment / strengthening of centers of excellence and test centers that combine EU, national and regional knowledge / investments in a multidisciplinary manner. As a regional AI hub, TU/e's EAISI is an active participant in the Dutch national AI excellence and expertise collaboration and would like to see the EC continue to set up and connect leading
AI centers and knowledge institutions.";3 - Neutral;5 - Very important;5 - Very important;TU/e's EAISI encourages the EC efforts to focus on a top European network of AI excellence centers, anchored in Hubs with associated spokes, in order to counteract the fragmented landscape of knowledge institutions and centers in the field of AI and to improve cooperation. Joint research and innovation programs, training programs for mobility instruments and investments in infrastructures may achieve this.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;Drop ecosystems of hubs-and-spokes built upon the existing systems. We emphasize that activities in the hubs should not only technically scientific, but also include human and ethics aspects and the involvement of citizens. All activities in other geographic locations should be connected to hubs like TU/e's EAISI to avoid isolated initiatives. The resulting hubs-and-spokes structure may then function as the national AI hub in EU initiatives per EU Member State, with recognition of regional hubs.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Where in the past our tools have been primarily passive extensions of ourselves, we are now moving to an era where we, as humans, will be working with tool sets as active, intelligent partners. It will require us to rethink and redesign the relationships and the interfaces we have with and to the intelligent technology that we use in our daily lives.;Current legislation may have some gaps;;Yes;;Yes;;It urges us to focus our efforts on the development of human-centered AI, that is, AI that connects to humans, helps and learns from humans, and collaborates with humans in a meaningful way, combining the advantages of AI with those of humans. Even perfectly functioning AI is useless if humans are not willing or able to integrate it in their daily lives and ways of working.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);Biometric identification should pay special attention to the design and development phase of AI in these systems for processing special personal data, insofar as this is necessary to counter discriminatory effects, in particular with regard to specific vulnerable groups.;Much;This proposal as one of the varied instruments is worthwhile to consider, where the development of legislation as well as other instruments such as standardization or certification is also an option.;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;Since supervisors must now be able to carry out the “conformity assessment” proposed by the EC, the starting point should be that as much as possible alignment with existing frameworks and structures.;Mental health risks;GDPR may be insufficiently tailored to AI applications. And certainly the interpretation and practical interpretation. References are necessary because supervisors are also still learning.;Yes;It should be clear to all actors (addressees) which rules they are subject to at the various stages of AI. In addition, the requirements would apply to all AI applications on the European market, regardless of whether the producer is located outside the EU.;No opinion;"A proactive involvement of citizens, scientists and companies in and co-creation of standards and rules for AI in a learning environment, in which the rules are ultimately supported by stakeholders, is an absolute ""must"" and will quickly show whether the current framework is sufficient.";Yes, for all AI applications;;A level playing field is not present between offline consumer protection and protection in the online world. Therefore, the position of the digital consumer may need to be further investigated.;
F528894;03-06-2020 11:24;English;Academic/Research Institution;Ben;RUTTEN;;Technische Universiteit Eindhoven;026513529943-78;Large (250 or more);Netherlands;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Eindhoven University of Technology (TU/e) has established the Eindhoven AI Systems Institute (EAISI) as a regional hub for leveraging AI knowledge to the regional industrial and societal ecosystem, focusing on AI for the real world. With AI moving Data-only to Data-Human-Machine,? we aim to use our traditional strengths ?to significantly leverage the huge potential ?of this next generation AI.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Joint EU investment agenda is important. The EC should facilitate the establishment / strengthening of centers of excellence and test centers that combine EU, national and regional knowledge / investments in a multidisciplinary manner. As a regional AI hub, TU/e's EAISI is an active participant in the Dutch national AI excellence and expertise collaboration and would like to see the EC continue to set up and connect leading AI centers and knowledge institutions.;3 - Neutral;5 - Very important;5 - Very important;TU/e's EAISI encourages the EC efforts to focus on a top European network of AI excellence centers, anchored in Hubs with associated spokes, in order to counteract the fragmented landscape of knowledge institutions and centers in the field of AI and to improve cooperation. Joint research and innovation programs, training programs for mobility instruments and investments in infrastructures may achieve this.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;Drop ecosystems of hubs-and-spokes built upon the existing systems. We emphasize that activities in the hubs should not only technically scientific, but also include human and ethics aspects and the involvement of citizens. All activities in other geographic locations should be connected to hubs like TU/e's EAISI to avoid isolated initiatives. The resulting hubs-and-spokes structure may then function as the national AI hub in EU initiatives per EU Member State, with recognition of regional hubs.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Where in the past our tools have been primarily passive extensions of ourselves, we are now moving to an era where we, as humans, will be working with toolsets as active, intelligent partners. It will require us to rethink and redesign the relationships and the interfaces we have with and to the intelligent technology that we use in our daily lives. ;Current legislation may have some gaps;;Yes;;Yes;;It urges us to focus our efforts on the development of human-centered AI, that is, AI that connects to humans, helps and learns from humans, and collaborates with humans in a meaningful way, combining the advantages of AI with those of humans. Even perfectly functioning AI is useless if humans are not willing or able to integrate it in their daily lives and ways of working.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);Biometric identification should pay special attention to the design and development phase of AI in these systems for processing special personal data, insofar as this is necessary to counter discriminatory effects, in particular with regard to specific vulnerable groups.;Much;This proposal as one of the varied instruments is worthwhile to consider, where the development of legislation as well as other instruments such as standardization or certification is also an option.;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;Since supervisors must now be able to carry out the “conformity assessment” proposed by the EC, the starting point should be that as much as possible alignment with existing frameworks and structures.;Mental health risks;GDPR may be insufficiently tailored to AI applications. And certainly the interpretation and practical interpretation. References are necessary because supervisors are also still learning.;Yes;It should be clear to all actors (addressees) which rules they are subject to at the various stages of AI. In addition, the requirements would apply to all AI applications on the European market, regardless of whether the producer is located outside the EU.;No opinion;"A proactive involvement of citizens, scientists and companies in and co-creation of standards and rules for AI in a learning environment, in which the rules are ultimately supported by stakeholders, is an absolute ""must"" and will quickly show whether the current framework is sufficient.";Yes, for all AI applications;;A level playing field is not present between offline consumer protection and protection in the online world. Therefore, the position of the digital consumer may need to be further investigated.;
F528893;03-06-2020 11:11;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;;5 - Very important;4 - Important;4 - Important;4 - Important;3 - Neutral;;3 - Neutral;3 - Neutral;3 - Neutral;4 - Important;2 - Not important;4 - Important;;There is a need for a new legislation;;Yes;;Yes;;;4 - Important;3 - Neutral;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Cyber risks;;Yes;;Yes;;Yes, for all AI applications;;;
F528892;03-06-2020 09:11;English;Company/Business organisation;Tjeerd;Oudkerk;;NV Nederlandse Spoorwegen;;Large (250 or more);Netherlands;The feedback can be published with your personal information;4 - Important;4 - Important;;2 - Not important;4 - Important;4 - Important;;4 - Important;3 - Neutral;3 - Neutral;2 - Not important;4 - Important;4 - Important;;2 - Not important;3 - Neutral;3 - Neutral;Next to joining forces between industries (both public and private) and research centers, also include citizens in the community of research and innovation. For example by publishing challenges and ask the public for solutions.  ;No opinion;3 - Neutral;No opinion;3 - Neutral;3 - Neutral;;4 - Important;5 - Very important;4 - Important;4 - Important;3 - Neutral;3 - Neutral;When using an AI application, who is responsible and who is accountable? The AI application in itself can’t be held accountable, as it’s not a person or legal entity. More specific in case a company buys AI applications build by others, who is responsible and accountable for decisions made by the AI application?  ;Current legislation may have some gaps;;Yes;;No opinion;;It’s not a specific AI application that concerns us the most, but the type of AI application. From our perspective a high-risk AI application is an application that has the ability to decide for certain solutions opposite to our company vision and values or in contradiction with certain rules and regulations.;4 - Important;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);Specific guidelines or legislation should be in place before use of biometric identifications systems in publicly accessible spaces will be allowed, but these guidelines or legislation does not necessarily be at EU level. Countries can decide for themselves when such use is allowed.;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;A combination of ex-ante compliance and ex-post enforcement mechanisms is necessary in our opinion. AI applications fit extremely well in an agile development cycle, with short release- and go-to-market cycles. Using only ex-ante compliance or only ex-post mechanisms will slow down this process and limit the use and development of AI applications. Ex-ante compliance for first (basic) releases of a new application and ex-post mechanisms for incremental releases will work best in our opinion.;Mental health risks;The accountability and responsibilities for the decisions made by AI applications is in our opinion the most important issue in safety legislation. As an AI application is not a person nor legal entity in itself, it’s extremely important to be clear on who is responsible and accountable for the actions from an AI application. This will be of great help in legislation about safety protection. ;Yes;Also refer to question about ex-ante compliance and ex-post mechanisms, a sensible combination of both should be considered in new risk assessment procedures to allow for agile development and short development cycles.;No opinion;Again, we stress out the importance of  making clear who is accountable and responsible for results from an AI application.;Yes, for specific AI applications;;We think changes or little additions should be made for specific AI applications, but we have too little in-depth knowledge of current liability rules to specify. ;20200527_Vision_on_white_paper_AI.pdf
F528891;03-06-2020 02:08;English;NGO (Non-governmental organisation);Alexia;Pickard;;GESAC - European Grouping of Societies of Authors and Composers;36529354479-57;Micro (< 10 employees);Belgium;The feedback can be published with your personal information;;;;;;;GESAC intends to provide only general comments and observations on the White Paper, as the issue is an important one for the creators community we represent, but the below detailed questions of the survey are not applicable. Please find attached our short written contribution.;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;GESAC.views.AI.WP.consultation_29.05.20.pdf
F528890;03-06-2020 00:33;English;EU Citizen;Daniel;Draghicescu;;;;;Romania;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;5 - Very important;Ensure a unified AI infrastructure policy for EU and non-EU cloud service providers;5 - Very important;5 - Very important;5 - Very important;;2 - Not important;4 - Important;3 - Neutral;5 - Very important;5 - Very important;;4 - Important;5 - Very important;4 - Important;3 - Neutral;No opinion;3 - Neutral;AI may raise fear and uncertainty for people who are not familiar with technology;There is a need for a new legislation;;Yes;;Yes;;AI applied in the defence sector;4 - Important;2 - Not important;5 - Very important;5 - Very important;4 - Important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Periodic re-assessment of compliance during the entire life cycle of the AI application, especially for those systems which might change their behaviour in time (e.g. after more data is collected in production);Personal security risks;;Yes;;No opinion;;No opinion;;;
F528889;02-06-2020 23:58;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;;5 - Very important;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;;4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;;4 - Important;5 - Very important;4 - Important;4 - Important;3 - Neutral;4 - Important;;There is a need for a new legislation;;No;;;;;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F528888;02-06-2020 15:12;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;"Promotion of key technologies in the field of ""trustworthy"" AI should be financially supported. Collaboration between public and private entities
";5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;4 - Important;;3 - Neutral;5 - Very important;;"-Providing workforce and citizens with the education/qualification to allow them to apply AI
-Increase public acceptance of AI -based technologies  by early age minimal education EU
-Instead of the new lighthouse center, the focus should be on the existin";3 - Neutral;3 - Neutral;3 - Neutral;5 - Very important;No opinion;;4 - Important;4 - Important;4 - Important;4 - Important;3 - Neutral;No opinion;AI has to be so accurate that applicable laws and traffic rules are obeyed and passengers/road user feel comfortable. Training AI on broader/better data using ML will produce better outcomes.;Other;"Current legislation may have some gaps. But
-to ensure that an AI framework doesn't duplicate/invalidate existing certification requirements/regulatory frameworks(Type Approval,UNECE regulations), it should be carefully considered where these and industry standards are better instruments to address gaps
-different AI applications may require diversified regulatory intervention, hence a specific assessment of AI use-cases is prerequisite for the identification of any regulatory intervention
";Yes;;Other;Classifying the whole sector as a high risk is unacceptable because the sector consists from various AI applications which are not all high-risk. The continuous assessment of the sectors’ high-risk level could hamper the AI development and disrupt the use of the non-critical services in the sector. The focus should be on AI applications and how safety critical is their function. Exact criteria for such assessment should be identified along with the clarification who evaluate the risk level.;Any application with a black-box algorithm, that is self-learning and makes automated decisions with a high impact on human rights and safety.;2 - Not important;3 - Neutral;2 - Not important;3 - Neutral;2 - Not important;4 - Important;No opinion;;Rather not;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;;;No opinion;;No;;No;;;
F528887;02-06-2020 12:14;German;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;No opinion;4 - Important;5 - Very important;2 - Not important;1 - Not important at all;;2 - Not important;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;;No opinion;No opinion;No opinion;;2 - Not important;2 - Not important;2 - Not important;2 - Not important;2 - Not important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;No;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Very much;;;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F528886;02-06-2020 11:33;English;EU Citizen;Bastien;Confais;;;;;France;The feedback can be published with your personal information;4 - Important;4 - Important;4 - Important;3 - Neutral;3 - Neutral;3 - Neutral;#NAME?;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;#NAME?;4 - Important;4 - Important;3 - Neutral;"Artificial Intelligence can contribute to make progresses in other research fields, such as networking, distributed computing or even other domains such as medicine, physics etc. In the same way, researchers of other fields can bring new ideas to improve the algorithms used by AI. 

Therefore, I think that promoting links between research teams and research fields is something that should be encouraged.";4 - Important;4 - Important;4 - Important;4 - Important;3 - Neutral;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;3 - Neutral;"Understanding the AI decisions and improving their accuracy are research topics that need to be pursued.

AI can breach fundamental rights and can lead to discriminations but these are the same dangers we can find in every process. The major difference is not understanding how a decision is made increases the sense of inequality and unfairness. ";No opinion;;No opinion;;;;"It is hard to say because high-risk is very dependent to the legislation and the society.
For me, the most important risk of AI is when AI ""guesses"" a piece of personal information people were not willing to share.
";3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;No opinion;;Much;;No opinion;;;;;No opinion;;No opinion;;No opinion;;;AI_and_autism.pdf
F528885;02-06-2020 10:23;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;5 - Very important;3 - Neutral;4 - Important;5 - Very important;;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;;3 - Neutral;5 - Very important;4 - Important;;4 - Important;5 - Very important;4 - Important;5 - Very important;4 - Important;;3 - Neutral;5 - Very important;4 - Important;4 - Important;4 - Important;3 - Neutral;;Current legislation may have some gaps;;Yes;;Yes;;;4 - Important;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;No opinion;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;No opinion;;Yes, for specific AI applications;;;
F528884;01-06-2020 22:37;English;;;;;;;;;The feedback can be published in an anonymous way;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;Biometric identification systems should never be allowed in publicly accessible spaces;WE-THE EU-ARE NOT CCP;;;;;;;;;;;;;;;
F528883;01-06-2020 16:03;Spanish;;;;National;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;No opinion;5 - Very important;5 - Very important;"Adecuar las actuaciones que propone el ""Libro Blanco"" para facilitar la convergencia con las prioridades de la Agenda 2030 y con los ""Principos rectores de Naciones Unidas sobre empresas y Derechos Humanos"", para que las organizaciones que hayan lesionado los derechos humanos y los derechos fundamentales puedan poner remedio al daño hecho y reparar a las posibles víctimas.";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;La creación y adopción de estandares comunes para todos los operadores del mercado, tal como aconseja la Recomendación CM/Rec2020 (1) del Comité de Ministros del Consejo de Europa del 8 de abril de 2020.;5 - Very important;5 - Very important;No opinion;Existencia de un Marco financiero europeo que ayude a los países con menos recursos a desarrollar sistemas de IA para mejorar las soluciones a los problemas o situaciones sobrevenidas como es el caso de la crisis sanitaria, económica y social que estamos sufriendo actualmente todos los países de la UE, causada por el COVID-19.;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;No opinion;5 - Very important;No opinion;;There is a need for a new legislation;;Yes;;Yes;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;"Los Gobiernos deben ser extremadamente cuidadosos con los datos que se recogen y almacena, velando por el respeto a la privacidad y seguridad de la ciudadanía y evitando su manipulación por parte de terceros.
";Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;No opinion;;No opinion;;Yes, for all AI applications;;;
F528882;01-06-2020 14:49;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"- Public & private sectors = captive markets for Big Techs pervading (and spying) technologies who poisoned them. Public sector should benefit from European public, open source and decentralised network of actors
- Have decentralized European infrastructu";5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;"- Supporting start-ups is fine; more importantly scale-ups should be supported in various ways (money, coaching, networking, etc) with strict criteria on partners (see next §)
- More importantly, support the creation of decentralised networks of SMEs that";4 - Important;5 - Very important;5 - Very important;- The definition of who are the potential partners is critical to ensure a true European value chain. Very strict criteria should be used (together with support of networking) in order to avoid any technology leak towards foreign regions (China, US);5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;"- Sustainable Finance is critical. Build bridges between sustainable finance and responsible innovation communities via AI projects
- AI projects should involve not only companies and academia, but also public authorities and civil society representatives";5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;"- AI in military applications not addressed
- AI in public administration not or ill addressed, especially Justice and Education
- AI usages as a threat for democracy if not accompanied properly in some applications";Other;Don't know if a brand new legislation is needed. What is clear is that the current one should be updated and complemented;Yes;;No;;- Industrial installations (factories, etc...) can be at stake if controlled by AI systems ill-designed or undergoing cyberattacks for ex. (leak of toxic or nuclear product, abrupt freezing of operations under cyberattack that lead to explosion, or ...etc;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;"- currently, it does not work so well and is biased
- ultimately, it is the quality of the social tissue and a low level of social inequalities that prevent crime and incivilities; face recognition is just a tool to care a symptom of an ill society";Much;#NAME?;A combination of ex-ante compliance and ex-post enforcement mechanisms;;- the concept of experimental regulatory sandboxes is appealing, as long as it gathers together not only companies and public bodies, but also civil society and academics to test on a small scale and before market sampling, and in a safe regulatory enviro;Mental health risks;- I think to all the applications of AI (use of social networks, of gaming, etc...) that result in addiction and public health problems;Yes;- if the product evolves during its life cycle, a regular assessment should be performed, and precaution principle should be applied in case of possible big damages;Yes;;Yes, for specific AI applications;;;
F528881;01-06-2020 13:33;English;Company/Business organisation;stephen;PATTISON;;Arm Ltd;532086816465-17;Large (250 or more);United Kingdom;The feedback can be published with your personal information;3 - Neutral;3 - Neutral;4 - Important;3 - Neutral;4 - Important;3 - Neutral;;4 - Important;3 - Neutral;4 - Important;4 - Important;4 - Important;3 - Neutral;;3 - Neutral;4 - Important;4 - Important;;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;Current legislation may have some gaps;;Yes;;No;;please see attached paper.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);the acceptability of such use will depend among other things on context, purpose, whether identification involves assigning a name, whether biometric is facial or other, the length of time the data will be stored etc  ;Much;see the attached paper: much depends on what the labelling system tell us about the AI system and how the high risk areas are identified. ;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Cyber risks;;Yes;;No opinion;;No opinion;;;Commission_AI_consultation_response.docx
F528880;01-06-2020 12:07;English;Company/Business organisation;Petja;Piilola;;TietoEVRY;;Large (250 or more);Finland;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;3 - Neutral;;4 - Important;4 - Important;5 - Very important;;No opinion;No opinion;No opinion;No opinion;No opinion;;3 - Neutral;3 - Neutral;2 - Not important;2 - Not important;1 - Not important at all;1 - Not important at all;Many of the above aren't currently an issue, but they could become so in the future, depending on the application. This is why the regulation should be use-case based.;Other;The regulation should be driven be use-cases and in principle be more pragmatic than the current framework not to prevent innovation.;Yes;;Other;There are a few concerns with regard to the proposed approach. Most useful applications are likely going to be categorized as high-risk. Furthermore, the effort required to monitor, test and certify what constitutes a high or low risk application would likely not be very practical.;;4 - Important;4 - Important;4 - Important;3 - Neutral;2 - Not important;3 - Neutral;No further guidelines or regulations are needed;;Very much;;Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;;Mental health risks;;No opinion;The regulatory framework in this regard should be use-case driven.;No opinion;The regulatory framework in this regard should be use-case driven.;Yes, for specific AI applications;Self-driving driving cars or diagnosis treatments, for example.;;TietoEVRY-EU_level_AI_regulation_position_paper_v5_2020.pdf
F528879;31-05-2020 18:55;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;;4 - Important;4 - Important;5 - Very important;4 - Important;4 - Important;5 - Very important;#NAME?;4 - Important;5 - Very important;5 - Very important;#NAME?;4 - Important;5 - Very important;4 - Important;5 - Very important;4 - Important;#NAME?;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;#NAME?;Current legislation may have some gaps;;Yes;;Yes;;Any avoiding human in the decision loop.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Cyber risks;#NAME?;Yes;;Yes;;Yes, for specific AI applications;Those with high risk implications.;Agility of any safety assessment procedure by authorities should be drastically improved in comparison with current practices.;
F528878;31-05-2020 15:06;German;NGO (Non-governmental organisation);Roxana;Duersch;;Bundeszahnärztekammer e.V. (German Dental Association);;Small (< 50 employees);Germany;The feedback can be published with your personal information;5 - Very important;4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;Der Erfolg der Digitalisierung im Gesundheitswesen hängt zu einem wesentlichen Teil von gut aus- und weitergebildetem Fachpersonal ab. So sollten digitale Technologien und KI bereits im Zahnmedizin-Studium vermittelt werden. Zudem müssen diese Fähigkeiten in Fortbildungen für Zahnärzte/-innen vermittelt werden.;4 - Important;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;Der Aufbau von Wissen im Bereich KI und Digitalisierung ist essentiell. Nur so kann Vertrauen in KI-Technologien wachsen, und einen Mehrwert zur zahnärztlichen Arbeit erbringen.;5 - Very important;5 - Very important;4 - Important;---;No opinion;5 - Very important;4 - Important;4 - Important;4 - Important;---;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;KI-Anwendungen können insbesondere im medizinischen Bereich das besondere Vertrauensverhältnis zwischen Zahnarzt und Patient nicht ersetzen. Dieser menschliche Faktor ist hier essentiell. Bei (zahn-)ärztlicher Nutzung von KI müssen Haftungsfragen eindeutig geklärt sein. Der Einsatz von KI kann zu Fehlern, falschen Diagnosen oder unvollständigen Ergebnissen führen.;No opinion;;Yes;;Yes;;Der Einsatz von KI im Gesundheitswesen birgt große ethische Herausforderungen, die benannt und diskutiert werden müssen. KI können aufgrund fehlerhafter Daten oder Algorithmen falsche, potentiell patientengefährdende Diagnosen stellen. Damit ist die Gesundheit und das Wohlergehen von Menschen von dieser Technologie abhängig. Es braucht die höchsten Sicherheits- und Qualitätsstandards im sensiblen Gesundheitsbereich.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;---;No opinion;---;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Insbesondere medizinische Daten müssen transparent, klinisch überwacht und in einem qualitativ hochwertigen System gesammelt werden, um Selektionsverzerrungen zu vermeiden. Die Daten müssen regelmäßigen, unabhängigen Qualitätskontrollen unterliegen.;Personal security risks;Zahnärzte/-innen müssen in KI-Technologien aus- und fortgebildet werden, um dann beim KI-Einsatz Patienten informieren und aufklären zu können. KI sollte den Zahnarzt in seinen Therapieentscheidungen unterstützen, aber die Autonomie des Zahnarztes in seiner therapeutischen Entscheidung muss gewahrt bleiben.;Yes;---;Yes;---;Yes, for all AI applications;;---;
F528877;31-05-2020 12:54;Portuguese;Academic/Research Institution;Pedro G;FERREIRA;;Faculty of Sciences University of Porto;;Large (250 or more);Portugal;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;;5 - Very important;3 - Neutral;3 - Neutral;;5 - Very important;4 - Important;5 - Very important;5 - Very important;4 - Important;;3 - Neutral;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;;No opinion;;No;;;;;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;4 - Important;3 - Neutral;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;;Yes, for specific AI applications;;;
F528876;31-05-2020 10:59;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;STRONG EFFORTS ON NEW ACADEMIC GRADES & STUDIES;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;4 - Important;4 - Important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;No opinion;5 - Very important;;There is a need for a new legislation;;No;;;;;3 - Neutral;5 - Very important;4 - Important;5 - Very important;;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Risks related to the loss of connectivity;;Yes;;Yes;;Yes, for all AI applications;;;
F528875;31-05-2020 09:30;English;Trade Union;Anca;Sipos;;Alma Mater FNS;;Large (250 or more);Romania;The feedback can be published with your personal information;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;Working with Trade Unions from Education;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;4 - Important;4 - Important;5 - Very important;;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;3 - Neutral;;Current legislation may have some gaps;;No;;;;;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);To protect the privacy of personal data.;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F528874;30-05-2020 17:35;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;;5 - Very important;4 - Important;No opinion;3 - Neutral;5 - Very important;4 - Important;;5 - Very important;5 - Very important;4 - Important;;No opinion;No opinion;No opinion;No opinion;No opinion;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;In order to tackle these issues it is very important that human-AI collaboration (work) is managed in an ethical way. This means that we need to understand how human-AI collaboration affects how work processes are organized and what happens to decision making in these new work processes if we cannot explain the rational on which AI bases its decision making. This can adressed by including social science and organization and management research into the research interest of AI researc centres. ;There is a need for a new legislation;;No opinion;;;;;4 - Important;4 - Important;5 - Very important;4 - Important;;5 - Very important;No opinion;;No opinion;;No opinion;;;;;No opinion;;No opinion;;No opinion;;;
F528873;30-05-2020 16:46;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;;5 - Very important;5 - Very important;3 - Neutral;;3 - Neutral;4 - Important;3 - Neutral;5 - Very important;4 - Important;;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;;There is a need for a new legislation;;Other;New rules should be developed along a more differentiated scheme of risks like the one the German Data Ethics Commission has suggested: a “pyramid of criticalities” with five steps for adjusting – and reglementing the use of algorithmic systems!;;;the development and use of AI for military purposes (which needs to be addressed - despite the Whitepaper's decision to not do so) - second: The use of AI in biomedical engineering - healthcare-related;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;No opinion;;No opinion;;;
F528872;30-05-2020 14:53;English;EU Citizen;M;Koellner;;;;;Germany;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;There is a need for a new legislation;;Yes;;;;;;;;;;;Biometric identification systems should never be allowed in publicly accessible spaces;I am strongly opposed to indiscriminate mass surveillance. Measures such as facial recognition threaten our core democratic principles by creating a significant power difference in between the state and the surveilled citizen. The Chinese Social Credit system is a perfect example of how these technologies can be used to control the population in the most effective and malevolent way. The European Union must not adopt such measures under any circumstances!;;;;;;;;;;;;;;;
F528968;29-05-2020 23:44;English;Business Association;Marco;Fugaccia;;International Regulatory Strategy Group (IRSG);;Small (< 50 employees);United Kingdom;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;IRSG_DATA_WORKSTREAM_RESPONSE_TO_CONSULTATION_ON_WHITE_PAPER_ON_ARTIFICIAL_INTELLIGENCE.pdf
F528967;29-05-2020 21:08;English;Business Association;Iva;ZORKO;;The European Lotteries;;Micro (< 10 employees);Switzerland;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;EL_Submission_Data_Strategy___AI_Consultations__FINAL_.pdf
F528966;29-05-2020 21:07;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;;4 - Important;3 - Neutral;5 - Very important;Continue funding innovative research projects focusing on AI in different sectors.;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"It is important to ensure that the datasets which have been chosen for AI training, or the algorithms which form the basis for AI functioning can not be maliciously altered. Thus further research should be directed at data and record-keeping (e.g. accurate records regarding the data set used to train and test AI systems; documentation on the programming, processes, protection of conf information, allowing potentially problematic actions or decisions by AI systems to be traced back and verified.";Current legislation may have some gaps;;Yes;;Yes;;"Health and defence related.
";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No further guidelines or regulations are needed;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Cyber risks;;Yes;;No opinion;;No opinion;;;
F528965;29-05-2020 20:41;English;Business Association;Attilio;Zani;;Telecom Infra Project;;Large (250 or more);United Kingdom;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;The Commission should initiate a sector dialogue on Smart Cities to promote AI adoption. The urban environment presents multiple opportunities for the deployment of AI use cases, individually or alongside each other.;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;Putting measures in place to ensure the appropriate level of connectivity is available to facilitate the EU’s AI ambitions, including taking into consideration issues of diversification and a secure supply chain.;5 - Very important;4 - Important;5 - Very important;As connectivity will underpin many deployments of AI, the Commission should consider establishing an innovation hub specifically to consider AI in the telecoms sector. Many TIP members already participate in R&D Hubs that have produced promising outcomes which would be beneficial to share widely and systematically across the EU.;4 - Important;5 - Very important;4 - Important;5 - Very important;3 - Neutral;;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;;Other;New legislation should focus on identified gaps relating to the use of AI. There are existing security standards and legislation to ensure the resilience of telecoms infrastructure and connectivity, meaning AI legislation does not need to address connectivity considerations.;No opinion;;;;;5 - Very important;5 - Very important;4 - Important;5 - Very important;3 - Neutral;5 - Very important;No opinion;;Rather not;;No opinion;;;;;No opinion;;No opinion;;Yes, for all AI applications;;Adaptations to national rules should focus exclusively on the novel element, i.e. AI and not disrupt established legal frameworks for other industries or sectors.;The_Telecom_Infra_Project_AI_Consultation_Attilio_Zani.docx
F528964;29-05-2020 20:19;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;No opinion;No opinion;3 - Neutral;4 - Important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;Startups should also follow the existing regulatory and legal requirements and have to demonstrate their compliance with existing regulation as a prerequisite for public funding.;5 - Very important;5 - Very important;4 - Important;;3 - Neutral;3 - Neutral;3 - Neutral;4 - Important;No opinion;"Any such initiatives should also follow the existing regulatory and legal requirements and
have to demonstrate their compliance with existing regulation as a prerequisite for public funding.";4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;3 - Neutral;Please note that it is not “the AI” per se that could create harm but rather the specific context in which AI is integrated in a certain product or service and such products or services might potentially create harm in case of malfunctioning in the intended context: e.g. health/surgery or mobility.;Current legislation may have some gaps;;Yes;;Yes;;"AI use in medical and surgery context; mobility and air traffic; applications that may have an impact of the well-functioning of our democratic institutions. Overall we recommend to define “high-risk” by the intensity of potential harm to legal and societal values at stake by malfunctioning AI applications (i.e. life, bodily harm, elections, judicial judgements).";4 - Important;3 - Neutral;4 - Important;4 - Important;4 - Important;4 - Important;No opinion;;Much;We strongly recommend that a labelling system should only be a voluntary system for systems that may have a direct customer impact. It must be avoided that other authorities take such voluntary system as new standard for supervision of AI applications. Such voluntary labelling system should NOT go beyond existing regulation but might rather provide best practice guidance on how to comply with the existing regulation.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;The technical nature of self/-dep learning AI models implies that products might change their functioning over time. Therefore the Product Liability Directive should be amended to include ongoing monitoring and updating obligations to the developer and deployer of AI based products.;Yes;For high-risk applications a regular update/assessment might be introduced to secure trust in AI based applications and prevention of harm.;Yes;;Yes, for all AI applications;;"We believe that there should not be a differing level of protection for the same quality of damage depending on the initial qualification of the AI application as high-risk or normal risk.
Furthermore, the liability regime should be governed by a REGULATION to achieve legal certainty and level playing field across Europe and to secure harmonization in protection of citizens.";20200529_Response_to_the_EU_COM_AI_White_Paper_Consultation.pdf
F528963;29-05-2020 18:33;English;Company/Business organisation;Alix;guillemet;;SoftBank Robotics Europe;;Large (250 or more);France;The feedback can be published with your personal information;4 - Important;3 - Neutral;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;Vulgarisation of AI for general public;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;;4 - Important;5 - Very important;4 - Important;;5 - Very important;No opinion;4 - Important;5 - Very important;No opinion;;4 - Important;3 - Neutral;4 - Important;4 - Important;2 - Not important;4 - Important;;Other;Really depends on the use cases and Industries (critical systems or not) ;Yes;;Yes;;Power plant, military , healthcare, transportation;4 - Important;4 - Important;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Very much;;Other enforcement system;Regarding the risk level of the application, if the risk is high, option 4 (combination of ex-ante compliance and ex-post enforcement mechanisms);;Personal security risks;;Yes;;Yes;risk to loose the control of AI / AI's responsibility ;Yes, for specific AI applications;critical systems ;;
F528962;29-05-2020 17:53;English;Other;Herman;RUCIC;;Copyright for Creativity (C4C);342464912839-08;Micro (< 10 employees);Belgium;The feedback can be published with your personal information;;;;;;;The Paper neglects an important facet in AI systems’ training: TDM technologies’ role and the surrounding legislative copyright framework. This is crucial, as limitations to what can be freely mined risks amplifying AI’s bias and unfairness, and could negatively impact AI applications and projects’ usefulness. It must be ensured that EU AI actors obtain adequate access to in-copyright material without impediments at the legal and/or technical level. See more details in our supporting document.;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;Supporting_Document_to_C4C_s_Response_to_the_EC_Consultation_on_the_White_Paper_on_Artificial_Intelligence_-_A_European_Approach_-_final.pdf
F528961;29-05-2020 16:41;Croatian;;;;National;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;uklju?ivanje IT sektora u sve segmente društva;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;-;5 - Very important;5 - Very important;5 - Very important;-;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;-;4 - Important;4 - Important;4 - Important;4 - Important;No opinion;3 - Neutral;-;There is a need for a new legislation;;No;;;;UI primjena u podru?ju kaznenog prava i kaznenih postupaka;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;Zbog vrlo osjetljive materije i podru?ja prvo bi trebalo to pitanje riješiti na EU razini, pa tek onda primjenjivati na nacionalnoj razini da se izbjegnu zlouporabe od strane nekih država.;Much;-;A combination of ex-ante compliance and ex-post enforcement mechanisms;;-;Personal security risks;-;Yes;-;Yes;trenutna direktiva ne obuhva?a se mogu?e rizike i štete koje bi mogle nastati;Yes, for all AI applications;;Odgovor je za sva rješenja UI kako bi se postigla potpuna transparentnost;
F528960;29-05-2020 16:39;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;engage national ecosystems especially in non-traditional non-ICT sectors;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;;2 - Not important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;4 - Important;;Current legislation may have some gaps;;No;;;;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);;Not at all;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;Yes;;Yes, for all AI applications;;;
F528959;29-05-2020 16:32;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;4 - Important;5 - Very important;5 - Very important;;4 - Important;While all six headline actions could contribute to building an 'ecosystem of excellence' in Europe, the most pressing actions are to raise skills and support SMEs. Encouraging SMEs to adopt AI will not only improve productivity of SMEs but drive innovation through demand. A consistent approach across Member States will encourage growth and innovation.;3 - Neutral;3 - Neutral;5 - Very important;3 - Neutral;5 - Very important;4 - Important;Building up the uptake of AI will help drive innovation, while increasing productivity, so it is important to align national policies. Building up the European data space is also important. To work most effectively, it should be open, accessible and non-discriminatory against companies outside of EU.;3 - Neutral;3 - Neutral;4 - Important;To foster investment and innovation in AI, it will be important to clarify patent ownership and copyright in AI created works. TikTok has an AI Lab in Europe and would welcome the opportunity to collaborate on European public-private partnerships.;5 - Very important;;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;AI is like any technology: it has limitations & problems in its implementation. However AI systems should seek to be reliable and robust, meaning that they should function correctly in the presence of invalid inputs or stressful environmental conditions. They should avoid flaws that make them vulnerable to misuse & aim to be able to withstand external attack. AI systems should strive to promote human wellbeing, agency & creativity, rather than exploit human vulnerabilities or supporting crime.;Current legislation may have some gaps;;Yes;;Yes;;Yes but greater clarity is needed on the definitions of AI. High-risk uses of AI should face a different level of regulation but high-risk needs to be carefully defined. From a public policy perspective, it makes sense to focus high-risk regulation on significant and life-changing uses of AI in the health, judicial, or law enforcement sector, rather than trying to regulate all uses of AI in the same way. ;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;;No opinion;;Much;"There needs to be flexibility within any voluntary system so that companies can design their own system in an effective way.
";No opinion;;;Personal security risks;;No opinion;;No opinion;;No opinion;;;
F528958;29-05-2020 16:19;French;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;No opinion;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;;No opinion;No opinion;No opinion;No opinion;No opinion;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;Yes;;Yes;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;Yes;;Yes, for all AI applications;;;
F528957;29-05-2020 16:13;Italian;Other;Raffaella;Scelzi;;Global Professionals for Artificial Intelligence;In itinere;Medium (< 250 employees);Italy;The feedback can be published with your personal information;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;n assenza di un quadro normativo europeo per lo sviluppo e la diffusione della IA è necessario stimolare maggiormente la fiducia dei consumatori e degli utenti per la conoscenza e l’utilizzo delle nuove tecnologie. Punto di partenza è l’attuazione e la promozione di una alfabetizzazione senza discriminazione dove siano coinvolte utenze di ogni età e ceto sociale. Implementare una piattaforma digitale per un confronto su progetti, inerenti le politiche europee sull’AI.;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;Sostenere l’eccellenza nella ricerca in AI presuppone la creazione di Master, Dottorati e moduli interdisciplinari tra settori combinati con l’IA. Promozione della digital literacy con il sostegno di professionisti TIC, in tema di automazione, robotica, intelligenza artificiale. Affidabilità e sicurezza delle banche dati mediante procedure di interoperabilità e governance sui dati e archivi. Previsione di procedure semplificate per sicurezza, privacy, responsabilità per fasi e settori (giustizia;4 - Important;5 - Very important;5 - Very important;Potenziare e mappare i centri di competenza o centri specialistici come RECAS in Italia e incentivare reti di poli e cluster dell’innovazione digitale, pubbliche e private, al fine di condividere a livello europeo migliori prassi e sinergie sulla ricerca scientifica ;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;"Incentivare startup e PMI nel settore IA con finanziamenti a livello europeo e dei singoli Stati membri; incentivare la collaborazione pubblico-privato.
Spronare gli Stati membri ad adottare politiche fiscali favorevoli per chi investe in questo settore (cd. fiscalità promozionale) e per detassare startup e PMI impegnate nell’AI.
";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"Mancanza di trasparenza per identificare e dimostrare violazioni di leggi, legislazione sulla sicurezza dell’UE applicata ai prodotti e non ai servizi; integrazione della IA nelle modifiche di funzionamento dei prodotti non presenti al momento dell’immissione sul mercato del sistema; attribuzione di responsabilità ai diversi operatori; minacce informatiche, rischi per la sicurezza personale, per la perdita di connettività, di sovrasorveglianza e limitazioni delle libertà, pericoli di imparzialit";There is a need for a new legislation;;Other;Occorre una legislazione che si occupi dell’utilizzo della IA nei vari settori, con il bilanciamento delle garanzie costituzionali interne ad ogni stato membro.;;;Identificazione biometrica remota, sorveglianza umana, mancanza di un sistema operativo in ambito di giustizia che possa rispettare i criteri di sicurezza, robustezza e precisione che possano prevenire ed evitare eventuali interferenze e furto dei dati da parte di sistemi informatici sovranazionali più evoluti. Inefficacia del sistema Microsoft teams nell’ambito della Giustizia. ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;Perché oltre alla violazione della privacy comportano una sorveglianza indiscriminata sulle persone senza preventiva autorizzazione motivata dall’autorità giudiziaria. ;Very much;Attraverso un’informazione non corretta, ed un’adeguata alfabetizzazione sui rischi ed eventuali benefici, la volontarietà è compromessa dalla persuasione.;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;"La combinazione dei meccanismi di valutazione ex ante ed ex post deve essere rinviata al periodo post emergenza. Successivamente i meccanismi di valutazione ex ante ed ex post dovranno essere sottoposti al sindacato di legittimità della Corte Costituzionale con un monitoraggio continuo delle garanzie procedurali anche mediante la previsione di 
un Garante dei rapporti tra IA e cittadini
";Mental health risks;Si. La finalità cognitiva del processo penale, imparzialità delle decisioni, pubblicità dei processi decisionali, rischi di furto di dati, rischi sulle libertà. Ricadute psicologiche e sociali dovuti all'uso costante della interconnessione alla Rete. ;Yes;Monitoraggio a fasi temporali sui prodotti che mutano  durante il loro ciclo di vita poichè si dovrebbero prevedere nuove figure giuridiche di responsabilità. ;Yes;Cfr. file allegato;Yes, for all AI applications;;Cfr. file allegato;COMMENTO_AL_LIBRO_BIANCO_SULL.docx
F528956;29-05-2020 15:52;English;Other;Fabienne;ECKERT;;European Council of Optometry and Optics (ECOO);03999415319-19;Micro (< 10 employees);Switzerland;The feedback can be published with your personal information;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;No opinion;"Additional action to consider: Working with professional bodies to analyse the challenges and impact on different professions.
";5 - Very important;No opinion;4 - Important;3 - Neutral;4 - Important;No opinion;;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;4 - Important;5 - Very important;No opinion;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"While machine-learning is increasingly being used to identify features of eye disease, it is important to highlight AI systems play a complementary role, and eye care professionals / clinicians are key in the decision-making process and recommending how patients should be referred for care. 
The use of AI in health care raises questions about patient safety and professional liability. The design of AI systems needs to go through proper clinical trials and regulatory approvals. ";Current legislation may have some gaps;;Yes;;Other;Vision is not considered high risk but visual impairment hugely impacts people’s lives.;Healthcare, early diagnosis and prevention of health conditions. ;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;No opinion;This answer relates to healthcare, which is usually regulated.;No opinion;;Other enforcement system;Clinical trials, regulatory approvals;;Personal security risks;Healthcare practitioners liability risk / patient safety;Yes;;No opinion;;Yes, for specific AI applications;Healthcare AI applications;;ECOO_contribution_to_the_European_Commission_consultation_on_Artificial_Intelligence.pdf
F528955;29-05-2020 15:03;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;2 - Not important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;1 - Not important at all;1 - Not important at all;1 - Not important at all;3 - Neutral;4 - Important;4 - Important;;Current legislation may have some gaps;;No opinion;;;;;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;No further guidelines or regulations are needed;Sigurnost je daleko važnija od privatnosti. Tako?e, te tehnologije mogu unaprediti zdravstvenu zaštitu kod hitnih slu?ajeva, ubrzati adekvatnu reakciju nadležnih organa, itd. ;No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Risks related to the loss of connectivity;;No opinion;;No opinion;;Yes, for all AI applications;;;
F528954;29-05-2020 14:14;English;Business Association;William;Moreau;;European Association of Automotive Suppliers (CLEPA);91408765797-03;Small (< 50 employees);Belgium;The feedback can be published with your personal information;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;CLEPA supports the development of an ecosystem of excellence on AI in the EU. However, we stress the importance of the uptake of research by the market. It is essential to ensure that research incentives are focused towards research projects that have a likelihood of industry uptake. A positive attitude on AI uptake by the industry will be necessary if the EU hopes to catch up with China or the United States.;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;Testing and experimentation sites are very important for the development of AI in the automotive sector, in particular if such testing sites are to be involved in the type approval of vehicles.;4 - Important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;5 - Very important;3 - Neutral;3 - Neutral;4 - Important;5 - Very important;4 - Important;Regarding concerns #4 and #5: decisions made by AI (based on machine learning algorithms) cannot necessarily be explained, assumptions can only be made on the training data. This lack of predictability heightens the need to set clear rules on liability in case of damage caused by an AI.;Current legislation may have some gaps;;Other;We agree with the proposed approach to determine high-risk AI. However, we would like to see some clarification on the transport sector. Due to the risk of death, injury, death, or material damage, most applications in this sector should be considered high-risk. However, the upcoming legislation should be clear on the fact that some applications used in the sector do not represent a high risk (e.g. in-vehicle infotainment systems), and propose a unambiguous definition of high risk in automotive.;;;;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Other special requirements in addition to those mentioned in the question above should be imposed (please specify);A clarification of what is considered a “public space” would be needed here. Facial recognition applications may be used by vehicles (e.g. to unlock the car, to monitor driver awareness/drowsiness, etc.). In which cases might a vehicle be considered a public space? For instance, private vehicles, company cars, carpooling, taxis, robo-taxis may present different perspectives.;Rather not;While a voluntary labelling system could, in principle, be a useful addition for low-risk applications, we find it difficult to support such a system without knowing how it would be implemented practically. AI applications are often incorporated into other products. Will the labels apply for AI algorithms, for products, for companies? How will KPIs be defined? We need more clarity on how such a system would work.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;The automotive sector is already subject to strict ex-ante conformity controls (type-approval – TA). We believe that AI-related requirements for our sector should be included into that existing framework, rather than as a new set of requirements controlled separately. It is important that any mandatory requirements in the legislative follow-up to the White Paper are harmonised with TA requirements and that certification testing and market surveillance are not duplicated.;Risks related to the loss of connectivity;Please see the liability section in the additional written contribution submitted to this consultation.;Yes;This is in line wth the recent UN reg. on sftware updates and to the TA regs (if important chnges are made to the product, conformity with technl requiremnts must be demonstrated via an approval extension or a new approval). In-service vhcles should not be subject to changes unless proven (by mrkt surveillance, customer complaints, in-service checks) that there are defects or safety risks for which the safety recall procedure shall apply (NB: such recall is already in the current TA frmwrk reg.);Yes;The current liability legislation doesn’t need to be fundamentally changed for AI. The PLD provides a sound legal basis to address consumer protection. Any revision should be assessed carefully. The review should focus on the extent AI applications are addressed by the current framework. We support clarification/broadening of the term “product” as used in the PLD to allow for product liability claims if any relevant automotive product or did not comply with justified safety expectations.;No;;CLEPA would favour liability and compensation rules being harmonised at the European level, as much as possible, rather than having different rules for each Member State. We would like to avoid a national fragmentation of the internal market.;AI_TF_consultation_final_reply_26.05.2020.pdf
F528953;29-05-2020 12:02;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;4 - Important;5 - Very important;3 - Neutral;4 - Important;4 - Important;Build a comprehensive open data repository  with an easy access that gathers scientific information.;5 - Very important;5 - Very important;4 - Important;3 - Neutral;4 - Important;5 - Very important;;4 - Important;4 - Important;5 - Very important;;4 - Important;4 - Important;;5 - Very important;3 - Neutral;;4 - Important;4 - Important;5 - Very important;3 - Neutral;4 - Important;4 - Important;Central concern: It needs to be ensured that misuse of AI is not done by accident like due to lack of skills. Currently there is a gold rush with regards to AI but this carries the great threat of taking shortcuts that should not be taken.;There is a need for a new legislation;;Yes;;Yes;;;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;4 - Important;No opinion;;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;High risk AI products should have mandatory maintenance intervals where it is verified that they still run under safe conditions and latest updates. It could even be necessary to have a continouus monitoring of AI used by a customer to be able to take care of its safety.;No opinion;;No opinion;;;
F528952;29-05-2020 11:48;English;Academic/Research Institution;Armand;NACHEF;;Commissariat à l'énergie atomique et aux énergies alternatives (CEA);52774696782-43;Large (250 or more);France;The feedback can be published with your personal information;5 - Very important;5 - Very important;3 - Neutral;4 - Important;5 - Very important;5 - Very important;"Developing cooperation with regulatory and standardization bodies in various sectors where AI can be used (e.g. vehicle regulation, traffic regulation, financial regulation, occupational safety and health, cyber-security regulation).
Note that “Focusing the efforts on the R&I community” is critical (more than very important) because in the current context companies are making less profit and are less investing in R&D. Thus, the support of the EU and the Member States to trusted AI R&D is vital.";5 - Very important;5 - Very important;3 - Neutral;4 - Important;4 - Important;4 - Important;"Promoting the uptake of AI by business and the public sector is important. Nevertheless, publicity has been made for the promotion of AI. It is now necessary to direct efforts and therefore the budget towards the design and development of solutions.
Also, it is important to point out that build up the European data space is very important for global issues like health, climate and safety but much less important for industrial applications because manufacturers don't actually share their data.";5 - Very important;5 - Very important;5 - Very important;"The lighthouse research centre is very important if it is a hub relying on RTOs or a funding agency. It is not important if it is a new RTO to be created from scratch (text is not clear).
The network of existing AI research excellence centres must have the obligation to communicate with RTOs and networks of industrial partners.
It is very important to treat AI as a separate field while setting up a PPP for industrial research. Tooling R&D is necessary to develop sovereign trustworthy AI.";4 - Important;5 - Very important;4 - Important;5 - Very important;3 - Neutral;"Evangelism has already been done to raise SME’s awareness about AI benefits. It is now necessary to let them have access to technologies and use case data.
Support partnerships between SMEs, larger enterprises and academia around AI projects is very important if academia include RTOs in order to do applied research (not only a pure academic research).
It is important or even a priority to finance SMEs, particularly with ERDF-type tools or hubs.";5 - Very important;4 - Important;4 - Important;4 - Important;3 - Neutral;4 - Important;"AI must be robust, reliable, and accurate in addition to safe.
All digital techniques may lead to discriminatory outcomes which rather derive from the use cases.
We should not mix up safety with explicability.
Concerning compensation, the real subject is related to the responsibility and accountability. AI itself is not responsible. The manufacturers or operators/users are the accountable.";Current legislation may have some gaps;;Other;"“HIGH-RISK” AI applications should not be binary; there must be a gradation of risks as in all sectors with a safety or security aspect. 
Different rules adapted to the risk level of the applications are needed. Safety levels like those in the IEC 61508 standard are also needed.";;;AI in real time cyber physical systems, autonomous systems, decision systems (with and without human verification) because human beings tend to over-trust the machine suggestions, in sensitive sectors like health or justice sectors.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;Biometrics for security and privacy-related questions is a subject that falls within the competence of the security, and SSH professionals, the States, and the Commission.;Not at all;The concept of labelling may lead to dangerous issues Labelling must always be carried out against a charter verifiable by a third party. Self-labelling involves too much risk with regard to its verification, in particular vis-à-vis non-European players subject to other legal and commercial rules.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;"The white paper does not talk at all about the “System” vision and the need/obligation to define system-engineering processes for AI-based systems (and not only for machine learning).
The use of system engineering processes should be mandatory.
Auto validation is a very ambitious feature but necessary as it corresponds to a real need. A system needs to be able to validate itself during its lifetime. The validation software must be third-party certified.";Cyber risks;Cyber risks: if data is not controlled and protected by the manufacturers or operators/users.;Yes;We need to do more R&D in this area in order to conceive and create continuous risk assessment processes. Changes in the system may occur several times a month, without a continuous risk assessment, the safety requirements will be too light.;Yes;Yes if the accountability remains that of the manufacturers and operators.;No;;AI applications are to be considered as software applications where the manufacturers or operators are accountable.;CEA_text_uploaded_on_EUSurvey_-_AI_white_paper.pdf
F528951;29-05-2020 11:22;English;Academic/Research Institution;Anniina;Jaako;;University of Oulu;524144227097-75;Large (250 or more);Finland;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;;3 - Neutral;4 - Important;5 - Very important;;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;Real regional coverage in remoted and rural areas;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;Misuse of AI;Current legislation may have some gaps;;No;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Risks related to the loss of connectivity;;Yes;;Yes;;Yes, for specific AI applications;Health applications;;
F528950;29-05-2020 10:34;German;Business Association;Lucie;Petersen;;Verband deutscher Verkehrsunternehmen e.V. (VDV);502542921140-86;Medium (< 250 employees);Germany;The feedback can be published with your personal information;5 - Very important;5 - Very important;4 - Important;2 - Not important;3 - Neutral;2 - Not important;;5 - Very important;3 - Neutral;3 - Neutral;4 - Important;5 - Very important;5 - Very important;;3 - Neutral;5 - Very important;4 - Important;Förderungen von Forschungseinrichtungen und Systemarchitekturen, Konzepten, Standards;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;5 - Very important;;4 - Important;4 - Important;4 - Important;3 - Neutral;3 - Neutral;3 - Neutral;Verarbeitung von personenbezogenen Daten, wie z.B. Gesundheitsdaten, Auswertungen digitaler persönlicher Profile;There is a need for a new legislation;;Yes;;Yes;;Verarbeitung von personenbezogenen Daten, wie z.B. Gesundheitsdaten, Auswertungen digitaler persönlicher Profile;3 - Neutral;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;No opinion;;Much;;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;;Cyber risks;;Yes;;No opinion;;Yes, for specific AI applications;;;
F528949;28-05-2020 22:04;Swedish;Public authority;Gordon;HAHN;Regional;Region Örebro Län;;Large (250 or more);Sweden;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;Vi anser att den offentliga och privata sektorns användning och tillämpning av AI som även tydliggör nyttiggörande av forskningen är viktigt. ;4 - Important;5 - Very important;4 - Important;Vi anser att det är viktigt att främja och utveckla den spetskompetens som redan finns, bland annat genom nätverk av befintliga spetsforskningsenheter inom AI i Europa. Vi ser Programmet för ett digitalt Europa och specifikt initiativet Europeiska digital innovationshubbar (EDIH) som bra exempel på detta och efterfrågar en tydligare samordning mellan de planerade insatserna inom EDIH med insatser som sker på regional och nationell nivå.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;Större företag är mycket viktigt att inkludera i tillämpningen av AI, men de har idag mycket svårt att prioritera detta utan finansiella incitament.;4 - Important;4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;Vi uppmanar Kommissionen att fortsätta det redan pågående arbetet med att stödja de etiska principerna i utvecklandet av AI, genom att bland annat vidareutveckla de metoder och verktyg för etisk garant som redan arbetats fram gällande utveckling och tillämpning av AI.;Current legislation may have some gaps;;Yes;;Yes;;Region Örebro län anser att störst behov av dialog runt AI finns i det etiska, där vår uppfattning är att EU gärna ser sig i framkant. Det handlar bland annat om prestationsmediciner och försvarsutveckling men även övervakning och ägande av sin personliga data. ;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;Vi anser att det är nödvändigt att säkerställa mellanstatliga överenskommelser och skapa gemensamma riktlinjer samt en gemensam EU standard för tillämpning av detta.;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Risks related to the loss of connectivity;Då AI är under ständig utveckling anser vi att det är viktigt att regelverket för säkerhet bör beakta nya riskbedömningsförfaranden för produkter som genomgår viktiga förändringar samt att det befintliga EU-regelverket uppdateras. ;Yes;Då AI är under ständig utveckling anser vi att det är viktigt att regelverket för säkerhet bör beakta nya riskbedömningsförfaranden för produkter som genomgår viktiga förändringar samt att det befintliga EU-regelverket uppdateras.;Yes;;Yes, for specific AI applications;;;EUs_vitbok_om_artificiell_intelligens.pdf
F528948;28-05-2020 19:48;English;Academic/Research Institution;Tony;Czarnecki;;Sustensis;Suste1518036132;Micro (< 10 employees);United Kingdom;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;4 - Important;4 - Important;;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;;5 - Very important;5 - Very important;4 - Important;;4 - Important;3 - Neutral;3 - Neutral;4 - Important;2 - Not important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;;There is a need for a new legislation;;Yes;;Yes;;;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;No further guidelines or regulations are needed;;Very much;;Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;A_Proposal_for_a_Global_AI_Governance_Agency_and_AI_Maturing_Framework.pdf
F528947;28-05-2020 17:51;Spanish;EU Citizen;Beatriz;Alegre Villarroya;;;;;Spain;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;;4 - Important;5 - Very important;No opinion;;4 - Important;No opinion;5 - Very important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;La IA puede generar obras literarias y artísticas, lo que afecta directamente a los derechos de Propiedad Intelectual e Industrial. Debe determinarse si existe una autoría y a quién corresponde la misma. En mi opinión personal, debería reconocerse como autor al desarrollador de la IA. Los riesgos mencionados y este último se desarrollan en mayor profundidad en la propuesta adjunta (pp. 9-33).;Other;Debe combinarse la adaptación de legislación vigente, en el caso de que existan lagunas jurídicas, con la elaboración de nueva normativa. En concreto, además de las Directrices Éticas sobre IA, conviene elaborar el Reglamento sobre responsabilidad en materia de IA. La explicación en mayor detalle de qué aspectos adaptar y cuáles recoger en nuevas normas se desarrolla en la propuesta adjunta (pp. 25-42).;Other;Hay medidas de prevención y control que deben aplicarse de forma obligatoria a todos los sistemas de IA, si bien pueden existir diferentes grados de exigencia o medidas distintas. En cuanto a la clasificación dual de sistemas en alto o bajo riesgo, deben aclararse los criterios de categorización y considerarse la división en más (sub)grupos para una mejor adaptación. Es importante determinar el carácter cerrado o abierto del anexo y su efecto retroactivo. Consultar propuesta adjunta (pp. 6-9).;;;La IA con mayor riesgo es la aplicada en el ámbito sanitario y de transporte, ya que afecta al derecho a la vida y derecho a la salud. En concreto, el uso de la IA con fines médicos y los vehículos de conducción autónoma. La posible discriminación causada por IA es también una cuestión relevante, aunque en menor medida que los anteriores. ;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);En la línea del Reglamento de Protección de Datos, el uso de estos sistemas debería estar permitido si la finalidad de su uso responde a circunstancias concretas previstas legalmente. Por ejemplo, en caso de que sea necesario para cualquier reclamo legal, por razones de interés público en el área de la salud o, en todo caso, si el consentimiento ha sido dado explícitamente.;Much;El etiquetado debería establecer para cualquier sistema de IA, si bien podría optarse por un etiquetado o método de certificación obligatorio para los sistemas de alto riesgo y voluntario para los sistemas de bajo riesgo (p. 17 de la propuesta adjunta).;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;Riesgos para la salud física, riesgos medioambientales;Yes;Los procedimientos de evaluación de riesgo deben medir de forma efectiva la probabilidad de que la IA cause un daño. Para ello, es fundamental que se establezca un protocolo de evaluación del nivel de incertidumbre y el margen de error del sistema, aspectos básicos para la seguridad y robustez técnica del mismo. Consultar la propuesta adjunta (pp. 12-17).;Yes;Es necesaria una adaptación de las definiciones de defecto, producto defectuoso y productor o fabricante, así como una inclusión de otros sujetos como el implementador o el proveedor de datos. El régimen de responsabilidad, en la línea del proyecto del Reglamento, requiere de algunas particularidades como el carácter objetivo de la responsabilidad o la inversión de la carga de la prueba. Consultar propuesta adjunta (pp. 30-33).;Yes, for all AI applications;;La elaboración de un Reglamento de Responsabilidad permite que la adaptación de las normas nacionales de los Estados miembros sea más uniforme. Este Reglamento debe establecer las medidas básicas de prevención, control, corrección y responsabilidad por incumplimiento de las obligaciones que se hayan establecido. Es conveniente que las medidas puedan graduarse en función del riesgo o incluso que sean distintas. Consultar propuesta adjunta (pp. 34-43).;Beatriz_Alegre_Villarroya_Propuesta_Libro_Blanco_UE.pdf
F528946;28-05-2020 17:39;English;;;;;;;;;The feedback can be published in an anonymous way;3 - Neutral;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Taking into account the AI frameworks of third countries. EU is not the only one working on AI. Several other countries and important economies (US, UK, Singapore) are active or have a functional framework in place. International cooperation will benefit EU and Global businesses to be more competitive and ultimately enable the EU to export its model.;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;Guidance on how existing EU legislation will be applied and interpreted to facilitate the development of AI is crucial. The deployment of AI does not happen in a policy vacuum. Data protection legislation is particularly impactful in this space and is linked also with the availability of data. In addition the industry incentives for the European data space to materialize and for companies to open up their data sets will be critical because in some use cases data sharing is counterintuitive. ;5 - Very important;5 - Very important;4 - Important;Supporting research will be critical. Considering that AI development happens cross multiple geographies it is crucial that European and global companies are allowed to participate in such efforts through partnerships. EU companies would benefit from the availability of non-EU data and international expertise. EU would attract further investment and would enable the creation of AI that meets EU values and standards.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"Supporting SMEs in AI development is strategic. The SMEs are agile and able to identify and develop or adopt innovative AI solutions that meet market needs. At the same time SMEs are constrained by their size. They are unable to shoulder a heavy governance burden. SMEs need a large market to scale. Therefore apart from supporting R&D and innovation EU needs a framework that is geared towards ""manageable"" AI governance for SMEs and broad access to EU and non-EU markets to scale the AI offerings.";5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;The cybersecurity considerations and risks related to AI are not yet fully understood. AI is by definition probabilistic. Therefore a margin of error is probable. The question is what is an acceptable and tolerable margin and that will depend on the specifics. The question of risk as a result of autonomy and the level of autonomy as well as the physical or not impact of AI are of highest importance in the risk matrix.;Current legislation may have some gaps;;Yes;;Yes;;AI systems that are connected to physical systems and as a result can cause direct physical harm. The example of connected vehicles although becoming a cliche is only one of the many robotic systems that would be AI enabled and can cause direct physical damage. The highest risk area is an area of physical impact and high autonomy especially on a self-learning system. This is because autonomous self-learning results in a system that may behave differently as a result of self-learning. ;3 - Neutral;3 - Neutral;5 - Very important;4 - Important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);The approach taken in the EU white paper on Artificial Intelligence about the need to ensure such biometric systems meet minimum quality and governance requirements through certification is reasonable. This is an important technology that should be put to use when it achieves an adequate level of security/quality. Against a general prohibition that would result in very slow technological development.;Much;Cybersecurity act provides the necessary legal framework for such labelling scheme. It would be necessary due to the generic nature of certain attributes and the multitude of use cases that any labelling scheme is generic enough to fit the different use cases and features. It would also be critical that any labelling scheme is applicable at EU level otherwise it would not make sense for manufacturers and technology developers. Any EU regime should not disincentivize existing data sharing models ;Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;A system that enables companies to self-assess before putting in the market is probably the optimal system that enables a good level of security and speed to bring into the market especially if the assessment criteria such as voluntary certification mechanisms or standards are already developed. That would enable companies to innovate and put technologies in the market while pursuing formal certification within a given timeframe. ;Risks related to the loss of connectivity;Depending on the functionality of the AI the cyber risks or the connectivity risks can be very different or result into a physical risk. Personal security risk should not be limited to risk related to personal data but expand to physical security, personal safety and possibly also risk to material property. Other risks include the extraction of the AI model or of the personal data used to train it or the creation of large scale deep fakes using AI to manipulate certain behaviour.;Yes;"The definition of ""important changes"" is critical and linked to self-learning AI. A system will change in its lifetime but if it is an operating system for a single processor those changes may have little impact. If it is running integrated in a critical system it carries a different risk. One should take into account the risk of evolving AI and the risk of systems that have not been designed for a particular purpose yet they are used as part of a larger system design causing dependencies.";Yes;Autonomous decision making by an AI system that causes physical harm, personal safety risk or physical property destruction should be properly covered ;Yes, for specific AI applications;Autonomous and self learning systems deployed in high risk scenarios;Considering the scale of the EU market and the patchwork of national liabilities there is a clear need for harmonization in this area to provide legal clarity. It is important to understand the distinction between AI that is developed by AI as well as AI that is operating in certain parameters versus self-evolving. The evolution is often result of training that happen in environments the producer/provider does not control and cannot apply governance. ;Broadcom-AI-Paper-FINAL.pdf
F528945;28-05-2020 17:27;English;Non-EU Citizen;Susie;Alegre;;;;;United Kingdom;The feedback can be published with your personal information;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;;5 - Very important;4 - Important;3 - Neutral;4 - Important;4 - Important;3 - Neutral;;5 - Very important;5 - Very important;3 - Neutral;;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;In terms of fundamental rights, specific consideration needs to be given to freedom of thought (see submission attached);There is a need for a new legislation;;No;;;;AI that seeks to make inferences about our inner state, emotions etc. or AI used to manipulate emotional responses to provoke particular behaviours. These types of use of AI are high risk regardless of the context (e.g. political, health, consumer).;4 - Important;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;The way in which biometric identification systems are developing to make inferences about emotional states or personality is deeply concerning and should never be permitted.;No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;Risks to mental autonomy which do not necessarily amount to mental health risks;Yes;;Yes;;Yes, for all AI applications;;;Alegre_Response_to_EU_AI_White_Paper.docx
F528944;28-05-2020 17:02;German;Other;Theresa;Tisch;;Verband öffentlicher Wirtschaft und Gemeinwirtschaft (VÖWG);16441688074-96;Small (< 50 employees);Austria;The feedback can be published with your personal information;3 - Neutral;3 - Neutral;4 - Important;3 - Neutral;3 - Neutral;3 - Neutral;;4 - Important;5 - Very important;4 - Important;3 - Neutral;5 - Very important;4 - Important;;3 - Neutral;4 - Important;2 - Not important;;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;;4 - Important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;5 - Very important;;There is a need for a new legislation;;No opinion;;;;;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;Allgemeine__berlegungen_Wei_buch_KI.pdf
F528943;28-05-2020 16:23;English;Company/Business organisation;Sylwia;GIEPMANS-STEPIEN;;GOOGLE;03181945560-59;Large (250 or more);United States;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;The Commission has identified a good combination of actions to ensure the development of an ecosystem of excellence. Google supports the Commission’s intention to focus on uptake and deployment of AI by European citizens, businesses, researchers and the public sector. This is even more vital in the current context, since AI and digital technologies will be critical components of the economic recovery.;4 - Important;3 - Neutral;5 - Very important;4 - Important;5 - Very important;3 - Neutral;Supporting organisations of all sizes to develop the skills necessary to make responsible and effective use of AI is vital. In parallel, there is a similar educational challenge to equip those in the wider ecosystem who will play crucial supporting roles in guiding the uptake of AI. To support this latter challenge, Google is offering workshops for policymakers new to the field to learn the basics of AI and machine learning from expert practitioners.;5 - Very important;4 - Important;3 - Neutral;There is currently a fragmented landscape of AI research centres in Europe. Streamlining and strengthening coordination between them would boost synergies and partnership opportunities. The proposal to create a lighthouse centre for AI research in Europe is an excellent initiative that could help, although it is vital to strike the right balance in the degree of centralisation. A single institute spread across several locations could be a good model. ;5 - Very important;3 - Neutral;5 - Very important;4 - Important;3 - Neutral;N/A;4 - Important;4 - Important;5 - Very important;3 - Neutral;2 - Not important;3 - Neutral;Google is concerned that the opportunity cost of not using AI is not sufficiently reflected in policy debates. When considering the risks of AI, it is vital to acknowledge there are also flaws in existing (non AI) approaches. We should compare the risks of using AI systems against existing approaches. If an imperfect AI system were shown to perform more accurately than the status quo at a crucial life-saving task, it may be irresponsible to not use the AI system.;Current legislation may have some gaps;;Other;Conceptually, Google supports a risk-based approach to a new regulatory framework but it is important to ensure that any potential regulation is targeted at the right use cases, provides legal certainty and does not discourage the responsible development and diffusion of AI. The Commission must be clear in its risk assessments that it is taking into account the likelihood of harm and not just the severity of the harm, as well as a nuanced consideration of the opportunity cost of not using AI.;;;N/A;3 - Neutral;3 - Neutral;5 - Very important;3 - Neutral;4 - Important;3 - Neutral;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);"Ultimately it is up to governments to decide on particular approaches to further regulation of these technologies. However, some important factors that governments should consider include: whether these technologies are required for public security; if they have been pre-approved as being reasonable and proportionate use; and whether there is a practical way of achieving the same ends without the use of such sensitive data.";Rather not;A labelling system risks placing a significant burden on SMEs to comply. This would favour large players who can afford to meet the requirements whilst delivering minimal benefit to consumers. There needs to be broad agreement on standards before such a scheme could be feasible or helpful. Given the pace of change, any scheme would have to be very flexible to work as intended. Existing self-regulatory approaches such as Google’s AI principles should also be taken into account.;Other enforcement system;A standalone scheme for AI systems would duplicate review procedures that already govern many higher risk products. The best approach for high risk AI applications not already part of such reviews is ex-ante self assessment, coupled with ex-post enforcement mechanisms where problems are suspected. Care should be taken to ensure the self-certification process is not too onerous, especially in terms of documentation requirements, so as to not discourage innovation or put an undue burden on SMEs. ;In choosing an assessment regime, it is vital to be pragmatic to ensure it is not overly burdensome for application providers, and also practical for designated assessment bodies to deliver, taking into account the level of expertise (sectoral and AI specific) and resourcing required to implement in a timely fashion. It is also important to remember that there is no “one size-fits-all'' approach to compliance, and thought should be given to the context in which the AI technology is operating. ;Mental health risks;"The focus should be solely on areas where the unique properties of AI, IoT, or robotics heighten the risk to the physical and mental integrity of consumers (see Attachment for more details). Note too that the selections of risk (relating to cyber, connectivity, mental health) are not intended to imply that Google believes additional laws are needed; our position is that existing laws are often sufficient, but that it would be useful to provide greater legal clarity as to their interpretation. ";Yes;Carrying out a new risk assessment should only be required when there has been a significant change to the functionality of the product which is likely to materially alter its performance in testing or the safety disclosures made. Generic over-the-air updates (OTAs) such as security fixes, bug fixes, or simple improvements after placing a product on the market should not trigger a renewed risk assessment.;No;Globally, strict liability frameworks are reserved for abnormally hazardous situations as they remove any consideration of intent or negligence. Expanding the scope of the PLD to software and all AI applications would mean that anyone involved in making an AI system could be held liable for problems for which they had no control. Google would strongly advise against burdening AI system developers with such legal exposure, as it would stifle innovation and competition.;No;;The existing liability framework is solid and technology neutral, making it flexible enough to cover the challenges arising with emerging technologies. Changing such a foundational legal and societal framework should be done thoughtfully and only in response to significant and demonstrable shortcomings with the current legislative framework. A strict liability regime for software would stall innovation in Europe, stifling economic growth.;Google_s_submission_to_EC_AI_consultation__3_.pdf
F528942;28-05-2020 14:38;Slovak;Consumer Organisation;Lucia;Lorencikova;;Spolo?nos? ochrany spotrebite?ov, S.O.S. Poprad;665841134915-33;Micro (< 10 employees);Slovakia;The feedback can be published with your personal information;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;Dialógu s malými, ale aj strednými podnikmi, ale najmä dialógu na úrovni EÚ a štát zastúpený správnymi orgánmi na úrovni samosprávy a následnému zavádzaniu opatrení v umelej inteligencii musí nevyhnutne predchádza? vzdelávanie a edukácia týchto partnerov.;4 - Important;3 - Neutral;5 - Very important;5 - Very important;4 - Important;5 - Very important;;4 - Important;4 - Important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;;4 - Important;4 - Important;No opinion;4 - Important;4 - Important;4 - Important;;There is a need for a new legislation;;No;;;;Vysokoriziková bude vždy tá AI aplikácia, od ktorej bude napríklad závisie? ?alší osud konkrétneho jednotlivca (napríklad využitá v súdnych konaniach a pod.);5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;Nie je pod?a nás možné spätne odkontrolova? zákonnos? ich využitia pri?om riziko ich zneužitia je neprimerane vysoké, rovnako ako vysoká je miera zásahu do základných práv jednotlivca. Ú?el, ktorému by mali slúži? (napr. ochrana verejného majetku, ?i poriadku) sa dá ú?elne dosiahnu? aj už dostupnými prostriedkami.;Much;;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F528941;28-05-2020 13:52;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;5 - Very important;2 - Not important;3 - Neutral;;4 - Important;4 - Important;3 - Neutral;3 - Neutral;4 - Important;4 - Important;;5 - Very important;5 - Very important;3 - Neutral;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;Yes;;No;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;and as with GDPR, setting law is not enough. Enforcement of law is of utmost importance;Not at all;mandatory labelling system;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;involve civil society in the whole process (EDRI for example);Mental health risks;human rights risks;Yes;Yearly review by independ bodies (EDRI and others);Yes;;Yes, for all AI applications;;;
F528940;28-05-2020 11:16;English;Company/Business organisation;Jan;Wiesner;;ARD, Co-operative of Public Service Broadcasting Organisations in Germany;ARD-6774178922-55;Large (250 or more);Germany;The feedback can be published with your personal information;4 - Important;4 - Important;4 - Important;3 - Neutral;5 - Very important;4 - Important;Digital Literacy is important to help the public understand basic functions of algorithmic systems and to raise awareness on how AI must be construed to ensure ethical and trustworthy use in general and particular in the media sector. Public Service Media have a long tradition promoting digital literacy in their Programmes (esp. young audiences) as well as other actions and are commited to continue this work in the future. Additional EU programmes supporting Digital Literacy would be welcomed.;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;;4 - Important;4 - Important;4 - Important;"It is important to strengthen research which helps to better understand and explain the ""blackbox""-phenomenon created by certain AI applications in order to make such decision-making systems more transparent.
Platforms should be required to grant Public Service Media access to audience data (in line with GDPR)  regarding their content and services offered on such platforms in order to better fulfill their public service remit. This data should be provided in a standardised format.";4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;;4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;3 - Neutral;ARD uses AI to (better) fulfil its public service remit. We are bound to use AI in a transparent & ethical way, avoiding any harm to fundamental rights but rather strengthening freedom of expression, media pluralism etc. However, the same can not always be said when AI is used by internet platforms. On the contrary, this use can have a negative impact on the availability, access, findability and prominence of media content and thus negatively impact fundamental rights (please see attachment).;Current legislation may have some gaps;;Yes;;Yes;;"In general we do not consider the use of AI by the media sector as high-risk. However, AI applied by big internet players/platforms to (trusted and high-qualitiy) content by ""traditional"" media can have an important influence on the availability, access, findability and prominence of such content and thus negativly impact fundamental rights, especially the right to freedom of expression in all its aspects (please see attachment).";5 - Very important;4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;No opinion;;Much;Development of a voluntary labelling for no-high risk applications would ensure that these applications could be subject to mandatory requirements. A standardised label would further strengthen the ethical and trustworthy aspects of AI systems. Finally, it would ensure that no-high risk applications, which voluntary participate in the labeling scheme, are automatically under human oversight.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;There should be a mandatory self-reassessment in cases where products change during their lifetime, in combination with a requirement to inform customers about changes and results of that assessment.;No opinion;;No opinion;;;ARD_on_AI-Consultation_final.pdf
F528939;28-05-2020 10:39;English;Other;Abbe;BROWN;;British and Irish Law, Education and Technology Association.  Response approved by Executive, prepared by E Harbinja, J Griffin, A Charlesworth, B Schafer, M Leiser led by F Romero Moreno. Supported by A Murray, M Mapp;;Medium (< 250 employees);United Kingdom;The feedback can be published with your personal information;4 - Important;4 - Important;5 - Very important;4 - Important;3 - Neutral;4 - Important;The emphasis on international leadership, combined with the important collaboration between member states, nonetheless risks a “fortress mentality” that could prevent beneficial collaboration beyond Europe. Especially with regards to the global south, it is crucial that fair collaboration and sharing is maintained and fostered, and dangerous exploitative practices (data as new raw resource) prevented through legal frameworks.;4 - Important;5 - Very important;3 - Neutral;3 - Neutral;5 - Very important;5 - Very important;To facilitate a data space, data sharing and collaboration, an appropriate IP framework needs to be in place that does not unnecessarily create walled gardens, or prevents transformative reuse of data through excessive rent seeking by right holders. The sui generis database right ought to be empirically validated for its effectiveness/harm in fostering AI innovation. The “right to mine” should be extended beyond research contexts, and also encompass images, sound etc.;3 - Neutral;5 - Very important;4 - Important;"Currently, many AI applications rely on often invisible human labour (data cleaning, annotation etc) that is often undervalued, created under exploitative conditions or entirely unrewarded. For EU funded research, a fair funding deal that covers these costs, and adherence to good labour practices is needed. This should also be a condition of funding when working with private sector partners. 

";3 - Neutral;5 - Very important;4 - Important;5 - Very important;3 - Neutral;Many SMEs already lack the in-house expertise, or adequate access to cost-effective external expertise, in areas such as IP and DP, to permit them to effectively navigate these existing specialised regulatory frameworks.  The White Paper envisages an environment in which still wider regulatory know-how will be essential.  Broader education, skill training and support in accessing and applying specialised regulatory frameworks relevant to AI needs to be provided in ways accessible to SMEs.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Please see attached file;There is a need for a new legislation;;Yes;;Yes;;Please see attached file;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;"Please see attached file
";Much;Please see attached file;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Please see attached file;Mental health risks;The use of AI for predicative enforcement may clash with expectations of the law. AI might make decisions which clash with individuals’ perceptions of the law, e.g. predictive systems for copyright infringements. Individuals may alter their behaviour as a result, leading to a divergence between the AI system and the formal legal rules. AI can also pose significant challenges to aspects such as privacy rights, rights of communication and the sanctity of the home. Those must be protected.;Yes;Risk assessment should identify whether there are fundamental rights that are at risk and identify these rights during the design phase of the AI. There should also be a process to balance the rights that are at risk. It would also be beneficial to identify in the risk assessment how much control individuals will have over their own data, and how it will be ensured that the use of this data is communicated to those individuals. Future re-use of the information should also be considered. ;Yes;There are specific risks that should be covered. AI products, if sold to the public, should pass through 1) impact assessments during the design stage, and 2) ideally there should be public consultation to identify those risks before release to the public of the AI. The results of both stages should be required to be made public. The specifics of the process may depend on the industry, e.g. differences between AI used in autonomous cars, and AI in computer games.;Yes, for specific AI applications;See below;The rules should be made dependent on factors relating to risk, such as high, medium or law. The level of liability should be related to those risk factors. Risk mitigating mechanisms should be put in place by the provider of the AI. There should be mandated processes that organisations should follow when making or deploying AI. Failure to follow those should open up greater liability. ;BILETA_EU_IA_response_additional_information_May_2020.docx
F528938;28-05-2020 08:06;English;Public authority;Andrea;Kostadinovic;Regional;Västra Götalandsregionen;83211143480-97;Large (250 or more);Sweden;The feedback can be published with your personal information;4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;A prerequisite for AI is the access to data. It is the availability of high-quality data that enables cutting-edge expertise that can support the development and use of AI. Access to high quality data should therefore be included in the list of measures required. Furthermore, access to competence is a key factor in achieving excellence.;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;For AI to contribute to strengthened competitiveness and better welfare, certain conditions are needed. A common theme should be sustainable AI, which implies that AI applications should be ethical, safe, reliable and transparent. A prerequisite is the availability of high-quality data and available infrastructure.;3 - Neutral;5 - Very important;4 - Important;A variety of infrastructures are needed for the development and use of AI. Access to large amounts of data and powerful computers are needed to create algorithms. To foster excellence, it is important to use existing infrastructures at Regional, National and European level. Regions have long experience of gathering and supporting actors which collaborate in different ecosystems at regional level. These already existing forms of cooperation and ecosystems should be considered in future policies.;2 - Not important;5 - Very important;4 - Important;5 - Very important;3 - Neutral;In addition to testing and demonstration facilities, the development and application of AI requires the possibility of continuous validation of algorithms and other applications. The public sector has the potential to reap many benefits by the implementation of AI applications, which results in cost-effectiveness in the long run.  It is therefore crucial that the European Digital Innovation Hubs consider the needs of the public sector in the digital transition.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;In order to minimize the concerns about AI, it is important that the basis for the regulations is based on human equality. Sustainable AI should be the guiding theme, which means that AI applications should be ethical, safe, reliable and transparent. Ethical and safety considerations cannot be a simple reflection in AI applications, but must be an integrated part of the early design work. It is important with transparency towards the civil society and a constant dialogue about the aims and goals;Other;A broader digitalisation approach and a gap analysis of the current situation is needed. EU's Data Protection Regulation is a good example of a comprehensive legislation that ensures strong privacy, which also makes it relevant in an AI framework. Common policies and guidelines in the EU are needed for how data should be anonymized, pseudonymised and, where applicable regulated, such as in the use of synthetic data, as the pooling of different data sources can increase the risks of breaches of p;Other;"It is of utmost importance that all use and legislation regarding AI is based on human equality. All AI applications should be guided by a general clause ensuring this principle.
Another guiding principle should be the possibility to appeal against automated decisions, not least when it comes to applications in the public sector and automation of government decisions, regardless of whether the AI application is of high or low risk.

";;;The use of poor algorithms is a high-risk factor as it leads to distorted results and discrimination of different groups in the society. High quality data is characterized by data that includes gender equality and people's differences. Creating AI solutions requires well thought through solutions for everyone. It is with representative data that ethical principles can be upheld. Solutions that consider differences must be rewarded and not just solutions adapted for the majority.;5 - Very important;5 - Very important;2 - Not important;5 - Very important;3 - Neutral;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);It is of utmost importance that all use and legislation regarding AI is based on human equality and rights. All AI applications should be guided by a general clause ensuring this crucial principle. To enable the potential of AI such a framework must consider both fundamental rights such as privacy, ethics, trust and community protection and the necessary access to data.;Much;A voluntary labelling system can create greater clarity for both users and consumers. Guidelines and standards could guide both private and public stakeholders regarding the use of AI and could promote technical, semantic and legal interoperability. A specific EU labelling system that guarantees that the AI technology is based on human equality and rights would create added value for both users and creators through the affirmation that ethical considerations are an integral part of the technolog;Other enforcement system;;;Mental health risks;Partial data quality is a major problem that leads to discriminatory results and excludes groups in the society from the use of a specific technology. High-quality data is characterized by representative data that includes gender equality and people's differences. It is with representative data that ethical principles can be upheld. A prerequisite for realizing the benefits of AI in the society, is therefore an appropriate regulatory framework with principles, norms, standards and rules.;Yes;"It is important that it should not be the technology but the purpose and the events that are analysed. The assessment should preferably be technology agnostic. The rule of law should apply to all use of AI solutions. Another guiding principle should be the ability to appeal against automated decisions, not least when it comes to applications in the public sector and the automation of government decisions, regardless of whether the AI application is of high or low risk.
";Yes;The principle of liability must be guiding all use of AI solutions and applications. It must be clear where the responsibility lies and to whom the user or the consumer should turn to in case of problems. A revision of the Product Liability Directive may be required to clarify the situation.;Yes, for all AI applications;;The principle of liability must be guiding all use of AI solutions and applications. It must be clear where the responsibility lies and to whom the user or the consumer should turn to in case of problems.;Region_V_stra_G_talnd_s_position_paper_on_EU_s_white_paper_for_AI.pdf
F528937;27-05-2020 17:50;French;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;2 - Not important;4 - Important;5 - Very important;5 - Very important;3 - Neutral;;4 - Important;5 - Very important;3 - Neutral;;4 - Important;3 - Neutral;;4 - Important;5 - Very important;3 - Neutral;;3 - Neutral;3 - Neutral;5 - Very important;5 - Very important;3 - Neutral;;3 - Neutral;3 - Neutral;3 - Neutral;1 - Not important at all;1 - Not important at all;1 - Not important at all;;There is a need for a new legislation;;Yes;;No opinion;;;5 - Very important;4 - Important;5 - Very important;2 - Not important;3 - Neutral;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Cyber risks;;Yes;;;;No opinion;;;
F528936;27-05-2020 17:09;English;EU Citizen;Luís;PEREIRA;;;;;Portugal;The feedback can be published with your personal information;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;;No opinion;No opinion;No opinion;;No opinion;No opinion;No opinion;No opinion;No opinion;;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;;No opinion;;No opinion;;;;;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;;No opinion;;No opinion;;;Personal security risks;;No opinion;;No opinion;;No opinion;;;AIS_The_Carousel_of_Ethical_Machinery.pdf
F528935;27-05-2020 13:58;English;Academic/Research Institution;Freek;BOMHOF;;TNO (unit ICT);40524063921-20;Large (250 or more);Netherlands;The feedback can be published with your personal information;5 - Very important;5 - Very important;3 - Neutral;;5 - Very important;3 - Neutral;;5 - Very important;3 - Neutral;5 - Very important;3 - Neutral;3 - Neutral;5 - Very important;;3 - Neutral;4 - Important;5 - Very important;;3 - Neutral;3 - Neutral;4 - Important;5 - Very important;3 - Neutral;;4 - Important;4 - Important;;5 - Very important;3 - Neutral;4 - Important;Unproductive use of AI: both over-reliance of users on results of AI systems, and algorithm aversion of users who do not want anything that is related to AI.;No opinion;;No;;;;;5 - Very important;;4 - Important;5 - Very important;5 - Very important;3 - Neutral;Other special requirements in addition to those mentioned in the question above should be imposed (please specify);technological approached exist that may support system behaviour that is in line with all regulations (normative IT), so such systems should not be banned a priori;Much;it will raise awareness both with system manufacturers and users;A combination of ex-ante compliance and ex-post enforcement mechanisms;;One cannot ensure full compliance ex ante, yet some enforcement should be in place in order to avoid costly errors. A trusted party that audits AI applications may be a solution.;Mental health risks;Risks as a result of individuals taking actions (based on faulty AI systems) that are not a problem for the individual, but for the society as a whole. (Eg, hoarding behaviour by misinformed people early in the Covid-19 crisis.);Yes;;No opinion;;No opinion;;;
F528934;27-05-2020 11:32;English;Company/Business organisation;Sylvie;CALSACY;;Worldline;;Large (250 or more);France;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;For AI : guidance is required on interpretation and consequences of GDPR regulation;5 - Very important;1 - Not important at all;3 - Neutral;4 - Important;5 - Very important;4 - Important;"Regarding the test facilities, only the explaination attached to the results are essential and
must be public. About investing, not only in start up, even if agility and speed are positive.";3 - Neutral;4 - Important;4 - Important;"About the lighthouse research, we expect that the EU invests directly in the research 
centers already in place, not only in one entity. Besides effort should focus on sectors
for which EU sovereignty is at stake.";4 - Important;2 - Not important;4 - Important;3 - Neutral;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Regarding our opinion expressed in the table below, it is important for the regulator not to misunderstand: we are not sceptical about AI but the concerns cited are all very important from our point of view.;Other;"We identify a concern regarding the difference between model and algorithm.
Today rules focus on algorithm; on our side, rights attached to model are more crucial.";Yes;;Yes;;;5 - Very important;4 - Important;5 - Very important;2 - Not important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;"A special attention regarding security issues (CFT, crimes, ...). Also it seems important
that a set of rules be defined to fix an adapted framework accepted by all the parties.";Rather not;;Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;;Personal security risks;;Yes;;Yes;According to the latest publications by experts group, maybe to update the directive issued in 1985.;Yes, for all AI applications;;;
F528933;27-05-2020 10:50;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;4 - Important;5 - Very important;4 - Important;5 - Very important;4 - Important;The creation of a digital hub (not physical) of AI specialists who share their knowledge across industries is important. Cross-industry cooperation should take precedence over a silo-sector-approach. ;3 - Neutral;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;#NAME?;5 - Very important;4 - Important;4 - Important;"Lighthouse research centres have to be established in a digital way; only by that the connection of the ressources can be guaranteed in a fast manner. ";3 - Neutral;5 - Very important;4 - Important;4 - Important;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;4 - Important;3 - Neutral;All these mentioned topics are important, but it has to be kept in mind, that they are currently not treated equally important in the US. Therefore the research in the US might proceed faster. Maybe in Europe a sort of evolving process could be applied which allows AI outside of the ecosystem of trust at the beginning.;Current legislation may have some gaps;;Yes;;Yes;;;3 - Neutral;3 - Neutral;5 - Very important;5 - Very important;4 - Important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);Fair and equal conditions for the use of biometric identification systems are important. Since private companies are already using this technology, public companies should also be allowed to do so. Certainly, sensitive data has to be protected, but this is partly already defined in the GDPR.;Rather not;Not now, it might be useful later when AI is more established in the EU.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;No opinion;;No opinion;;Yes, for specific AI applications;;;
F528932;27-05-2020 09:15;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;No opinion;5 - Very important;5 - Very important;The development of AI capabilities in Europe requires a clear and flexible business and investment framework. That is why CECIMO stands for specific rules as to the adoption of AI tools and applications in European manufacturing, ensuring that these new means comply with the European Single Market principles and with free and fair trade conditions.;5 - Very important;5 - Very important;5 - Very important;No opinion;No opinion;4 - Important;Define clear rules concerning ownership of data in relation to the European Data Space;4 - Important;5 - Very important;5 - Very important;"Set up a right policy that enables and fosters progress in research while at the same time guaranteeing innovation on industrial AI.
A comprehensive European investment plan should be laid out by EU policymakers, focusing on the development of machine learning, AI applications and other digital areas in which European companies are global leaders. ";5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;3 - Neutral;3 - Neutral;4 - Important;2 - Not important;3 - Neutral;4 - Important;"• From European machine tool manufacturers, AI systems are not a product; AI is a broader technology that is integrated into their industrial products. Therefore, rules and responsibilities concerning the use of AI systems in manufacturing must be clearly specified.";Current legislation may have some gaps;;Yes;;No;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No further guidelines or regulations are needed;The biometric identification system is already covered under the EU GDPR (General Data Protection Regulation) and it is considered a “special category of personal data” that requires both a special legal basis for processing and an accompanying data protection impact assessment.;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;CE marking. Self certification;Risks related to the loss of connectivity;;No;;No;;No;;;
F528931;26-05-2020 20:45;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"1. Promote European programs, with relevant AI content, in critical domains such as security, leading to procurement of capabilities.
2. Building European public acceptance of AI.";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Financial support should not be limited to start-ups but extended also to SMEs;5 - Very important;5 - Very important;5 - Very important;For the security community, a lighthouse research centre would be highly beneficial if capable to provide a tangible value added by establishing a well-functioning partnership with industry.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;4 - Important;4 - Important;4 - Important;3 - Neutral;3 - Neutral;3 - Neutral;A very important concern that is often overlooked is the use that malevolent actors can do of AI-based technologies. AI can be used to spread disinformation by fabricating fake news or using deep fakes. Autonomous systems can be trained to conduct hostile targeted actions against citizens or infrastructures. AI can increase the speed and effectiveness of cyberattacks also by implementing targeted phishing strategies. Adversarial AI can be exploited to spoof biometric systems or scanning devices.;Current legislation may have some gaps;;;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);Even when legally permissible to use AI-enabled biometric identification in public spaces, the systems should respond to technical requirements ensuring the adherence to European security standards.;No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;While self-assessment compliance mechanisms are generally preferred, there can be areas or uses where external assessment procedures are necessary. However, strong preference is for agile mechanisms which do not introduce unnecessary delays and minimize costs and administrative burden for industry. Compliance evaluation should also look at trustworthiness from the angle of European technological autonomy. European preference may be, in fact, necessary for certain applications.;Cyber risks;;Yes;;No;;No opinion;;;EOS_Considerations_on_AI_White_Paper.pdf
F528930;26-05-2020 18:25;English;EU Citizen;Mika;Helenius;;;;;Finland;The feedback can be published with your personal information;5 - Very important;3 - Neutral;2 - Not important;2 - Not important;2 - Not important;1 - Not important at all;All AI is software. AI just one tiny math methdod. The real value is in systems and software engineering creating data, delivering quality and processing information to data used by AI. What EU need is software strategy and DG software. https://www.linkedin.com/pulse/cost-paranoid-optimist-ai-mika-helenius/;2 - Not important;2 - Not important;2 - Not important;1 - Not important at all;1 - Not important at all;1 - Not important at all;"There should be coordination on local European owned software industry development and growth. Investment to ""software engineering"" (Schools of Software) is needed to fix massive 400 000 enginering shortage influencing societies at all levels.  https://www.linkedin.com/pulse/software-advantage-mika-helenius/ ";2 - Not important;2 - Not important;2 - Not important;EU should vitalize competiviness with product and market based innovation with software not science based research funding. AI is just 3% of all software used bys societies and business. https://www.linkedin.com/pulse/vitalization-technological-competitiveness-mika-helenius/;2 - Not important;2 - Not important;2 - Not important;2 - Not important;2 - Not important;"Software engineering is today the strategic KET and general purpose technology. The DIH should be created sound ""software centers"" located in proximity of ""software engineering"" education unit ""Schools of Software"" or ""Software Science"" departments.  https://www.linkedin.com/pulse/path-sustaining-sovereignty-cyberspace-mika-helenius/";1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;It the the overall software systems and architectures that trust - not AI. https://www.linkedin.com/pulse/fading-wealth-nations-mika-helenius/;There is a need for a new legislation;;Yes;;No;;"The biggest risk is lack of EU policy on ""software engineering"" and ""software economy"". https://www.linkedin.com/pulse/dark-digital-wormhole-mika-helenius/";2 - Not important;5 - Very important;5 - Very important;2 - Not important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;Biometric identification should never be allowed. ;Not at all;;Other enforcement system;"Create DG Software similar to MITRE and CIO.gov., EITC (EU IT Council similar to ITIF) and new Software Engineering university network like in China. 
https://www.linkedin.com/pulse/european-software-agenda-mika-helenius/";Local competitiviness and own industry is key - NOT regulation which doesnt support owne industry. https://www.linkedin.com/pulse/fading-wealth-nations-mika-helenius/;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;2019.11.07_DIX_POSITION_PAPER_ON_EU_COMPETIVENESS_IN_DIGITAL_ECONOMY.pdf
F528929;26-05-2020 17:44;Hungarian;;;;National;;;;;The feedback can be published in an anonymous way;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;nincs;3 - Neutral;4 - Important;3 - Neutral;4 - Important;5 - Very important;4 - Important;nincs;4 - Important;5 - Very important;3 - Neutral;nincs;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;nincs;5 - Very important;3 - Neutral;3 - Neutral;2 - Not important;2 - Not important;2 - Not important;nincs;There is a need for a new legislation;;Yes;;Yes;;;5 - Very important;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;4 - Important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Very much;nincs;A combination of ex-ante compliance and ex-post enforcement mechanisms;;nincs;Risks related to the loss of connectivity;nincs;Yes;nincs;Yes;nincs;Yes, for all AI applications;;nincs;
F528928;26-05-2020 16:40;English;Academic/Research Institution;Guido;Noto La Diega;;University of Stirling;369394724693-87;Large (250 or more);United Kingdom;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;4 - Important;2 - Not important;4 - Important;Partnerships with the private sector can be dangerous, especially if opaque;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;3 - Neutral;;5 - Very important;4 - Important;2 - Not important;Public-private partnerships risk allowing big tech to dictate the agenda;4 - Important;4 - Important;5 - Very important;3 - Neutral;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;AI can manipulate consumers and the electorate, even when this does not lead to a breach of a fundamental right per se;Current legislation may have some gaps;;Other;They should apply to all AI systems: distinguishing between high risk, medium risk, etc. is arbitrary.;;;Lethal autonomous weapons;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;The concept of defect is mostly focused on defects that existed when the product was put into circulation and 'product' is often regarded as excluding software and service products. These are the most urgent amendments that would need to be introduced to make the Product Liability Directive fit for AI.;Yes, for all AI applications;;;
F528927;26-05-2020 16:36;Italian;Academic/Research Institution;Associazione;SISDiC;;Società Italiana degli Studiosi del Diritto Civile (SISDiC);279818538285-95;Micro (< 10 employees);Italy;The feedback can be published with your personal information;4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;promuovere forme di didattica universitaria integrata tra tecnologia e diritto;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;"ricerca integrata tra nuove tecnologie e scienze giuridiche; corsi universitari volti a formare un profilo di giurista con una preparazione interdisciplinare caratterizzata dall'acquisizione di competenze per la gestione delle problematiche che scaturiscono dalle nuove tecnologie";5 - Very important;5 - Very important;5 - Very important;"promuovere lo studio e la tutela dei diritti fondamentali; promuovere lo studio degli effetti giuridici delle norme etiche (etica applicata)";4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;contribuire alla formazione specializzata di nuove figure di giuristi;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;"Il digital divide; contrazione dei contenuti, diminuzione del livello culturale, tecnocrazia, danni alla salute";Other;occorrono indicazioni costitutive di un ordine pubblico di valenza internazionale che governi i rapporti sostanziali e i rimedi anche giurisdizionali;Other;il collaudo spesso non è possibile eseguirlo al completo per la complessità del sistema e il livello di rischio effettivo è inaffidabile;;;uso militare, controllo sociale, human enhancement, trattamento dati biometrici, trasferimento dati personali fuori dall'UE;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;Biometric identification systems should never be allowed in publicly accessible spaces;tali sistemi non sono affidabili e possono violare la tutela della privacy;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;affiancare alle competenze tecnologiche le competenze giuridiche. Lavorare in gruppi interdisciplinari;Personal security risks;"rischi ambientali e sanitari; rischi di contrazione delle libertà; rischi di diminuzione del livello culturale e di apprendimento";Yes;nella valutazione dei rischi occorre utilizzare il principio di precauzione ed evitare di mettere sul mercato prodotti i sistemi dei quali sono caratterizzati da black box;No;l'interprete può individuare nella disciplina esistente e nei principi normativi il regolamento specifico e più adeguato;Yes, for all AI applications;;l'adattamento si può realizzare attraverso l'attività interpretativa;SISDiC_-_Libro_bianco_IA.pdf
F528926;26-05-2020 16:33;French;NGO (Non-governmental organisation);CHRISTINE;MONTY;;OBSERVATOIRE EUROPEEN DE LA NON-DISCRIMINATION ET DES DROITS FONDAMENTAUX;910380716313-26;Micro (< 10 employees);France;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"Préconisation 1 : Plus que des rapprochements public/privé , favorisez la création de réseaux interdisciplinaires d'intelligence artificielle notamment par domaine d'application (exemple : médical /traitement du cancer).
Préconisation 2 : Veillez à ne pas mettre en place des systèmes bureaucratiques ""lourds"" en terme de délais et de critères d'obtention pour obtenir des financements afin de ne pas freiner l'innovation.
";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Préconisation :  Mettre en place des systèmes de scolarité gratuite pour des étudiants issus de milieux défavorisés qui souhaitent étudier dans le domaine de l'Intelligence Artificielle.;;;;"Recommendation : Etre en mesure de proposer aux enseignants chercheurs d'excellence en intelligence artificielle des salaires comparables à ceux de la Silicone Valley pour éviter une ""fuite des cerveaux"".";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;Other;Il faut mettre en place en amont une charte éthique européenne de l'Intelligence artificielle qui encadre la Recherche et l'innovation . ;Yes;;Yes;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Rather not;"Il est plus simple d'établir une Charte Ethique ""pré-conception"" artificielles applicables à toutes les applications d'intelligence artificielle (à haut risque ou non) que de créer un label supplémentaire. note complémentaire jointe";A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;cf note complémentaire - droit fondamental à l'intégrité et à la continuité psychologique ;Yes;Des missions d'audit d'évaluation externes et indépendantes devraient être mises en place. ;No opinion;;No opinion;;"D'un point de vue nationale, il faut veiller à ""faciliter"" les procédures juridiques permettant  aux personnes d'obtenir réparations des préjudices liés à l'Intelligence : coût, délais, détermination des responsabilité.";Consultation_on_white_paper_and_IA.pdf
F528925;26-05-2020 14:51;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;4 - Important;5 - Very important;3 - Neutral;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;4 - Important;4 - Important;4 - Important;3 - Neutral;3 - Neutral;5 - Very important;;There is a need for a new legislation;;Yes;;Yes;;;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Risks related to the loss of connectivity;;Yes;;Yes;;Yes, for all AI applications;;;
F528924;26-05-2020 13:54;English;Company/Business organisation;Aleix;Solé Sánchez;;Formal Vindications SL;;Micro (< 10 employees);Spain;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;When considering the training of personnel dedicated to AI applications, taking into account interdisciplinarity is essential, since many of the decisions that can be made by automated processes may have a huge capacity to critically substitute human decisions. This reality can easily end up having opposite effects to those desired. There should be no strict separation between experts in a specific field from AI technicians, since we can turn the desired advantage into a malfunction.;5 - Very important;4 - Important;4 - Important;4 - Important;3 - Neutral;3 - Neutral;All the presented proposals are important. What is fundamental when speaking of excellence, resides in avoiding idealism or inconsistent statements by being very strict with the reality of the technology we are dealing with, whose nature and possibilities of application are huge (as well as its potential disadvantages if not applied correctly). A detailed study of the application of AI to each area of society is essential, and this is what excellence should mean here.;5 - Very important;5 - Very important;4 - Important;What should be of concern here is the ultimate goal of such initiative, since excellence can be used for all kinds of objectives, the EU should be extremely clear in the fact that these initiatives have the development of technology at the service of European democratic values.;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;Help to raise EU's awareness about potential dangers and benefits of AI from SMEs's knowledge.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;The lack of formal verification, for example, can lead to very serious problems, which can go unnoticed in the implementations of the algorithms, implying malfunctions that can lead to incalculable damages. It is paramount to create a concept map of the dangers and problems of this technology along with its causes in order to be aware of how to apply it before creating a system with serious startup problems.;Other;"Any new legislation should include:
1- Concept map of AI technology: (potential problems, their causes and possible solutions).
2-Detailed analysis of the relationship between AI and each of the areas to which it will be applied, to match the principles and needs of each and avoid undesired overlapping of technical needs over fundamental principles and values. 
3-Take into account high security protocols for software systems like formal verification.";Yes;;Other;"The generic requirements that are referred in the ""Robustness and Accuracy"" section, should strongly consider (once those requirements are clarified) the application of formal verification, or at least protocols to ensure that the software is produced through clear logical-mathematical formal specifications.";Every part of our society that will be in hands of some AI process is concerning per se due to the nature of this technology. In particular, when talking about legislative/judiciary applications we have to take into account that we can end up violating our own value system.;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Rather not;;Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;;Personal security risks;There is a great tendency to talk about data, when the most serious problems come from the algorithms themselves (due to problems related to bugs, misinterpretation of specifications or faulty specifications). Focusing on the robustness and accuracy aspect of AI systems is an essential previous step.;Yes;;Yes;;Yes, for all AI applications;;;Formal_Vindications_presentation.pdf
F528923;26-05-2020 13:40;English;;;;Regional;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;We should considerate as very important point the participation of the teacher category into the exploration of how AI functionalities may be explored, adapted and then transferred into the classroom or wherever learning processes are expected. One of the prior investigations should focus on the interdisciplinarity function applied to didactics, then on the learning analytics to enhance personalization of teaching/learning.;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;Update student’s program to be carried out in high school and at the university level in order to train for specific domain application of AI.;4 - Important;4 - Important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;Support partnerships between academia, schools and applied research centers around AI projects and support funded projects and training. Help to raise SME’s business and know-how suggesting the relationship with University or Research Agency in every European or National Tender;4 - Important;4 - Important;4 - Important;4 - Important;3 - Neutral;5 - Very important;AI is not always underlined by its producers and this lack of transparency does not facilitate the general public understanding about the risks and the potentials of AI in everyday life;There is a need for a new legislation;;Yes;;Yes;;"In order of importance of the risk I’m feared on:

1.	health system because it could be that non-professional operators may try to deduce AI digital data with no medical knowledge,
2.	to make personal choices in any aspect of the life may become a rare competence if the AI engines know too much about your orientation. The personal thinking elaboration may become not so capital for people’s live if the AI machines substitute the human brain to make choice in an acceptable manner.
  ";4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;;No opinion;;;
F528922;26-05-2020 12:25;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;No opinion;;5 - Very important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;Current legislation may have some gaps;;Yes;;Yes;;;3 - Neutral;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;Yes;;Yes, for all AI applications;;;
F528921;26-05-2020 11:50;English;Academic/Research Institution;Joost;JOOSTEN;;University of Barcelona;;Large (250 or more);Spain;The feedback can be published with your personal information;3 - Neutral;4 - Important;No opinion;4 - Important;5 - Very important;No opinion;One might consider a funding scheme that helps universities set up relevant masters. To keep up with development, educating the right scholars is important. Current bureaucracy makes top researchers reluctant to help funding new masters that may be fundamental to keep up with the fast developments. If one is provided with money to pay bureaucratic support possibly important new master will see the light. (Action 3 only speaks about being able to contract, but bureaucratic support is fundamental);5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;No opinion;One could consider schemes between member states.;No opinion;No opinion;5 - Very important;Being IP of a large collaboration project and 4 Industrial Doctorate students I sometimes think that the current programs are slightly too much pitched toward industry. More academic freedom would encourage many of my colleagues to join too. As such industrial academic proposals could have a purely academic track (related to the matter but independent).;No opinion;No opinion;5 - Very important;5 - Very important;4 - Important;Again, to have industrial projects be interesting to academia, it would be good if these funding schemes allowed a track that is NOT directly industry related.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;We are working with what some call the next digital revolution: software of ZERO ERROR. Moreover, we provide mathematical proof that the software developed via our protocol contains zero errors. Moreover, this proof can be checked via a very simple computer program (that is trust-worthy). For critical software, this should be a standard and homologation agencies (like Fitch or Standard and Poors) should provide rigor and control to ensure software does what it claims it does.;There is a need for a new legislation;;Yes;;No opinion;;;4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;No opinion;As long as there is no guarantee that software does what it claims it does, all further discussion on regulations are secondary in my opinion.;No opinion;As long as there is no guarantee that software does what it claims it does, all further discussion on regulations are secondary in my opinion.;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;I think that for high-risk applications the only real solution to compliance issue is mathematical rigor. All other standards and ISOs concerning dynamical testing and the like cannot provide 100 % security. Verified programming can (provided the small kernel is correct which has been tested, analyzed and used very often). Of course verified programs will then be as good as its mathematical specification (whose accordance with the spirit of the task may be questioned but that is another story). ;Cyber risks;As long as there is no guarantee with mathematical rigor that software does what it claims it does, all further discussion on regulations are secondary in my opinion.;Yes;Programming errors ALWAYS occur unless you mathematically prove that the program does what it claims it does. Proprietary software cannot disclose its source code. Therefore, external agencies are needed. However, they seem to only be able to do their job if the program fits a suitable mathematical framework. This should be mandatory for high-risk applications.;Yes;Eradicate the possibility of programming errors making rating/homologation mandatory;No opinion;;;Presentacio__projecte_verificacio__formal.pdf
F528920;26-05-2020 11:24;French;;;;International;;;;;The feedback can be published in an anonymous way;5 - Very important;4 - Important;4 - Important;3 - Neutral;3 - Neutral;5 - Very important;;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;4 - Important;;3 - Neutral;4 - Important;3 - Neutral;3 - Neutral;4 - Important;;4 - Important;5 - Very important;4 - Important;3 - Neutral;4 - Important;3 - Neutral;;There is a need for a new legislation;;Yes;;Yes;;;4 - Important;4 - Important;5 - Very important;4 - Important;5 - Very important;4 - Important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;No opinion;;Yes, for specific AI applications;;;
F528919;25-05-2020 17:44;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;3 - Neutral;;5 - Very important;5 - Very important;3 - Neutral;4 - Important;4 - Important;4 - Important;;5 - Very important;4 - Important;4 - Important;;4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;Current legislation may have some gaps;;No;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F528918;25-05-2020 17:08;English;EU Citizen;ALVARO;RANGEL HERNÁNDEZ;;;;;Spain;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;;5 - Very important;4 - Important;5 - Very important;;4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;;There is a need for a new legislation;;Yes;;Yes;;;No opinion;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Risks related to the loss of connectivity;;Yes;;Yes;;Yes, for all AI applications;;;
F528917;25-05-2020 16:58;Dutch;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Aandacht voor ethische aspecten van KI;5 - Very important;No opinion;3 - Neutral;5 - Very important;5 - Very important;5 - Very important;Ontwikkelen en promoten van standaarden voor electronische gegevensuitwisseling;4 - Important;5 - Very important;4 - Important;;4 - Important;No opinion;4 - Important;5 - Very important;No opinion;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;gebrek aan transparantie als er geen mensen meer aan te pas komen;There is a need for a new legislation;;No;;;;;2 - Not important;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;gebruik van infographics;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;;No opinion;;;
F528916;25-05-2020 13:33;German;;;;;;;;;The feedback can be published in an anonymous way;;;;;;;;;;;;;;;1 - Not important at all;1 - Not important at all;1 - Not important at all;;3 - Neutral;4 - Important;4 - Important;3 - Neutral;5 - Very important;;3 - Neutral;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;;Current legislation may have some gaps;;Yes;;No opinion;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;Es gibt schon heute Extremistische Strömungen in Europa. Sollte in der Zukunft ein Land eine extremistische Regierung haben, so wird eine Opposition zu dieser durch Regierungs KI extrem erschwert;Very much;;;;;;;;;;;;;;
F529055;25-05-2020 11:43;Dutch;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;4 - Important;5 - Very important;4 - Important;5 - Very important;4 - Important;;4 - Important;4 - Important;5 - Very important;4 - Important;5 - Very important;No opinion;;3 - Neutral;4 - Important;4 - Important;;5 - Very important;4 - Important;4 - Important;3 - Neutral;4 - Important;;4 - Important;5 - Very important;3 - Neutral;4 - Important;4 - Important;3 - Neutral;;Current legislation may have some gaps;;Yes;;Yes;;"Invloed op toekomst; leven(skwaliteit)";5 - Very important;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;;;No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;No opinion;;Yes, for specific AI applications;"Invloed op toekomst; leven(skwaliteit)";;
F529054;25-05-2020 11:38;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;4 - Important;3 - Neutral;5 - Very important;5 - Very important;-;5 - Very important;4 - Important;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;-;5 - Very important;4 - Important;4 - Important;-;4 - Important;5 - Very important;4 - Important;;3 - Neutral;-;4 - Important;4 - Important;5 - Very important;3 - Neutral;2 - Not important;4 - Important;-;Current legislation may have some gaps;;Yes;;Yes;;-;5 - Very important;4 - Important;4 - Important;5 - Very important;3 - Neutral;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);Instead of queuing, I would be happy to Biometric identification as much as possible. ;Not at all;-;A combination of ex-ante compliance and ex-post enforcement mechanisms;;-;Personal security risks;-;Yes;-;Yes;-;Yes, for all AI applications;;-;
F529053;25-05-2020 11:36;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;No opinion;No opinion;2 - Not important;4 - Important;3 - Neutral;We support that the EU community observe trends in other regions and align their approach with global regulators where possible.;5 - Very important;4 - Important;5 - Very important;2 - Not important;4 - Important;No opinion;We recommend that the EU supports the investment in the right AI talent to compete with other regions.;3 - Neutral;4 - Important;5 - Very important;;No opinion;No opinion;No opinion;4 - Important;No opinion;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;We support that the EU authorities also focus their policy development on the benefits of AI, not just the potential concerns.;Current legislation may have some gaps;;Yes;;Yes;;"We believe that the existing legislation needs enhancing and alignment with the work of other organizations such as the OECD principles on AI.
Any new regulation/ adjustments to the current framework should be subject to the appropriate risk taxonomy.";4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;3 - Neutral;No opinion;;Much;Any voluntary regulation should be industry driven in cooperation with regulators.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Compliance and enforcement mechanisms should be ensured through existing regulatory and supervisory bodies.;Personal security risks;;No;;No;;No;;;
F529052;25-05-2020 11:06;Swedish;;;;International;;;;;The feedback can be published in an anonymous way;5 - Very important;4 - Important;5 - Very important;4 - Important;4 - Important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;;3 - Neutral;3 - Neutral;3 - Neutral;;4 - Important;5 - Very important;4 - Important;4 - Important;3 - Neutral;;3 - Neutral;3 - Neutral;1 - Not important at all;1 - Not important at all;No opinion;3 - Neutral;"The ""owner"" of the AI system and their intention and responsibility.";There is a need for a new legislation;;No;;;;New rules for all AI system is needed. See EU Product directive.;4 - Important;3 - Neutral;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Very much;Use the principles of exsisting accreditation system according EU 765/2008.;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;Use the principles of exsisting accreditation system according EU 765/2008 and use notified bodies.;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F529051;24-05-2020 21:37;Portuguese;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;3 - Neutral;;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;;4 - Important;4 - Important;5 - Very important;4 - Important;5 - Very important;;4 - Important;4 - Important;5 - Very important;5 - Very important;3 - Neutral;4 - Important;;No opinion;;No opinion;;;;;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);;Not at all;;Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;;Cyber risks;;Yes;;Yes;;Yes, for all AI applications;;;
F529050;24-05-2020 17:15;Portuguese;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;Partner and support Startups that are able to scale rather then consultancy companies.;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Promote a strategy for data usage under the GDPR constrains - synthetic data, federated learning, Differential privacy mechanisms;4 - Important;3 - Neutral;5 - Very important;;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;;3 - Neutral;4 - Important;3 - Neutral;1 - Not important at all;3 - Neutral;3 - Neutral;;Other;Current legislation is not enough and in a lot of countries is not even being followed, in particular around the privacy topics and the use of data to develop AI based solutions.;No;;;;Privacy and poor development of AI based solutions can lead to risky events.;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;Other special requirements in addition to those mentioned in the question above should be imposed (please specify);"Remote biometric identification will be more and more important. The times we are experiencing today show us that.
But current legislation is not enough to ensure the correct use of data to build this system: who can access the data, how it can be accessed, how many can access it, and what is privacy when talking about biometric data.";Not at all;Voluntary labelling can lead to unaccurate labelling, that will lead to poor AI based solutions. Rather start working with a small but high quality ground truth dataset and have it augmented of automatically labelled.;No opinion;;;Risks related to the loss of connectivity;;Yes;;Yes;;Yes, for all AI applications;;;
F529049;24-05-2020 11:50;English;Academic/Research Institution;Giorgio;GIACINTO;;Università degli Studi di Cagliari;;Large (250 or more);Italy;The feedback can be published with your personal information;4 - Important;2 - Not important;4 - Important;4 - Important;4 - Important;4 - Important;Action 3 is focused on “leading universities and higher education institutes to attract the best professors and scientists and offer world-leading masters programmes in AI” (Action 3): The development of skills in EU needs to be supported at all levels, not limited to ‘leading universities’, with the aim of increasing the impact of AI-related education and research, thus allowing the emergence of a wide number of leading professors and scientists;5 - Very important;3 - Neutral;4 - Important;4 - Important;5 - Very important;3 - Neutral;;2 - Not important;4 - Important;4 - Important;;2 - Not important;2 - Not important;4 - Important;5 - Very important;3 - Neutral;;3 - Neutral;3 - Neutral;4 - Important;4 - Important;2 - Not important;3 - Neutral;Too much emphasis is given on the concerns on AI with respect to the expected benefits. A too strict regulation on AI might be an obstacle to the evolution of research and adoption. The definition of an open schema for the evaluation of AI implementations with respect to privacy, safety, security, will help a community driven trust. Trust requires transparency of the methodologies and procedures. ;Current legislation may have some gaps;;Yes;;No;;Artificial Intelligence is already embedded in many applications and appliances. The definition of “high-risk” AI applications reported in the document might have the consequence of including a large number of applications, thus preventing the rapid development of the field. High-risk should be defined in terms of direct life threats. Rules should focus on design, development, deployment and maintenance methodologies rather than on specific technical requirements. ;4 - Important;4 - Important;;4 - Important;5 - Very important;3 - Neutral;Biometric identification systems should never be allowed in publicly accessible spaces;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Community driven assessment should be fostered through open source schemes related to the methodology used for the design, deployment, and testing.;Risks related to the loss of connectivity;;No;Legislation should be kept general enough to avoid that a too strict regulation based on the current state of the art prevents further development that can provide reliable quantitative assessment approaches. ;No;At this stage of development of the AI field, I support the development of guidelines to follow, and the openness of the methodologies used in the development, deployment and maintenance stages. Stricter regulations can be enforced when the field will be mature enough.;No;;;
F529048;23-05-2020 12:58;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;;5 - Very important;;4 - Important;3 - Neutral;4 - Important;5 - Very important;;4 - Important;4 - Important;4 - Important;;4 - Important;3 - Neutral;3 - Neutral;4 - Important;3 - Neutral;;4 - Important;No opinion;3 - Neutral;No opinion;3 - Neutral;;AI bias and opinions;Current legislation may have some gaps;;Yes;;Yes;;;4 - Important;4 - Important;;;4 - Important;4 - Important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;No opinion;;Yes, for all AI applications;;;
F529047;23-05-2020 01:20;Portuguese;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Redirecionar fundos/investimento para áreas específicas como educação, saúde, agricultura e ciência que possam usar skills relacionados com AI e tecnologia.;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;De momento, não me lembro de mais nenhuma opção relevante.;5 - Very important;5 - Very important;5 - Very important;De momento, não me lembro de mais nenhuma opção relevante.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;De momento, não me lembro de mais nenhuma opção relevante.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;De momento, não me lembro de mais nenhuma opção relevante.;There is a need for a new legislation;;No;;;;"Para mim, aplicações de alto-risco são as relacionadas com justiça e saúde. Por isso mesmo, é preciso investir em inteligência artificial transparente, ""non-biased"" e justa.";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;"Sou altamente contra os sistemas de identificação biométricos, assim como sou contra a videovigilância em espaços públicos. A Europa é sinónimo de liberdade e liberdade significa que eu, enquanto cidadão, tenho direito à minha privacidade e a não ser ""tracked"". Entendo, contudo, que caso aconteça algo que coloque em perigo a saúde e segurança de todos (algo catastrófico ou uma calamidade), a EU poderá rever estas questões regulatórias.";Very much;"Teria de haver sempre uma curadoria peer-to-peer para garantirmos homogeneidade e um não-excesso de ""bias"" (género, etnia...).";A combination of ex-ante compliance and ex-post enforcement mechanisms;;De momento, não me lembro de mais nenhuma opção relevante.;Mental health risks;De momento, não me lembro de mais nenhuma opção relevante.;Yes;De momento, não me lembro de mais nenhuma opção relevante.;Yes;De momento, não me lembro de mais nenhuma opção relevante.;Yes, for all AI applications;;De momento, não me lembro de mais nenhuma opção relevante.;
F529046;22-05-2020 12:38;English;EU Citizen;João;Vinagre;;;;;Portugal;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;;5 - Very important;4 - Important;4 - Important;2 - Not important;4 - Important;5 - Very important;;2 - Not important;5 - Very important;4 - Important;;4 - Important;2 - Not important;4 - Important;5 - Very important;3 - Neutral;;3 - Neutral;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;;Current legislation may have some gaps;;Yes;;Other;I agree with the definition given in Section 5.C, not 5.B;Law enforcement and military applications;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);If formal and auditable guarantees are given to citizens regarding risks;Much;Label validity must be renewed periodically (e.g. every year) ;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;No opinion;;Yes, for specific AI applications;Healthcare, Finance, Public administration and Law enforcement;;
F529045;22-05-2020 12:38;German;Company/Business organisation;Gertraud;LEIMÜLLER;;winnovation consulting gmbh - open innovation research;;Micro (< 10 employees);Austria;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;2 - Not important;5 - Very important;5 - Very important;;3 - Neutral;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;Provide access to showcases on AI-driven business models ;3 - Neutral;5 - Very important;5 - Very important;4 - Important;3 - Neutral;5 - Very important;AI may be used by authorities to manipulate citizens;There is a need for a new legislation;;No;;;;The definition of high-risk is extremely difficult, also medium-risk needs to be covered. Additionally, in order to make the regulation effective and applicable, a lot of effort needs to be put into the propper definition of high and medium risk.;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Very much;Inclusion of users, stakeholders and other society organisations into the AI development should be labeled. Testing in real-world-conditions and the result of this testing should be labeled.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Because AI includes constant learning and further development of the implemented algorithms, an ex-ante assessment cannot be sufficient. It needs to be a combination.;Mental health risks;Privacy risks, health risks on overall;Yes;;Yes;;Yes, for specific AI applications;High and medium risk applications;;
F529044;22-05-2020 11:52;English;NGO (Non-governmental organisation);Stephanie;KOHL;;European Association of Hospital Pharmacists (EAHP);82950919755-02;Micro (< 10 employees);Belgium;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;4 - Important;Special care must be taken for high-risk sectors, such as health, where ethical and privacy issues must be given extra attention. Furthermore, the skills of the healthcare workforce are particularly important and updating technological skills through continued training is vital. As well as engaging with universities and other educational facilities for healthcare professionals to ensure that new graduates are prepared for the new technologies. ;5 - Very important;3 - Neutral;3 - Neutral;3 - Neutral;5 - Very important;4 - Important;Specific actions for AI in health should be considered. Issues such as patient safety and liability of healthcare professionals when using AI technologies to support diagnostics and treatment options must be addressed. This must be taken into account when creating common standards as well as how to assess AI innovations within existing authorisation processes for medicines and medical devices. Also, it should be investigated whether specific legislative actions are needed for AI in health. ;5 - Very important;4 - Important;3 - Neutral;AI research and innovation in health should be strengthened and be given greater legal clarity and guidance on issues such as data protection and privacy. Investigating how new developments in AI fit within existing EU legislation and structures on medical devices and clinical trials should also be explored. Networks focused on AI in health would enable the identification and facilitate the exchange of best practices between healthcare institutions championing AI solutions. ;4 - Important;4 - Important;3 - Neutral;5 - Very important;3 - Neutral;The involvement of healthcare professionals in projects or information gathering intended for use in healthcare is very important, this includes projects carried out by SMEs. There have been instances of small-scale projects intended for the health sector or for healthcare professionals which were developed without input from healthcare workers or engagement at a very late stage of the process. This can lead to inefficiencies or even unusable end products. ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Any breach of safety, privacy or discrimination can have serious consequences. The accuracy of AI in health must be ascertained and can be very difficult to achieve due to a fragmented data landscape. Liability, when medical errors occur due to incorrect AI system outputs and the risk of over-reliance on AI in place of clinical knowledge, needs to be addressed. As well as the issue of informed consent from patients to share data or use AI technologies for diagnostics or treatments. ;Current legislation may have some gaps;;Yes;;Yes;;The use of AI in health has enormous potentials but is one of the most high-risk areas. Issues such as over-reliance on AI, accountability for medical errors, informed consent, quality of data and the assessment of AI technologies in the context of existing legislation regarding clinical trials, medical devices and medicines must be considered at every step of the process, from the initial research stage to the application of AI technologies in European healthcare services. ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;The security and privacy risks are substantial. ;Very much;It is important that high-risk AI technologies and applications, such as those which are intended to be used in healthcare, be excluded from the non-high-risk category which would allow them to only require voluntary compliance. High-risk AI applications must receive more rigorous mandatory requirements which are harmonised with existing processes for clinical trials, medicines and medical devices. ;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Enforcement of compliance should be harmonised with existing structures and legislation in the health sector. Testing of real-world applicability with the involvement of healthcare professionals should be a part of the assessment criteria. Several variables can interfere with the accuracy of AI solutions when applied in different locations or situations, an assessment of these risks before market authorisation and transparency of these limitations once the product is on the market is vital. ;Cyber risks;Patient safety should be taken into consideration when assessing AI in health. As well as existing structures and legislation regarding clinical trials, medicines and medical devices. Liability of healthcare professionals when using AI technologies in patient care should also be considered, as well as the risk of over-reliance on AI solutions in lieu of clinical knowledge.;Yes;;Yes;;Yes, for all AI applications;;Liability and safety in the health sector is complex and AI must be fully integrated into existing systems on the European and national level to prevent liability gaps. Special attention needs to be paid to this to provide legal clarity to manufacturers and healthcare professionals and to ensure patient safety. ;
F529043;22-05-2020 08:55;English;Public authority;Eduard;FALCO;Regional;"Government of Catalonia, Catalan Regional Government.
http://politiquesdigitals.gencat.cat/en/tic/catalonia-ai/index.html
http://politiquesdigitals.gencat.cat/web/.content/Telecomunicacions/catalonia-ai/Estrategia_IA_Catalunya_ENG_VFinal.pdf";;Large (250 or more);Spain;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;Catalonia’s top vector in AI is related to the knowledge. The region is consolidating a knowledge ecosystem that would enable it to head the deployment of a human-centered AI with guaranteed success—creating ecosystems, communities, clusters to work together, to test solutions (research & private). We believe it is not just act on the agents, it should be work with the agents, enhancing the ecosystem. Mobile Networks Ecosystem and Health could be a good AI showcases.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"It is important to balance the participation to have a healthy ecosystem, but Public Administration should be one of the leaders. Public Administration should adopt fair and trustable AI solutions and demonstrate that it is an active agent and also promote the definition and network of trustworthy AI certification entities to help that ecosystem.
Please, check Catalan AI strategy at: http://politiquesdigitals.gencat.cat/en/tic/catalonia-ai/index.html.";3 - Neutral;5 - Very important;4 - Important;If it is promoted a good network of existing AI research excellent centres, it is not clear the necessity of a lighthouse centre. Efforts should be devoted to creating effective ways of promoting the exchange of knowledge and top researchers and practitioners among the European research centres (e.g. the Covid experience). A policy for talent attraction is necessary. Perhaps large enterprises should contribute enhancing the community contracting research to Academia or actively creating startups;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;Support partnerships between startups and academia is more important than between larger enterprises and academia. Larger enterprises could fund their own R+D+I activities and support academia and startups (the idea to subsidy the big firms creates doubts). Of course, a neutral PPP should be positive, but in case of AI, where we can have very big firms, we do not consider that subsidying big firms should be a solution.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"Public Administrations must ensure some specific control or supervise on AI solutions. That is why Public Administrations should develop skills to be able to do those tasks on their own for the common benefit. To avoid that individuals and legal entities may face difficulties with adequate access to justice in situations where such decisions may negatively affect them, research on ""Explainable and Trustworthy AI"" solutions should be fostered where decisions can be traced back.";There is a need for a new legislation;;Yes;;Yes;;"Autonomous Lethal Weapons, pervasive face recognition and biometric identification for massive control of citizens and privacy-related applications, in general, should be considered ""high-risk"". 
Fake news and other personalised campaigns ought to be forbidden especially for non-adult publics.";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;"Please, check Ctaalan AI strategy at:
Web: http://politiquesdigitals.gencat.cat/en/tic/catalonia-ai/index.html
Document: http://politiquesdigitals.gencat.cat/web/.content/Telecomunicacions/catalonia-ai/Estrategia_IA_Catalunya_ENG_VFinal.pdf";Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;The enforcement mechanism could be supervised by an external committee.;Mental health risks;"Bots bring with them legal implications and the law, both at a national and international level is far behind the technology. It’s time to wake up at the jurisdictional and legal levels, collaborate, and quickly create a legal bot and human identity system that protects citizens. For further information, please, see:
https://hvl.net/pdf/The%20identity%20lifecycle%20of%20Jane%20Doe%20Mar%202019.pdf";Yes;As pointed out in the answer to the previous question, we consider we need a system able to identify bots from humans, describe and classify bots, legally identify them (and identify those that refuse legal identification), and then keep up with the incredible pace of change to maintain these capabilities.;Yes;;Yes, for all AI applications;;From the Catalan point of view, AI and related technologies are creating an entirely new environment, not just a new technology. Europe should adopt the legal framework to preserve individual rights (and rest of universal human rights) in that new legal, socio-economic and cultural reality. ;
F529042;21-05-2020 18:04;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;4 - Important;4 - Important;3 - Neutral;2 - Not important;5 - Very important;Promoting AI in Education;5 - Very important;4 - Important;3 - Neutral;4 - Important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;3 - Neutral;Promote mobility of researchers of excellence despite their own research centers;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;Inapropriate takeover of AI capabilities by the great powers (Army, Governement,  Companies);There is a need for a new legislation;;Yes;;Yes;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;No opinion;;No opinion;;;
F529041;21-05-2020 17:47;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;5 - Very important;2 - Not important;3 - Neutral;3 - Neutral;;5 - Very important;5 - Very important;4 - Important;3 - Neutral;5 - Very important;4 - Important;;5 - Very important;4 - Important;3 - Neutral;;3 - Neutral;5 - Very important;3 - Neutral;3 - Neutral;3 - Neutral;;4 - Important;5 - Very important;4 - Important;2 - Not important;5 - Very important;2 - Not important;;Current legislation may have some gaps;;Yes;;Yes;;;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;4 - Important;4 - Important;Biometric identification systems should never be allowed in publicly accessible spaces;;Not at all;;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F529040;21-05-2020 17:35;Portuguese;Consumer Organisation;Luis;PISCO;;ASSOCIAÇÃO PORTUGUESA PARA A DEFESA DO CONSUMIDOR - DECO;005887421809-56;Medium (< 250 employees);Portugal;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;4 - Important;;4 - Important;Proteção do cidadão e consumidor ;5 - Very important;5 - Very important;4 - Important;3 - Neutral;5 - Very important;3 - Neutral;Proteção do cidadão e consumidor;5 - Very important;5 - Very important;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;No;;;;;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Very much;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Risks related to the loss of connectivity;;Yes;;Yes;;Yes, for all AI applications;;;
F529039;21-05-2020 15:13;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;;4 - Important;4 - Important;4 - Important;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;;Current legislation may have some gaps;;Yes;;;;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Cyber risks;;Yes;;Yes;;Yes, for specific AI applications;causing legal or significant effects;;white_paper.txt
F529038;21-05-2020 12:55;Italian;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;;3 - Neutral;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;4 - Important;4 - Important;No opinion;5 - Very important;;There is a need for a new legislation;;Yes;;Yes;;;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;Biometric identification systems should never be allowed in publicly accessible spaces;ritengo una grave violazione della privacy e della libertà dei cittadini prevedere qualsiasi forma di riconoscimento facciale.;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;No opinion;;Yes, for all AI applications;;;
F529037;20-05-2020 22:29;Dutch;Other;Hans;van der heijden;;Drie Oude Eiken;;Micro (< 10 employees);Netherlands;The feedback can be published with your personal information;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Ki gaat zich  ontwikkelen. Het is niet tegen te houden. De belangrijkste maatregelen betreffen vooral het aantonen en demonstreren aan de eindgebruikers wat de voordelen zijn. Het grootste gevaar is de weerstand  bij de eindgebruiker die ontstaat door de onwetenheid wat de voordelen zijn.  ;2 - Not important;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;Voorlichting en demonstratie van de voordelen. Laat de eindgebruiker kennisnemen van de mogelijkheden. Zorg voor motivatie aan de basis en pas op voor top-down benadering.;2 - Not important;4 - Important;4 - Important;creëer pilots en publiceer de resultaten, zorg voor inktvlekwerking;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;1 - Not important at all;1 - Not important at all;4 - Important;4 - Important;5 - Very important;Ki is per definitie NIET accuraat, maar betrouwbaarder dan een mens. Zorg voor transparantie. Besluiten van mensen zijn niet objectief en niet transparant, besluiten met behulp van KI zijn dat wel;No opinion;;No opinion;;;;ik kan onvoldoende inschatten wat de reikwijdte is van de huidige regelbegeving;2 - Not important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;Ik ben onvoldoende op de hoogte van de huidige EU-wetgeving;No opinion;ik heb onvoldoende kennis van dit onderdeel;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;betrouwbaarheid moet worden beoordeel door de eindgebruikers;;Aansprakelijkheid van AI is een verkeers uitgangspunt. AI is een beslissingsondersteunend functie. AI helpt de beslisser een betere beslissing te nemen. Dus de eindbeslisser is verantwoordelijk, die kan niet AI ' de schuld' geven.;No opinion;;No opinion;;No;;;Beslissingsondersteuning.docx
F529036;20-05-2020 19:40;English;Academic/Research Institution;Alberto;GIRETTI;;Università Politecnica delle Marche;;Large (250 or more);Italy;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;3 - Neutral;5 - Very important;4 - Important;;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;2 - Not important;5 - Very important;4 - Important;2 - Not important;;Current legislation may have some gaps;;No;;;;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;No opinion;;Rather not;;No opinion;;;Risks related to the loss of connectivity;;Yes;;No opinion;;No opinion;;;
F529035;20-05-2020 18:51;Italian;Academic/Research Institution;vito;getuli;;University of Florence;;Medium (< 250 employees);Italy;The feedback can be published with your personal information;2 - Not important;3 - Neutral;5 - Very important;4 - Important;5 - Very important;5 - Very important;;3 - Neutral;1 - Not important at all;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;;2 - Not important;1 - Not important at all;5 - Very important;;2 - Not important;4 - Important;5 - Very important;4 - Important;2 - Not important;;4 - Important;2 - Not important;2 - Not important;4 - Important;3 - Neutral;3 - Neutral;;Current legislation may have some gaps;;Yes;;Yes;;;5 - Very important;1 - Not important at all;2 - Not important;1 - Not important at all;5 - Very important;5 - Very important;No opinion;;Much;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Mental health risks;;No;;No;;Yes, for all AI applications;;;
F529034;20-05-2020 18:12;Italian;Academic/Research Institution;Saverio;MECCA;;Università di Firenze;679509925258-80;Large (250 or more);Italy;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;promuovere l'adozione dell'IA nella formazione superiore;4 - Important;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;;3 - Neutral;3 - Neutral;4 - Important;;5 - Very important;4 - Important;4 - Important;5 - Very important;3 - Neutral;;5 - Very important;5 - Very important;4 - Important;4 - Important;3 - Neutral;3 - Neutral;;There is a need for a new legislation;;No;;;;;3 - Neutral;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Risks related to the loss of connectivity;;Yes;;No opinion;;Yes, for specific AI applications;;;
F529033;20-05-2020 17:31;English;Non-EU Citizen;Prateek;Sibal;;;;;India;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;International Cooperation is mentioned in the report as an important area but it could also be a part of the action plan. AI has the potential to exacerbate the digital divide between nations, the ripple effects of misuse of AI technologies elsewhere may also be felt in Europe. For instance, the use of AI for creation of deep-fake videos influences elections and democratic process with the possibility of poor governance or a crisis situation emerging in a country with repercussions for peace. ;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;A coordinated European approach can help establish best practices and lead to policy transfer outside the EU as has been witnessed in the case of GDPR. Emphasis must also be paid to engagement of international organisations in EU's work as a matter of knowledge sharing. ;4 - Important;5 - Very important;4 - Important;;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;Current legislation may have some gaps;;Yes;;No opinion;;Applications with direct impact on human dignity, human rights and democracy. Applications facilitate the spread of misinformation and disinformation in the public sphere are high-risk with long term consequences for politics, institutions and society. The risk matrix should accommodate assessments for second level and systemic risks.  ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;;Very much;It would be a good starting point for understanding potential applications that can be brought under the ambit of regulations later. ;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;No opinion;;Yes, for all AI applications;;;
F529032;20-05-2020 15:48;German;Other;Marie-Luise;MOLTMANN;;Bundesarchitektenkammer e.V - Federal Chamber of German Architects;08215638217-13;Small (< 50 employees);Germany;The feedback can be published with your personal information;4 - Important;4 - Important;;5 - Very important;No opinion;No opinion;;5 - Very important;No opinion;4 - Important;4 - Important;5 - Very important;5 - Very important;;2 - Not important;4 - Important;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;;4 - Important;5 - Very important;No opinion;4 - Important;No opinion;No opinion;;Current legislation may have some gaps;;No;;;;;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;No opinion;Biometric identification systems should never be allowed in publicly accessible spaces;Temporary anonymised analysis i.e. of traffic movement should be possible. ;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;Yes;;Yes, for all AI applications;;Relevant for products and services;200520_BAK-Stellungnahme_KI-Konsultation_final.pdf
F529183;20-05-2020 12:36;English;Business Association;Tim;HAMERS;;CECIP - European weighing industry;48444564134-24;Micro (< 10 employees);Belgium;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;5 - Very important;No opinion;5 - Very important;5 - Very important;5 - Very important;4 - Important;;4 - Important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;;3 - Neutral;4 - Important;3 - Neutral;3 - Neutral;2 - Not important;3 - Neutral;;Current legislation may have some gaps;;Yes;;No opinion;;;3 - Neutral;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;No opinion;;No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Risks related to the loss of connectivity;;Yes;;No opinion;;No;;;
F529182;20-05-2020 11:55;German;;;;;;;;;The feedback can be published in an anonymous way;3 - Neutral;5 - Very important;4 - Important;4 - Important;3 - Neutral;3 - Neutral;;3 - Neutral;3 - Neutral;1 - Not important at all;3 - Neutral;3 - Neutral;4 - Important;;4 - Important;5 - Very important;4 - Important;;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;3 - Neutral;;No opinion;;No opinion;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;;;;;;;;;;
F529181;20-05-2020 11:44;French;;;;;;;;;The feedback can be published in an anonymous way;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;;1 - Not important at all;1 - Not important at all;1 - Not important at all;;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;Yes;;Yes;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);;Rather not;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Mental health risks;Privacy;Yes;;Yes;;Yes, for all AI applications;;;
F529180;20-05-2020 10:04;English;Academic/Research Institution;Sara;LANDINI;;Univesity of Florence;;Large (250 or more);Italy;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;financial sector, Supervisors Public Authorities;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;reduce accidents and human errors;4 - Important;4 - Important;4 - Important;increase knowledge in risk management;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;digital innovation hubs could be used for risk management e.g.Cyber Risk and Threat Analysis;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;"
AI can act independently and it must be established that it will pay for the damages caused. It is important to think of ad hoc insurance coverage.";Current legislation may have some gaps;;Yes;;Yes;;Risk of financial crisis. IA can help in monitoring the market helping Supervisory authority in finding criticalities;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;We have to find measures to avoid the risks of using biometrics including data and network hacking, rapidly evolving fraud capabilities, biometric enrollment security, familiar fraud;Much;;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;landini_-_provvisorio__1_.pdf
F529179;19-05-2020 19:25;Spanish;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;4 - Important;5 - Very important;green deal;4 - Important;4 - Important;5 - Very important;4 - Important;4 - Important;5 - Very important;cibersecurity;4 - Important;5 - Very important;5 - Very important;;4 - Important;4 - Important;3 - Neutral;4 - Important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;4 - Important;No opinion;5 - Very important;;There is a need for a new legislation;;No opinion;;;;;3 - Neutral;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;No opinion;;No opinion;;;Personal security risks;;Yes;;No opinion;;No opinion;;;
F529178;19-05-2020 18:34;English;;;;;;;;;The feedback can be published in an anonymous way;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;Biometric identification systems should never be allowed in publicly accessible spaces;;;;;;;;;;;;;;;;
F529177;19-05-2020 17:58;Spanish;Other;Cristina;Barreto;;Grupo Social ONCE;68612977691-59;Large (250 or more);Spain;The feedback can be published with your personal information;4 - Important;4 - Important;5 - Very important;3 - Neutral;4 - Important;5 - Very important;Implementar mecanismos para desactivar la desinformación que llega a la sociedad a través de noticias falsas ((fake news), artículos basados en información errónea, y en general material sensacionalista sin base científica.;3 - Neutral;4 - Important;4 - Important;3 - Neutral;3 - Neutral;5 - Very important;"Ámbito de la comunicación, periodismo y divulgación científica.

?
";3 - Neutral;4 - Important;5 - Very important;"Fomento de estancias de investigadores entre centros de investigación de diferentes países.
?
";3 - Neutral;3 - Neutral;4 - Important;5 - Very important;3 - Neutral;"Asesoramiento experto a las PYMES en los procesos de adquisición de productos y servicios TIC basados en IA, apropiados a la actividad de las PYMES.

";4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;1 - Not important at all;"Gran parte de las preocupaciones enumeradas se basan en que la IA es un recurso muy limitado, o sea es un servicio restringido a una parte pequeña de la población lo que puede dar lugar a una nueva brecha en la sociedad a sumarse a la brecha digital.

";There is a need for a new legislation;;No;;;;;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;5 - Very important;4 - Important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);Debe existir una precisión y robustez de los sistemas garantizadas, a través mecanismos independientes.;Much;"Mecanismos de verificación y régimen sancionador eficiente.

?
";A combination of ex-ante compliance and ex-post enforcement mechanisms;;Desde las Administraciones Públicas se debería fomentar el desarrollo de IAs de control por entidades independientes. Estas IAs deberán estar especializadas en identificar incumplimientos de los compromisos autoadquiridos con las etiquetas.;Mental health risks;"Riesgo relacionados con la inaccesibilidad de los sistemas. En el caso de personas que por su discapacidad, no pueden interactuar con los sistemas, o cuando sus características especiales anulan la fiabilidad de los sistemas, no pueden ser discriminadas.


";Yes;"Los sistemas con IAs que aprenden y se ajustan en base a la interacción con los usuarios, deben testearse por normativa obligatoria siguiendo procedimientos similares a los que se aplican en el sector médico y farmacológico. Garantizando el suficiente número de usuarios de testeo y la necesaria diversidad de la muestra.

";Yes;"Las personas con discapacidad deben tener una especial protección ya que asumen riesgos mayores ante aplicaciones de la IA. El más evidente por motivos de discriminación ante IAs sin una transparencia y trazabilidad de sus resultados.

";Yes, for specific AI applications;"Aplicaciones de IA donde sus resultados tengan influencia en la vida de las personas con mayor o menor influencia: Relacionadas con la salud, el empleo, modelado de comportamiento de los usuarios, etc. 

";En lugar de valorar la funcionalidad de las IAs, debe establecerse una protección especial en aquellos casos de que sus resultados se estén aplicando a perfiles que necesitan una especial protección. Las personas con discapacidad o las mujeres, ambos en cuestiones relacionadas con el empleo, son un ejemplo.;
F529176;19-05-2020 17:28;Spanish;Non-EU Citizen;Maria;Vanoli;;;;;Costa Rica;The feedback can be published with your personal information;3 - Neutral;3 - Neutral;3 - Neutral;;3 - Neutral;1 - Not important at all;transparency in EVERY step;3 - Neutral;3 - Neutral;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;respect privacy at every step;1 - Not important at all;1 - Not important at all;1 - Not important at all;again transparecy is key;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;respect citizens privacy ALWAYS;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;No;;;;eternal tracking, personal choices about my body, divulgation of personal data;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;no one should own info about me FIRSTLY.  AI Trakking may facilitate rogue factions of intelligence services to harass and put in danger whistleblowers or anyone against the status quo.... meaning democracy goes to trash;Not at all;;;;;Mental health risks;;No opinion;;Yes;;;;;
F529175;19-05-2020 17:26;English;Academic/Research Institution;Salvatore;Orlando;;JODI - Juridical Observatory on Digital Innovation- https://web.uniroma1.it/deap/ogid;164451038125-33;Small (< 50 employees);Italy;The feedback can be published with your personal information;4 - Important;5 - Very important;;4 - Important;4 - Important;5 - Very important;;;;;;;;;;;;;;;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;With reference to the seven “key requirements” mentioned in the White Paper, and in connection with the concern that AI’s systems may be employed to exploit human behaviour, it is suggested to identify a specific notion of the “fairness” principle consisting in the duty of care of AI’s systems for human behavioural vulnerability including but not limitedly to AI’s applications affecting consumers- PLEASE SEE DOCUMENT UPLOADED   ;Current legislation may have some gaps;;Other;With reference and in addition to the purposes of AI’s applications to be considered “high risk as such”, as indicated and listed on a non-exclusive basis in the White Paper, it is suggested to expressly mention also the marketing purpose so as to cover all AI’s marketing applications, including those directed at influencing non-economic decisions- PLEASE SEE DOCUMENT UPLOADED ;;;;;;;;;;;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;"(1) Ex-ante compliance should include “fairness by design”; (2) Ex-post enforcement should be focussed on collective remedies and public enforcement for their higher ability to act as deterrents against AI’s unfair marketing applications compared to individual ex-post remedies - PLEASE SEE DOCUMENT UPLOADED";;;;;;;;;;2020_05_19_JODI_comments_to_EU_Commission_Whitepaper_COM_2020__final_pdf.pdf
F529174;19-05-2020 15:25;German;;;;;;;;;The feedback can be published in an anonymous way;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;Biometric identification systems should never be allowed in publicly accessible spaces;;;;;;;;;;;;;;;;
F529173;19-05-2020 13:59;English;NGO (Non-governmental organisation);Aneta;TYSZKIEWICZ;;Council of European Dentists;4885579968-84;Micro (< 10 employees);Belgium;The feedback can be published with your personal information;5 - Very important;4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;For the digital revolution to succeed in the healthcare system, a well-trained workforce is paramount. Therefore, priority should be given to the implementation of digital skills education into the dental studies curriculum. Once qualified, dentists should be able to learn and update their skills through appropriate continuing professional development (CPD) provision. ;4 - Important;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;Each professional should have the opportunity to undertake training about the functioning of algorithms, as well as training in AI tools management, which should be supported by appropriate technological structures in the practicing environment. Understanding AI processes and application would be the first step in reinforcing confidence in AI technologies among dentists which is a precondition for their wider use. ;5 - Very important;5 - Very important;4 - Important;;No opinion;5 - Very important;4 - Important;4 - Important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;To encourage confidence in AI systems among the profession there needs to be clarity about liability in case of  AI failure or misdiagnosis. The dentist´s liability when using AI has to be clearly defined. When health related decision is made relaying on AI technologies, the assessment if the dentist complied with the standard of care become more complex. ;There is a need for a new legislation;;Yes;;Yes;;Healthcare is the sector where AI could present ethical challenges that need to be addressed. Databases and algorithms may introduce bias into the diagnostic process, and AI may not perform as intended, posing a potential for patient harm. As health and social wellbeing are at stake, healthcare should be treated as the most sensitive sector and appropriate safeguards should be applied.  ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;In order to not reinforce the disparities in healthcare related to socioeconomic status or other biases. The data criteria selection must be developed and regularly verified. Transparent, clinically validated AI and systematic quality checks could foster the acceptance and trust in AI among the end-users. Dentists should be able to assess the reliability of the proposed AI decisions according to agreed-upon standards.;Personal security risks;Understanding AI processes and application would be the first step in reinforcing confidence in AI technologies among dentists which is a precondition for their wider use. Proper explanation of AI methods to patients is important in terms of their right to information. Therefore, the role of AI should be limited to a supporting role which does not undermine the autonomy of the dentist in the decision-making process.  ;Yes;;Yes;"Human interaction is particularly relevant for professionals such as dentists as their work is fundamentally dependent on patients’ trust. The dentist should, as part of the consent process, inform the patient that AI systems are being used to assist in diagnosis and treatment planning. In dentistry, any decision on treatment should take into consideration overall oral health of the patient and any physical and financial limitations.
";Yes, for all AI applications;;;CED-DOC-2019-047-FIN-E__6_.pdf
F529172;19-05-2020 13:22;German;;;;Regional;;;;;The feedback can be published in an anonymous way;4 - Important;4 - Important;5 - Very important;4 - Important;5 - Very important;4 - Important;"Für die Einfuhr von Technologie mit integrierter KI in die EU Kriterien festgelegt werden, beispielsweise in Bezug auf Auswahl und Qualität von Trainingsdatensätzen, um einen rechtskonformen (diskriminierungsfreien) Einsatz sicherzustellen.
Wissen muss breit gefächert in der Gesellschaft verankert werden. Insbesondere die Sensibilisierung für mögliche Risiken, die durch den Einsatz von KI entstehen, kann nur gelingen, wenn ein Grundverständnis für die Funktionsweise von KI vorhanden ist. 
";4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;;3 - Neutral;5 - Very important;3 - Neutral;;4 - Important;5 - Very important;4 - Important;4 - Important;3 - Neutral;"Sensibilisierung für Risiken beim Einsatz neuer Technologien
Rechtssicherheit und ein verbindlicher Handlungsrahmen, der Grenzen des Einsatzes von KI unter Bezug auf europäische Grundrechte festlegt (inkl. Marktortprinzip)";3 - Neutral;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;"Führungsrolle unter Wahrung europäischer Werte kann nicht erreicht, wenn die Interessen an KI  - insbesondere in Bezug auf die Auswertung personenbezogener Daten außerhalb Europas (z.T. auch bei einigen Mitgliedstaaten) -  dem auf globaler Ebene entgegenstehen. 
";There is a need for a new legislation;;No;;;;Alle Anwendungen, die den Zugang zu staatlichen Leistungen oder die Ausübung von Rechten bzw. die Einschränkung von Freiheiten zur Folge haben, aber keine Transparenz bezügliche der Verantwortlichen oder Gründe für die Entscheidungen erkennen lassen;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;Die Hürden für den Einsatz sollten möglichst hoch sein. Einsatzzeit und -raum sollten grundsätzlich auf 1-2 Wochen und einen bestimmten Bereich begrenzt sein und nur verlängert werden können, wenn die Gefahrensituation weiterhin ein erhebliches Risiko für Leib, Leben, Gesundheit im öffentlichen Raum in sich trägt.;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;"fehlende Kernanforderungen für KI-Technologie unterhalb von Hochrisikoverfahren;
fehlenden Gesamthaftung für KI-Produkte und damit fehlende Klärung der Verantwortlichkeit bei Schäden;
";Yes;Es fehlt an einer standardisierten Methode zur Risiko- und Schutzbedarfsfeststellung, die objektiven Kriterien genügt. Daher sollten Anforderungen Schlüsselmerkmale von KI-Systemen nicht auf Hochrisiko-Verfahren beschränkt werden. Es ist notwendig, zunächst grundsätzlich für jede KI geltende Anforderungen zu formulieren, z.B. zu den Trainingsdaten. Davon ausgehend können dann Anforderungen für besonders riskante KI-Anwendungszwecke abgeleitet werden.;Yes;Beweislast der rechtskonformen Funktion sollte bei Vertantwortlichen  nicht bei Verbrauchern liegen;Yes, for all AI applications;;;
F529171;19-05-2020 13:06;German;;;;;;;;;The feedback can be published in an anonymous way;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;Biometric identification systems should never be allowed in publicly accessible spaces;;;;;;;;;;;;;;;;
F529170;19-05-2020 13:04;English;Non-EU Citizen;Siddhartha;Singh;;;;;India;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;Response_to_the_EU_Commission_White_Paper_AI_Regulation_final_20200519.pdf
F529169;19-05-2020 13:01;English;Trade Union;Martin;JEFFLÉN;;Eurocadres – Council of European Professional and Managerial Staff;803183412905-34;Micro (< 10 employees);Belgium;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;#NAME?;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;4 - Important;Coordinated Plan is needed between EU members if public interest is the main key for development of research and innovation. Public and private sectors should collaborate for this purpose. Involvement of social partners in the national plans elaboration as well as the sectoral social partners at EU level is important. Trade unions and workers’ representatives shall be able to negotiate the algorithm of AI at the workplace.;4 - Important;4 - Important;5 - Very important;Research centers in Europe are important and require regular expert evaluation. Trade unions must be involved, experts reports must be transparent & terms of expertise should be proposed with union contributions. Public sector involvement & public interest is a must. The logic of cooperation, sharing of data & results must be guiding principles. Success won’t come from reductive competition between centres of excellence, but from successful cooperation, e g CERN, Airbus & Ariane Espace in France;4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;"Risk assessment must be done at SMEs level especially where trade unions are not present. 
Work on networking SMEs with each other and even with some larger companies to pool projects and the results of experimentation and development in order to reduce risks and increase opportunities.";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Regulatory framework is needed to prevent all risks of AI. Trade unions should be involved in the conception phase. New rules are needed not only for what we consider High-risk applications but for all AI systems. Developers and all workers should be able to blow the whistle on unethical AI applications or when zero risk on workers’ rights cannot be guaranteed. ;There is a need for a new legislation;;No;;;;"New rules are needed not only for what we consider High-risk applications because we cannot always evaluate the risks of AI. Risk assessments for all AI systems should be considered with TU implication.
Cross-referencing of files and their processing by opaque algorithms can generate the revelation of data (metadata)* that are subject to medical confidentiality. This may lead to discrimination from employers, insurance organisations (private insurers), (for instance for access to housing, etc)";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;Biometric identification systems should be prohibited in public spaces except for some sensitive places where it is justified (national security, research centers etc…). The circumstances for facial recognitions must be fixed by laws and rules must be taken with social partners. ;No opinion;Yes if labels are relevant and reliable. They have to be put place by public law with public control and labelled AI systems that do not respect certain requirements should have effective and deterrent sanctions. There is a need of a legal framework for all AI applications and labelling can be added by sectors.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;The EU should set up a legal system for all AI applications. Risks could be unknown for some AI systems. It is important that the conformity assessment procedure reveals these failures. Trade unions must be involved in this process.;Mental health risks;"Discrimination. Many unknown risks, especially w. self-learning AI
Ecological risk. 
Social risk: platform workers who do not know each other feed and ensure the algorithms’ functioning. Delivery platforms collect data from couriers to target where customers are, their consumption etc. Couriers subordinate to algorithm, must follow its instructions & subject to metrics evaluating their work. Managers’ Enterprise Resource Planning leaves zero room for manoeuvre and generates psychosocial risks.";Yes;There is a need to identify all risks including semi-automated decision-making. Trade unions should be involved in the risk assessment process.;Yes;Trade unions should be involved in amending the EU legislative framework for liability. Developers and manufacturers should ensure that AI applications are safe before used.;Yes, for all AI applications;;National liability rules should be adapted and all actors’ liabilities’ should be defined. At the workplace trade unions should be consulted and involved in algorithm conceptions.;
F529168;19-05-2020 12:01;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;4 - Important;4 - Important;4 - Important;;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;4 - Important;4 - Important;3 - Neutral;4 - Important;2 - Not important;4 - Important;;Current legislation may have some gaps;;No;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);These spaces should have clearly visible warnings at the entrances.;Not at all;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;Yes;;No;;;
F529167;19-05-2020 11:53;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;4 - Important;;4 - Important;3 - Neutral;3 - Neutral;5 - Very important;4 - Important;5 - Very important;;;4 - Important;2 - Not important;;2 - Not important;4 - Important;4 - Important;3 - Neutral;3 - Neutral;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;;Current legislation may have some gaps;;Yes;;Yes;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No further guidelines or regulations are needed;;Much;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;;;;;;;;;;
F529166;19-05-2020 11:27;English;Public authority;Hans;Ossebaard;National;National Health Care Institute;n.a.;Large (250 or more);Netherlands;The feedback can be published with your personal information;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;4 - Important;5 - Very important;;5 - Very important;5 - Very important;4 - Important;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;Yes;;Yes;;health and health care;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;Only when strict regulation is in place;Not at all;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Cyber risks;;Yes;;Yes;;No opinion;;;
F529165;19-05-2020 11:13;English;Academic/Research Institution;Janvi;Balani;;None;None;Micro (< 10 employees);India;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;None;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;None;5 - Very important;5 - Very important;5 - Very important;None;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;None;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;None;Current legislation is fully sufficient;;Yes;;Yes;;None;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);None;Very much;None;A combination of ex-ante compliance and ex-post enforcement mechanisms;;None;Mental health risks;None;Yes;None;Yes;None;Yes, for all AI applications;;None;
F529164;19-05-2020 10:56;German;Public authority;Franziska;Meier;Local;Landeshauptstadt München;;Large (250 or more);Germany;The feedback can be published with your personal information;1 - Not important at all;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;Gerade im Bereich der Datensicherheit (Security) und Ethik könnte sich Europa differenzieren und ein Alleinstellungsmerkmal (Vertrauen) schaffen. Aufklärung und eine offener transparenter Umgang mit Risiken, Ängsten, Vorfällen sollte die EU weit in den Vordergrund stellen;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;4 - Important;Europ. Datenraum gelingt nur, wenn man sich auf ein gemeinsames Sicherheitsniveau einigt. Hierin sehe ich ein derzeit starkes Gefälle und die Gefahr des Misslingens;5 - Very important;5 - Very important;3 - Neutral;"Risiken in privaten Partnerschaften durch Beteiligung internationaler Investoren/Anteilseigner, die dann ggf. für Abfluss von Infos/Ergebnissen in den außereuropäischen Raum sorgen könnten.
Die politischen Entscheidungen in IT-Fragen und zur KI sollten kompetent getroffen werden, d.h. man setzt auf Qualifikation der Entscheidungsträger oder auf Beteiligung von externen Wissensträgern.";5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;mit gleichem Tempo wie die privatwirtschaftl. Förderung für KMUs Startups etc. muss es ebenso attraktiv sein, für die Behörden zu arbeiten, die sich mit den Risiken und Sicherheitsfragen, die mit dem Einsatz von KI aufkommen, auseinandersetzen und hier regulatorische Vorgaben machen und deren Einhaltung überwachen können. D.h. es braucht hier viele kluge und integre Köpfe, die sicherlich mit Startups oder in der Industrie viel mehr verdienen können. ;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;"durch Manipulation zugrundeliegender Daten kann die KI ""manipulierte""Ergebnisse liefern ";There is a need for a new legislation;;No;;;;wenn der KI für Einzelne oder Gruppen von Menschen existentielle Entscheidungen überlassen werden.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;Generell sollten neue Technologien auch zur Identifikation niemals ausgeschlossen werden, da sich mit neuen durchaus auch Verbesserungen des Sicherheitsniveaus u.v.a.m . erreichen lassen. Allerdings muss vor dessen Einsatz auch immer die bestehende Rechtslage und Vorschriften überprüft und ggf. angepasst werden. Eine pauschale Aussage, dass die bisherigen Leitlinien und Vorschriften ausreichend sind, ist daher nicht möglich.;Much;wenn es Vertrauen schafft, z.B. dass mit dem System auch gewisse Mindestanforderungen (zertifizierte Algorithmen, ständige Überprüfung, Sicherheit, Betreib,...) gewährleistet sind;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;Eine Kombinierung Vorab und Ex-Post Maßnahmen schafft die Gefahr, der Betriebsblindheit. Besser wären hier zwei unabhängige Instanzen. Beide Maßnahmen sehe ich als zwingend erforderlich, Wissensaustausch ja, aber immer die Unabhängigkeit muss immer gewährleistet sein.;Mental health risks;;Yes;;Yes;Bsp. Autonomes Fahren: wenn dies eine Funktionalität des Fahrzeugs ist, muss die Haftungsfrage bei Fehlfunktion geklärt werden zumal ggf. irgendwann gar kein menschlicher Fahrzeugführer mehr erforderlich sein wird.;Yes, for specific AI applications;Immer dann, wenn eine Funktionalität die heutige Verantwortung des Anwenders beeinflusst;;
F529163;19-05-2020 10:47;English;;;;;;;;;The feedback can be published in an anonymous way;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;Biometric identification systems should never be allowed in publicly accessible spaces;;;;;;;;;;;;;;;;
F529162;19-05-2020 10:44;English;Company/Business organisation;Guillermo;Errezil;;Guretruck SL;;Medium (< 250 employees);Spain;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;"Formal verification of software (FV). EU could be world leader in this ambit & we have mathematical tools in EU. By developing definition of “Software Homologation” using FV we could: (1) regulate what software can do it (Formal Specification); (2) that the Formal Specification is correct;& (3) prove that software works following Formal Specification. EU would legislate the FV concept, protecting citizens’ rights against mercy of unregulated software developers & ensuring control & regulation.";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;AI software must be Formally Verified, as should be any public software, for the reasons mentioned above. ;There is a need for a new legislation;;Yes;;Yes;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;AI software must be Formally Verified, as should be any public software, for the reasons mentioned above. ;;;Yes;;Yes;;Yes, for all AI applications;;;
F529161;19-05-2020 10:11;German;Business Association;Philipp;EHMANN;;eco - Association of the Internet Industry e.V.;483354220663-40;Small (< 50 employees);Germany;The feedback can be published with your personal information;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;Strengthening digital infrastructures;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;Fostering open standards;4 - Important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;;3 - Neutral;3 - Neutral;3 - Neutral;2 - Not important;2 - Not important;4 - Important;;Current legislation is fully sufficient;;Other;see additional comments in attachment;;;;2 - Not important;2 - Not important;3 - Neutral;3 - Neutral;2 - Not important;2 - Not important;No further guidelines or regulations are needed;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;;;No opinion;;No;;Yes, for all AI applications;;;20200518_eco_POS_Wei_buch_K_nstliche_Intelligenz.pdf
F529160;19-05-2020 09:21;French;EU Citizen;Jerome;Bruneau;;;;;France;The feedback can be published with your personal information;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;;4 - Important;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;3 - Neutral;;4 - Important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;;4 - Important;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;"Le traitement ou la décision prise par un automate IA doivent être auditables et explicables par un expert. En cas de litige, la preuve du raisonnement doit être apportée.
Dans la fourniture d'un service en ligne, le demandeur (client, usager) doit pouvoir choisir entre un bot et un humain pour interlocuteur. Aucun service ne doit être fourni par le seul moyen d'un bot, l'accès à un humain doit toujours être possible.";Current legislation may have some gaps;;Yes;;Yes;;;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;4 - Important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Rather not;;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;;Personal security risks;;Yes;;No opinion;;Yes, for specific AI applications;santé, services financiers, assurances, sécurité, transport;;IA.pdf
F529159;19-05-2020 00:24;German;;;;;;;;;The feedback can be published in an anonymous way;;;;;;;;;;;;;;;;;;;;;;;;;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;;;;;;;;;;;;;;;Biometric identification systems should never be allowed in publicly accessible spaces;;;;;;;Risks related to the loss of connectivity;;;;;;;;;
F529158;18-05-2020 22:45;English;EU Citizen;Ruediger;KOENIG;;;;;Germany;The feedback can be published with your personal information;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;"developing public appreciation and acceptance with pan-European CITIZEN panels, ""juries"" etc. (working closely with EU Parliament. Also consider lessons learned e.g. from ""ENEF"". Note: Citizen particiapation sholud not be a back-door to NGOs and other institutions and special interests";3 - Neutral;5 - Very important;;3 - Neutral;4 - Important;;Financing: not with public funds but by incentivizing private finance/equity (reducing tax barriers,  and improving other single market considerations, etc.);5 - Very important;5 - Very important;4 - Important;;3 - Neutral;5 - Very important;5 - Very important;4 - Important;4 - Important;;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;data security (foreign interests);There is a need for a new legislation;;Yes;;Yes;;;3 - Neutral;4 - Important;5 - Very important;4 - Important;3 - Neutral;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Compliance of non-high-risk applications with the identified requirements should be self-assessed ex-ante;Mental health risks;Safety and liability framework should follow the benchmarks set e.g. by nuclear safety and liability regime and regulation and incl. 3rd party liability;Yes;;Yes;Safety and liability framework should follow the benchmarks set e.g. by nuclear safety and liability regime and regulation and incl. 3rd party liability;Yes, for all AI applications;;Safety and liability framework should follow the benchmarks set e.g. by nuclear safety and liability regime and regulation and incl. 3rd party liability;
F529157;18-05-2020 20:46;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Reduce redtape, bureaucracy and encourage innovation;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;Reduce redtape;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;;There is a need for a new legislation;;Yes;;Yes;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);With limited control and access by humans and government;Very much;;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;;Mental health risks;;Yes;;Yes;;Yes, for specific AI applications;;;
F529156;18-05-2020 17:44;French;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;4 - Important;4 - Important;5 - Very important;3 - Neutral;3 - Neutral;;5 - Very important;2 - Not important;2 - Not important;5 - Very important;4 - Important;3 - Neutral;;5 - Very important;4 - Important;4 - Important;;3 - Neutral;3 - Neutral;3 - Neutral;5 - Very important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;Current legislation is fully sufficient;;No;;;;;4 - Important;3 - Neutral;5 - Very important;2 - Not important;5 - Very important;5 - Very important;No further guidelines or regulations are needed;Chaque Etat membre peut décider des orientations à donner;Very much;;Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;;Personal security risks;;No opinion;;No opinion;;Yes, for specific AI applications;;;Commentaires__Consultation_IA.DOCX.pdf
F529155;18-05-2020 17:17;German;Other;Dr. Rupert;Wolff;;Österreichischer Rechtsanwaltskammertag (ÖRAK / Austrian Bar);29642463540-93;Small (< 50 employees);Austria;The feedback can be published with your personal information;No opinion;No opinion;4 - Important;No opinion;No opinion;No opinion;;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;Siehe angeschlossenes Dokument.;No opinion;No opinion;No opinion;;No opinion;No opinion;No opinion;No opinion;No opinion;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Siehe angeschlossenes Dokument.;No opinion;;No opinion;;;;Der ÖRAK unterstützt grundsätzlich die Annahme der Europäischen Kommission, dass die Anwendung von künstlicher Intelligenz in der Justiz als mit hohem Risiko verbunden eingestuft werden soll und daher eine vorherige Prüfung erfolgen muss.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;"Im Hinblick auf die Nutzung von biometrischer Fernidentifikation ist eindringlich vor möglichen Gefahren zu warnen. Die Versuchung, diese Technologie immer weiter zu nutzen, sobald sie einmal im Einsatz ist, ist evident. Es ist unbedingt erforderlich, z.B. jegliche Art von „social scoring“ zu verhindern. 

Auch ist eine Debatte notwendig, wie Datennutzung präventiv (präventive Polizeiarbeit, geheimdienstliche Tätgkeiten) und wie sie repressiv (Strafverfolgungsbehörden) erfolgen darf. Es muss diskutiert werden, inwieweit geheimdienstliche Erkenntnisse in die Strafverfolgung Eingang finden dürfen. Hierbei ist vor allem auch darauf zu achten, dass ausreichender Rechtsschutz gewährleistet wird. 

Auch aus der berufsrechtlichen Sicht der Anwaltschaft stellen sich Fragen im Hinblick auf den Schutz der Mandanten. Sollten zB Eingänge zu Gebäuden mit Anwaltskanzleien (ebenso zB Ärzten) grundsätzlich im Hinblick auf das Verschwiegenheitsgebot nicht von Maßnahmen der biometrischen Fernidentifikation erfasst sein? ";No opinion;;No opinion;;Siehe angeschlossenes Dokument.;;Siehe angeschlossenes Dokument.;;Siehe angeschlossenes Dokument.;;Siehe angeschlossenes Dokument.;;;Siehe angeschlossenes Dokument.;_RAK-Stellungnahme_Konsultation_Wei_buch_K_nstliche_Intelligenz_final_Have_your_say.pdf
F529154;18-05-2020 16:11;Italian;Business Association;Luigi;Degan;;Alleanza Lavoro;294727938196-92;Micro (< 10 employees);Italy;The feedback can be published with your personal information;5 - Very important;;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;;5 - Very important;4 - Important;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;Sarebbe importante avere più di un laboratorio di riferimento;2 - Not important;3 - Neutral;5 - Very important;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;Preoccupazioni, in merito alla diffusione dell’IA, legate alla ’esclusione di parti della popolazione dalle nuove attività lavorative e sociali e la disumanizzazione delle azioni.;There is a need for a new legislation;;Other;E’ di estrema importanza il ruolo fondante del principio di precauzione;;;E’ necessario un approccio che si riferisca alle attività monopolistiche delle istituzioni pubbliche e private, soprattutto di quelle che possono determinare la violazione dei diritti fondamentali, perché ci siano previsioni più stringenti e con il ricorso all’intervento umano in ultima istanza. In particolare è necessario che nel settore della giustizia e in quello della produzione dell’applicazione ed esecuzione di normative generali ed astratte, l’intervento umano non possa essere escluso. ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Other special requirements in addition to those mentioned in the question above should be imposed (please specify);Dovrebbero essere strettamente regolamentate le finalità di impiego e utilizzo e la possibilità di impedire uso non consentito. ;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;Il rischio delle inefettività del diritto è elevatissimo e, quindi, è necessario non escludere l’autonomia umana nell’utilizzo dell’IA.;Yes;;Yes;;;;;
F529153;18-05-2020 12:48;Polish;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Zapoznanie ze sztuczn? inteligencj? dzieci i osób z grupy 60+;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;Current legislation may have some gaps;;No;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;Other special requirements in addition to those mentioned in the question above should be imposed (please specify);Nalezy podj?? dzia?ania aby nie dosz?o do sytuacji znanej z Chin;Very much;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Personal security risks;;Yes;;No opinion;;Yes, for all AI applications;;;
F529152;18-05-2020 11:31;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;5 - Very important;3 - Neutral;4 - Important;3 - Neutral;"It is of paramount importance that the EU creates a task force to analyze, control and   prevent any harmful or unintended negative consequence that might derive from the application of AI systems both on a micro and macro-scale.
";5 - Very important;5 - Very important;4 - Important;3 - Neutral;5 - Very important;5 - Very important;;5 - Very important;4 - Important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;No;;;;"compulsory requirements for AI system should be applied across the board

";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Risks related to the loss of connectivity;;Yes;;Yes;;Yes, for all AI applications;;;
F529151;18-05-2020 10:51;English;EU Citizen;Greg;Elliott;;;;;United Kingdom;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;Development of an accreditation system for AI applications, covering: (i) input content, (ii) the algorithms themselves, (iii) output content, (see attached PDF).;4 - Important;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;A pan-European accreditation system for AI applications to avoid fragmenting the Digital Single Market.;5 - Very important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;Supporting SME's in information accounting and auditing, (see attached PDF).;4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;5 - Very important;AI's internal model of reality is based on input data that may not match reality.;There is a need for a new legislation;;No;;;;;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;No opinion;;Very much;Voluntary labelling should be considered as a stepping-stone towards eventual mandatory labelling, (see attached PDF on indicative 5-Star rating system).;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Organisations should be subject to a regular audit of their information accounting, (in the same way that they are vis-a-vis their financial accounting).;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F529150;18-05-2020 10:35;Spanish;Other;Isidor;Garcia;;Ilustre Colegio de la Abogacia de Barcelona;;Medium (< 250 employees);Spain;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;DOC_CTD_IA_LIBRO_BLANCO_Vfinal.pdf
F529149;18-05-2020 10:25;German;;;;;;;;;The feedback can be published in an anonymous way;2 - Not important;2 - Not important;4 - Important;5 - Very important;2 - Not important;1 - Not important at all;;4 - Important;2 - Not important;1 - Not important at all;1 - Not important at all;2 - Not important;1 - Not important at all;;1 - Not important at all;1 - Not important at all;2 - Not important;;4 - Important;3 - Neutral;3 - Neutral;2 - Not important;1 - Not important at all;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;There is a need for a new legislation;;No;;;;;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Risks related to the loss of connectivity;;Yes;;Yes;;Yes, for all AI applications;;;
F529148;18-05-2020 09:40;Spanish;NGO (Non-governmental organisation);PILAR;VILLARINO;;El CERMI se crea para ser la plataforma unitaria de incidencia política que aporta valor añadido al movimiento social de la discapacidad. Orienta su actuación bajo el principio de unidad y cohesión de la discapacidad organizada. www.cermi.es;708719831695-59;Micro (< 10 employees);Spain;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;El despliegue y gestión de la IA ha de estar sometido a procedimientos democráticos de gobernanza, que garanticen la transparencia, la rendición de cuentas y la participación de todos los grupos de interés en la toma de decisiones y en la validación de soluciones. Las personas con discapacidad y sus organizaciones representativas son un grupo de interés legítimo de la IA que ha de tener asegurado un rol en el gobierno de estos sistemas. ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Visibilizar la discapacidad e incluirla en el desarrollo de reglas uniformes y principios éticos sobre el uso de sistemas de IA ;5 - Very important;5 - Very important;5 - Very important;Es esencial tener presente el riesgo de discriminación a grupos ciudadanos en situación de mayor vulnerabilidad, tales como las personas con discapacidad. Debe darse entrada a la participación de las entidades del tercer sector social de la discapacidad.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;"Es esencial tener presente de manera transversal  enfoques específicos basados en el principio de igualdad y no discriminación;  además de tomar en consideración los principios de diseño para todas las personas y la accesibilidad universal.";4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;Se echa en falta la consideración de los riesgos de vulneración de los derechos humanos en especial respecto de grupos sociales como los de las personas con discapacidad;Other;Los principios, valores y mandatos de la Convención Internacional sobre los Derechos de las Personas con Discapacidad de 2006 y los Objetivos de Desarrollo Sostenible/Agenda 2030 de Naciones Unidas han de constituir en todo caso el marco referencial y de prescripición de la IA en relación con las personas con discapacidad;Other;Se considera prioritario visibilizar la discapacidad e incluirla en el desarrollo de reglas uniformes y principios éticos sobre el uso de sistemas de IA ;;;" justificar la selección genética de personas sin discapacidad;  identificar y eventualmente discriminar a personas con discapacidad; sistemas de IA basados en modelos  que excluyan o no tengan en cuenta las personas con discapacidad;  datos que incluyan estereotipos, sesgos y prejuicios respecto de la discapacidad;no permitir la participación o toma de decisiones de personas con discapacidad; sistemas IA dirigidos a las personas con discapacidad no probados y validados por las propias PcD.  ";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Other special requirements in addition to those mentioned in the question above should be imposed (please specify);Los principios, valores y mandatos de la Convención Internacional sobre los Derechos de las Personas con Discapacidad de 2006 y los Objetivos de Desarrollo Sostenible/Agenda 2030 de Naciones Unidas han de constituir en todo caso el marco referencial y de prescripición de la IA en relación con las personas con discapacidad. ;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;Cualquier legislación debe tener presente los mandatos de la Convención sobre los Derechos de las Personas con Discapacidad;Yes;La valoración de las personas usuarias debe ser considerada;Yes;Los principios, valores y mandatos de la Convención Internacional sobre los Derechos de las Personas con Discapacidad de 2006 y los Objetivos de Desarrollo Sostenible/Agenda 2030 de Naciones Unidas han de constituir en todo caso el marco referencial y de prescripición de la IA en relación con las personas con discapacidad. ;Yes, for all AI applications;;;Contribuci_n_CERMI_consulta_p_blica_europea_sobre_IA.docx
F529147;18-05-2020 09:29;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;4 - Important;4 - Important;2 - Not important;5 - Very important;;4 - Important;5 - Very important;3 - Neutral;4 - Important;5 - Very important;No opinion;;5 - Very important;4 - Important;2 - Not important;;5 - Very important;4 - Important;5 - Very important;2 - Not important;3 - Neutral;;5 - Very important;5 - Very important;4 - Important;1 - Not important at all;4 - Important;3 - Neutral;;There is a need for a new legislation;;No;;;;;3 - Neutral;5 - Very important;4 - Important;4 - Important;3 - Neutral;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;Yes;;Yes, for all AI applications;;;
F529146;17-05-2020 23:06;French;;;;;;;;;The feedback can be published in an anonymous way;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;;1 - Not important at all;1 - Not important at all;1 - Not important at all;;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;Yes;;No opinion;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Not at all;;Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F529145;17-05-2020 20:22;Spanish;EU Citizen;Ruth;Sala;;;;;Spain;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;;Current legislation may have some gaps;;Other;;;;"el deep learning es uno de los componentes ""opacos"" de la IA que puede plantear mayores problemas para ser auditado.";4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);Deberá atenderse a criterios de proporcionalidad.;Rather not;El etiquetado voluntario no tiene ningún valor, más teniendo en cuenta que, a día de la fecha, muchos productos tienen etiqueta falsificada de la Comunidad Europea, el hecho de facilitar un etiquetado voluntario sin ser posible contrastarlo con un registro de autenticidad a nivel Europeo, conforme se ha pasado una supervisión mínima, no tendría sentido alguno.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Los desarrollos en los que se vea impliacada la IA relacionándose con seres humanos en la oferta de productos o servicios, debería ser auditable desde su desarrollo hasta su comercialización e incluso de forma periódica.;Risks related to the loss of connectivity;;Yes;;Yes;;Yes, for all AI applications;;;
F529144;17-05-2020 17:52;English;;;;;;;;;The feedback can be published in an anonymous way;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;Biometric identification systems should never be allowed in publicly accessible spaces;;;;;;;;;;;;;;;;
F529143;17-05-2020 17:42;English;NGO (Non-governmental organisation);Paul;MACDONNELL;;Global Digital Foundation;352788324603-77;Micro (< 10 employees);Ireland;The feedback can be published with your personal information;3 - Neutral;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;;3 - Neutral;1 - Not important at all;3 - Neutral;2 - Not important;4 - Important;4 - Important;;3 - Neutral;3 - Neutral;3 - Neutral;;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;;5 - Very important;2 - Not important;3 - Neutral;2 - Not important;2 - Not important;3 - Neutral;;Current legislation may have some gaps;;Yes;;No;;;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;4 - Important;No further guidelines or regulations are needed;;Not at all;These are likely to be too restrictive on innovation and on the importation of AI systems.;Other enforcement system;AI that my affect safety needs ex ante and ex post review. But AI that may affect human rights should be subject to ex post review and only then if a problem has been raised.;;;;Yes;;Yes;Yes but only in so far as they may affect safety.;No;;;
F529142;17-05-2020 17:35;German;;;;;;;;;The feedback can be published in an anonymous way;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;Nein zu künstlicher Intelligenz;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;Nein zu künstlicher Intelligenz;1 - Not important at all;1 - Not important at all;1 - Not important at all;Nein zu künstlicher Intelligenz;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;Nein zu künstlicher Intelligenz ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Nein zu künstlicher Intelligenz ;No opinion;;;;;;Datenschutz und Privatsphäre sind nicht gewährleistet  ;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;Biometric identification systems should never be allowed in publicly accessible spaces;Nein zu künstlicher Intelligenz ;Very much;Nein zu künstlicher Intelligenz ;No opinion;;Nein zu künstlicher Intelligenz ;Mental health risks;Gefahr der Überwachung, sowie Verlust von Datenschutz, Privatsphäre und Selbständigkeit;No opinion;Nein zu künstlicher Intelligenz ;No opinion;Nein zu künstlicher Intelligenz ;No opinion;;Nein zu künstlicher Intelligenz ;
F529141;17-05-2020 17:34;German;;;;;;;;;The feedback can be published in an anonymous way;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;Nein zu KI;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;Nein zu KI;1 - Not important at all;1 - Not important at all;1 - Not important at all;Nein zu KI;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;Nein zu KI;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Nein zu KI;No opinion;;;;;;Privatsphäre und Datenschutz nicht mehr gewährleistet;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;Biometric identification systems should never be allowed in publicly accessible spaces;Nein zu KI;Very much;Nein zu KI;No opinion;;Künstliche Intelligenz darf nicht zur Überwachung (aus-)genutzt werden;Mental health risks;Gefahr der Überwachung, sowie Verlust von Datenschutz, Privatsphäre und Selbstständigkeit;No opinion;Nein zu KI;No opinion;Nein zu KI;No opinion;;Nein zu KI;
F529140;17-05-2020 17:14;German;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;4 - Important;2 - Not important;2 - Not important;;5 - Very important;5 - Very important;4 - Important;3 - Neutral;4 - Important;4 - Important;;5 - Very important;4 - Important;2 - Not important;;4 - Important;4 - Important;4 - Important;2 - Not important;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;Current legislation may have some gaps;;No opinion;;;;;4 - Important;3 - Neutral;5 - Very important;4 - Important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;Hier ist der Schritt zu Totalüberwachung nur noch ein Kleiner;Very much;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Personal security risks;;Yes;;Yes;;Yes, for all AI applications;;;
F529139;17-05-2020 16:20;Hungarian;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;3 - Neutral;4 - Important;4 - Important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;4 - Important;;4 - Important;5 - Very important;4 - Important;;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;4 - Important;No opinion;5 - Very important;;There is a need for a new legislation;;Yes;;No opinion;;;5 - Very important;No opinion;3 - Neutral;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;No opinion;;No opinion;;;
F529138;17-05-2020 11:10;English;EU Citizen;Maarten;Hotze;;;;;Netherlands;The feedback can be published with your personal information;5 - Very important;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;2 - Not important;Whitepaper lacks balance to implement a level playing field (internal vs external to EU) with regard to regulation and promotion.;2 - Not important;2 - Not important;2 - Not important;2 - Not important;2 - Not important;2 - Not important;;2 - Not important;2 - Not important;2 - Not important;;2 - Not important;2 - Not important;2 - Not important;2 - Not important;2 - Not important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;;There is a need for a new legislation;;Other;"The whitepaper only defines two cases of high-risk: recruiting and remote biometric identification, while leaving open the details the two cumulative criteria which ""sectors"" are considered and what is meant by  ""risks"". ";;;"I agree with the general statement of high-risks ""protection of safety, consumer rights and fundamental rights"", but the whitepaper lacks clear definitions. 
Moreover, more granularity is needed to define risks in general. Take for instance manipulation of AI engines in social media, influencing elections. In general, is there a need for a new fundamental right: the right not to be influenced by AI? ";3 - Neutral;3 - Neutral;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Very much;"Objection to this question: high-risk is not well defined, so how to answer this question? If it is well defined however, voluntary labeling is not needed. 
e.g. etnic profiling by AI, would that be considered low-risk?";Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;"A labeling system is a good idea; although it falls short when clear criteria are lacking. ";Mental health risks;"Privacy
Freedom of choice (e.g. manipulation of choice)
Profiling";Yes;;No opinion;;Yes, for all AI applications;;"How about liability for AI systems originating outside of EU?
How about fundamental rights, or high-risk items originating outside EU? So do not only consider ""damage"" here, use the previous definition of ""high-risk"" items. ";
F529137;17-05-2020 10:28;English;Other;James;Burnie;;gunnercooke (a law firm);;Large (250 or more);United Kingdom;The feedback can be published with your personal information;4 - Important;3 - Neutral;4 - Important;5 - Very important;5 - Very important;4 - Important;;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;No opinion;;4 - Important;5 - Very important;4 - Important;Assist firms creating interoperability standards – these allow AI systems to communicate between each other, and consequently enable economies of scale with out requiring uniformity of provider (avoiding competition issues) or concentrating risk. ;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;Consider “approving” certain new forms of AI to give existing participants comfort it meets certain standards and so encourage its use. ;4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;4 - Important;Existing firms should be encouraged to recognise the need to have sufficient Know Your Tech (“KYT”) when outsourcing functions to AI, in particular to be able to monitor the AI they use and identify when it is not functioning properly / delivering perverse outcomes. Senior management should be aware of potential issues and the Chief Technology Officer should have appropriate powers and authority within the company. ;Current legislation is fully sufficient;;Yes;;No opinion;;;5 - Very important;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;No opinion;;Much;;Other enforcement system;An initial assessment should be made regarding the potential for bad outcomes, and, in the event of a bad outcome, the level of harm. For low risk / low harm AI, ex-post assessment makes sense. For higher risk / higher harm, ex-ante test make sense, so long as these are proportionate. These could, in high risk cases, include use of a sandbox test. ;;Personal security risks;;Yes;;No opinion;;No;;Existing rules generally allow for compensation in the event of harm, and as such it is not these that should be altered. The real issue is the ability of smaller firms to pay in the event of harm. Many start-up are using insurance to assist with this, which provides reputational comfort, as the firms need to meet certain requirements before they can be insured, as well as financial assistance if a problem develops. This insurance market is to be encouraged.;
F529136;17-05-2020 10:21;English;EU Citizen;Mando;Rotman;;;;;Netherlands;The feedback can be published with your personal information;4 - Important;5 - Very important;3 - Neutral;3 - Neutral;5 - Very important;5 - Very important;Protect AI champion companies in EU from acquisition by non-EU parties. Focus GDPR on personal control over data, not on restriction.;5 - Very important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;;5 - Very important;4 - Important;4 - Important;;3 - Neutral;3 - Neutral;3 - Neutral;4 - Important;4 - Important;;5 - Very important;4 - Important;4 - Important;5 - Very important;3 - Neutral;2 - Not important;;Current legislation may have some gaps;;Yes;;Yes;;AI applications trained on data representing only a subset of the population (e.g. social media) but applied to the full population (e.g. job screening, product development etc), therewith enforcing a single society view and introducing bias that is hard to measure.;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);Clear governance and administration of access to and use of the data by (government) personnel. Highly robust (Cyber)security measures!;Not at all;Do not go looking for more ways to create more regulation, laws and policies. Focus on stimulating and supporting research and application, while ensuring against only the biggest risks.;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Cyber risks;;Yes;;No opinion;;No opinion;;;
F529135;17-05-2020 05:56;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;5 - Very important;;2 - Not important;1 - Not important at all;1 - Not important at all;3 - Neutral;4 - Important;5 - Very important;;4 - Important;5 - Very important;3 - Neutral;;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;;4 - Important;5 - Very important;3 - Neutral;4 - Important;4 - Important;5 - Very important;;There is a need for a new legislation;;Yes;;Yes;;;4 - Important;4 - Important;5 - Very important;4 - Important;3 - Neutral;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Much;;No opinion;;;Mental health risks;;No opinion;;No opinion;;No opinion;;;
F529134;17-05-2020 01:59;English;;;;;;;;;The feedback can be published in an anonymous way;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;Biometric identification systems should never be allowed in publicly accessible spaces;;;;;;;;;;;;;;;;
F529133;17-05-2020 00:13;Danish;;;;;;;;;The feedback can be published in an anonymous way;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;Biometric identification systems should never be allowed in publicly accessible spaces;;;;;;;;;;;;;;;;
F529132;16-05-2020 22:35;German;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;;No opinion;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;No opinion;No opinion;5 - Very important;;4 - Important;5 - Very important;5 - Very important;4 - Important;No opinion;;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;;There is a need for a new legislation;;Yes;;No opinion;;;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;No opinion;;No opinion;;;
F529131;16-05-2020 21:43;French;EU Citizen;carina;otero;;;;;France;The feedback can be published with your personal information;3 - Neutral;4 - Important;2 - Not important;3 - Neutral;3 - Neutral;3 - Neutral;;;;;;;;;;;;;;;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;L'intelligence artificielle pourrait être utile à l'homme cependant t je m'inquiète des impunité évidentes de ceux qui pourraient l'utiliser pour torturer détruire... Voir violences des services secrets et polices. ;Other;La législation actuelle n'est même pas respectée par ceux qui sont t sensé la faire respecter. Les textes ne suffisent pas ;Other;Il faudrait œuvrer à ce que cette ia soit vraiment maîtrisée est utilisée hors système occulte secrets hors la lois. La transparence. Qui surveillerait la bonne utilisation des ia ? Comment éviter quelle ne puisse pas servir pour harceler espionner mentir sur la vérité ? Comment éviter qu'elle ne sert e pas à manipuler influencer la population ;;;;3 - Neutral;3 - Neutral;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;Ah non merci demande pourquoi à George orwell je veux pouvoir rêvasser tranquillement sans  ressentir l'œil vicieux de big brother. ;Very much;Toutes les ia mal utilisées sont à très haut risques;Other enforcement system;C'est compliqué la question est encore qui surveillerait les surveillants qui surveille la police par exemple en France c'est la police. Il est impossible dans le système actuelle de nous garantir la 'bonne' utilisation sécurisée des ia. Personne ne surveille les services secrets par exemple. ;Je ne sais pas mais aujourd'hui certains puissants peuvent détournés tous objet électronique en mico caméra poisons etc de manière INVISIBLES. ;;'garantir que tous les produits et services, y compris ceux qui intègrent des technologies numériques émergentes, fonctionnent de manière sûre, fiable et cohérente'vous prétendez vraiment pouvoir garantir l' utilisations des ia ??? Combien de nous au fait sommes surveillé sans autorisation combien de nos famille amis enfants ? Je ne me rappel pas du chiffre ! Hahaha. Cela ne garantie pas ma sécurité de savoir qu une personnel un peu doué en informatique peut m' épier chez moi épier mon fils ;Yes;Aujourd'hui une personne peut accéder à la totalité de votre vie sur votre ordi. Il peut vous nuire sans laisser de traces s'il fait partie des personnes influentes. ;Yes;Il faudrait déjà veiller a ce que la législation soit respectée par les instances par la police par les services secrets que prétendez vous garantir avec des textes non respectés. On ne savait même pas que l'on était susceptible d'être surveillé chez soit en prenant une douche etc... ;No opinion;;;
F529130;16-05-2020 20:59;English;Academic/Research Institution;faiz;ikramulla;;maryville university;;Large (250 or more);United States;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;4 - Important;5 - Very important;;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;;3 - Neutral;5 - Very important;5 - Very important;;3 - Neutral;5 - Very important;5 - Very important;4 - Important;5 - Very important;;3 - Neutral;3 - Neutral;4 - Important;5 - Very important;3 - Neutral;3 - Neutral;;There is a need for a new legislation;;No opinion;;;;"i think technical standardization would be more helpful than legislation.  standards are ""compulsary requirements"" in a way. - just not by a specific government entity, although governments can leverage off such standards to enforce how they deem appropriate.  standards help benchmarking and comparison, and increase quality by default!";4 - Important;3 - Neutral;5 - Very important;5 - Very important;5 - Very important;No opinion;No opinion;again, here i believe that high quality biometric systems should/can be deployed, if they can meet some technical standard, over time and regularly checked.;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;these are all important for the public to be aware of. glad you are considering all of these.;Yes;"absolutely, ""unintended consequences"" can be not good...";No opinion;i am not familiar enough with that, and how it has been applied to SW systems.;No opinion;;i am not sure / not familiar;
F529129;16-05-2020 19:10;Romanian;EU Citizen;Cosmin;Balan;;;;;Romania;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);;;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;;;;;;;;;;
F529128;16-05-2020 17:34;English;;;;;;;;;The feedback can be published in an anonymous way;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;Biometric identification systems should never be allowed in publicly accessible spaces;;Not at all;;;;;;;;;;;;;;
F529127;16-05-2020 15:09;English;;;;;;;;;The feedback can be published in an anonymous way;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;Biometric identification systems should never be allowed in publicly accessible spaces;;;;;;;;;;;;;;;;
F529126;16-05-2020 15:05;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;4 - Important;4 - Important;2 - Not important;5 - Very important;;5 - Very important;;5 - Very important;4 - Important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;2 - Not important;;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;;3 - Neutral;5 - Very important;3 - Neutral;5 - Very important;3 - Neutral;5 - Very important;;There is a need for a new legislation;;Yes;;Yes;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);;Rather not;;Other enforcement system;;;Personal security risks;;;;;;;;;
F529125;16-05-2020 14:36;English;NGO (Non-governmental organisation);Paul;Beyer;;FSD Fahrzeugsystemdaten GmbH;339316636029-30;Medium (< 250 employees);Germany;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;international association of AI ;4 - Important;5 - Very important;3 - Neutral;4 - Important;5 - Very important;3 - Neutral;;5 - Very important;4 - Important;4 - Important;;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;;There is a need for a new legislation;;Yes;;Yes;;autonomous weapons, autonomous driving, social scoring;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;ex-ante and ex-post enforcement mechanisms should be independent from developer;Mental health risks;;Yes;;Yes;;Yes, for specific AI applications;autonomous driving (sae levels 3-5);;
F529124;16-05-2020 13:58;German;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;5 - Very important;2 - Not important;1 - Not important at all;4 - Important;- Nicht im militären Bereich, da unschuldige Menschenleben abhängen können. - Die Wirtschaft darf nicht die Datenhoheit haben und muss strikte Datenschutzvorgaben einhalten. - Big Data-Analysen dürfen nur durchgeführt werden, wenn diese zuvor von der Quel;4 - Important;4 - Important;4 - Important;1 - Not important at all;4 - Important;5 - Very important;"KI muss vor allen Dingen in öffentlicher Hand bleiben mit fest definierten Überwachungsgremien, da die Wirtschaft (Facebook, Google und Co.) in der Vergangenheit im negativen Sinne die schiere Masse an der Analyse von persönlichen Daten verarbeitet und in den verschiedenen ""kostenlosen"" Diensten gezeigt hat.";4 - Important;4 - Important;1 - Not important at all;;3 - Neutral;4 - Important;2 - Not important;2 - Not important;2 - Not important;;5 - Very important;5 - Very important;4 - Important;4 - Important;3 - Neutral;5 - Very important;- Enge Grenzen ziehen, in denen KI verwendet werden darf. - In den falschen Händen kann sehr viel Schaden entstehen.;There is a need for a new legislation;;Other;Alle Rechtsvorschriten sollten für alle Bereiche der KI-Anwendung gleichermaßen gelten.;;;Militär und Krieg, psychologische Schlussfolgerungen eines Patienten, ethnische Menschheitsfragen;4 - Important;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;Die biometrische Gesichtserkennung ist sehr fehleranfällig und verstößt maßiv gegen die Grundrechte von europäischen Bürgern und Besucher aus Drittstaaten.;Not at all;Nein. Ich bin für eine jährliche Überprüfung einer Untersuchungseinrichtung, das entscheidet, ob Forschungseinrichtungen / die öffentliche Hand / das Unternehmen weiterhin KI-Forschung oder die KI-Anwendung betreiben darf.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Der Ethikrat sollte ebenfalls bewerten welche Kriterien gelten sollen zur Evaluierung der Marktüberwachung und Vorabbewertungs-Mechanismen sowie Ex-post-Durchsetzungsmechanismen.;Mental health risks;Politischer Missbrauch der KI-Datenanalysen für ihre eigenen Zwecke um Wähler zu manipulieren. Die Werbung würde in noch mehr bereichen im Leben vordringen und zu mehr eigentlich ungewollten Käufen führen. Weniger eigenständiges Denken und Infragestellen bei KI-Auswertungen.;Yes;;Yes;;Yes, for all AI applications;;;
F529123;16-05-2020 13:58;English;;;;;;;;;The feedback can be published in an anonymous way;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;4 - Important;5 - Very important;;4 - Important;3 - Neutral;2 - Not important;1 - Not important at all;2 - Not important;3 - Neutral;;1 - Not important at all;1 - Not important at all;1 - Not important at all;;1 - Not important at all;4 - Important;1 - Not important at all;3 - Neutral;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;No;;;;;3 - Neutral;3 - Neutral;5 - Very important;3 - Neutral;5 - Very important;4 - Important;Biometric identification systems should never be allowed in publicly accessible spaces;;No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;No opinion;;No opinion;;Yes, for all AI applications;;;
F529122;16-05-2020 13:30;English;;;;;;;;;The feedback can be published in an anonymous way;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;Biometric identification systems should never be allowed in publicly accessible spaces;;;;;;;;;;;;;;;;
F529121;16-05-2020 13:09;English;;;;;;;;;The feedback can be published in an anonymous way;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;Biometric identification systems should never be allowed in publicly accessible spaces;;;;;;;;;;;;;;;;
F529120;16-05-2020 10:53;Danish;EU Citizen;Rasmus;Vedel;;;;;Denmark;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;3 - Neutral;4 - Important;4 - Important;Jo fler jo bedre. Der er formentlig kun meget få der ikke vil have glæde af at gå ind i samarbejdet!;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;;5 - Very important;4 - Important;5 - Very important;;3 - Neutral;4 - Important;3 - Neutral;4 - Important;4 - Important;;5 - Very important;4 - Important;5 - Very important;2 - Not important;4 - Important;5 - Very important;Der skal altid være en MULIGHED FOR en menneskelig vurdering;Current legislation may have some gaps;;Other;AI kan hjælpe meget i mange situationer, men der skal altid være MULIGHED FOR en Human vurdering;;;Irrervasible handlinger eller handlinger der skader folks omdømme som fx fængsling/off. udskamning;5 - Very important;5 - Very important;3 - Neutral;4 - Important;3 - Neutral;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;Vi er ikke Kina. Vi er frie mennesker der ikke skal overvåges i det offentlige rum!!! (Vi har allerede tlf og betalingskort);Rather not;Ai bruges allerede mange steder som beslutningsstøtte. Det skal være muligt at finde oplysningen, men Vi behøver ikke at vide alt hele tiden;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;No opinion;Det er vigtigt at vi (borgerne) oplever en respekt og har fortsat tillid til systemet. (Jeg kender ikke nuværende regler i dybden);Yes, for specific AI applications;;;
F529119;16-05-2020 10:50;English;EU Citizen;Carsten;Borch;;;;;Denmark;The feedback can be published with your personal information;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;;4 - Important;4 - Important;No opinion;4 - Important;No opinion;4 - Important;;4 - Important;4 - Important;4 - Important;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;5 - Very important;;Current legislation may have some gaps;;Yes;;No opinion;;;No opinion;No opinion;No opinion;No opinion;5 - Very important;No opinion;Biometric identification systems should never be allowed in publicly accessible spaces;;No opinion;;No opinion;;;Personal security risks;;Yes;;No opinion;;No opinion;;;
F529118;16-05-2020 10:38;German;;;;;;;;;The feedback can be published in an anonymous way;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;Biometric identification systems should never be allowed in publicly accessible spaces;;;;;;;;;;;;;;;;
F529117;16-05-2020 10:37;English;EU Citizen;Marie-Katharine;Traunfellner;;;;;Austria;The feedback can be published with your personal information;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;;4 - Important;3 - Neutral;4 - Important;2 - Not important;5 - Very important;4 - Important;;3 - Neutral;3 - Neutral;3 - Neutral;;4 - Important;4 - Important;4 - Important;5 - Very important;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;Yes;;No opinion;;;3 - Neutral;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;No opinion;;No opinion;;No opinion;;;
F529116;16-05-2020 09:58;French;EU Citizen;Hervé;Jaouen;;;;;France;The feedback can be published with your personal information;5 - Very important;4 - Important;4 - Important;5 - Very important;2 - Not important;1 - Not important at all;;3 - Neutral;2 - Not important;2 - Not important;2 - Not important;3 - Neutral;4 - Important;;4 - Important;2 - Not important;3 - Neutral;;2 - Not important;4 - Important;2 - Not important;3 - Neutral;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;Current legislation may have some gaps;;Other;Tout depends comment vous définissez le haut risque, ca peut être détourné !;;;;4 - Important;2 - Not important;1 - Not important at all;3 - Neutral;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;L'Europe n'est pas la Chine.;Rather not;;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F529115;16-05-2020 09:12;English;Non-EU Citizen;Vijay;Varathan;;;;;India;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;2 - Not important;3 - Neutral;;5 - Very important;5 - Very important;3 - Neutral;4 - Important;5 - Very important;No opinion;;;;;;;;;;;;;;;;;;;There is a need for a new legislation;;No;;;;;;;;;;;Biometric identification systems should never be allowed in publicly accessible spaces;;;;Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;;Mental health risks;;;;;;No opinion;;;
F529114;16-05-2020 09:09;German;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;3 - Neutral;5 - Very important;5 - Very important;2 - Not important;3 - Neutral;5 - Very important;;There is a need for a new legislation;;No;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;Yes;;Yes, for all AI applications;;;
F529113;16-05-2020 08:49;English;;;;;;;;;The feedback can be published in an anonymous way;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;Biometric identification systems should never be allowed in publicly accessible spaces;;;;;;;;;;;;;;;;
F529112;16-05-2020 07:52;Danish;EU Citizen;Alexandru Adrian;IANCU;;;;;Romania;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Protect citizen's rights to anonymity and (this one you know, of course) make sure you don't create imbalance within the EU with decision-making or it will blow up in all of our faces and bye-bye EU (don't do that :-) );5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;4 - Important;5 - Very important;AI = controllable or controlling. We will control it, of course. But who? Only a small group of people, I imagine. Not everyone will be able to or should be able to control wide-scale public infrastructure based on AI. Therefore, it will naturally be in the hands of a few. A few can, in turn, be controllable by whims and personal priorities. Citizen protests could, under dire circumstances, be obsolete when there's too much AI. Little AI development. Too much = the end of personal development;5 - Very important;3 - Neutral;5 - Very important;;2 - Not important;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;Change the scale from 'important' to 'should do', it's misleading;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;1 - Not important at all;;Current legislation may have some gaps;;No;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Risks related to the loss of connectivity;;Yes;;Yes;;Yes, for all AI applications;;;
F529111;16-05-2020 02:24;Italian;;;;;;;;;The feedback can be published in an anonymous way;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;;No opinion;No opinion;No opinion;;No opinion;No opinion;No opinion;No opinion;No opinion;;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;;No opinion;;No opinion;;;;;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;Biometric identification systems should never be allowed in publicly accessible spaces;;No opinion;;No opinion;;;;;No opinion;;No opinion;;No opinion;;;
F529110;16-05-2020 01:20;English;;;;;;;;;The feedback can be published in an anonymous way;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;Biometric identification systems should never be allowed in publicly accessible spaces;;;;;;;;;;;;;;;;
F529109;16-05-2020 01:16;English;;;;;;;;;The feedback can be published in an anonymous way;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;Biometric identification systems should never be allowed in publicly accessible spaces;;;;;;;;;;;;;;;;
F529108;16-05-2020 00:59;German;;;;;;;;;The feedback can be published in an anonymous way;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;Biometric identification systems should never be allowed in publicly accessible spaces;;;;;;;;;;;;;;;;
F529107;16-05-2020 00:59;German;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;;5 - Very important;5 - Very important;2 - Not important;2 - Not important;2 - Not important;2 - Not important;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;Biometric identification systems should never be allowed in publicly accessible spaces;;;;;;;;;;;;;;;;
F529106;16-05-2020 00:54;German;;;;;;;;;The feedback can be published in an anonymous way;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;Biometric identification systems should never be allowed in publicly accessible spaces;;;;;;;;;;;;;;;;
F529105;15-05-2020 23:48;German;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;;4 - Important;4 - Important;5 - Very important;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;;4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;;Current legislation may have some gaps;;Yes;;Yes;;;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Rather not;;Other enforcement system;;;Cyber risks;;Yes;;No;;Yes, for specific AI applications;;;
F529104;15-05-2020 23:47;English;;;;;;;;;The feedback can be published in an anonymous way;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;Biometric identification systems should never be allowed in publicly accessible spaces;;;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Mental health risks;;Yes;;;;;;;
F529103;15-05-2020 23:30;Danish;EU Citizen;Hans Paarup;Thomsen;;;;;Denmark;The feedback can be published with your personal information;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;;No opinion;No opinion;No opinion;;No opinion;No opinion;No opinion;No opinion;No opinion;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;4 - Important;;No opinion;;No opinion;;;;;No opinion;No opinion;No opinion;4 - Important;4 - Important;4 - Important;Biometric identification systems should never be allowed in publicly accessible spaces;;No opinion;;No opinion;;;Personal security risks;;Yes;;No opinion;;No opinion;;;
F529102;15-05-2020 23:30;English;;;;;;;;;The feedback can be published in an anonymous way;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;;No opinion;No opinion;No opinion;;No opinion;No opinion;No opinion;No opinion;No opinion;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;No opinion;;;;;No opinion;No opinion;No opinion;;5 - Very important;No opinion;Biometric identification systems should never be allowed in publicly accessible spaces;;No opinion;;No opinion;;;Mental health risks;;Yes;;Yes;;No opinion;;;
F529101;15-05-2020 23:22;Swedish;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;1 - Not important at all;;4 - Important;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;4 - Important;;4 - Important;4 - Important;4 - Important;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;There is a need for a new legislation;;Yes;;Yes;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Not at all;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F529100;15-05-2020 23:10;English;;;;;;;;;The feedback can be published in an anonymous way;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;Biometric identification systems should never be allowed in publicly accessible spaces;;;;;;;;;;;;;;;;
F529099;15-05-2020 23:07;Dutch;;;;;;;;;The feedback can be published in an anonymous way;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;Biometric identification systems should never be allowed in publicly accessible spaces;;;;;;;;;;;;;;;;
F529098;15-05-2020 22:48;Danish;EU Citizen;Katarina;Dominiak;;;;;Denmark;The feedback can be published with your personal information;5 - Very important;5 - Very important;4 - Important;No opinion;4 - Important;4 - Important;;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;4 - Important;;5 - Very important;5 - Very important;4 - Important;;No opinion;No opinion;No opinion;No opinion;1 - Not important at all;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;Yes;;Yes;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;Yes;;Yes, for all AI applications;;;
F529097;15-05-2020 22:41;English;;;;;;;;;The feedback can be published in an anonymous way;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;Biometric identification systems should never be allowed in publicly accessible spaces;;;;;;;;;;;;;;;;
F529096;15-05-2020 22:38;Swedish;;;;;;;;;The feedback can be published in an anonymous way;3 - Neutral;4 - Important;5 - Very important;3 - Neutral;3 - Neutral;2 - Not important;;5 - Very important;4 - Important;2 - Not important;2 - Not important;2 - Not important;3 - Neutral;;4 - Important;3 - Neutral;2 - Not important;;2 - Not important;1 - Not important at all;1 - Not important at all;1 - Not important at all;2 - Not important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;2 - Not important;3 - Neutral;;Current legislation may have some gaps;;No;;;;;4 - Important;3 - Neutral;No opinion;No opinion;No opinion;No opinion;Biometric identification systems should never be allowed in publicly accessible spaces;;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;;No opinion;;;
F529095;15-05-2020 22:35;English;;;;;;;;;The feedback can be published in an anonymous way;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;Biometric identification systems should never be allowed in publicly accessible spaces;;;;;;;;;;;;;;;;
F529094;15-05-2020 22:33;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;Biometric identification systems should never be allowed in publicly accessible spaces;;;;;;;;;;;;;;;;
F529093;15-05-2020 22:29;Dutch;EU Citizen;Tjeerd;Schraa;;;;;Netherlands;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;Biometric identification systems should never be allowed in publicly accessible spaces;;;;;;;;;;;;;;;;
F529092;15-05-2020 22:13;English;EU Citizen;J;H;;;;;Denmark;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;;5 - Very important;;;;;;;;;;;;;;;;;;;Biometric identification systems should never be allowed in publicly accessible spaces;;;;;;;;;;;;;;;;
F529091;15-05-2020 22:12;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;4 - Important;4 - Important;No opinion;1 - Not important at all;1 - Not important at all;;4 - Important;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;;1 - Not important at all;1 - Not important at all;1 - Not important at all;;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;Yes;;No;;;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Not at all;;Other enforcement system;BAN;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F529090;15-05-2020 21:59;Romanian;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;4 - Important;5 - Very important;4 - Important;4 - Important;2 - Not important;;2 - Not important;1 - Not important at all;1 - Not important at all;3 - Neutral;4 - Important;4 - Important;;1 - Not important at all;4 - Important;4 - Important;Nu imi dau seama;3 - Neutral;2 - Not important;3 - Neutral;3 - Neutral;3 - Neutral;Nu imi dau seama;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Deoarece deocamdata nu stim ce implica si unde se va ajunge cu IA, cadrul legislativ trebuie sa ramana deschis si adaptat in permanenta pentru ca drepturile fundamentale ale omului sa nu fie nicidecum afectate. Aceasta este opinia mea la intrebarea urmatoare care nu se regaseste printre punctele insirate ca raspunsuri. Am bifat totusi ca sunt necesare noi acte legislative pt ca va fi nevoie continuu de noi acte legislative pe masura ce se va descoperi ce poate face AI. ;There is a need for a new legislation;;Yes;;Yes;;Nu-mi dau seama ce poate sa ajunga sa faca AI in 5 ani. Dar acum urmarirea tuturor activitatilor oamenilor pentru a-i cunoaste si a-i controla mai bine nu este cea mai transparenta si inocenta folosire a IA.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;Nu este nici o nevoie de identificare biometrica atunci cand statele nu vor sa isi controleze cetatenii. ;Rather not;Nu;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Nu;Mental health risks;Inca nu imi dau seama.;No opinion;Nu;No opinion;Nu;Yes, for all AI applications;;Nu;
F529089;15-05-2020 21:55;Slovenian;;;;;;;;;The feedback can be published in an anonymous way;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;Biometric identification systems should never be allowed in publicly accessible spaces;;;;;;;;;;;;;;;;
F529088;15-05-2020 21:43;German;EU Citizen;Rüdiger;Biernat;;;;;Germany;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;1 - Not important at all;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;1 - Not important at all;5 - Very important;1 - Not important at all;;5 - Very important;5 - Very important;1 - Not important at all;;5 - Very important;5 - Very important;5 - Very important;1 - Not important at all;1 - Not important at all;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;No;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Very much;JEDES KI-System muss als solches gekennzeichnet werden!;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Jede Entscheidung oder Bewertung muss zu jedem Zeitpunkt von einem MENSCHEN überwacht und nachvollziehbar sein.;Mental health risks;"Finanzielle Risiken (Die Schufa zerstört oder verhindert schon jetzt Existenzen!)
Physische Gesundheit
Schutz der persönlichen Daten / Doxing";Yes;;Yes;;Yes, for all AI applications;;;
F529087;15-05-2020 21:33;English;;;;;;;;;The feedback can be published in an anonymous way;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;Biometric identification systems should never be allowed in publicly accessible spaces;;;;;;;;;;;;;;;;
F529086;15-05-2020 21:33;German;;;;;;;;;The feedback can be published in an anonymous way;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;Biometric identification systems should never be allowed in publicly accessible spaces;;;;;;;;;;;;;;;;
F529085;15-05-2020 21:26;German;;;;;;;;;The feedback can be published in an anonymous way;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;Biometric identification systems should never be allowed in publicly accessible spaces;;;;;;;;;;;;;;;;
F529084;15-05-2020 21:13;English;;;;;;;;;The feedback can be published in an anonymous way;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;Biometric identification systems should never be allowed in publicly accessible spaces;Biometric recognition systems is the exact opposite of what I expect from a free and democratic europe. It would be more than a shame for the eu to inplement these systems. ;;;;;;;;;;;;;;;
F529083;15-05-2020 21:02;German;EU Citizen;Renate;Pollard;;;;;Germany;The feedback can be published with your personal information;5 - Very important;4 - Important;5 - Very important;4 - Important;3 - Neutral;2 - Not important;;4 - Important;4 - Important;2 - Not important;2 - Not important;3 - Neutral;4 - Important;;3 - Neutral;4 - Important;2 - Not important;;4 - Important;;5 - Very important;3 - Neutral;3 - Neutral;;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;No;;;;;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;;Yes, for specific AI applications;;;
F529082;15-05-2020 20:59;Romanian;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;2 - Not important;3 - Neutral;4 - Important;3 - Neutral;1 - Not important at all;;3 - Neutral;2 - Not important;1 - Not important at all;2 - Not important;1 - Not important at all;2 - Not important;;4 - Important;2 - Not important;4 - Important;;1 - Not important at all;3 - Neutral;1 - Not important at all;2 - Not important;1 - Not important at all;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;No opinion;;Yes;;No opinion;;;1 - Not important at all;1 - Not important at all;1 - Not important at all;3 - Neutral;3 - Neutral;4 - Important;Biometric identification systems should never be allowed in publicly accessible spaces;;No opinion;;No opinion;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F529081;15-05-2020 20:59;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;4 - Important;;1 - Not important at all;1 - Not important at all;Banning AI from the public sector and keeping the public space free from antihuman authority.;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;Biometric identification systems should never be allowed in publicly accessible spaces;;;;;;;;;;;;;;;;
F529080;15-05-2020 20:53;English;;;;;;;;;The feedback can be published in an anonymous way;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;Biometric identification systems should never be allowed in publicly accessible spaces;;;;;;;;;;;;;;;;
F529079;15-05-2020 20:43;French;;;;;;;;;The feedback can be published in an anonymous way;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;;1 - Not important at all;1 - Not important at all;1 - Not important at all;;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;Yes;;Yes;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Not at all;;Other enforcement system;;;Mental health risks;;No opinion;;No opinion;;No opinion;;;
F529078;15-05-2020 20:33;English;EU Citizen;Wojciech;Michalak;;;;;Germany;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;;4 - Important;2 - Not important;;5 - Very important;4 - Important;2 - Not important;2 - Not important;2 - Not important;2 - Not important;;5 - Very important;4 - Important;2 - Not important;;5 - Very important;2 - Not important;3 - Neutral;2 - Not important;3 - Neutral;;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;4 - Important;5 - Very important;;There is a need for a new legislation;;Yes;;Yes;;;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;Biometric Identification Systems in Public Spaces are an intrusion into my right to privacy and are dictatorial in nature. ;Not at all;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Risks related to the loss of connectivity;;Yes;;Yes;;Yes, for all AI applications;;;
F529077;15-05-2020 20:31;Dutch;;;;;;;;;The feedback can be published in an anonymous way;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;;5 - Very important;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;;3 - Neutral;3 - Neutral;3 - Neutral;;4 - Important;5 - Very important;3 - Neutral;3 - Neutral;3 - Neutral;;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;;There is a need for a new legislation;;No opinion;;;;;;;;;;;Biometric identification systems should never be allowed in publicly accessible spaces;;No opinion;;No opinion;;;Mental health risks;;No opinion;;No opinion;;No opinion;;;
F529076;15-05-2020 20:13;English;;;;;;;;;The feedback can be published in an anonymous way;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;None of those envisioned. Drop it.;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;;1 - Not important at all;1 - Not important at all;1 - Not important at all;;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;No;;;;Any of them.;No opinion;5 - Very important;No opinion;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Very much;;Other enforcement system;AI will never be trustworthy. This question, like many in this survey, is flawed.;;Mental health risks;;Yes;;No opinion;;Yes, for all AI applications;;;
F529075;15-05-2020 20:10;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;1 - Not important at all;3 - Neutral;1 - Not important at all;;4 - Important;5 - Very important;1 - Not important at all;;3 - Neutral;1 - Not important at all;4 - Important;4 - Important;1 - Not important at all;;3 - Neutral;5 - Very important;5 - Very important;5 - Very important;2 - Not important;5 - Very important;;Current legislation is fully sufficient;;No;;;;;5 - Very important;1 - Not important at all;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;Ban facial recongnition;Very much;;No opinion;;;Mental health risks;;No opinion;;Yes;;Yes, for all AI applications;;;
F529074;15-05-2020 20:07;German;;;;;;;;;The feedback can be published in an anonymous way;1 - Not important at all;2 - Not important;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;;1 - Not important at all;1 - Not important at all;1 - Not important at all;3 - Neutral;1 - Not important at all;1 - Not important at all;;3 - Neutral;3 - Neutral;1 - Not important at all;;No opinion;No opinion;No opinion;No opinion;No opinion;;4 - Important;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;5 - Very important;;There is a need for a new legislation;;Yes;;No;;;No opinion;1 - Not important at all;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Very much;;Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;;Personal security risks;;Yes;;Yes;;Yes, for all AI applications;;;
F529073;15-05-2020 20:06;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;4 - Important;No opinion;2 - Not important;2 - Not important;;5 - Very important;3 - Neutral;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;Biometric identification systems should never be allowed in publicly accessible spaces;It's a step closer to fascism ;;;;;;;;;;;;;;;
F529072;15-05-2020 20:03;English;;;;;;;;;The feedback can be published in an anonymous way;;;;;;;;;;;;;;;5 - Very important;4 - Important;2 - Not important;;3 - Neutral;4 - Important;4 - Important;3 - Neutral;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;No;;;;;;;;;;;Biometric identification systems should never be allowed in publicly accessible spaces;;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;;;;;;
F529071;15-05-2020 20:03;French;;;;;;;;;The feedback can be published in an anonymous way;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;Biometric identification systems should never be allowed in publicly accessible spaces;;;;;;;;;;;;;;;;
F529070;15-05-2020 20:02;French;EU Citizen;Joseph;Delgove;;;;;France;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;;There is a need for a new legislation;;No;;;;;;;;;;;Biometric identification systems should never be allowed in publicly accessible spaces;;;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;Yes;;Yes, for all AI applications;;;
F529069;15-05-2020 20:01;German;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;1 - Not important at all;1 - Not important at all;;3 - Neutral;1 - Not important at all;1 - Not important at all;4 - Important;3 - Neutral;1 - Not important at all;;3 - Neutral;1 - Not important at all;3 - Neutral;;3 - Neutral;3 - Neutral;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
F529068;15-05-2020 19:56;German;EU Citizen;Simon;Häusler;;;;;Germany;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;Biometric identification systems should never be allowed in publicly accessible spaces;Ban biometric identification! You can't permit a dangerous tool like that! ;;;;;;;;;;;;;;;
F529067;15-05-2020 19:51;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;No opinion;3 - Neutral;No opinion;1 - Not important at all;1 - Not important at all;;5 - Very important;;3 - Neutral;3 - Neutral;;3 - Neutral;;4 - Important;3 - Neutral;3 - Neutral;;2 - Not important;4 - Important;2 - Not important;3 - Neutral;4 - Important;;;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;;No opinion;;No opinion;;;;;3 - Neutral;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;No opinion;;No opinion;;;;;No opinion;;No opinion;;Yes, for all AI applications;;;
F529066;15-05-2020 19:50;German;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;4 - Important;4 - Important;4 - Important;3 - Neutral;3 - Neutral;;4 - Important;5 - Very important;3 - Neutral;3 - Neutral;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;4 - Important;;4 - Important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;Yes;;No opinion;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Not at all;;Other enforcement system;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F529065;15-05-2020 19:38;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;3 - Neutral;4 - Important;3 - Neutral;1 - Not important at all;3 - Neutral;;3 - Neutral;3 - Neutral;3 - Neutral;2 - Not important;3 - Neutral;2 - Not important;;4 - Important;3 - Neutral;1 - Not important at all;;3 - Neutral;1 - Not important at all;4 - Important;1 - Not important at all;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;We still are unaware of the real process happening behind AI development and especially machine learning. We might enter uncharted fields that we wont have the capability to mitigate after a point. ;There is a need for a new legislation;;No;;;;Machine learing and neurolink processing;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;This is a total violation of privacy. ;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;No opinion;;No opinion;;Yes, for all AI applications;;;
F529064;15-05-2020 19:38;English;;;;;;;;;The feedback can be published in an anonymous way;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;Biometric identification systems should never be allowed in publicly accessible spaces;;;;;;;;;;;;;;;;
F529063;15-05-2020 19:37;English;;;;;;;;;The feedback can be published in an anonymous way;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;Biometric identification systems should never be allowed in publicly accessible spaces;Never ever. ;;;;;;;;;;;;;;;
F529062;15-05-2020 19:35;German;;;;;;;;;The feedback can be published in an anonymous way;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;Biometric identification systems should never be allowed in publicly accessible spaces;;;;;;;;;;;;;;;;
F529061;15-05-2020 19:32;Danish;EU Citizen;Amilla;Jensen;;;;;Denmark;The feedback can be published with your personal information;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;;1 - Not important at all;1 - Not important at all;1 - Not important at all;;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;;Current legislation is fully sufficient;;Yes;;Yes;;;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;Biometric identification systems should never be allowed in publicly accessible spaces;;Not at all;;Other enforcement system;;;Personal security risks;;No;;No;;No;;;
F529060;15-05-2020 19:32;Dutch;;;;;;;;;The feedback can be published in an anonymous way;3 - Neutral;4 - Important;4 - Important;3 - Neutral;1 - Not important at all;2 - Not important;;4 - Important;4 - Important;2 - Not important;2 - Not important;4 - Important;2 - Not important;;2 - Not important;4 - Important;2 - Not important;;2 - Not important;4 - Important;2 - Not important;2 - Not important;1 - Not important at all;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;;There is a need for a new legislation;;No;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Not at all;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F529059;15-05-2020 19:24;Swedish;Non-EU Citizen;Thomas;Askjellerud;;;;;Sweden;The feedback can be published with your personal information;3 - Neutral;2 - Not important;4 - Important;2 - Not important;2 - Not important;1 - Not important at all;;3 - Neutral;2 - Not important;3 - Neutral;2 - Not important;3 - Neutral;2 - Not important;;3 - Neutral;2 - Not important;4 - Important;;5 - Very important;4 - Important;3 - Neutral;4 - Important;4 - Important;;4 - Important;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;;There is a need for a new legislation;;No;;;;;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F529058;15-05-2020 19:19;Latvian;;;;;;;;;The feedback can be published in an anonymous way;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
F529057;15-05-2020 19:16;English;;;;;;;;;The feedback can be published in an anonymous way;3 - Neutral;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;1 - Not important at all;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;Biometric identification systems should never be allowed in publicly accessible spaces;;Not at all;;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;;Personal security risks;;;;;;;;;
F529056;15-05-2020 19:05;German;EU Citizen;Michael;Hußmann;;;;;Germany;The feedback can be published with your personal information;4 - Important;5 - Very important;;4 - Important;3 - Neutral;3 - Neutral;;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;2 - Not important;;4 - Important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;4 - Important;4 - Important;;There is a need for a new legislation;;No;;;;Gesichtserkennung;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Rather not;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Personal security risks;;Yes;;Yes;;Yes, for all AI applications;;;
F528871;15-05-2020 00:37;French;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Il faudrait penser à donner de la visibilité et un soutien politique à cet écosystème d'IA. J'avais des projets à faire et je ne trouvais pas d'institution centrale de l'IA Européenne qui incarne ces efforts. Ma recherche d'infos sur les politiques européennes en matiere d'IA et d'Europe digitale furent dures et complexes, cela devrait être + simple. En plus,  une agence pourrait être bienvenue pour accélérer le developpement de l'IA Européenne, vu le retard de l'UE face aux US et à la Chine.  ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;la création d'un environement attirant et incitant les meuilleurs chercheurs à venir/rester en Europe pour des projets liés à l'IA? Encore une fois, les solutions d'IA made in Europe doivent gagner en visibilité auprès du public européen. Il faudrait aussi penser à éventuellement à protégér les industries tech européennes... (même si cela rentre en conflit avec la compétitivité);5 - Very important;5 - Very important;5 - Very important;il faudrait que le centre de recehrche sur l'IA ne soit pas que technologique, mais soit multidisciplinaire et permette des chercheurs de sciences sociales à venir également. Ensuite, il faudrait que ce centre de recherche ait les pouvoirs pour faire des expériences/test à grande ou petite échelle sur le territoire européen avec les citoyens européens.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;il faut que les centres universitaires donnent plus de visibilité aux solutions d'IA déjà existante pour permettre aux entrepreuneurs (dont je fais partie) de pouvoir explorer les oportunités offertes par ces nouvelles technologies. Une platforme pour relier les gens du milieu avec des personnes d'autres disciplines? pourquoi, l'objectif est de construire un réseau autour de cette IA européenne pour la developper. Si ça existe... bah ce n'est pas très visible...;5 - Very important;4 - Important;5 - Very important;3 - Neutral;5 - Very important;4 - Important;si utilisée par des acteurs publiques, de la transparence à son encontre. À mon avis, il faut prévoire que cetraines decisions vont surement etres prises dans le secteur public grace à de l'IA, il faudrait penser à des mesures qui poussent les acteurs publics à être transparents sur le sujet pour que les citoyens comprennent l'origine de la décision.;Current legislation may have some gaps;;No opinion;;;;les armes cyber automatiques. Tout ce qui touche au domaine de la defense (tout ce qui va tuer sans besoin d'avoir un humain pilotant) et en particulier la cyberdefense (tout ce qui peux bloquer/endommager/compremmetre données et réseaux sans commande humaine), car cela se relie à la souveraineté des nations.;3 - Neutral;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);Je suis favorable a l'usage de reconnaissance faciale dans l'espace publique. Le maitien de l'ordre et la sécurité deviendraient plus faciles, cela dit, il faut absoluement mettre en place un systeme empechant les abus par les administrateurs publiques, pour que les citoyeuns UE aient confiance. Il faut faire des efforts pour faire comprendre que l'usage de la surveillance n'équivaut pas à de la dictature et que l'UE se maintiendra democratique et supportrice des HR.  ;No opinion;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Personal security risks;;No opinion;;No opinion;;Yes, for all AI applications;;;
F528870;14-05-2020 21:18;English;Non-EU Citizen;Michel;TIA;;;;;Côte d’Ivoire;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;Current legislation is fully sufficient;;Yes;;Yes;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);;Very much;;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;;Risks related to the loss of connectivity;;Yes;;Yes;;Yes, for all AI applications;;;
F528869;14-05-2020 19:14;French;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;3 - Neutral;5 - Very important;3 - Neutral;4 - Important;4 - Important;;5 - Very important;;4 - Important;3 - Neutral;5 - Very important;4 - Important;;5 - Very important;4 - Important;3 - Neutral;;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;4 - Important;;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;;There is a need for a new legislation;;Yes;;Yes;;l'approche proposée se trouve en section 5.C et non 5.B;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F528868;13-05-2020 17:44;English;;;;;;;;;The feedback can be published in an anonymous way;3 - Neutral;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;I think the focus should be on making Europe attractive for people skilled in AI, or wanting to become skilled in it. This way, more high-tech companies will want to be in Europe because of all the talent and more companies and public institutions will be able to implement AI due to the increased number of skilled people.;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;4 - Important;;4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;;4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;2 - Not important;The accuracy point is a problem, but it can be worked around. If it is known that the specific AI implementation is not always accurate, then you know how much trust you can put into it and where you need to build in safeguards. Therefore, I think it is less of a fundamental concern than the other points.;There is a need for a new legislation;;Yes;;Yes;;Anything that has to do with surveillance of individuals. Surveillance of 'normal' individuals in society should not be allowed under any circumstance. Only when law enforcement has demonstrable solid evidence that a person has done something, surveillance is allowed to be requested from the relevant judicial authority.;4 - Important;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;Like I said before, surveillance of individuals in the general public should never be allowed. Only when law enforcement has demonstrable and solid evidence about an individual can that person be tracked by those identification systems.;Very much;Have some kind of level system, for example with risk levels and the number of requirements followed.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Make very clear requirements for different applications. That makes it easier for organisations to prove that they are indeed following them (statistics of training set, number of people that will interact with the system, etc.);Cyber risks;Current AI systems can often be fooled by adversarial examples, therefore this risk should be taken into account, especially in high-risk applications such as autonomous driving.;Yes;;No opinion;;Yes, for all AI applications;;;
F528867;13-05-2020 11:17;English;Other;e;k;;University College London;122063331075-91;Large (250 or more);United Kingdom;The feedback can be published with your personal information;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;4 - Important;Introduced in the start as a key pivot of the report ‘An Ecosystem of Excellence’ is fleshed out in section 4 (Table 2). Here the discussion revolves around who the key stakeholders are and in what manner they currently operate and should be structured strategically. There appears to be a tension between the centralised role and vision of the EU as a coordinator and director of agenda (for example, we can see how the EU would drive the skills agenda) and the need to have a vibrant SME community.;5 - Very important;3 - Neutral;4 - Important;5 - Very important;5 - Very important;3 - Neutral;;3 - Neutral;4 - Important;3 - Neutral;;4 - Important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;;3 - Neutral;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;With respect to human oversight, the following quote is germane 'transparency, traceability and human oversight are not specifically covered under current legislation in many economic sectors’ ([1] p.9). This is an issue of governance. More generally, ethics is central, ‘unintended effects’, ‘malicious purpose’, ‘lack of trust’ are all mentioned in the context of promoting Europe’s innovation capacity. ;Current legislation may have some gaps;;Other;the white paper is ambiguous with respect to its signalling of regulation/legislation. Indeed, some parts indicate clear openness to regulation, other parts reiterate that legislation can be updated, amended and that other regulatory frameworks/statutes won’t be incurred upon etc.Such ambiguity is likely to have impacts on planning, risk evaluation, innovation and investment.;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;;No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;In our own work on AI Impact Assessment [4] we envision a declaration of interest as part of the deployment of a new algorithm. This declaration will disclose qualitative information about the team that have built the system, as well as few scenarios that can happen if bad actors or the algorithm is used inappropriately. ;Mental health risks;;Yes;;No opinion;;Yes, for specific AI applications;;;EU_White_Paper_Review_-_Kazim_Koshiyama.pdf
F528866;13-05-2020 11:08;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;;3 - Neutral;4 - Important;4 - Important;5 - Very important;4 - Important;5 - Very important;;2 - Not important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;;Current legislation may have some gaps;;Yes;;Yes;;;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;No opinion;;Yes, for specific AI applications;All applications where there are real risks to someone's health, to economy or to society;;
F528865;13-05-2020 00:04;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;5 - Very important;;5 - Very important;4 - Important;5 - Very important;;4 - Important;4 - Important;4 - Important;4 - Important;3 - Neutral;;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;;There is a need for a new legislation;;Other;;;;;5 - Very important;;3 - Neutral;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;No opinion;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F528864;12-05-2020 19:14;English;Business Association;Javier;Santacruz Cano;;Institut Agrícola Català de Sant Isidre;;Small (< 50 employees);Spain;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;Focus on customers to understand changes in their decision-making process;4 - Important;3 - Neutral;3 - Neutral;5 - Very important;5 - Very important;5 - Very important;;3 - Neutral;5 - Very important;5 - Very important;Set up a public-private partnership for farming research;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;Special development of market mechanisms to exchange new developments based on IA;4 - Important;5 - Very important;4 - Important;2 - Not important;4 - Important;2 - Not important;;There is a need for a new legislation;;Yes;;Yes;;The use of algorithms for e-commerce, and risk profile in banking and financial products;;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;Violation of privacy right in public areas;Not at all;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Personal security risks;Financial personal data;Yes;;Yes;;Yes, for all AI applications;;;
F528863;12-05-2020 17:06;Swedish;Company/Business organisation;Anders;Arpteg;;Peltarion AB;;Medium (< 250 employees);Sweden;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;It is evident that we are lacking a strong cloud platform provider in Europe. However, important not to underestimate the difficulty and amount of work needed to build a new European cloud provider. Important to also consider allow and promote use of existing cloud provider while building up data spaces, otherwise the functionality will be far behind alternatives.;3 - Neutral;4 - Important;4 - Important;As seen in US and China, it is the industry driving AI forward these-days and we should give strong incentives for European companies to build strong industrial research teams to  start to catch up with them.;5 - Very important;3 - Neutral;4 - Important;4 - Important;5 - Very important;;4 - Important;3 - Neutral;4 - Important;2 - Not important;2 - Not important;3 - Neutral;Important to always consider the benefits with AI at the same time as the dangers. How ethical is it not to use AI, and what would the opportunity cost be?;There is a need for a new legislation;;Other;Regulation for high-risk AI applications should be also balanced with the need for innovation. Sectors such as the health-care sector are high-risk, but also one of the fields that need AI the most. Very important to enabling innovation with AI, and any regulation should focus on the outcome of AI application, not the technology itself. In that way, we can still enable innovation while preserving safety.;;;;4 - Important;3 - Neutral;3 - Neutral;4 - Important;4 - Important;3 - Neutral;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);The use of biometric identification should be clearly marked, which will also serve as a deterrent helping to prevent crime. ;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;;;No;;No;;No;;;
F528862;12-05-2020 16:44;Greek;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;4 - Important;4 - Important;2 - Not important;2 - Not important;4 - Important;;5 - Very important;3 - Neutral;3 - Neutral;2 - Not important;3 - Neutral;4 - Important;;4 - Important;5 - Very important;4 - Important;;3 - Neutral;4 - Important;4 - Important;3 - Neutral;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;Current legislation may have some gaps;;Yes;;Yes;;;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Personal security risks;;Yes;;Yes;;Yes, for all AI applications;;;
F519210;12-05-2020 10:08;Italian;Public authority;Sergio;Pillon;International;CIRM Centro Internazionale Radio Medico;734697138122-24;Small (< 50 employees);Italy;The feedback can be published with your personal information;3 - Neutral;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;;4 - Important;3 - Neutral;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;;3 - Neutral;4 - Important;4 - Important;2 - Not important;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;2 - Not important;;There is a need for a new legislation;;No;;;;salute;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Personal security risks;;Yes;;;;Yes, for all AI applications;;;
F519209;12-05-2020 10:03;English;Academic/Research Institution;Anne-Grace;Kleczewski;;UCLouvain (Université Catholique de Louvain);016573426066-92;Large (250 or more);Belgium;The feedback can be published with your personal information;3 - Neutral;5 - Very important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;;4 - Important;5 - Very important;3 - Neutral;3 - Neutral;5 - Very important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;;3 - Neutral;5 - Very important;3 - Neutral;5 - Very important;3 - Neutral;;4 - Important;4 - Important;4 - Important;;5 - Very important;4 - Important;;Current legislation may have some gaps;;Yes;;No opinion;;Recruitment processes and in the public sector in general ;4 - Important;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;;;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;A period assessment and an obligation to notify major changes that would trigger the need for an additional reassessment so that the effective compatibility with the framework is ensured at all times and does not allow for changed to generate gaps that are not spotted before some time ;Yes;;Yes, for all AI applications;;;
F519146;11-05-2020 18:01;English;Company/Business organisation;Andrea;Corazza;;Biogen;966165310889-60;Large (250 or more);United States;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;In medicines discovery and development, AI can be used to identify promising compounds, aid in early detection of diseases for testing treatments, and potentially provide predictive analytics to identify safety risk or signs of efficacy sooner. Taken together AI has the potential to speed drug development, lower costs by focusing on the most promising candidates, and improve available treatments for patients in need.;5 - Very important;4 - Important;5 - Very important;2 - Not important;5 - Very important;5 - Very important;The intention to create a forum for exchange of information and best practices, and to issue guidance and opinion is sensible. In light of this, we strongly encourage the Commission to identify and communicate means to “maximum stakeholder participation” during development and implementation as proposed in the white paper. ;3 - Neutral;5 - Very important;5 - Very important;We would recommend that for the focus on health care and medicines development, the Commission considers early future partnership with the World Health Organization (WHO), the International Council for Harmonisation (ICH), the International Medical Device Regulator Forum (IMDRF), and professional organizations like the Institute of Electrical and Electronics Engineers (IEEE), which is dedicated to advancing technology for the benefit of humanity. ;3 - Neutral;3 - Neutral;3 - Neutral;5 - Very important;3 - Neutral;;;;;;;;As the Commission considers potential updates to product safety legislation and incorporating new safety concepts that are robust to the changing nature of AI algorithm-based products, the industries investing in adoption of these new technologies will benefit from step-wise, sector-specific guidance to minimize risk and ensure that consumers benefit from proper implementation of these advancements.;Current legislation may have some gaps;;Yes;;No opinion;;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;No opinion;;Very much;;No opinion;;;Personal security risks;;No opinion;;No opinion;;No opinion;;;European_Commission_AI_White_Paper_Final_Biogen_Comments_5_May_2020.pdf
F519102;11-05-2020 14:37;English;;;;;;;;;The feedback can be published in an anonymous way;2 - Not important;3 - Neutral;4 - Important;5 - Very important;5 - Very important;5 - Very important;;4 - Important;3 - Neutral;4 - Important;5 - Very important;5 - Very important;3 - Neutral;;4 - Important;3 - Neutral;5 - Very important;;4 - Important;4 - Important;5 - Very important;5 - Very important;3 - Neutral;;2 - Not important;4 - Important;3 - Neutral;2 - Not important;3 - Neutral;4 - Important;;Current legislation may have some gaps;;Yes;;Yes;;;4 - Important;3 - Neutral;3 - Neutral;5 - Very important;4 - Important;4 - Important;No opinion;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Risks related to the loss of connectivity;;No opinion;;Yes;;Yes, for specific AI applications;health applications, applications working with private data;;
F518945;10-05-2020 11:11;Dutch;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;4 - Important;5 - Very important;;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;4 - Important;;3 - Neutral;4 - Important;3 - Neutral;;4 - Important;5 - Very important;4 - Important;5 - Very important;3 - Neutral;;3 - Neutral;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;No;;;;;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F518610;08-05-2020 12:47;English;;;;;;;;;The feedback can be published in an anonymous way;3 - Neutral;5 - Very important;5 - Very important;4 - Important;3 - Neutral;5 - Very important;;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;;4 - Important;5 - Very important;3 - Neutral;;3 - Neutral;4 - Important;5 - Very important;3 - Neutral;5 - Very important;;4 - Important;5 - Very important;5 - Very important;3 - Neutral;No opinion;4 - Important;;There is a need for a new legislation;;No opinion;;;;;3 - Neutral;5 - Very important;4 - Important;3 - Neutral;4 - Important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Much;;No opinion;;;Personal security risks;;Yes;;Yes;;Yes, for specific AI applications;;;
F518609;08-05-2020 12:46;Spanish;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;4 - Important;4 - Important;3 - Neutral;4 - Important;;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;Estándares y reglamentaciones para el uso y desarrollo de IA a nivel europeo;5 - Very important;4 - Important;4 - Important;Estándares y reglamentaciones para el uso y desarrollo de IA para sectores públicos y privados;4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;;3 - Neutral;4 - Important;4 - Important;2 - Not important;4 - Important;4 - Important;;There is a need for a new legislation;;Yes;;No;;;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;4 - Important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Risks related to the loss of connectivity;;Yes;;Yes;;Yes, for all AI applications;;;
F518607;08-05-2020 12:24;Spanish;Academic/Research Institution;Joan;NAVARRO;;Consejo Superior de Investigaciones Científicas (CSIC);14854119757-24;Large (250 or more);Spain;The feedback can be published with your personal information;5 - Very important;5 - Very important;4 - Important;4 - Important;3 - Neutral;4 - Important;;5 - Very important;5 - Very important;3 - Neutral;4 - Important;4 - Important;4 - Important;;4 - Important;5 - Very important;3 - Neutral;;No opinion;No opinion;No opinion;No opinion;No opinion;;4 - Important;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;4 - Important;;Current legislation may have some gaps;;Yes;;Yes;;;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Much;;Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;;Personal security risks;;No opinion;;Yes;;No opinion;;;
F518594;08-05-2020 10:22;German;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;4 - Important;4 - Important;3 - Neutral;3 - Neutral;3 - Neutral;;2 - Not important;5 - Very important;4 - Important;3 - Neutral;4 - Important;4 - Important;;No opinion;3 - Neutral;3 - Neutral;;;;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;There is a need for a new legislation;;No;;;;;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;Gesichtserkennung etc. stellen eine immense Bedrohung für die Grundrechte und ein öffentliches Bewegen in Freiheit dar, was in Relation zu ihrem kaum bis gar nicht vorhandenen Nutzen über alle Maßen unverhältnismäßig ist!;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;Yes;;Yes, for all AI applications;;;
F517321;07-05-2020 16:46;Romanian;EU Citizen;Doina;Visan;;;;;Romania;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;-;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;4 - Important;4 - Important;;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"
- În ceea ce prive?te controlul uman- ar fi necesar ca oamenii s? aib? libertatea de a alege dac?, ?i în ca condi?ii î?i deleag? deciziile sistemelor IA, pentru îndeplinirea unor obiective, proprii.
- În domeniul justi?iei - s-ar impune garan?ia unei transparen?e, în sensul c?, orice implicare a unui sistem IA sau re?ele neuronale, în hot?râri judec?tore?ti trebuie s? ofere o explica?ie verificabil? ?i controlabil? de o autoritate uman? competent?.";There is a need for a new legislation;;No;;;;"- armele autonome
- S-ar impune reglement?ri interna?ionale pentru a îndruma cercet?rile ?i utilizarea IA în domeniul armelor/militar 
- Preocup?ri legate de toate aspectele ce implic? utilizarea IA, pentru a g?si r?spunsuri din timp, la întreb?rile esen?";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Other special requirements in addition to those mentioned in the question above should be imposed (please specify);Ar trebui trasat? o grani?? clar? între protejarea societ??ii ?i protejarea libert??ii personale, respectiv respectarea confiden?ialit??ii, pentru maximizarea unui rezultat benefic.;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;"- Obligatoriu IA trebuie s? fie proiectat? ?i s? ac?ioneze, astfel încât s? fie compatibil? cu drepturile ?i libert??ile, astfel cum sunt prev?zute de Carta Drepturilor Fundamentale a Uniunii Europene ?i normele comunitare.
- Totodat?, o atent? revizuire ";Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F517199;07-05-2020 10:47;Spanish;;;;National;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;Current legislation may have some gaps;;Yes;;Yes;;Reconocimiento facil avanzado, patrones,  identificación de personas;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Other special requirements in addition to those mentioned in the question above should be imposed (please specify);;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;;Yes, for specific AI applications;;;
F516955;06-05-2020 17:27;English;Company/Business organisation;Abayomi;OTUBUSHIN;;BMW AG;7193977808-18;Large (250 or more);Germany;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;4 - Important;;5 - Very important;4 - Important;5 - Very important;3 - Neutral;5 - Very important;4 - Important;;4 - Important;5 - Very important;3 - Neutral;Here a good balance must be found between establishing lighthouse center and building up on existing centers of competence throughout Europe and fostering their networking and collaboration;No opinion;No opinion;No opinion;No opinion;No opinion;;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;3 - Neutral;;Current legislation may have some gaps;;Yes;;Other;"A classification of whole sectors into ""high-risk"" is not adequate as it will most likely hamper AI development in these sectors, also for low risk applications, which would be the majority. The risk assessment and classification should depend solely on the AI application .";;2 - Not important;4 - Important;3 - Neutral;4 - Important;3 - Neutral;4 - Important;;;Not at all;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;;;No;;No;;No;;;BMW_Comment_AI_White_Paper_final_EN.pdf
F516954;06-05-2020 14:48;English;Other;Corado;MATTIUZZO;;KAN Commission for Occupational Health & Safety and Standardization;90520343621-73;Small (< 50 employees);Germany;The feedback can be published with your personal information;5 - Very important;5 - Very important;No opinion;No opinion;No opinion;No opinion;;5 - Very important;5 - Very important;No opinion;No opinion;No opinion;No opinion;;No opinion;5 - Very important;No opinion;;No opinion;5 - Very important;No opinion;No opinion;No opinion;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Please note that this text is referring to the question below on applicable EU legislation where no field for comments has been offered online (only in the pdf-version): As an example for a gap, current legislation is requiring the capacity to reliably assess the magnitude of risks originated by a product. If AI is potentially decisive for the hazard-related behaviour of a product, the consequences of this influence cannot be reliably assessed because currently no adequate tools are available.;Current legislation may have some gaps;;No;;;;The distinction between those applications, and between only two risk categories, is not convincing. Requiring that both criteria, sector + application, must be of high risk is too strict. A more gradual approach is necessary (see proposal of the German Commission on Data Ethics). Applications not covered by one of the sectors proposed in the White Paper are machinery or smart PPE. It could potentially be very harmful to rely on AI when involved in the safety of such products.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;;No opinion;A voluntary system would not cover all relevant, i.e. somehow critical, products, and as all the identified concerns related to AI are considered of importance, necessary measures need to be obligatory. Anything else, i.e. problems of no real concern, does not need such a burdensome scheme.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;As an example, current legislation is requiring the capacity to reliably assess the magnitude of risks originated by a product. If AI is potentially decisive for the hazard-related behaviour of a product, the consequences of this influence cannot be reliably assessed because currently no adequate tools are available.;No opinion;"Important changes during lifetime should be considered as a new putting into service. Therefore, no new procedures appear to be necessary but rather raising the awareness about the obligation mentioned in the first sentence. Really necessary is first and foremost a tool which enables the assessment of ""black boxes"" or other configurations where changes occur continually rather than step by step.";Yes;The burden of proof for any harm caused by AI products should be on those who placed them on the market. For users it is too difficult to demonstrate the relationship between cause and effect.;Yes, for all AI applications;;;
F516953;06-05-2020 13:51;Greek;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;Highly important: Provide incentives to the private sector regarding the support of data sharing. Also: several public data sets should be compulsory opened, so that every data scientist should have access to them and work on experimental AI projects.;4 - Important;5 - Very important;4 - Important;;4 - Important;5 - Very important;4 - Important;5 - Very important;4 - Important;Prerequisite action: every member state should map all startups in its economy, since in several countries the exact number of startups is literally unknown.;4 - Important;5 - Very important;4 - Important;5 - Very important;4 - Important;4 - Important;;Current legislation may have some gaps;;Yes;;Yes;;Autonomous driving. Surveillance systems.;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;;Much;;Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;;Personal security risks;;Yes;;Yes;;Yes, for all AI applications;;;
F516952;06-05-2020 13:10;English;;;;;;;;;The feedback can be published in an anonymous way;1 - Not important at all;5 - Very important;4 - Important;3 - Neutral;5 - Very important;2 - Not important;Core AI capabilities and investment in key research is essential.  Working with leaders in the private sector is also critical as a fair amount of the innovation is being led by such companies.  ;5 - Very important;2 - Not important;5 - Very important;4 - Important;3 - Neutral;1 - Not important at all;;5 - Very important;5 - Very important;5 - Very important;;1 - Not important at all;2 - Not important;3 - Neutral;4 - Important;4 - Important;;3 - Neutral;3 - Neutral;3 - Neutral;1 - Not important at all;1 - Not important at all;5 - Very important;I believe that AI itself is rather neutral.  As such, the first three statements above are nonsensical.  Of course safety, rights, and discrimination are important aspects which need to be addressed.  However, it is in how the decisions/recommendations are applied that are key. Therefore a more holistic approach is required in any such systems.;Other;I do not believe that legislation is the answer.  ;Other;I believe that ALL high risk systems, whether they act based on human decisions or AI-driven decisions, need to have the appropriate safety control mechanisms.  ;;;;5 - Very important;No opinion;2 - Not important;5 - Very important;3 - Neutral;4 - Important;No further guidelines or regulations are needed;;Not at all;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Cyber risks;;Yes;;No;;No;;This is a particularly difficult matter that needs further debate.  It is not a simple yes/no matter.  ;
F515793;05-05-2020 21:32;German;;;;;;;;;The feedback can be published in an anonymous way;;5 - Very important;5 - Very important;3 - Neutral;1 - Not important at all;2 - Not important;"Eine ökonomische Fokussierung ohne gleichranige Seitenbetrachtung Klima, Ökosysteme, Aufklärung im Sinne von ISR der Aussenbeziehungen wird keine dauerhaft Nachhaltigkeit erreichen.
 ";5 - Very important;4 - Important;4 - Important;3 - Neutral;4 - Important;5 - Very important;KI soll für den Bereich der transparenten Analyse von Entscheidungsvorbereitungen der EU und ihrer Mitgliedsstaaten eingesetzt werden. In jedem Fall soll der Förderungsfokus auf KI-Forschung und KI-Förderung im Bereich der natürlichen Systeme (Klima, Ökosysteme, Biodiversität, Ziel-/Resultat-Analyse);3 - Neutral;4 - Important;1 - Not important at all;Dem Bereich Citizens-Science muss gelegenheit gegeben werden, im Bereich KI durch Schaffung von Big-Data, primitive KI, an der Entwicklung Teilhabe zu entwickeln um die Szene zu finanzieren und in der Bevölkerung zu verankern.;3 - Neutral;4 - Important;3 - Neutral;1 - Not important at all;5 - Very important;Die Gesundung, besonders technologisch, für KMU muss aus der Investitionskompetenz kommen. Vorteile aus KI müssen dem produzierenden und Service-Markt nicht den Finanzmärkten dienen.;3 - Neutral;3 - Neutral;4 - Important;5 - Very important;5 - Very important;5 - Very important;"Nicht der Rechtsrahmen sondern der Normen-Rahmen stellt eine Herausforderung dar. Zum derzeitigen Stand der ""tiefen KI"" darf die Nutzung nur experimentell erfolgen. Eine Mehrstufige automatische Prozessfolge (nicht nachvollziehbar nach heutigem Stand der Technik) darf erst zu Entscheidungen und Empfehlungen führen, wenn eine ""Beweisführung im wissenschaftlichen Sinne [reverse evidence]"" möglich ist. Unter dem KI-Act der EU müssen auch Algorithmen des Profiling offen gelegt werden.";There is a need for a new legislation;;No;;;;Profiling, Konfliktentscheidungen, Marktmanipulation, Genomanalyse/Veränderung;3 - Neutral;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;Bionetrie darf nur unter kontrollierten Bedingungen (Einzelabfragen) definitive Ergebnisse erzeugen. Mit wachsender Fehlersicherheit (Abhängig von der Konsequenz) darf der Nutzungsrahmen erweitert werden. Keine Abwägungsrechte!;Very much;Einstufungsprozess ist die Schwachstelle !;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;Die EU als Genehmigungs-Instanz muss Prüf- und Verifikationsverfahren vor der Zulassung und im Betriebszeitraum entwickeln und anwenden.;Personal security risks;KI-Systeme, zusammen mit Sensorik und Robotik eröffnen kummulierte Risiken, die individuell beschrieben und eingegrenzt werden müssen.;Yes;;Yes;;Yes, for all AI applications;;;
F515792;05-05-2020 19:48;Slovenian;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;;5 - Very important;4 - Important;5 - Very important;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;;Current legislation may have some gaps;;No;;;;;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Mental health risks;;Yes;;Yes;;Yes, for specific AI applications;;;
F515791;05-05-2020 18:35;Italian;EU Citizen;Fabio;RASPADORI;;;;;Italy;The feedback can be published with your personal information;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;coinvolgere il mondo della comunicazione per sensibilizzare e informare correttamente i cittadini;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;coinvolgere il livello regionale e locale;5 - Very important;4 - Important;4 - Important;;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;;4 - Important;5 - Very important;5 - Very important;3 - Neutral;4 - Important;3 - Neutral;;Current legislation may have some gaps;;Yes;;Yes;;;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);Autorizzazione da concedere, da parte di autorità competenti ed in modo tassativo, in caso di motivi di ordine pubblico, salute pubblica, sicurezza. I risultati dovrebbero essere coperti da riservatezza;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;procedure di monitoraggio e pronta informazione al consumatore;Yes;soprattutto in riferimento ai controlli durante l'impiego;Yes, for all AI applications;;;
F515373;05-05-2020 16:19;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;;4 - Important;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;4 - Important;;4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;;Current legislation may have some gaps;;Yes;;Yes;;;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;;Mental health risks;;Yes;;Yes;;Yes, for specific AI applications;;;
F515302;05-05-2020 13:31;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;4 - Important;3 - Neutral;4 - Important;4 - Important;4 - Important;;5 - Very important;4 - Important;5 - Very important;4 - Important;3 - Neutral;4 - Important;;4 - Important;4 - Important;4 - Important;Already selected AI excellence centers did not cover the major research areas/verticals in Europe, or even addressed the existing strength, e.g. AI in Robotics. Thus it depends what the EC is managing to promote, if AI in Media, the UK SMEs, or the same CLAIRE group members, then the answer is negative - not important at all. ;4 - Important;4 - Important;3 - Neutral;5 - Very important;3 - Neutral;;4 - Important;4 - Important;3 - Neutral;5 - Very important;3 - Neutral;4 - Important;;Current legislation may have some gaps;;Yes;;Yes;;;4 - Important;3 - Neutral;3 - Neutral;4 - Important;4 - Important;3 - Neutral;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;No opinion;;Yes, for specific AI applications;;;
F515298;05-05-2020 12:37;German;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;4 - Important;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;Anmerkung zur vorherigen Frage: Dem öffentlichen Sektor einschließlich kommunaler Unternehmen kommt beim Einsatz von KI u.a. deshalb eine besondere Bedeutung zu, da die Bürger den Akteuren, die mit der Versorgung der Bürger beauftragt und dem Gemeinwohl verpflichtet sind, in besonderem Maße Vertrauen entgegenbringen können.;4 - Important;3 - Neutral;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;;4 - Important;3 - Neutral;5 - Very important;"1.Sicherung des Zugangs zu komplementären Technologien und Ressourcen, einschließlich Investitionen durch die EU:
A) Cloud-Ressourcen, d.h. Betrieb und Eigentum von Rechenzentren durch europäische Akteure
B) Cyber-Security von DDos-Attacken bis Endpunkte-Schutz

2.Stärkung der Anreizstrukturen für Technologietransfer aus Hochschulen und Unternehmen (z.B. Verwertungsmöglichkeiten von Patenten)";4 - Important;3 - Neutral;3 - Neutral;5 - Very important;5 - Very important;"Technische und rechtliche Beratung zur Zertifizierung von Produkten für die europaweite Einführung (CE, MPG etc.)
";5 - Very important;5 - Very important;4 - Important;4 - Important;3 - Neutral;2 - Not important;;Other;Da die bestehenden EU RL und ihre nationale Umsetzung im Bereich Produktsicherheit und Haftung heute schon sehr aufwändig sind, stellt eine weitere, KI-spezifische Regulierung voraussichtlich eine deutliche Hürde für den Einsatz von KI dar (u.a. verbunden mit Nachteilen für EU Unternehmen im internationalen Wettbewerb). Zusätzlich bestehen in sensiblen Anwendungsbereichen bereits Zulassungsverfahren für z.B. Kraftfahrzeuge und Medizinprodukte. Daher ist der Nutzen weiterer Regulierung fraglich.;Yes;;Yes;;;5 - Very important;3 - Neutral;4 - Important;5 - Very important;5 - Very important;4 - Important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Very much;Das freiwillige Kennzeichnungssystem erscheint sinnvoll in Bezug auf die Erfüllung von Anforderungen im Bereich Cybersicherheit.;Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;;Risks related to the loss of connectivity;;;;No;;No;;Nach der Produkthaftungsrichtlinie ist der Anbieter, der ein Produkt in Verkehr bringt, verantwortlich für dessen Funktionsfähigkeit und Qualität. Zusammen mit den Regelungen für sensible Anwendungsfelder, z.B. Zulassung von Kraftfahrzeugen und Medizinprodukten, erscheint diese Regelung ausreichend.;
F515238;04-05-2020 14:12;Finnish;Public authority;Hanna;Heiskanen;National;Finnish Financial Supervisory Authority;;Medium (< 250 employees);Finland;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;4 - Important;;There is a need for a new legislation;;Yes;;Yes;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;;;;;;;;;;
F515197;03-05-2020 23:44;French;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;Veiller à la collaboration entre des compétences diverses (scientifique, légal, éthique, personnes de terrain, ...) afin de permettre que les actions soient le plus proches de la réalité et tiennent compte de cette dernière, dans son ensemble.;4 - Important;3 - Neutral;4 - Important;5 - Very important;5 - Very important;3 - Neutral;;3 - Neutral;4 - Important;4 - Important;;5 - Very important;4 - Important;5 - Very important;5 - Very important;3 - Neutral;;3 - Neutral;5 - Very important;5 - Very important;4 - Important;3 - Neutral;2 - Not important;Il conviendrait de bien différencier les risques inhérents à l'IA et ceux qui dépendent de l'action et des choix des hommes. L'encadrement de la création d'une IA éthique et digne de confiance devrait surtout concerner l'IA en tant que telle. ;Current legislation may have some gaps;;;;;;;5 - Very important;4 - Important;4 - Important;5 - Very important;3 - Neutral;5 - Very important;No opinion;;Rather not;Trop d'incertitudes sont à combler pour développer ce label : quels seraient les standards sur base desquels le label serait développé ? Est-ce que le prix de cette labellisation permettrait à tout type d'entreprise d'y prétendre ? Comment éviter l'amalgame tel qu'il en existe au niveau des label bio ? Pour donc pouvoir évaluer la pertinence d'un tel label, il faudrait avoir davantage de précision sur sa mise en place.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;L'enjeux serait de ne pas de surcharger les entreprises de procédures lourdes et complexes, mais plutôt d'assurer une meilleure prévisibilité dans le développement des applications IA.;Yes;;Yes, for all AI applications;;Il s'agirait ici non pas de créer un cadre réglementaire qui soit trop lourd, mais un ensemble de règles générales qui tient compte de l'IA dans son ensemble.;
F515196;03-05-2020 21:05;Danish;EU Citizen;Anders;Grundtvig;;;;;Denmark;The feedback can be published with your personal information;4 - Important;5 - Very important;4 - Important;3 - Neutral;4 - Important;4 - Important;;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;3 - Neutral;4 - Important;;5 - Very important;5 - Very important;5 - Very important;;3 - Neutral;3 - Neutral;3 - Neutral;4 - Important;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;Current legislation may have some gaps;;Yes;;No opinion;;;No opinion;No opinion;No opinion;4 - Important;5 - Very important;4 - Important;No opinion;;Much;;No opinion;;;Personal security risks;;No opinion;;No opinion;;No opinion;;;
F515123;03-05-2020 14:30;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;Defining it as EU-centric v China and US dominance;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;2 - Not important;;4 - Important;5 - Very important;2 - Not important;Too much bureaucracy spoils effort and progress;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"Just make sure is doesn't end ""hype""";3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;"Yes, treating AI as a ""technology"" is the wrong way -- it should be about AI becoming aware as Artificial Minds!";Other;New rules should be about AI for the good of all mankind as a new 18th UN goal;No;;;;"""high risk"" is a misnomer for ""business as usual""";4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;Biometric identification systems should never be allowed in publicly accessible spaces;However, it will be introduced anyway (as e.g. in China);Not at all;;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;Such an entity should be UN-enforced;Mental health risks;AI is a technology -- Artificial Minds are the real problem when making autonomous decisions ;Yes;;Yes;;Yes, for all AI applications;;;
F515025;01-05-2020 16:56;English;Other;Mireille;HILDEBRANDT;;Vrije Universiteit Brussels;;Large (250 or more);Netherlands;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;;3 - Neutral;4 - Important;No opinion;;3 - Neutral;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;Interact from the very beginning with those who may suffer the consequences of AI;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;No opinion;;;;critical infrastructure, e.g. justice authorities;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;this should be restricted to emergency, based on law and include sunset clauses;No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;an AI-impact assessment should be combined with strict liability, see my uploaded file;Yes, for all AI applications;;we probably need an autonomous interpretation of tort law at the level of the EU;Hildebrandt_EC_White_Paper_on_AI.pdf
F514994;01-05-2020 14:49;English;EU Citizen;Argyro;Karanasiou;;;;;Greece;The feedback can be published with your personal information;4 - Important;No opinion;5 - Very important;4 - Important;5 - Very important;4 - Important;;No opinion;4 - Important;3 - Neutral;3 - Neutral;4 - Important;5 - Very important;;;;;;4 - Important;5 - Very important;5 - Very important;;4 - Important;;4 - Important;No opinion;5 - Very important;No opinion;4 - Important;No opinion;;There is a need for a new legislation;;Yes;;Yes;;;5 - Very important;3 - Neutral;4 - Important;4 - Important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;Yes;;Yes, for all AI applications;;;
F514955;01-05-2020 03:45;English;NGO (Non-governmental organisation);KONSTANTINOS;CHORTIS;;NATIONAL CONFEDERATION OF DISABLED PEOPLE;;Medium (< 250 employees);Greece;The feedback can be published with your personal information;5 - Very important;4 - Important;5 - Very important;3 - Neutral;2 - Not important;5 - Very important;NO;4 - Important;3 - Neutral;5 - Very important;;5 - Very important;5 - Very important;NO;4 - Important;5 - Very important;4 - Important;NO;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;4 - Important;;5 - Very important;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;4 - Important;;There is a need for a new legislation;;No;;;;;5 - Very important;4 - Important;4 - Important;3 - Neutral;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;Yes;;Yes, for all AI applications;;;
F514780;28-04-2020 13:20;Finnish;Company/Business organisation;Janne;Järvenoja;;NordCheck Oy;151478537968-22;Micro (< 10 employees);Finland;The feedback can be published with your personal information;5 - Very important;3 - Neutral;4 - Important;4 - Important;5 - Very important;5 - Very important;Realistic, consistent regulatory setup & standards.;4 - Important;3 - Neutral;4 - Important;5 - Very important;4 - Important;2 - Not important;due regulatory setup or fair, transparent standards for deploying AI (like CE mark with more content and criteria);3 - Neutral;3 - Neutral;4 - Important;;4 - Important;4 - Important;5 - Very important;5 - Very important;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;;There is a need for a new legislation;;No;;;;;2 - Not important;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Very much;Preferably promoting one standard (like IEEE's or ISO) for consistency and clarity provided that a standard is reasonably well defined. Minor or moderate improvement would not warrant establishing almost alike standard. Clarity, simplicity, adequacy and practicability. ;No opinion;;"With clear standard amended with industry/purpose specific addendum(s), a well-documented validation and risk assessment that can be demonstrated; clear change management, risk and incident management requirement. Clear liability rules and resolution process; potentially with joint insurance mechanisms (subject to aforesaid requirements) for covering damages. ";Personal security risks;There should be a process and obligation to address and respond to in a timely manner to risks notified to an AI provider/user. ;Yes;"Risk assessment should be consistent with any related standards applied. It would be helpful to have a recommended risk assessment criteria established from the beginning (ref. to the fuss with GDPR related DPAs that varied a lot and were quite impossible and ridiculous every now and then - costly, confusing and unecessary mess). 
There can be industry / purpose / risk level specific additional criteria as needed.";Yes;Clarity in liability is needed. Liability rules should enable group / insurance pool setups and preferential deployment of those. ;No;;ref above - pool/group insurance systems should be preferred and then local laws as secondary fall-back option. ;
F514779;28-04-2020 01:10;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;;3 - Neutral;5 - Very important;5 - Very important;;4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;4 - Important;;There is a need for a new legislation;;No;;;;;3 - Neutral;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;Possible exceptions for high risk areas or temporary installations;Very much;There should be independent assessments;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;Economic risks;Yes;;Yes;"better framework for attribution of liability; consideration of design records; 'eventual' contemplation of personality for selected established autonomous systems";Yes, for all AI applications;;;
F514778;27-04-2020 16:05;English;Academic/Research Institution;Velislava;Hillman;;informal working group, affiliates and alumni of Berkman Klein Centre for Internet & Society at Harvard University;;Micro (< 10 employees);Malta;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;children's digital and human rights;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;education and children's rights and freedoms;5 - Very important;5 - Very important;5 - Very important;interdisciplinary collaboration, research, participation (to include youth);5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;children's rights, freedoms and futures;Current legislation may have some gaps;;Yes;;Other;It requires examples and case studies to better determine high-risk AI applications. These can be obtained from industry and research centres and from an interdisciplinary perspective (e.g. not just from AI developers but from various end users and members of society);diminishing children's rights, freedoms and futures;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;EC_white_paper_response-FINAL.docx
F514777;27-04-2020 14:53;English;Other;Michael;Puntschuh;;iRights.Lab GmbH;;Small (< 50 employees);Germany;The feedback can be published with your personal information;4 - Important;4 - Important;5 - Very important;3 - Neutral;4 - Important;4 - Important;Coordination and Co-creation with Civil Society;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;4 - Important;4 - Important;"Strengthen implementation of human rights standards during all stages of development and application; Promote use of AI by non-profit sector";4 - Important;5 - Very important;4 - Important;Support the establishment of micro-funding schemes for AI innovation and research in non-profit organizations;4 - Important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;Promote building of networks with civil society organizations;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"The framing of the goal to be ""trust"" can be problematic, as it seems to focus on citizens as consumers and trust as necessary for consumption of AI systems and services. This makes scenarios imaginable, where e.g. more transparency is not provided because it might risk trust in a system (security/trust by obscurity). Rather, the goal should be societal well-being and the protection of human rights.";There is a need for a new legislation;;Other;There needs to be a risk-based classification with new requirements for not only the highest level of risk, but rather proportional to the risk level.;;;There is no simple answer to this. Rather, we need a classification system. This classification must be based on the overall potential damage an AI system may cause in its respective social process. Decisive factors in assessing this potential are the intensity of the potential harm of the AI-system and the dependence of the affected person(s) on the respective decision (see works of Krafft/Zweig and risk matrix in document attached).;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;The labelling system should be mandatory in certain areas, e.g. public sector use of AI or with a certain risk level. This would need to be further specified. We have made a suggestion of how such a labelling system could look like (see attached document).;A combination of ex-ante compliance and ex-post enforcement mechanisms;;#NAME?;Personal security risks;Specification on the balance between transparency (disclosure of functioning of AI systems) and the risk for manipulation.;Yes;;No opinion;;No opinion;;;From_Principles_to_Practice.pdf
F514776;26-04-2020 21:59;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;4 - Important;4 - Important;5 - Very important;3 - Neutral;5 - Very important;Integration of AI in the education sector (schools etc.) - already pupils need to get in touch with AI and need to understand benefits/risks and challenges of AI.;4 - Important;3 - Neutral;5 - Very important;3 - Neutral;4 - Important;5 - Very important;Building the bridge to integrate the research outcomes into established companies. Currently, if they use, they use many times AI solutions from IT companies that come from USA and having monopoly or leading market position. How to convince companies to switch/integrate to EU solutions?;2 - Not important;5 - Very important;5 - Very important;;5 - Very important;3 - Neutral;4 - Important;5 - Very important;3 - Neutral;"Very important to generate AI awareness at SME's (technology, ethics, market, legal, intellectual property,...). How to move the companies from the ""glory past"" to the common good AI future?";3 - Neutral;5 - Very important;4 - Important;4 - Important;3 - Neutral;4 - Important;Adoption of legal system to fast developing AI technologies.;Current legislation may have some gaps;;Yes;;Yes;;From technology point of view: machine learning / deep learning / reinforcement etc. From domain point of view: Usage in government environment (electricity, legal/law court, hospitals, ..), supervision (airport, military, people surveillance,...).;5 - Very important;3 - Neutral;4 - Important;4 - Important;4 - Important;4 - Important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Very much;Central EU labelling system and an option also for non high-risk AI systems.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;Biased training data and biased algorithms.;Yes;;Yes;;Yes, for all AI applications;;Generate more sub-categories (not only high- and low-risk). Based on the categories different liability rules become effective. ;
F514775;26-04-2020 00:46;Greek;EU Citizen;????????;??????????;;;;;Greece;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;4 - Important;5 - Very important;4 - Important;4 - Important;3 - Neutral;3 - Neutral;?? ?????????? ??? ??????? ???? ?????????? ???? ?? ?????? ???????? ?????????? ??????? ?? ?????????? ??????????? ???????????? ??? ??????? ?? ????????? ?????? ???? ??????? ????????? ?? ?????????? ??????????? ????????. ????????????? ??? ?? ??????????????? ???’ ???????? ???? ?? ???????, ?????????????? ??? ?????????? ???????????, ???? ??? ??? ???????? ????????? ??? ??????? ??? ?? ??? ?? ??????? ??? ??? ????????????? ?????????, ???? ??????????? ??? ? ???????????? ??? ????????? ??? ?????????.;Other;? ???????? ??? ???? ???????????? ???????? ??? ?.?. ??? ??? ??, ?? ????????? ??? ?? ???? ??? ?? ?????????????? ???????? ?????????? ???? ?? ???????????? ??? ?????????? ????? ????? ??? ?? ??????????? ????? ??? ?????? ?????. ???? ??????, ? ?????? ????????? ??? «?????? ??????» ?????????????? ??? ??? «????????» ??? GDPR ?????? ??? ??? ??????? ?????????, ??? ??? ????? ???????? ??? ????????????? ??????? ??????????? ??? ???????????? ??? ?????????? ????? ????? ??? ??? ???????? ??????????? ????? (?????).;Yes;;Yes;;? ?????? ??? ???? ??? ??? ????????????, ???? ??? ???? ??? «????????» ??? GDPR, ????????? ????????? ??????? ??????? ????????? ?????????? ? IP ???????????, ?? ????? ??? ??????????????? «???????????? ?????????» ??? ??? ????????????? ???? ??????, ??????? ????????? ?? ?????????? ??? ?? ?????????????? ?????? ???????? ??????? ?? ??? ??????????????? ??? ? ??? ?? ?????????????? ??? ?????? ????? ??? ??????????, ??? ??? ?????? ?? ?????????? ?????????? ??????????? ??? ??????? ?? ????????? ??????.;4 - Important;5 - Very important;3 - Neutral;4 - Important;4 - Important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;?????????? ???????????? ??? ??????? ?? ?????????? ??????????? ???????????? ??? ??????? ???? ?? ????? ??? ??????? ????????? ??? ????? ???????????? ??? ??????????? ?????????? ?????? ???? ??? GDPR, ?????????? ?? ?????????? ????????? ??? ??????? ?? ?????????? ??????????? ???????????? ??? ??????? ?? ????? ???? ????????? ??????. ? «????????» ??? GDPR ?????? ???? ????? ??? ??????? ?????????, ??????? ???????????? ?? ?????? ????? ???? ????? ????????????? ??????????.;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;? ????????? ???????? ?????? ???? ??? ?????? ??? ???????? ???????????? ?? ???? ??? ???????????? ??? ????????-?????????? ???? ????? ?? GDPR, ?? ??????????? ??? ??????? ??????? ?????? ??? ?.?. ???? ??? ???????? ????????? ???????????? ??? ?????????? ????? ????? ??? ??? ???????? ??????????? ????? ?? ??? ?? ????? ???? (??. ??????. ?14/420 & ?19/3803, ????. ??. 157 & 313/2017 ??????????? ?????????? ??????, ????. ?.?. 3889/2019 ??????????? ?????? ??????, ????. ??. 21/2019 ?????).;;;;;;;;;;_______________________________________________________________________________________________________.doc
F514774;25-04-2020 18:45;Spanish;EU Citizen;Jesús;Cordero González;;;;;Spain;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;"No hay una “tecnología responsable” sin una necesaria “transición humana responsable”.
Debemos educarnos para:
(i) reproducir y replicar nuestras acciones cotidianas en la dimensión digital,
(ii) pensar y establecer una réplica en ellas de nuestros valores sociales y políticos colectivos, 
(iii) aplicando asimismo estándares éticos y morales a nuevas situaciones cotidianas
que se generan;

Y es por ello que debemos desarrollar una Estrategia de Educación Digital ciudadana ";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Debería estudiarse el impacto cognitivo colectivo de la dimensión digital en la sociedad europea. Es importante aquí la siguiente conclusión: por falta de referentes y de educación digital, dejamos que sean otros los que den continente y contenido a nuestros actos en la esfera digital, con las consiguientes repercusiones en nuestra vida emocional, social y política.;5 - Very important;5 - Very important;5 - Very important;Será necesario un apoyo presupuestario decidido, acorde a las capacidades que se quieren desarrollar y atraer talento internacional con condiciones que superen a nuestros competidos directos a Oeste y Este.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;;There is a need for a new legislation;;No;;;;La aplicación de la IA en procesos relacionados con la gobernanza, como sucede con los sistemas de asignación de ayudas públicas o en la resolución de disputas judiciales.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F514773;25-04-2020 18:05;English;EU Citizen;NIELS;ZIBRANDTSEN;;;;;Denmark;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;4 - Important;Super AI and Artificial Mindset (MindFuture ™);5 - Very important;5 - Very important;4 - Important;5 - Very important;3 - Neutral;5 - Very important;European SECURE dataspace - as a distributed dataspace, reducing single point of failure and single hacker attack entity.;4 - Important;5 - Very important;5 - Very important;Distributed lighthouse research - the world is decentralized and EU should aim for inclusion of all member states as participant and subjects as knowledge transfer.;4 - Important;2 - Not important;5 - Very important;5 - Very important;5 - Very important;The effort should also include large enterprises in close cooperation with small and medium enterprises.;3 - Neutral;4 - Important;4 - Important;5 - Very important;2 - Not important;5 - Very important;Please refer to MindFuture.ai addressing debate on the above issues.;Current legislation may have some gaps;;Yes;;No;;Infinite compute power and distributed global dataset can not be regulated - it might set back Europe.;3 - Neutral;3 - Neutral;2 - Not important;3 - Neutral;5 - Very important;5 - Very important;No further guidelines or regulations are needed;;Not at all;It will add additional cost of European products and a lot of administrative work and challenge EU competitiveness.;Other enforcement system;This section is written by legal :-) ;Common sense and a strong competitive evaluation team to be allocated by EU to the individual member states in case of challenging AI or Artificial Mindsets;Cyber risks;;No;;Yes;;Yes, for all AI applications;;;
F514772;25-04-2020 09:49;English;Public authority;Marya;AKHTAR;National;The Danish Institute for Human Rights;;Medium (< 250 employees);Denmark;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Please see attached letter ;There is a need for a new legislation;;;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;;;;;;;;;;;;;;;Consultation_Response_AI.PDF
F514771;25-04-2020 07:44;English;Business Association;Jon;Camilleri;;TechTarget;;Micro (< 10 employees);Malta;The feedback can be published with your personal information;3 - Neutral;3 - Neutral;4 - Important;4 - Important;4 - Important;;Investment;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;Investment;3 - Neutral;3 - Neutral;3 - Neutral;Investment that is not controlled by intermediary auditors who keep percentages for own leisure.;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;Investment and support;;;;;;;;There is a need for a new legislation;;Yes;;Yes;;Investment and other resources, since you are a business analyst like me we cannot be negligent towards our political priorities.;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);;Rather not;Not sure;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Mental health risks;Investment, that is not an advertisement of an investment, financial services that cut their crap for a change they cannot be allowed to control  the market through political influence and social circles of public relations, other people have equal rights in the market according to same European law, if anything compliance should increase in its conscious ness and less online bullying should be observed or downplaying due to own email signature and position mantras.  Trolling is unacceptable.;Yes;Yes;Yes;yes;Yes, for specific AI applications;There are many ideas naturally you can survey schools and universities.;Yes, plenty;document.pdf
F514770;24-04-2020 16:56;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;4 - Important;4 - Important;4 - Important;3 - Neutral;4 - Important;;5 - Very important;5 - Very important;3 - Neutral;4 - Important;4 - Important;3 - Neutral;;4 - Important;4 - Important;4 - Important;;4 - Important;4 - Important;5 - Very important;5 - Very important;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;Yes;;No opinion;;;5 - Very important;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;No opinion;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Personal security risks;;Yes;;;;No;;;
F514769;24-04-2020 16:16;English;NGO (Non-governmental organisation);Paul;DE RAEVE;;European Federation of Nurses Associations - EFN;87872442953-08;Micro (< 10 employees);Belgium;The feedback can be published with your personal information;3 - Neutral;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;5 - Very important;The engagement of the frontline nursing workforce should be included. They should be pro-actively engaged in the co-design  process formulating end-user  requirements that guide digital developments and innovation, ensuring fit-for-purpose solutions.;5 - Very important;4 - Important;3 - Neutral;4 - Important;5 - Very important;4 - Important;EU Member States should proactively engage with the frontline nursing workforce. That would ensure that the deploy of AI  tools  and  systems  that reduce the  workload  of  nurses and safeguard the quality of care. AI should go beyond the traditional perspective of the medical model and be more inclusive towards all healthcare professions in the health and social care ecosystems.  ;5 - Very important;5 - Very important;3 - Neutral;Nursing research input is key to make deployment of AI frontline more fit-for-purpose. Building trust with the end-user is key to make research findings implemented. Implementation research is key to build trust!;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;SME need to build their work, development on end-user engagement, a co-creation and co-design approach, to make sure investments lead to concrete fit-for-purpose outcomes. End-user requirement are key: listen to what the nurses need and want! ;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;"end-user co-creation is key to develop a regulatory framework; Nursing Regulators are key to translate nurses needs into an AI regulatory framework build on TRUST. ";There is a need for a new legislation;;Yes;;Yes;;Nurses experiences to make informed decisions in the clinical pathways, that impact on human activities and behaviour, are AI high-risk areas: machine learning and clinical expertise need to be complementary, not competitive and disruptive. ;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);EHR developments and consent are key in clinical pathways - informed consent is key in nursing care.;Much;Building trust is key! End-user engagement builds trust! Listen careful to what the end-user needs so tools become fit-for-purpose.;Other enforcement system;Engaging with the nursing workforce and with the end-users in the deployment of AI tools. A co-creation process is key.;Make systems deployable, easy to use, so that tools decrease nurses workload!;Mental health risks;healthcare service liability stays with the healthcare provider;Yes;clinical risk assessment procedures are key to be developed by the regulatory body of the profession.;Yes;Engage the nursing profession regulation in this development.;Yes, for all AI applications;;EU coordination is key for the healthcare sector in the EU;
F514768;24-04-2020 13:56;Spanish;EU Citizen;Juan;LLORET;;;;;Spain;The feedback can be published with your personal information;3 - Neutral;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;1 - Not important at all;"-Formar y fomentar la credibilidad real en la sociedad. 
-Fomentar la credibilidad entre ingenieros, investigadores y otro personal sobre las realidades de la Inteligencia Artificial, que supone un cúmulo de promesa desde hace más de 50 años y probablemen";5 - Very important;5 - Very important;1 - Not important at all;5 - Very important;5 - Very important;5 - Very important;-Fomentar, siguiendo el ejemplo de Suecia, el conocimiento y la participación ciudadana.;3 - Neutral;4 - Important;1 - Not important at all;;1 - Not important at all;5 - Very important;3 - Neutral;3 - Neutral;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;Current legislation may have some gaps;;Yes;;Yes;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Very much;;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;;Risks related to the loss of connectivity;Riesgos particularizados para el sector de la medicina y de los vehículos autónomos y del uso de armas.;Yes;;Yes;;Yes, for all AI applications;;;
F514767;24-04-2020 11:51;German;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;Förderung von Schulen und Vorbereitung der Kleinen auf ein Leben und Arbeiten mit KI. Wir können nur erfolgreich sein, wenn unsere Kinder von klein auf lernen damit umzugehen und nicht erst im Studium.;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;;3 - Neutral;5 - Very important;4 - Important;;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;;4 - Important;5 - Very important;4 - Important;3 - Neutral;4 - Important;5 - Very important;;There is a need for a new legislation;;No;;;;"Datenqualität ist das wichtigste, deshalb müssen Vorschriften geschaffen werden, die das regulieren. Gerade in Bezug auf KI muss es Regularien geben. Andernfalls wird sich zu sehr auf die ""Technik"" verlassen. Gerade in der Medizintechnik sehr wichtig.";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F514766;23-04-2020 16:45;Portuguese;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;1 - Not important at all;4 - Important;3 - Neutral;More funding for Private/Academic partnerships, with Academic institutions leading.;5 - Very important;2 - Not important;3 - Neutral;1 - Not important at all;4 - Important;2 - Not important;Funding for openining data from public entities, without any restrictions for citizens of that entity.;4 - Important;5 - Very important;2 - Not important;;2 - Not important;2 - Not important;5 - Very important;5 - Very important;2 - Not important;;3 - Neutral;3 - Neutral;3 - Neutral;5 - Very important;3 - Neutral;3 - Neutral;;Current legislation may have some gaps;;Yes;;No opinion;;;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;4 - Important;3 - Neutral;No further guidelines or regulations are needed;;Much;;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;;Mental health risks;;Yes;;No;;No;;;
F514765;23-04-2020 08:54;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;3 - Neutral;4 - Important;3 - Neutral;4 - Important;;5 - Very important;2 - Not important;3 - Neutral;4 - Important;3 - Neutral;1 - Not important at all;;1 - Not important at all;5 - Very important;1 - Not important at all;;4 - Important;4 - Important;4 - Important;4 - Important;1 - Not important at all;;4 - Important;4 - Important;2 - Not important;2 - Not important;2 - Not important;3 - Neutral;;There is a need for a new legislation;;Yes;;No opinion;;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;Biometric identification systems should never be allowed in publicly accessible spaces;;No opinion;;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;;Personal security risks;;No opinion;;No;;No;;;
F514754;22-04-2020 20:54;English;Academic/Research Institution;Jan;BERGER;;2b AHEAD ThinkTank;none;Small (< 50 employees);Germany;The feedback can be published with your personal information;5 - Very important;5 - Very important;4 - Important;No opinion;;5 - Very important;#NAME?;4 - Important;4 - Important;5 - Very important;3 - Neutral;4 - Important;3 - Neutral;"- dispel 1970's prejudices against technology
- promote data driven smart society benefits through a multitude of smaller pilots
- found and fund equivalents of DARPA";No opinion;4 - Important;No opinion;While lighthouse research centers are important and while PPP may be of benefit, such institutions must not be overregulated politically driven monsters of bureaucracy.;4 - Important;5 - Very important;5 - Very important;2 - Not important;4 - Important;Partnerships between SMEs, larger enterprises and academia will be beneficial once academia adapts to modern collaboration formats, agility and data-driven mindsets. These qualities are, at least in Germany and Austria, few and far between. As long as this is the case, academia will be detrimental to the work of Digital Innovation Hubs.;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;2 - Not important;All of the listed concerns are important, as they exist and need to be addressed. In the overwhelming majority of cases they stem from a fundamental misunderstanding of what AI actually is. Each concern can be addressed through practicable and implementable measures. AI only endangers safety, fundamental rights etc. to the extent that humans handle it wrong. It's human error, not AI that's the problem!;Current legislation may have some gaps;;Yes;;No;;any AI whose outcome makes decisions and whose input training data was not monitored by trained personnel that can detect and wipe out unethical bias carryies the potential of danger to individuals and society.;5 - Very important;3 - Neutral;5 - Very important;No opinion;5 - Very important;4 - Important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;Biometrics are generally pretty bad identification systems when it comes to fraud. Fingerprints, irises etc. can be easily copied and misused. While other identification systems can also be copied and misused, prime number based encryption keys do have the advantage that they can be tossed and a new one generated. You can't toss and regenerate a thumb or an iris of an individual, yet.;Not at all;"Would you label an OCR software? This was one of the first industrially used neural networks. That is (or rather was) AI. Again, I believe it would be better to ""label"" people who understand AI (through certificates) before they're allowed to speak or make policy on it.";A combination of ex-ante compliance and ex-post enforcement mechanisms;;legislation or guidelines shoudl enforce AI anomaly response procedures in public and private entities.;;;Yes;;No opinion;;No opinion;;;
F514753;22-04-2020 17:49;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;Please see our seperate document with the specific answers to the Consultation.;4 - Important;4 - Important;4 - Important;3 - Neutral;5 - Very important;5 - Very important;Please see our seperate document with the specific answers to the Consultation.;4 - Important;4 - Important;4 - Important;Please see our seperate document with the specific answers to the Consultation.;4 - Important;3 - Neutral;4 - Important;4 - Important;3 - Neutral;Please see our seperate document with the specific answers to the Consultation.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Please see our seperate document with the specific answers to the Consultation.;There is a need for a new legislation;;No;;;;Please see our seperate document with the specific answers to the Consultation.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;Please see our seperate document with the specific answers to the Consultation.;Rather not;Please see our seperate document with the specific answers to the Consultation.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Please see our seperate document with the specific answers to the Consultation.;Mental health risks;Please see our seperate document with the specific answers to the Consultation.;Yes;Please see our seperate document with the specific answers to the Consultation.;Yes;Please see our seperate document with the specific answers to the Consultation.;Yes, for all AI applications;;Please see our seperate document with the specific answers to the Consultation.;20200422_AI_consultation_specific_responses_UNI_Europa.pdf
F514752;22-04-2020 17:04;English;Company/Business organisation;John;Prime;;Amp X;929948136531-22;Small (< 50 employees);United Kingdom;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;4 - Important;No opinion;4 - Important;;5 - Very important;3 - Neutral;3 - Neutral;5 - Very important;5 - Very important;3 - Neutral;;5 - Very important;4 - Important;4 - Important;;3 - Neutral;4 - Important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;4 - Important;2 - Not important;;No opinion;1 - Not important at all;;There is a need for a new legislation;;No opinion;;;;;5 - Very important;5 - Very important;;4 - Important;3 - Neutral;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Cyber risks;;Yes;;Yes;;Yes, for specific AI applications;;;
F514751;22-04-2020 16:40;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;;4 - Important;4 - Important;4 - Important;3 - Neutral;5 - Very important;5 - Very important;;5 - Very important;4 - Important;5 - Very important;In the white paper the role of AI at the edge, and HW for AI is an important aspect. Is the EC aware of the lost opportunity in ICT48 to fund any proposals in this area?;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
F514750;22-04-2020 12:19;Portuguese;Company/Business organisation;João;Mourinho;;Sistrade Software Consulting, SA;;Medium (< 250 employees);Portugal;The feedback can be published with your personal information;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;4 - Important;"1) Focusing in providing adequate funding opportunities to allow the uptake of AI technology by SMEs, which constitute the majority of the European Ecomic tissue and do not have the resources to undertake research and innovation actions in the AI field.
2) Focusing on developing EU-wide cloud platforms, fundamental for big data storage and processing, which are the base for AI. United States has Google, Amazon and Microsoft. China has Alibaba, KS, Tencent, etc. Europe has none.";4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;4 - Important;;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;The data aquisition, access and stored used by AI should be focus of tight scrutiny.;There is a need for a new legislation;;Other;They should be enforced to all applications, not only high risk. Who controls the risk assessment?;;;AI for health insurance risk profiling. AI for political weaponisation (reduction of public trust on democracy and societies).;3 - Neutral;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Rather not;There is no way to absolutely guarantee that the information will not be exploited by companies or rogue governments or agencies to exploit user data.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Risks related to the loss of connectivity;;Yes;;Yes;;Yes, for all AI applications;;;
F514749;22-04-2020 11:02;Spanish;Trade Union;José Manuel;Fernández;;Sindicato de Trabajadoras y Trabajadores de la Enseñanza de la Región Murciana - Intersindical;;Large (250 or more);Spain;The feedback can be published with your personal information;5 - Very important;4 - Important;4 - Important;2 - Not important;1 - Not important at all;4 - Important;El desarrollo de la IA es una cuestión de interés público y, por lo tanto, no debe estar sujeto al riesgo de que los intereses privados dirigan la gobernanza europea.;4 - Important;4 - Important;4 - Important;2 - Not important;3 - Neutral;3 - Neutral;;5 - Very important;4 - Important;1 - Not important at all;;3 - Neutral;4 - Important;4 - Important;3 - Neutral;3 - Neutral;;3 - Neutral;5 - Very important;5 - Very important;2 - Not important;3 - Neutral;3 - Neutral;- La IA puede conllevar un mal uso de la gran cantidad de datos que maneja. Tanto la recopilación, el almacenamiento como el análisis de los datos debe ser ejercido con las máximas garantías de transparencia, con claras indicaciones de para qué va a ser u;There is a need for a new legislation;;No;;;;"Toda aquella apliacción de IA que tiene cualquier forma de impacto en las vidas humanas debe ser considerada de ""alto riesgo"" y, por lo tanto, estar enmarcada en un marco regulatorio fuerte que tenga presente el respeto a los derechos humanos y la dignidad como principio fundamental.";5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Rather not;Los sistemas de etiquetado en ningún caso pueden ser voluntarios porque de serlo pierden el sentido de la finalidad con el que se crea un sistema de etiquetado: el control y la trazabilidad de todos los sistemas de IA sin excepción. Al igual que sucede con otro tipo de productos que existen en la actualidad.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;"En cualquier caso, aunque se produzca la combinación de mecanismos de cumplimiento antes y después de la comercialización es imprescindible enfatizar la fase previa y evitar que esta sea realizada por agentes del ""mercado"" o por agencias externas que no siempre van a compartir los interés públicos que deben ser defendidos.";Mental health risks;Riesgos relacionados con el aumento de la desigualdad;Yes;;Yes;;Yes, for all AI applications;;;
F514748;22-04-2020 09:47;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;The multilingual reality of the EU (not only official languages, but also regional languages as well) should be taken into consideration to ensure the adoption of AI by all the society. If not, we'll develop technology focused on big languages linke english as always, and that's not the realituy of european society.;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;;4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;No opinion;No opinion;4 - Important;;Current legislation may have some gaps;;No opinion;;;;;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;Biometric identification systems should never be allowed in publicly accessible spaces;;Rather not;;No opinion;;;Personal security risks;;Yes;;No opinion;;No opinion;;;
F514747;21-04-2020 08:57;English;Company/Business organisation;Lukáš;Kova?ík;;Bohemian AI Technologies s.r.o.;064 03 077;Micro (< 10 employees);Czech Republic;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;1 - Not important at all;;5 - Very important;5 - Very important;3 - Neutral;;3 - Neutral;4 - Important;5 - Very important;5 - Very important;5 - Very important;;3 - Neutral;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;;Current legislation may have some gaps;;Yes;;No opinion;;;5 - Very important;3 - Neutral;5 - Very important;4 - Important;5 - Very important;3 - Neutral;Biometric identification systems should never be allowed in publicly accessible spaces;;Not at all;;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;;Personal security risks;;No opinion;;No opinion;;No opinion;;;
F514746;20-04-2020 20:08;English;Company/Business organisation;Peter;Villax;;Mediceus Dados de Saúde SA;968744537889-50;Micro (< 10 employees);Portugal;The feedback can be published with your personal information;3 - Neutral;4 - Important;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;Educating decision-makers should be encouraged. Taking decisions in extremely complex fields requires very complex knowledge. Decision-makers, as a minimum, should learn the basics of AI, cryptography and of intrinsically-secure systems architectures. If decision-makers cannot understand what they are deciding, it prevents them from taking the right decision. Delegating decisions to experts is not appropriate.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Of all European Data Spaces, the Health Data Space is a priority, as it is the most backward and has the greatest potential to improve life and quality of life.;4 - Important;4 - Important;5 - Very important;Give very clear guidelines to businesse wishing to process sensitive classes of data, specifically GDPR article 9 data.Determine rules of engagement between private business and public sources of data. If the right of access to data has been delegated to a data controller by data subject and data subject has consented to processing of its data by same controller, data processor or controller currently storing it must cooperate with mandated controller to allow access, subject to consent terms.;3 - Neutral;4 - Important;3 - Neutral;3 - Neutral;5 - Very important;;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;3 - Neutral;These questions shows how decision-makers react on the basis of fear and ignorance. Rather than think that AI may breach fundamental rights, YOU decision-makers should challenge US developers to guarantee that our products are safe for use and clearly attribute to us the liability for mistakes. You should also learn to spot excuses: it is not the user's fault if he fails to upgrade software in spite of manufacturers' requirements. It is the manufacturer's to allow it run without being upgraded ;There is a need for a new legislation;;Yes;;Yes;;In a critical area such as health, any AI application where the outcome has not been tested and validated is high risk. AI should be regulated as a new class of medical device.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;Widespread face recognition should only be allowed with strong pseudonymization measures in place. Re-identification of persons of interest should only be via court order.;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Just as there are Qualified Persons in the pharmaceutical industry, who are personally responsible for the quality of the products, there should be QPs in the AI industry for high-risk applications, as well.;Risks related to the loss of connectivity;;No;In the case of high-risk applications, liability should always rest with the AI deployer, through the life of the product. If the deployer can no longer reasonably guarantee the safety of the product, he should lock its use remotely, after warning user. ;Yes;In the case of high-risk applications, liability should always rest with the AI deployer, through the life of the product. If the deployer can no longer reasonably guarantee the safety of the product, he should lock its use remotely, after warning user. ;Yes, for specific AI applications;High risk applications, namely health and autonomous vehices.;;
F514745;20-04-2020 18:56;French;Public authority;Romain;Sohet;National;comrcd;628696437625-48;Large (250 or more);France;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;Be sure that counterparts respect current laws as GDPR.;4 - Important;3 - Neutral;4 - Important;3 - Neutral;5 - Very important;5 - Very important;Skills have to be completed, mainly not only technics or technologics but also sociologists and philosophics.;5 - Very important;4 - Important;5 - Very important;Work with existing NTIC firms like Atos or CapGemini which have yet a very high and focused skill in IA.;4 - Important;4 - Important;4 - Important;3 - Neutral;3 - Neutral;We have to be sure firstable about the project adequation and the ability of SME counterpart before of financing it.;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;AI weakness will always be the truth of delivered informations.;Current legislation may have some gaps;;No;;;;"Problem of AI is that opening way to bullying, and that is a ""low risk using"" by everybody.";4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Very much;A not considered high-risk system now can develop itself and becom an high-risk system one later, so we have to anticipated.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Justice has to be involved by the way.;Personal security risks;Mental health risks has to be evaluated firstable by doctors.;Yes;;Yes;;Yes, for all AI applications;;;
F514744;20-04-2020 16:18;French;Company/Business organisation;laetitia;Pouliquen;;NBIC WATCH;NANO BIO INFORMATION COGNITIVE TECHNOLOGY WATCH;Micro (< 10 employees);Belgium;The feedback can be published with your personal information;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;3 - Neutral;4 - Important;;5 - Very important;5 - Very important;3 - Neutral;4 - Important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;4 - Important;;4 - Important;4 - Important;3 - Neutral;4 - Important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;les personnes vulnérables  qui ne sont pas en mesure d'accéder à des décisions prises par IA doivent se voir proposer les mêmes services par des moyens humains;Current legislation may have some gaps;;Yes;;Yes;;;3 - Neutral;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;Other special requirements in addition to those mentioned in the question above should be imposed (please specify);systèmes d’identification biométrique autorisés à des fins de contrôle policier et judiciaire uniquement;Very much;Label de type Ethics Inside comme pour les normes qualité/conformité ou bio garantissant le respect des droits fondamentaux pour tous, y compris les plus vulnérables;Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;une option OPT OUT devra permettre à l'utilisateur de refuser une prise de décision par IA;Mental health risks;;Yes;inversion de la charge de la preuve à envisager pour responsabiliser les industriels/chercheurs/développeurs;No;selon nos interlocuteurs chercheurs ou industriels, cette directive remplit bien son office pour le moment. il est certain que d'ici 5 ans, la vision objective de ses manquements sera plus claire;Yes, for specific AI applications;données personnelles gérées par les services publiques, secteur santé, défense, éducation, données utilisées à des fins commerciales (reconnaissance faciale dans les magasins par exemple);les directives européennes rempliront cet office au moment nécessaire. sinon la CJUE ou la CEDH pourront être saisies;
F514743;20-04-2020 14:29;English;Academic/Research Institution;Virginia;Dignum;;Umeå  University;;Large (250 or more);Sweden;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Dealing with the challenges, risks and benefits of AI for humanity and society requires more than bridging between data-driven and model-driven computational approaches to AI. In order to navigate this potential, explore opportunities and mediate challenges, it is essential to bring humanities and social sciences into the conversation. Only then and in collaboration with public and private stakeholders, we can set the direction towards a sustainable and trustworthy AI future.;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;;3 - Neutral;5 - Very important;5 - Very important;"focus on fundamental research; focus on multidisciplinary aspects of AI; and above all, take a broad view on AI methods and theories (extending the current focus on deep learning, data-oriented methods)";5 - Very important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;"engage with existing local hubs; ensure collaboration with regional and local governement";4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;"presenting AI as THE solution to a problem; over-reliance on AI based solutions; ignore limitations";There is a need for a new legislation;;No;;;;The issue is not only one of high risk, but also on understanding what are the consequences of labeling an application as 'low-risk'. Given a too abstract and too narrow definition of AI, such as used in the Whitepaper, will create the opportunity for loopholes.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;;Not at all;"How would such a system would be monitored and enforced?
See also our comments on the high risk section. ";Other enforcement system;there is not a one size fits all;"consider the introduction of certification mechanisms; facilitate and engage standardisation; provide assessment tools and maturity level mechanisms for all organisations, open source, transparent,";Mental health risks;"AI is a multi-purpose technology; focus needs to be on the application and ensure a basic trusted high level of quality for the technology";Yes;;Yes;;Yes, for all AI applications;;;Comments_on_AI_200415.pdf
F514742;20-04-2020 10:10;English;EU Citizen;Moritz;SCHWAN;;;;;Germany;The feedback can be published with your personal information;4 - Important;3 - Neutral;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;;4 - Important;3 - Neutral;4 - Important;3 - Neutral;5 - Very important;4 - Important;;3 - Neutral;4 - Important;4 - Important;;3 - Neutral;5 - Very important;5 - Very important;4 - Important;4 - Important;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;;There is a need for a new legislation;;No;;;;;3 - Neutral;4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Cyber risks;;Yes;;Yes;;No opinion;;;
F514741;20-04-2020 09:40;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;4 - Important;3 - Neutral;4 - Important;5 - Very important;;5 - Very important;5 - Very important;4 - Important;3 - Neutral;5 - Very important;4 - Important;;5 - Very important;3 - Neutral;4 - Important;;4 - Important;3 - Neutral;4 - Important;5 - Very important;3 - Neutral;;3 - Neutral;5 - Very important;4 - Important;5 - Very important;3 - Neutral;3 - Neutral;"AI could make the concept of ""free will"" at risk + the benefits of AI should be measured according to qualitative or quantitative criteria (what does it mean to be ""more efficient"" or ""optimized""? + concerns that AI is not explained enough to users";Current legislation may have some gaps;;No;;;;;5 - Very important;4 - Important;4 - Important;4 - Important;3 - Neutral;3 - Neutral;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;Cognitive risks ;Yes;;Yes;;Yes, for specific AI applications;AI applications that can cause physical or mental damage;;
F514740;19-04-2020 22:30;German;EU Citizen;Sarah;Lewandowski;;;;;Germany;The feedback can be published with your personal information;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;Clear standards and guidelines foster further development of the usage of AI. These guidelines and standards should be reviewed by the privat sectore as well, to guarantee, that these can be applied and answer all important questions. ;5 - Very important;3 - Neutral;4 - Important;3 - Neutral;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;I think, that if you do not collaborate with privat sectore, it is hard, also in the research area, to get the talents of todays world against the goole and apples of this world.;3 - Neutral;3 - Neutral;4 - Important;4 - Important;4 - Important;;3 - Neutral;4 - Important;3 - Neutral;3 - Neutral;3 - Neutral;1 - Not important at all;"The understanding of AI needs to be common. ""AI"" does not make ""mistakes"", the developer or the principles do. For me, AI should be part of education, to raise awareness.";There is a need for a new legislation;;No;;;;Ai in healthcare area. AI in self-driving e.g.cars. AI used to trade, agree on any requests.;;1 - Not important at all;4 - Important;4 - Important;2 - Not important;4 - Important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;"In my opinion freedome is one of the biggest rights menkind has. Therefore such kind of identificationsystem does not bring any value, beside the fact, that one could maybe ""catch"" one or two killers more - although they will find different ways if you really want to break the law. All the data generated does not bring the value, to justify such kind of reducment of ones freedome. ";Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;We also look for these answers within the healthcare regulatory sector. Testingframeworks should be adapted, also by healthauthorities, to give clear guidelines on these topics.;Mental health risks;;Yes;;Yes;First question - when is an Ai an AI? I think it is crucial to find a clear definition used in this context. ;Yes, for specific AI applications;I think, all AI applications should have the need to a specific insurance on the one hand, and a specific liability check on the other hand. Is it the responsibility of the user or the producer, if any damage happens?;;
F514739;19-04-2020 11:58;Romanian;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;Introducerea în programele de înv???mânt ale ciclului secundar al unor elemente de IA la nivelul UE.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;4 - Important;4 - Important;3 - Neutral;3 - Neutral;;There is a need for a new legislation;;Yes;;Yes;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Other special requirements in addition to those mentioned in the question above should be imposed (please specify);;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F514738;18-04-2020 17:56;German;EU Citizen;Conrad;Conrad;;;;;Germany;The feedback can be published with your personal information;5 - Very important;4 - Important;4 - Important;3 - Neutral;4 - Important;2 - Not important;Es müssen internationale Kontrollgremien und Organisationen begründet werden, die weltweit zusammen agieren. Wenn in China oder den USA eine „Super KI“ entsteht, die in die weltweite Infrastruktur eingebunden wird, muss hierauf eingewirkt werden und müssen Regeln geschaffen werden, wenn ein Europäer betroffen ist (Marktort-Prinzip). Weltweite Standards müssen entwickelt werden.;4 - Important;5 - Very important;2 - Not important;3 - Neutral;4 - Important;5 - Very important;Wichtig sind die Forschung in der Rechtswissenschaft und insbesondere im Datenschutzrecht, um einem entsprechenden Rechtsrahmen frühzeitig zu entwickeln und auch allgemeingültige Regeln festzulegen, die in eine KI zu implementieren ist. Auf dieser Basis muss die KI trainiert werden. Diese Forschung muss interdisziplinär mit anderen Wissenschaften zusammenarbeiten (Ethik, VWL). Gleiches gilt auch für die IT-Sicherheit durch IT-Sicherheitsforschung.;5 - Very important;4 - Important;3 - Neutral;Die Rechtswissenschaft und IT-Sicherheit sind an erster Stelle zu ernennen, weil sie Rechtssicherheit und IT-Sicherheit bringen muss, ehe überhaupt KI-Entwicklung stattfinden. Die KI-Programmierung muss auf diese Regeln aufbauen wie bei der „Menschenleben“-Forschung. Auch die Entwickler brauchen Rechtssicherheit. Aber es fehlt schon an der Definition der „KI“ und Klassifizierung verschiedener Stufen (kontrollierbar bis selbstständig/unkontrollierbar).;3 - Neutral;4 - Important;5 - Very important;4 - Important;3 - Neutral;Es sollte ein europäisches EU-Forschungslabor begründet werden, das eine eigene „KI“ entwickelt und trainieren lässt, um eine „KI“ zu erschaffen, die andere KI und überwachen kann – und um daran im Labor (ohne Menschenbezug) die KI zu studieren, ohne dabei in Verbindung mit echten Daten und echter IT-Infrastruktur zu gelangen. Dies muss staatlich sein und nicht durch ein privatwirtschaftliches Unternehmen geführt werden. Es müssen Auswirkung und Folgen auf Mensch und Gesellschaft geprüft werden.;3 - Neutral;5 - Very important;4 - Important;5 - Very important;5 - Very important;3 - Neutral;KI führt zur Analyse des menschlichen Verhaltens, was mit Unternehmen für Werbung, Angebots- und Preisgestaltung und somit Beeinflussung des Marktes und Wettbewerbs führt. Preise und Inhalte wie auch das Wissen wird beeinflusst und kann zu Gunsten von privatwirtschaftlichen Unternehmen manipuliert werden. Bestimmte Ware wird bestimmten Personen teurer verkauft, nur bestimmte Inhalte angezeigt und bestimmte Personen bekommen keinen günstigen Vertrag oder teurere Versicherungs-/Bankkonditionen. ;Current legislation may have some gaps;;No;;;;Das höchste Risiko sind das Tracking und die Identifikation des Menschen, der weltweit überall durch seine Aktivitäten und biometrische Daten wiedererkannt wird. Ebenso hochriskant sind Verhaltensanalyse, die für Werbung und Wettbewerb und Preise genutzt werden, um Inhalte aber auch den Wettbewerb zu manipulieren. Bestimmte Personen haben dann keinen Zugang mehr oder schlechtere Konditionen/Preise, es existiert keine freie Auswahl an Wissen und Produkt. ;4 - Important;4 - Important;5 - Very important;3 - Neutral;4 - Important;5 - Very important;Other special requirements in addition to those mentioned in the question above should be imposed (please specify);"Die Verarbeitung biometrischer Daten ist in der Regel nur durch die Einwilligung zulässig (Art. 9 DSGVO), hierfür bedarf es eines besseren Einwilligungsmodells, bessere Transparenz in der Aufklärung und geeignete Alternativen, um der Freiwilligkeit gerecht zu werden (Conrad, K&R 04/2020, S. 253 ff).
Es müssten Grafiken und Modelle entwickelt werden zur Darstellung und Aufklärung. Leitlinien für Unternehmen und Transparenzanforderungen müssen europaweit einheitlich entwickelt werden.";Much;Problem: Ein freiwilliges Kennzeichnungssystem ist immer subjektiv und kann vom Anbieter manipuliert werden. Es müssten Standards getroffen werden, die eine Klassifizierung und Risikoeinstufung anhand von Datenkategorien und Folgen vorsehen. Werden Gesundheitsdaten oder Biometrie verarbeitet, muss immer die höchste Risikoklasse angenommen werden. Es bedarf Grafiken/Symbole und transparente Klassen. Anbieter müssen überprüft werden. Das Kennzeichen müsste jährlich überprüft werden.;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;Es müssen rechtliche und technische Vorgaben geschaffen und Klassifizierungen/Risikoeinstufungen bestehen. Hierfür bedarf es Modelle. Es muss ein Prüf- und Kontrollsystem existieren. Aber das wichtigste: Die KI muss erst einmal bestimmt und verstanden werden. Es müssen Betroffenenrechte (Aus-Schalter, Infopflichten, Einwilligung, Widerruf) implementiert und jederzeit ausgeführt werden können. Es darf keine komplette Abhängigkeit bestehen. Die KI muss sich selbst reglementieren.;Personal security risks;"Die KI kann in der Lage sein, in einem späteren Zustand vorher erfasste und gelernte Informationen zu verarbeiten und auszunutzen (erkennt z.B. Krankheiten oder Unfälle oder wirtschaftliche Schäden), was für den Datenschutz und Menschenrechte von Bedeutung ist. Die KI kann neue Erkenntnisse gewinnen, die die Anwendung ursprünglich nicht vorsah und kann somit später heimlich erweitert werden.
Privatwirtschaftliche Unternehmen dürfen diese Informationen nicht verarbeiten und ausnutzen.";Yes;Die KI und deren Ziele / Erkenntnisse müssen jederzeit überprüfbar sein. Es muss überprüfbar sein, welche Daten exakt verarbeitet werden und zu welchem Zweck. Es muss jederzeit erkennbar sein, welches Ziel die KI verfolgt und welche Daten verarbeitet werden (Ton, Gesicht, Emotionen, GPS, Gesundheit usw). Hierfür bedarf es eines Modells der Klassifizierung (Welche Datenkategorien und welche Ziele sind freigegeben oder ewig gesperrt).;Yes;Es ist zunächst überhaupt erst einmal die KI zu definieren und die Rechtsfigur der KI zu bestimmen und wer in der Verantwortlichkeitskette haftet. Haftet der Hersteller oder Entwickler? Und wie ist eine KI, wenn diese selbst verantwortlich handelt, zu bestrafen und zu erziehen? Die Haftungsgrenze ist zu erhöhen. Das Gesetz ist mit anderen Gesetzen in Einklang zu bringen bzw. dort einzubeziehen (DSGVO, UWG). ;Yes, for all AI applications;;"Für Verstöße nach dem Datenschutz besteht derzeit im Hinblick auf die KI eine Haftungslücke – die Verantwortlichkeit ist nicht einmal geklärt. Die Vorgaben nach Art. 25 DSGO sind unbestimmt. Die Anforderungen nach der Einwilligung sind zu allgemein und müssen strenger sein bei der KI (Art. 9 DSGVO). Die DSGVO muss daher angepasst werden (Vgl. Aufsätze hierzu: Conrad, K&R 12/2019, S. 741 – 746; Conrad, DuD 12/2017, S. 740 – 744; Conrad, DuD 09/2018, S. 541 – 546).";kur-12-2018-741-conrad-KI.pdf
F514737;18-04-2020 12:55;English;EU Citizen;Alexandru-Gabriel;Ghergu?;;;;;Romania;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"1) Promote a framework to allow easier intra-EU data collection for AI purposes (current privacy laws make this very difficult even for noble intentions)
2) Promote a public EU-level community/website for datasets to be used by citizens or governments when implementing AI (useful for applications in healthcare, security and environment)";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;5 - Very important;;1 - Not important at all;1 - Not important at all;3 - Neutral;5 - Very important;1 - Not important at all;5 - Very important;;Other;I am concerned that by introducing new legislation we will, again, focus too much on privacy and not encourage innovation and widespread AI use. I think it has great implications for healthcare, security and environment but we will lag behind and not use that to improve the life of EU citizens. Current coronavirus would have been easier to manager with contact tracking, AI use to help doctors with diagnoses in hospitals or even supply chains automation.;Yes;;No;;I think the definition is so broad it could apply everywhere. For me, high-risks applications are those whose decisions would not be validated and would have direct impact on the health of individuals. For example, an AI performing surgery or diagnoses alone would be high-risk but an AI used to help doctors or help police track criminal activities in public would not be high-risk.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No further guidelines or regulations are needed;"As an EU citizen, I would like to reap the benefits of AI as soon as possible. Biometric identification could greatly improve safety when it is adopted by authorities. It also improves the speed in airports or other places where you have to identify yourelf (see US). I would like the commission to help more in spreading AI rather than over-regulate it as was the case for GDPR. GDPR is good but the data-sharing strategy you're proposing now should have been adopted before GDPR. This has caused slow-down on multiple businesses fronts (especially AI) and a decrease in our competitive ability against US and China.

I would like to add another thing, as an EU citizen, I don't want you to ban this technology for me and my government. Technology moves faster than laws. If you ban it for 5 years, by the time the ban wears off, we'll have missed a lot of benefits. AI has potential to be as transformative as the industrial/electrical revolution.

Not the very least, many people have lost their lives in the current coronavirus pandemic. I think it is unethical to not use technology to improve human lives. True, at some point you have to weigh privacy vs security but from my point of view, human life itself is above all. You can't have privacy when you're dead.";Much;In my opinion this could be helpful in gaining public trust but the labeling system should be developed by researchers/scientists in the industry working on AI rather than politicians that are not really aware of today's AI capabilities and have watched too many Terminator movies.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Assessment should be made by conflict-free specialized people;Risks related to the loss of connectivity;;No opinion;"I don't know what ""important changes"" mean. I'd say yes but wouldn't want risk assessment over any minor change.";Yes;;Yes, for all AI applications;;;
F514736;17-04-2020 21:24;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;No opinion;5 - Very important;3 - Neutral;4 - Important;No opinion;;5 - Very important;4 - Important;5 - Very important;3 - Neutral;4 - Important;5 - Very important;;No opinion;4 - Important;4 - Important;;4 - Important;4 - Important;4 - Important;3 - Neutral;No opinion;;4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;3 - Neutral;;Current legislation may have some gaps;;Yes;;Yes;;;5 - Very important;5 - Very important;4 - Important;4 - Important;3 - Neutral;4 - Important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;Yes;;Yes, for all AI applications;;;
F514735;17-04-2020 20:25;English;Company/Business organisation;Irene;Solaiman;;OpenAI;;Medium (< 250 employees);United States;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;No opinion;5 - Very important;4 - Important;;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;;No opinion;5 - Very important;5 - Very important;;No opinion;5 - Very important;No opinion;No opinion;No opinion;;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;No opinion;4 - Important;;Current legislation may have some gaps;;Other;Due to AI's omni-use nature, a system in a seemingly low-risk setting can quickly be adapted to a high-risk application. Similar safety criteria across the risk spectrum will help cover these adaptations. We discuss this further in the attached document.;;;;5 - Very important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;No opinion;;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;;;Yes;;No opinion;;No opinion;;;OAI_EU_White_Paper_Feedback.pdf
F514734;17-04-2020 15:13;English;Academic/Research Institution;Daniel;MÜGGE;;University of Amsterdam;;Large (250 or more);Netherlands;The feedback can be published with your personal information;5 - Very important;3 - Neutral;3 - Neutral;4 - Important;4 - Important;4 - Important;"Clear rules vis-a-vis third-country AI (a form of ""EU AI diplomacy"")";4 - Important;3 - Neutral;3 - Neutral;4 - Important;3 - Neutral;4 - Important;;2 - Not important;5 - Very important;3 - Neutral;;5 - Very important;4 - Important;4 - Important;5 - Very important;3 - Neutral;;4 - Important;5 - Very important;4 - Important;4 - Important;3 - Neutral;2 - Not important;Limitation of human autonomy, for example when AI is used by large corporations to market products;There is a need for a new legislation;;No;;;;Citizen surveillance by public authorities and private companies;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Rather not;I struggle to envision how that would be made meaningful for citizens.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Risks related to the loss of connectivity;;Yes;;Yes;Needs to provide more legal certainty in case of important product changes during lifetime, and be enforceable for consumers even when products are complex and AI-based.;No opinion;;;Mugge_-_Response_to_AI_consultation_April_2020.pdf
F514733;16-04-2020 17:44;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;;4 - Important;5 - Very important;5 - Very important;;3 - Neutral;4 - Important;4 - Important;5 - Very important;4 - Important;;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;3 - Neutral;;Current legislation may have some gaps;;Yes;;Yes;;;4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;No opinion;;Rather not;;No opinion;;;Risks related to the loss of connectivity;;Yes;;Yes;;Yes, for all AI applications;;;AI-end-to-end.pdf
F514732;16-04-2020 16:17;English;Trade Union;Inese;PODGAISKA;;Association of Nordic Engineers;218451731019-24;Micro (< 10 employees);Denmark;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;More prioritisation and investments are needed for ethical AI innovation and research incorporated in actions 2, 3 and 4 ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;Current legislation may have some gaps;;No;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Very much;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;ANE_response_consultation_EU_Commission_White_paper_on_AI_2020_final.pdf
F514731;16-04-2020 13:24;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;Trustworthy and secure supply chains in Europe (by Europe and for Europe). The crucial parts of the supply chains for technologies (i.e. innovation development, IP ownership) and equipment (hardware, e.g. 5G, 6G and beyond) for AI applications must be available to and maintained by European stakeholders (companies, public organisations).;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;;5 - Very important;4 - Important;3 - Neutral;4 - Important;5 - Very important;;There is a need for a new legislation;;No;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;Expanded use of biometric identification systems is unavoidable and will enter public spaces. Standards and ethics must evolve with the technology. Therefore, applications need to be continuously monitored and guidelines likewise reviewed and adapted.  ;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;Yes;;Yes, for all AI applications;;;
F514730;15-04-2020 19:13;German;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;4 - Important;5 - Very important;3 - Neutral;3 - Neutral;2 - Not important;;4 - Important;4 - Important;2 - Not important;3 - Neutral;4 - Important;2 - Not important;;4 - Important;4 - Important;3 - Neutral;;;;;;;;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;;Current legislation may have some gaps;;No;;;;Autonome Waffensysteme, biometrische Identifikationssysteme;;;;;;;Biometric identification systems should never be allowed in publicly accessible spaces;Der Einsatz von biometrischen Identifikationssystemen im öffentlichen Raum bietet großes Missbrauchspotential und bedeutet einen erheblichen Einschnitt in die informationelle Selbstbestimmung der Bürger*innen. ;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;;;No opinion;;No opinion;;No opinion;;;
F514729;15-04-2020 18:43;English;NGO (Non-governmental organisation);Federica;bordelot;;EUROCITIES;12493392840-79;Small (< 50 employees);Belgium;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Local governments are crucial to fostering an ecosystem of excellence in AI in Europe. The EU must work with local governments in the development of the future regulatory framework for a trustworthy AI that supports cities to uptake and upscale AI in Europe.;4 - Important;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;The supported partnership should include also local governments. City authorities can facilitate research and innovation activities and processes, and create the right conditions to boost AI deployment, including by SMEs. Acting as open participation and collaboration platforms, using and making data and information available, city authorities enable crowd-creation, foster experimentation, share technological expertise and co-develop ideas and solutions.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Through experimentation and early adoption of AI,city governments identify possible safety and fundamental rights risks, and propose trustworthy solutions. As the level closest to citizens, local authorities engage with citizens, understand their fears and concerns and develop together possible solutions. The EU must work with local governments in the development of a future regulatory framework for a trustworthy AI that takes into account the principles of the Cities for Digital Right coalition;There is a need for a new legislation;;Yes;;Yes;;More detailed and clear description of the possible uses that are considered high-risk is essential to clearly understand when a local public administration is affected by a specific use. Considering the multitude of possible uses of AI in several sectors of responsibility for public authorities and the complexity of the classification exercise, we call for task force composed of experts on AI and public administration experts, also at local level, to better define the possible high-risks uses.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;"AI systems and algorithms can hide bias which might lead to high risks for people’s safety. Safety and reliability tests of AI systems are burdensome; local public authorities do not have the expertise or the budget to carry them out. A central EU body or agency should develop the necessary verification and validation procedures, and guarantee security and public safety.";Yes;"Liability and accountability in AI are key to guaranteeing people’s safety. Tangible measures must be applied to close the accountability gap such as:
- full access to the algorithm code by the competent authorities whenever needed for
inspection or verification purposes
- obligations to report which algorithms are used
- a framework for algorithmic auditing that supports AI system development end-to-end
- fostering an open source code philosophy";Yes, for all AI applications;;;EUROCITIES_statement_on_AI_final.pdf
F514728;15-04-2020 17:20;German;EU Citizen;Hendrik;Kaldewei;;;;;Germany;The feedback can be published with your personal information;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;;5 - Very important;5 - Very important;1 - Not important at all;4 - Important;1 - Not important at all;4 - Important;;5 - Very important;5 - Very important;1 - Not important at all;;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;Other;;No;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Not at all;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;Anschreiben_von_der_Leyen.docx
F514727;15-04-2020 13:42;French;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;4 - Important;5 - Very important;3 - Neutral;4 - Important;5 - Very important;;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;3 - Neutral;5 - Very important;4 - Important;;4 - Important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;4 - Important;3 - Neutral;;Current legislation may have some gaps;;Yes;;No;;;4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;4 - Important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Risks related to the loss of connectivity;;No opinion;quite a challenge in the context of relatively long technical (contrary to usefulness) lifetime and meaning of important change;No;not now but the work should continue on possible gaps;Yes, for specific AI applications;maybe for specific applications going beyond narrow scope of AI use, in particular operating in public space with multiple, unknown actors;;
F514726;15-04-2020 13:24;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;No opinion;No opinion;No opinion;No opinion;No opinion;;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important; The role of AI at the edge, &HW for AI is important ;Current legislation may have some gaps;;No opinion;;;;;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Very much;;Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;;Mental health risks;;Yes;;No opinion;;Yes, for all AI applications;;;
F514725;14-04-2020 18:52;French;EU Citizen;Benjamin;Pingault;;;;;France;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Garantir le stockage des donnees des Europeens uniquement sur le sol europeen et par des acteurs europeens afin d'empecher leur utilisation en dehors du cadre legal developpe par l'UE;5 - Very important;5 - Very important;5 - Very important;En plus de d'un centre de recherche phare, creation de geants technologiques europeens comparables a Google ou Tencent afin de retenir les talents europeens tentes de travailler pour ces entreprises et de monetiser la recherche en IA et en computation quantique. Pour une creation suffisamment rapide pour esperer rattraper le retard sur les Etats-Unis et la Chine, une approche coordonnee des Etats telle que celle ayant permis la creation d'Airbus apparait indispensable.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;3 - Neutral;4 - Important;4 - Important;1 - Not important at all;3 - Neutral;No opinion;;Current legislation may have some gaps;;Yes;;Yes;;Gestion des donnees en vue d'influencer la vie politique, comme lors du scandale Cambridge Analytica;4 - Important;4 - Important;2 - Not important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);identification de personnes recherchees;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;L'evaluation doit etre suffisamment rapide et peu bureaucratique pour garantir la rapidite d'innovation;Cyber risks;;No opinion;;No opinion;;Yes, for specific AI applications;applications a haut risque;;Further_comments_to_EU_AI_white_paper.docx
F514724;14-04-2020 18:41;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;1 - Not important at all;4 - Important;5 - Very important;5 - Very important;5 - Very important;;4 - Important;4 - Important;5 - Very important;5 - Very important;2 - Not important;4 - Important;;3 - Neutral;4 - Important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;1 - Not important at all;5 - Very important;1 - Not important at all;3 - Neutral;1 - Not important at all;1 - Not important at all;Humans are also biased decision makers, AI is the most democratic, logical and fact-based system. ;Current legislation may have some gaps;;Other;"Omipresent AI systems (and non-AI, such as CCTV) should be regulated to avoid user privacy violations and avoid always-on monitoring. GDPR should be defined for these ""tracking & monitoring systems"".";;;;4 - Important;3 - Neutral;4 - Important;5 - Very important;1 - Not important at all;4 - Important;Biometric identification systems should never be allowed in publicly accessible spaces;;No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;EU must be competitive (AI) but remain a free region (privacy). We need to balance that stance.  ;No opinion;;Yes;It should make it easier for some recommendation systems (AI) to obtain certifications. Othervise we're using competitive edge (healthcare) and manpower (industry) in those areas. Economical aspect should be put into perspective (possible, relative low risk of mistakes by AI, compensated by cheaper, more accessible services (ie public health);Yes, for specific AI applications;;;
F514723;14-04-2020 14:56;English;EU Citizen;Adam;Gašparovi?;;;;;Slovakia;The feedback can be published with your personal information;5 - Very important;;4 - Important;5 - Very important;3 - Neutral;4 - Important;When partnering with the private sector, please be sure to establish a real knowledge transfer. While private sector companies may have greater experience in working with AI, it is imperative for the public sector to avoid creating a dependency on private sector consultants or outsourcing. Developing internal skills and making the environment flexible and appealing to private sector employees with key skills is imperative. ;4 - Important;4 - Important;4 - Important;3 - Neutral;4 - Important;5 - Very important;;3 - Neutral;4 - Important;5 - Very important;Establishing public-private knowledge transfer. States cannot be dependent on private companies for service provision. Furthermore, states are already behind in terms of regulation as the best talent is fished up by the private sector. Knowledge transfer and making public sector more appealing are key.;3 - Neutral;4 - Important;4 - Important;4 - Important;No opinion;These partnerships between SMEs, larger enterprises, and academia should not become a way for large enterprises to simply outsource innovation and improve their standing. This must be regulated and rent-seeking behavior curbed. ;No opinion;4 - Important;5 - Very important;No opinion;5 - Very important;5 - Very important;Transparency of the method is key. People need to understand how AI makes choices and have real avenues for 2nd opinion, a minority report if you will. AI is a built by and trained on humans and as such will not be 100% or just. We need an alternative avenue for conflict resolution in case of disputes of AI's choices (I am thinking about things like asylum status, benefits entitlement, or healthcare).;There is a need for a new legislation;;No opinion;;;;policing and prosecution;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);Spaces where risks are high-enough to justify intrusion of privacy, such as airports, are a valid case for biometric identification systems. However, adding other spaces should be restricted unless risks to human life are clearly pronounced. I imagine these new places to be parliament buildings, councils, prisons, and very few other. The Czech courts have banned the use of AI in Prague's city-wide police CCTV network and I think that was a good call. ;Rather not;"I personally believe all AI-applications should be regulated and made compliant to certain regulations. The distinction between high-risk and low-risk is not as important for me then. However, I would be worried that the appeal of a ""badge"" is not enough to warrant rent & profit-seeking actors to utilize best practices vis-a-vis use of AI and data management. I would make it all mandatory just to err on the side of caution. ";Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;;Mental health risks;Quite a few uses of AI center on marketing and advertising algorithms, as well as product development and sentiment/trend tracking. I am afraid of the predatory nature of these technologies and think the constant hunt for the customer/consumer can cause mental health risks to those who are being hunted. I believe these risks include depression and anxiety, as well as inability to focus on key personal matters. ;Yes;;No opinion;;No opinion;;;
F514722;14-04-2020 11:17;English;;;;National;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;There is no AI without data.  The development of the AI strategy should be closely interdependent and interconnected with the EU data strategy.;5 - Very important;5 - Very important;5 - Very important;"Promote open science approach between AI research excellence centres
";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"SMEs needs to understand and benefit from the potential of the data economy but in order to achieve this, the European data strategy needs to be effectively implemented.
";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;Current legislation may have some gaps;;Yes;;Yes;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Risks related to the loss of connectivity;;Yes;;Yes;;Yes, for all AI applications;;We suggest to work closely with the OECD Network of Experts on AI (ONE AI);
F514721;13-04-2020 09:44;French;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;;5 - Very important;4 - Important;5 - Very important;;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;Yes;;Yes;;;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;4 - Important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Very much;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Personal security risks;;Yes;;Yes;;Yes, for all AI applications;;;
F514720;12-04-2020 22:12;Italian;EU Citizen;laura;CHIMIENTI;;;;;Italy;The feedback can be published with your personal information;5 - Very important;4 - Important;5 - Very important;No opinion;2 - Not important;4 - Important;;5 - Very important;3 - Neutral;No opinion;5 - Very important;;5 - Very important;;3 - Neutral;5 - Very important;1 - Not important at all;;4 - Important;4 - Important;4 - Important;2 - Not important;;;5 - Very important;5 - Very important;2 - Not important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;No;;;;;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;In assenza di una specifica  regolamentazione possono violare altri diritti già riconosciuti;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;violazioni del copyright;;;Yes;;Yes, for all AI applications;;;
F514719;12-04-2020 20:44;Portuguese;Academic/Research Institution;Marcio;Goethel;;Universidade do Porto;870039427179-40;Large (250 or more);Portugal;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;;3 - Neutral;4 - Important;5 - Very important;5 - Very important;5 - Very important;;1 - Not important at all;2 - Not important;4 - Important;4 - Important;1 - Not important at all;1 - Not important at all;;There is a need for a new legislation;;Yes;;Yes;;;5 - Very important;5 - Very important;1 - Not important at all;1 - Not important at all;5 - Very important;5 - Very important;No further guidelines or regulations are needed;;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Cyber risks;;Yes;;Yes;;Yes, for all AI applications;;;
F514718;11-04-2020 16:06;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;A.I. Ethics;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;4 - Important;The synergy of AI-specific skills and human capabilities;5 - Very important;5 - Very important;4 - Important;Taking care of scientific liberties of the individual who is developing AI, in line with ethics;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Raising public awareness on the existence of Digital Innovations Hubs;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;General rule: AI shouldn't be used as a replacement of humans in working environments etc. (taking into account possible exceptions to that rule);There is a need for a new legislation;;Yes;;Yes;;Complete replacement of the human factor with AI. AI should primarily be an extension or aid to human skills, which is to be viewed as a general rule with possible exceptions.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;Possible broader problems with deep fakes.;Yes;;Yes;;Yes, for all AI applications;;;
F514717;10-04-2020 11:43;French;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;4 - Important;2 - Not important;3 - Neutral;2 - Not important;2 - Not important;;5 - Very important;5 - Very important;2 - Not important;4 - Important;4 - Important;5 - Very important;;5 - Very important;5 - Very important;2 - Not important;;2 - Not important;4 - Important;4 - Important;4 - Important;3 - Neutral;;2 - Not important;5 - Very important;4 - Important;5 - Very important;5 - Very important;4 - Important;;There is a need for a new legislation;;No;;;;Lorsqu'elle est utilisée pour la surveillance de masse;4 - Important;3 - Neutral;5 - Very important;4 - Important;2 - Not important;3 - Neutral;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;À des fins de sécurité seulement et uniquement par l'état;Very much;;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;;Personal security risks;;Yes;;Yes;;Yes, for all AI applications;;;
F514716;09-04-2020 18:44;English;Academic/Research Institution;MACI Marzio;Veneman;;iSTACS Rythmos Advisory;;Micro (< 10 employees);Netherlands;The feedback can be published with your personal information;5 - Very important;5 - Very important;4 - Important;3 - Neutral;4 - Important;5 - Very important;;5 - Very important;4 - Important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;;5 - Very important;5 - Very important;4 - Important;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"- Collaborative Intelligence: Social impact: AI connected to P2P blockchains in conjunction with the ELITE SILOS. (Universal Basic Income)

- Returns 1 if the data is from a real living system, and 0 if it's not.
The use of learning algorithms is expressl";There is a need for a new legislation;;Yes;;No opinion;;"- Collaborative Intelligence: Social impact: AI connected to P2P blockchains in conjunction with the ELITE SILOS. (Universal Basic Income)

- Returns 1 if the data is from a real living system, and 0 if it's not.
The use of learning algorithms is expressl";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);UN report: Piercings either back to tribal or to prepare for cybernetics;No opinion;;Other enforcement system;Collaborative Intelligence;;Mental health risks;Universal Basic Income rapid loss of employment;Yes;;Yes;;Yes, for specific AI applications;"Returns 1 if the data is from a real living system, and 0 if it's not.

The use of learning algorithms is expressly not allowed, for the obvious reason, they would not yield an explanation of why they work.";;
F514715;09-04-2020 16:58;German;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;2 - Not important;4 - Important;5 - Very important;Auf nationaler als auch internationaler Ebene nicht alles auf KI fokussieren und damit andere Bereiche wo Europa stark ist/war womöglich vergessen. Eine gute Ausgewogenheit ist wichtig und eine entsprechend langfristige Strategie um alle Beteiligten mitzunehmen und nicht in immer kürzeren Abständen aktuelle Hypes aufzugreifen.;2 - Not important;4 - Important;4 - Important;3 - Neutral;4 - Important;5 - Very important;Hinweis zu Stärkung der Exzellenz in der Forschung: die Etablierung exzellenter Forschung in DE und vor allem in der EU ist natürlich vorrangiges Ziel, das meint aber nicht die Einrichtung/Stärkung von Leuchttürmen ohne Strategie für Hochschulen, Universitäten oder andere Forschungseinrichtungen in der Umgebung zu haben. Sicher ist eine gewisse Konsolidierung der wissenschaftlichen Landschaft in der EU und gerade DE notwendig, dies braucht aber eine Strategie! (vielleicht auch mittels KI?);1 - Not important at all;5 - Very important;4 - Important;"nicht nur eine Bildung sondern auch aktive Moderation/Management jenes (offenen) Netzwerkes, ggf. inkl. gewissem Budget um bspw. auch kleine Projekte entlang einer stetig überarbeiteten Strategie bürokratiearm zu fördern um auch Interdisziplinararität und die Kompetenz Externer zu nutzen.
KI ist geschickter Nutzen des Wissens von Vielen, EIN Leitzentrum wäre gerade für KI ein klein wenig wie ein Widerspruch.";5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;3 - Neutral;"Unterstützung von KMU ist sicher z.T. von Vorteil. In Projekten mit KMU Quoten kommt es aber immer häufiger vor, dass man jene nicht adressieren kann, da die personellen Ressourcen bei KMU beschränkt sind und eben nur eine limitierte Anzahl an ÖgP bearbeitet werden kann. Weiterhin ist die Aufrechnung von Förderquoten gerade für geringkapitalisierte kleine Unternehmen im Konsortium immer schwierig (""Industrie GEGEN Forschung""). Eine Änderung der Förderbedingung würde Abhilfe schaffen.";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;All das ist denkbar, gerade deswegen ist ein Rechtsrahmen frühzeitig notwendig und ein Zwang zur Transparenz verwendeter Algorithmen oder der Datengrundlage, um Entscheidungen autonom arbeitender Technologie nachvollziehbar zu gestalten. Keine Lösung ohne Lösungsweg.;No opinion;;No;;;;;5 - Very important;4 - Important;4 - Important;5 - Very important;3 - Neutral;No opinion;No opinion;"Schwierig, auch der Punkt klare Haftungs- und Sicherheitsvorschriften der vorherigen Frage.
Alles, auch jede Technologie wird missbraucht werden, jede Möglichkeit dazu genutzt. Wichtig ist allein, dass jeder Missbrauch (intern wie extern) möglichst frühzeitig aufgedeckt werden kann und umgehend strafrechtlich verfolgt wird. Öffentliche Überwachung kann völlig unproblematisch sein, wenn sichergestellt ist, dass ein Missbrauch nicht möglich ist oder nur äußerst geringen Schaden anrichtet.
Enge Rechtsvorschriften für alle Eventualitäten sind komplex und schränken andernorts zu sehr ein bzw. führen zu enormen Aufwänden (ist bereits in aktueller Gesetzeslage der Fall).";Rather not;;Other enforcement system;"Kontrolle muss nicht immer durch Behörden erfolgen, dass dies an Grenzen stößt zeigen zahllose Beispiele, selbiges gilt für sonstige ""Prüfeinrichtungen"". Es muss transparent sein, die gesamte Gesellschaft einbezogen werden ggf. gar mitarbeiten, Entscheidungen mit Grundrechten vereinbar. Es braucht ggf. gar nicht mehr neue Gesetze, es müsste nur sichergestellt werden, dass bestehende ""Lücken"" nicht billigend oder schamlos genutzt werden. Wie? ??";;Risks related to the loss of connectivity;"Ethische.
Auf welcher Basis erfolgen Entscheidungen: Wird abgewogen was ein kleineres Übel ist? Auf welcher Datenbasis erfolgt dies (GIGO)? Wird eine Datenbasis zur Verfügung gestellt? Was ist bei Updates/alten Versionen? Wer trägt die Kosten?
Wer haftet? Programmierer, Nutzer, was ist bei sonstigen Fehlern: Systemausfällen/ Konnektivitätsausfällen/ Störungen/ Manipulationen u.a.?
Gibt es Möglichkeiten individuell einzugreifen, was hat dann höhere Priorität: Mensch oder Maschine?";No opinion;zu wenig Ein-/Überblick;No opinion;;No opinion;;;
F514714;09-04-2020 16:44;French;Company/Business organisation;William;Eldin;;XXII Group;;Medium (< 250 employees);France;The feedback can be published with your personal information;4 - Important;2 - Not important;5 - Very important;4 - Important;5 - Very important;4 - Important;"- Faciliter la commande publique, 
- Etablir un plan de communication global positif sur l'IA
- Soulager les entreprises de deep technologie avec des fonds européens 
- Identifier et tester la performance des différentes entreprises européennes dans le se";4 - Important;3 - Neutral;4 - Important;5 - Very important;5 - Very important;3 - Neutral;Encourager les sociétés et grands groupes à développer des projets avec les startups, cela permettra un valorisation des différents acteurs. ;3 - Neutral;5 - Very important;5 - Very important;Créer une plateforme sociale destinée à la communauté de recherche, précisant les fonctions, spécialités et sujets de recherche ;4 - Important;3 - Neutral;No opinion;5 - Very important;5 - Very important;/;2 - Not important;4 - Important;5 - Very important;3 - Neutral;4 - Important;2 - Not important;/;Other;"Une nouvelle législation s’impose mais des textes peuvent uniformiser la législation tout en permettant d’appréhender différents niveaux d’IA. 
Exemple : on ne traite pas la recherche ou le développement de projets en big data et la robotique ou la vision par ordinateur de la même manière. Il est important de consulter les projets en fonction de leur champ d’application. ";Other;oui puisqu'il faut un focus sur l’IA à haut risque et non pas sur l’IA à faible risque. Mais aussi non car il est tout de même pertinent de mettre en place une législation spécifique pour l’IA. Tout développement d’IA comporte un risque pour l’utilisateur ou le client. Qu’importe le secteur, le domaine d’applications et les compétences. Il serait pertinent d’avoir des exigences à tout type d’IA mais cela doit être gradué : IA sans risque, IA avec risque modéré et IA à haut risque.;;;Militaire et véhicule autonome;5 - Very important;4 - Important;4 - Important;2 - Not important;3 - Neutral;5 - Very important;No opinion;;Rather not;Il ne faut pas un label non obligatoire mais justement obligatoire. ;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Précision : l’évaluation préalable doit être respecté en interne (par le constructeur) et une entité extérieur sur la faisabilité.;Personal security risks;/;Yes;/;Yes;/;Yes, for specific AI applications;militaires, voitures autonomes et exploitation des données personnelles;;
F514713;09-04-2020 14:09;English;EU Citizen;ioana;bratu;;;;;Romania;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;4 - Important;5 - Very important;;4 - Important;4 - Important;4 - Important;5 - Very important;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;;There is a need for a new legislation;;No;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;No opinion;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;White_Paper_Recommendations_Ioana_Bratu_VU_Amsterdam_April_2020.pdf
F514712;09-04-2020 13:02;Spanish;EU Citizen;Javier;Abanades Medina;;;;;Spain;The feedback can be published with your personal information;3 - Neutral;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;Potenciar regiones europeas económicamente mas desfavorecidas;4 - Important;3 - Neutral;4 - Important;3 - Neutral;5 - Very important;5 - Very important;;3 - Neutral;3 - Neutral;4 - Important;;3 - Neutral;3 - Neutral;5 - Very important;4 - Important;4 - Important;;4 - Important;3 - Neutral;3 - Neutral;5 - Very important;4 - Important;2 - Not important;;Current legislation may have some gaps;;No;;;;;5 - Very important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;4 - Important;No opinion;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Risks related to the loss of connectivity;;No opinion;;No opinion;;Yes, for all AI applications;;;
F514711;09-04-2020 12:56;French;;;;;;;;;The feedback can be published in an anonymous way;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;1 - Not important at all;;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;3 - Neutral;3 - Neutral;;1 - Not important at all;1 - Not important at all;1 - Not important at all;;2 - Not important;2 - Not important;2 - Not important;1 - Not important at all;1 - Not important at all;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;No;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;et dans un contexte limité, pour une période de temps définie après un vote à majorité spéciale des instances concernnées;No opinion;;Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;Une IA digne de confiance constitue un oxymore;Personal security risks;;Yes;Consulter des experts utilisateurs de l'IA, mais non concernés par son dévellopement;Yes;;Yes, for all AI applications;;;
F514710;08-04-2020 15:56;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;;4 - Important;4 - Important;4 - Important;;4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;;4 - Important;4 - Important;4 - Important;3 - Neutral;4 - Important;3 - Neutral;;Current legislation is fully sufficient;;Other;Yes, but for end product, not research and development. Testing facilities could test end product.;;;;3 - Neutral;3 - Neutral;4 - Important;4 - Important;3 - Neutral;4 - Important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);"Current use of terminology poses risk to biometric industry. There is unclear separation of 'remote biometric identification' and what is known as 'identification at a distance' in academia. Remote identification can be easily confused with processing performed in the cloud hence remote, which is a common scenario for biometric matching. Also there are touch-less fingerprint scanners, should they be considered as remote identification? Handheld iris-cameras? Vein scanners? How far should scanning device be to be considered remote or at a distance? Definition should drop the form of getting the data  in favor of formulating as 'active/purposeful voluntary participation from a subject in identification process'. 

Current regulation indicating area surveillance could be expanded with additional specification that data can be used for identification at a distance - which is what surveillance cameras are there for anyway. ";Much;Centralized testing facilities would provide common evaluation grounds  - a measure to compete between vendors.;Other enforcement system;Please check how US National Institute for Science and Technology evaluates biometric technologies as blackboxes. E.g. https://www.nist.gov/system/files/documents/2019/09/11/nistir_8271_20190911.pdf . European governments already use NIST reports for determining biometric vendor to buy services from. Similar evaluation EU may do for AI technologies. No need for additional  research and development regulation. ;Please check how US National Institute for Science and Technology evaluates biometric technologies as blackboxes. E.g. https://www.nist.gov/system/files/documents/2019/09/11/nistir_8271_20190911.pdf . European governments already use NIST reports for determining biometric vendor to buy services from. Similar evaluation EU may do for AI technologies. No need for additional  research and development regulation. ;;;Yes;;Yes;;No opinion;;Please check how US National Institute for Science and Technology evaluates biometric technologies as blackboxes. E.g. https://www.nist.gov/system/files/documents/2019/09/11/nistir_8271_20190911.pdf . European governments already use NIST reports for determining biometric vendor to buy services from. Similar evaluation EU may do for AI technologies. No need for additional  research and development regulation. ;EU_AI_Whitepaper_Remarks.pdf
F514709;08-04-2020 14:05;English;NGO (Non-governmental organisation);Piotr;Kolczy?ski;;Standing Committee of European Doctors (CPME);9276943405-41;Large (250 or more);Belgium;The feedback can be published with your personal information;5 - Very important;4 - Important;5 - Very important;4 - Important;4 - Important;5 - Very important;Involving interdisciplinary teams in design, testing and implementation of AI-enabled products and services. E.g. in healthcare sector such teams should include health-system leaders, doctors and patients, among others. To ensure appropriate design, validation, calibration and implementation of AI-powered technology, professionals' and end-users' perspective should be integrated at each stage of this process. ;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"Assure data security, ownership, governance and privacy while it is used by AI-based technology and high data quality and representativeness for training of AI systems. 

Provide governmental and independent non-governmental oversight on building evidence-based, trustworthy, equitable and non-discriminatory AI systems.";4 - Important;4 - Important;5 - Very important;;No opinion;No opinion;No opinion;No opinion;No opinion;;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;"AI may be used not transparently and unethically 

Responsibility and liability of AI producers and users may be unclear

Incorporating health data into AI systems during its training can prompt bias as data is imperfect and not free from numerous bias which can be inherited and further reproduced by the systems. 

The major danger of algorithms bias. 

Applying health data to enhance capabilities of AI and sharing patients’ records with AI systems can affect medical confidentiality.";There is a need for a new legislation;;Yes;;No;;"All AI applications in healthcare entails high risks. Wrong AI decision/advice can be health- or life-threatening (e.g. recommendation of a wrong drug, failure in screening, harm during surgery, mistakes in triaging etc.). 
AI is trained on imperfect and potentially biased data that can lead to inequality and discrimination. Any use of sensitive health data by AI must comply with GDPR rules as it puts patients privacy at risk. All AI application in healthcare should be determined as high-risk.";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Competent authorities should seek the knowledge, expertise and experience of professionals and end-users in assessing sector-specific AI products and services. In healthcare, doctors should be involved in ex-ante assessment of AI-based technology to be applied in medical practice. Moreover, the appropriate professional oversight over clinical validation and usefulness is essential once an AI system is integrated into health care practice. ;Mental health risks;"Physical health risks (due to AI unrobustness and inaccuracy or flaws)
Risk of discrimination (due to narrow scope of data used for AI training)
Risk of data security and privacy (due to use and sharing of data by AI and AI ability to de-anonymise data)
Risk of reduction of autonomy (due to overreliance on technology and inexplicability of AI outputs)";Yes;As AI systems can be frequently updated, evolve due to  an external input or even develop through self-learning, its risk assessment solely at the time of the placement on the market is insufficient. Its openness - as stated in the Section 3 - requires repeated assessment over the life-time of AI. ;Yes;Product Liability Directive should be assessed and revised. Existing legislation is not up to date regarding the new potential risks posed by AI and needs to be adapted to effectively address complexity, opacity and often autonomy and openness of AI systems. Amongst others, definitions of product and defect as well as the issues of burden of proof and liable persons should be updated. ;Yes, for all AI applications;;;CPME.Board_.Nov19.FINAL_.EN_.AIinhealthcare.pdf
F514708;08-04-2020 09:44;English;Academic/Research Institution;Christian;WENGER;;IHP GmbH - Leibniz Institut fuer innovative Mikroelektronik;;Large (250 or more);Germany;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;;5 - Very important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;;Current legislation may have some gaps;;Yes;;Yes;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;;No opinion;;Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;;Personal security risks;;No opinion;;No opinion;;Yes, for all AI applications;;;
F514707;07-04-2020 14:18;Italian;EU Citizen;Davide;Marini;;;;;Italy;The feedback can be published with your personal information;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;3 - Neutral;5 - Very important;"Considerazione del problema del trasferimento tecnologico dal mondo dell'innovazione a hub privilegiati per effettuare un coerente e robusto meccanismo di sperimentazione. 
inoltre è importante concentrarsi su strumenti di politica pubblica
decidere infine quali siano le capacità tecnologiche che vogliamo sviluppare in proprio, condividere o sviluppare in collaborazione. questo discorso è molto importante ai fini dello sviluppo di capacità che contengono tecnologie di AI. ";4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;3 - Neutral;il problema della costituzione di uno spazio europeo dei dati ha forti implicazioni con il modello di difesa e sicurezza che si vuole seguire. più orientato verso la considerazione dei Big data centre come fattore di potenza nelle dimensioni del potere DIME (DIPLOMATICO INFORMATIVO MILITARE ECONOMICO)?;3 - Neutral;5 - Very important;3 - Neutral;;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;da non dimenticare il convincimento del settore Difesa e Sicurezza in quanto determinate realtà ed incubatoi partono dal trasferimento di concepts e innovazione nel mondo militare e della sicurezza. ;3 - Neutral;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;sfida relativa alla qualità dei dati. un sistema di tecnologie di AI sfrutta una mole impressionante di dati. Chi li sta producendo in europa? esiste un sistema di garanzie in materia? quali attività di tutela a fronte di attività esterne di avversari o malintenzionati di carattere non statale abbiamo messo in piedi, a livello collettivo, a parte il sistema di Cyber defence principalmente coordinato dagli attori della Difesa? quali sono i risvolti nella comunità intelligence in materia? in tale ;Current legislation may have some gaps;;Yes;;Yes;;"Limitations of scope of existing EU legislation
changing functionality ";5 - Very important;3 - Neutral;5 - Very important;3 - Neutral;4 - Important;3 - Neutral;No further guidelines or regulations are needed;campo nel quale si possono effettuare grandissime evoluzioni per il benessere sociale e la sicurezza, nel pieno rispetto dei dettami di privacy. l'infrastruttura tecnica tuttavia non è ancora pronta. ;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;un sistema RAS ad esempio nasce come architettura adattiva ed è pensabile che non possa essere certificato solo alla verifica del suo requisito iniziale, ma soprattutto in conformità al suo ciclo di vita operativo con la capacità a cui è correlato (esempio di sistemi robotici ed autonomi che aiutano attraverso l'utilizzo complesso di AI e Deep learning lo svolgimento di funzioni specifiche con l'uomo);No opinion;;Yes, for specific AI applications;quelle per le quali esiste la possibilità di effettuare un bias delle repository dei dati. e ovviamente per le quali il sistema di AI è di fatto pienamente implementato. per il resto si continuerà a parlare di supervised autonomy ad esempio quando uAI si incontra con un sistema Robotico UGV o UAV;;
F514706;07-04-2020 10:57;German;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;5 - Very important;3 - Neutral;2 - Not important;4 - Important;;2 - Not important;4 - Important;4 - Important;3 - Neutral;5 - Very important;4 - Important;;4 - Important;5 - Very important;3 - Neutral;;4 - Important;4 - Important;5 - Very important;4 - Important;3 - Neutral;;4 - Important;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;No opinion;;Current legislation may have some gaps;;No;;;;;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;3 - Neutral;No opinion;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;No opinion;;No;;;
F514705;07-04-2020 10:38;Italian;EU Citizen;Daniele;CHIAPPINI;;;;;Italy;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;Valutare la creazione di un ente europeo sull'IA deputato a seguire, consigliare e proporre valutazioni su aspetti etici, giuridici e scientifici dell'IA, come peraltro previsto da altri Paesi.;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;4 - Important;;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;L'aspetto della sostituzione del lavoro umano con l'IA deve essere adeguatamente affrontato in modo che, coerentemente con la volontà di costruire un'IA affidabile come previsto dalla stessa Commissione, si possa avviare una transizione assicurando i cittadini circa la sostituzione con nuovi lavori 1:1, per quanto in un lasso di tempo ampio.;Current legislation may have some gaps;;No;;;;Armi letali autonome (che dovrebbero essere bandite a livello internazionale nonostante la contrarietà di alcune Paesi), ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;L'utilizzo dell'IA in questo ambito si muove lungo una linea di confine tra la garanzia di sicurezza e il controllo invasivo delle vite dei cittadini. Al fine di garantire un elevato livello di rispetto dei diritti umani e delle previsioni della Carta dei Diritti fondamentali dell'Unione Europea da parte di tutti gli stati membri è necessaria l'adozione di uno strumento normativo comune. ;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;Un approccio adeguato dovrebbe prevedere l'introduzione di quella che è stata definita personalità elettronica dal Parlamento Europeo e personalità artificiali da altre fonti dottrinali al fine di determinare gli aspetti dello status di IA, ivi compresi quelli relativi alla responsabilità.;Yes, for all AI applications;;;Contributo_Consultazione_IA_EU.docx
F514704;06-04-2020 18:25;Italian;Public authority;Pasquale;Marino;Regional;Servizio Infrastrutture e Crescita Digitale - Regione Puglia;;Large (250 or more);Italy;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;"Alfabetizzazione dei decisori politici e dei manager sull'importanza strategica dell'utilizzo dell'IA in ogni settore. Proporre linee guida per lo sviluppo del software ""AI by design"". Accelerare il dispiegamento dell'IA nel settore pubblico (ad esempio entro il 2023 molti [XX%]  procedimenti amministrativi dovrebbero essere automatizzati attraverso l'utilizzo di motori di IA)  ";5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;"Lanciare ""gamifications"" per temi non risolvibili con le normali procedure informatiche. Ad esempio ""Sviluppo di sistemi intelligenti per la predizione dei terremoti"" oppure ""Implementazione di sistemi intelligenti per lo studio delle malattie rare"". ";5 - Very important;5 - Very important;3 - Neutral;Coinvolgere in maniera capillare i cittadini;4 - Important;4 - Important;5 - Very important;4 - Important;3 - Neutral;;5 - Very important;3 - Neutral;4 - Important;3 - Neutral;2 - Not important;3 - Neutral;;Current legislation may have some gaps;;Yes;;Yes;;Poichè l'errore umano esiste occorre prevedere dei valori soglia che determinino l'accettabilità del rischio di utilizzo dell'IA. Ad esempio l'introduzione dei veicoli a guida autonoma ha un valore soglia di riferimento dato dalla mortalità per incidenti stradali, l'introduzione dell'IA su larga scala nella mobilità determinerebbe un abbattimento enorme di incidenti stradali. Un sistema basato su IA considerato salvavita dovrebbe avere un  rapporto rischi/benefici sempre positivo.;3 - Neutral;3 - Neutral;3 - Neutral;5 - Very important;2 - Not important;5 - Very important;No further guidelines or regulations are needed;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Risks related to the loss of connectivity;;No opinion;Per considerare un'applicazione di IA ad alto rischio o non applicabile bisognerebbe dimostrare che il livello di rischio attuale senza IA è inferiore o assente. ;No;;No;;;
F514703;06-04-2020 16:46;Italian;;;;Regional;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;2 - Not important;2 - Not important;5 - Very important;;5 - Very important;3 - Neutral;4 - Important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;;2 - Not important;2 - Not important;3 - Neutral;2 - Not important;2 - Not important;;4 - Important;4 - Important;1 - Not important at all;2 - Not important;2 - Not important;1 - Not important at all;;Current legislation may have some gaps;;No;;;;;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;;Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;;Personal security risks;;Yes;;No opinion;;No opinion;;;
F514702;06-04-2020 16:01;German;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;4 - Important;3 - Neutral;5 - Very important;4 - Important;"Wichtig wäre, dass nicht nur die wissenschaftliche Exzellenz gefördert wird, sondern der Fokus auf dem Transfer in die Anwendung liegt. AI mangelt es in Europa wenig an geeigneten Algorithmen sondern mehr an Mut und Fähigkeit vorhandene Dinge in die Anwendung zu bringen.
Hier sollten vor allem eine EU-weit harmonisierte Regelung angestrebt werden.";3 - Neutral;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;;3 - Neutral;4 - Important;5 - Very important;;4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;;2 - Not important;3 - Neutral;4 - Important;5 - Very important;3 - Neutral;4 - Important;KI birgt nicht per se Risiken. Vielmehr hängt das Ausmaß der Risiken von der Person ab, die KI anwendet. KI kann heute schon angewendet werden, ohne Code zu entwickeln (z.B. über Cloud Services). Damit können Menschen KI anwenden, denen die grundlegenden Fähigkeiten und statistischen Hintergründe fehlen. Erst dadurch wird die Anwendung von KI riskant. Daher sollte vielmehr in die Ausbildung der Personen investiert werden als in nachträgliches prüfen der Anwendungen. ;No opinion;;Other;"Das darf nur geschehen, wenn vollumfänglich geklärt ist, wann eine Anwendung ein hohes Risiko aufweist; dies muss klar nachvollziehbar und nachweisbar sein (welche Risiken, wie messbar, wo ist die Grenze). Des Weiteren darf keine Industrie benachteiligt wird. Der Fokus sollte direkt auf der Anwendung liegen. Daher ist Kriterium 1 nicht notwendig. ";;;;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;3 - Neutral;No further guidelines or regulations are needed;;Rather not;Die Frage ist, welchen Aufwand die freiwillige Kennzeichnung mit sich bringt. Kann nur ein Unternehmen mit hohem KI-Reifegrad den Aufwand betreiben, da Kapazitäten zur Verfügung stehen? Falls ja, wäre das eine Benachteiligung kleiner Unternehmen. Daher müsste der Aufwand minimal und kostenfrei sein. ;A combination of ex-ante compliance and ex-post enforcement mechanisms;;"Die Unternehmen sollten auf Anfrage zu ihren Lösungen ex-post auskunftsfähig sein. Dafür muss zwingend spezifiziert werden, welche Informationen in welcher Granularität zur Verfügung gestellt werden müssten. 
Von besonderer Bedeutung ist, dass die verpflichtende Selbstbewertung und ex-post Überprüfung die Entwicklungsgeschwindigkeit nicht einschränkt. Damit hängt stark zusammen, welche Institution die Prüfung durchführt und pragmatisch dies erfolgt. ";;;No;KI verändert sich immer während der Lebensdauer und lässt sich nicht auf bestimmte Produkte beschränken. Daher muss zu Beginn die Risikobewertung vorgenommen werden. ;Yes;Auch bei Black-Box Ansätzen muss Rechtssicherheit bestehen, um die Markteintrittsbarrieren gering zu halten. Z.B. Darf KI selbständig Entscheidungen treffen? Wer ist dann für die Entscheidung verantwortlich? Wer haftet für eine Fehlentscheidung? Wie lässt sich eine Fehlentscheidung von einer Fehlfunktion abgrenzen? Wer ist Urheber, wenn KI Neues erschafft?;Yes, for all AI applications;;Wenn die Haftung gänzlich auf den Lieferanten übergeht, werden kleine Start-Ups im Markt nicht überleben können. Daher ist eine Haftungsaufteilung notwendig, die Innovationen nicht verhindert. ;
F514701;06-04-2020 12:04;Spanish;Academic/Research Institution;José Antonio;CHICA PÁEZ;;Tecnalia Research and Innovation;87365826901-63;Large (250 or more);Spain;The feedback can be published with your personal information;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;5 - Very important;Formación y capacitación, planes para una oferta educativa para atender la oportunidad  del empleo en AI;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;3 - Neutral;5 - Very important;4 - Important;3 - Neutral;No opinion;3 - Neutral;;There is a need for a new legislation;;Yes;;Yes;;".- Aplicaciones para trabajo o entornos colaborativos, tanto industrial como de uso particular (conducción autónoma), Persona - Robot con AI.
.- Aplicaciones para la Automatización de los Procesos con la Administración Pública o proveedores de Servicios Público mediante RPA (Robotic Process Autoation) basados en bots con NLP (Natural Language Processing)… que pueden introducir sesgos o restricciones a la elección de las personas, que podrían no identificar que les atiende una AI.";5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;Estos sistemas deberían garantizar lo establecido en la GDPR, https://ieeexplore.ieee.org/abstract/document/8793058 ;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;Mayor definición de los entornos y aplicaciones persona - robot, no sólo en ámbito industrial sino en aplicaciones para particulares (conducción autónoma, RPA basados en NLP,...);Yes;;Yes;;Yes, for all AI applications;;;
F514700;04-04-2020 22:04;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;4 - Important;Science communication! For AI interaction between programmers, philosophers (ethics), users, professionals (e.g. medical doctors), mathematicians and many more is essential. There is a high potential for misunderstandings! This is usually completely underestimated. Something can go wrong because someone is not aware about a different meaning of a word within some context (this is the most simplest case). This is already an issue at universities. But for SMEs, what is the basis for communication?;3 - Neutral;5 - Very important;3 - Neutral;1 - Not important at all;5 - Very important;3 - Neutral;What is excellence in research? While here we are talking about AI, who is talking about the next generation of AI? I'm not joking. I consider (maybe) a FET-Open on non-commutative AI (with some connection to quantum computations, etc.) but it is very difficult to find research groups which can go far beyond what is just starting now. This is already a problem within mathematics ...;3 - Neutral;5 - Very important;5 - Very important;See also my comments before. The current research is very fragmented. Excellence would mean to go beyond the common research topics (in particular in mathematics). But how could that work when the mainstreem dominates research (journals, job offerings, etc.)?;3 - Neutral;5 - Very important;5 - Very important;5 - Very important;2 - Not important;It's very naive to think about SMEs. What is important is the support by INDEPENDENT agencies. This infrastructure does not exist yet. Just like a national contact point (NCP) for research proposals (ERC, etc.) there should be AI-NCPs that can provide help for finding experts on ethics, etc. Please do not underestimate the complexity within such an environment.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;No;;;;"There shouldn't be any AI (in products, etc.) except ""verifyable AI"". But even that would need a standard to be able to educate people (at universities) being able to investigate problems. It's about asking the right questions. Right now this might not seem important. But what is in (say) 10 years? Then we have an AI-Proliferation (German: Wildwuchs) no-one can handle anymore (from regulation point of view, etc.) Notice that lawyers, technicians, programmers, etc. need to understand each other.";5 - Very important;4 - Important;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;"Please be careful. Human oversight is overestimated. Because once AI is in place there are hardly enough well trained people who could take over! There is another subtle topic: It's nice that only humans are allowed to reject a proposal or something else. But HOW (!) does that effect decisions if humans are only working on rejection rather than both? Do we understand that already deep enough? ""Taking over"" is very delicate: Pilots have a special training. How about car drivers?";Very much;There must be at least some standard catalogue of issues. Interested people should be able to investigate deeper ...;Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;"There needs to be several levels, at least the algorithmical/mathematical, the technical and the concrete (data related). My impression is that the first is often neglected and taken for granted. But there is also high risk when a (for a specific task) bad random number generator is used. Another topic is ""system"" as a whole. Is another car recognized by AI from an image (in 2D) or is AI used to reconstruct a 3D-object that is recognized by AI as a car?";Mental health risks;There is a simple non-AI example: Babyphones with video. They are dangerous because missing connectivity is often treated as using the last picture which then suggests that everyhing is perfect. However, some small sign in the right upper part (showing missing connectivity) might be overseen. The product might be legally perfect. But humans use them. This is a difficult topic (how does the perception of large numbers work, etc.);Yes;;Yes;;Yes, for all AI applications;;In general: If there is something unclear, please contact me!;
F514699;04-04-2020 11:24;Spanish;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"La manera en que se aúna el conocimiento de los sectores (problemas) tradicionales con el conocimiento digital y más concretamente con las técnicas de inteligencia artificial o aprendizaje máquina. Como transformamos los ""ingenieros"" tradicionales en digitales instruyendo y capacitando en la forma de abordar los problemas jugando con dos tecnologías a día de hoy disjuntas en conceptualización y unidas en aplicación.";4 - Important;5 - Very important;;4 - Important;4 - Important;5 - Very important;Explicabilidad, confiabilidad, replicabilidad, sesgos y usabilidad. Ciclo de vida. Es decir, cóm o hacer de la IA algo fácil de usar por usuarios finales sin conocimiento experto en IA.;4 - Important;4 - Important;5 - Very important;Creo que es muy importante que se unan los mundos de investigación en IA y aplicación en IA, es importante el acceso al dato, de otra manera se puede caer en el desarrollo tecnológico, mediante la conceptualización de nuevos esquemas numéricos, que no respondan a las necesidades reales de los data sets, sino a conceptos teóricos. Se podrían onceptualizar esquemas numéricos que nunca se utilicen. Es muy importante el acceso al dato y la hibridación de conocimientos entre expertos de dominio e IA.;4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;;2 - Not important;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;La confiabilidad, ligada con alguna de los puntos recogidos en la encuesta, pero también la usabilidad y el ciclo de vida, es decir, cómo hacemos que los ususarios de la IA conciban la IA como una caja negra, pero como un usuario de un móvil que no concibe el conjunto de tecnologías que alberga dicho móvil, pero lo usa de manera segura, confiada y útil. ;There is a need for a new legislation;;Other;Creo que en el ámbito del respecto a las leyes de protección de datos se debe trabajar en gobernanza del dato, acceso al dato, anonimización del dato y con blockchain trazabilidad del dato para garantizar la privacidad sin limitar el acceso a los datos y, por lo tanto, el potencial impacto de la IA en el tejido industrial.;;;La IA es un compendio de técnicas matemáticas aplicado a todos los dominios donde se recaben datos. Creo que el punto de vista que hay que transmitir es el potencial que tiene sin entrar al estudio de datos individuales, la agregación del contenido del dato en esquemas estadísticos ya genera anonimización y por lo tanto limita mucho los posibles recelos en privacidad y en muchos casos, seguridad.;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;3 - Neutral;No opinion;Creo que todo depende del fin último de las cosas. Yo creo que el planteamiento es que no deben usarse los datos de estudios sobre individuos concretos, salvo con su consentimiento, sino sobre el conjuntos de usuarios a través del estudio individual de los mismos. Es decir, el fin es sobre un grupo y no sobre los individuos, por ejemplo, seguridad, no es lo mismo detectar armas que estudiar personas.;No opinion;;No opinion;;;Risks related to the loss of connectivity;;No opinion;;No opinion;;No opinion;;;
F514698;03-04-2020 15:27;English;NGO (Non-governmental organisation);Peter;Bihr;;ThingsCon e.V.;;Micro (< 10 employees);Germany;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;2 - Not important;Prioritize the focus on societal aspects of AI (strengthen the AI & Society Ecosystem) by focusing on stronger engagement with civil society;5 - Very important;5 - Very important;2 - Not important;3 - Neutral;5 - Very important;4 - Important;Establish organizations specifically for the purpose of funding the AI & Society Ecosystem similar to social innovation funds such as Sitra in Finland or Nesta in the UK;5 - Very important;5 - Very important;4 - Important;Involve civil society organizations in conducting impact assessments, and provide more funding to support this;2 - Not important;4 - Important;5 - Very important;3 - Neutral;3 - Neutral;Open up research and innovation programs for non-academic and non-commercial applicants.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"AI can function like a ""Trojan horse"" and introduce or alter processes and mechanisms with limited oversight.";Current legislation may have some gaps;;No opinion;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Risks related to the loss of connectivity;;Yes;;Yes;;Yes, for all AI applications;;;towards_a_european_ai_society_ecosystem_1.pdf
F514697;03-04-2020 15:16;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;Dialogue with partners outside of EU might be beneficial in certain fields.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;Coordination is important, but regulation should serve here as an enabler, not as a burden.;4 - Important;4 - Important;4 - Important;/;No opinion;No opinion;No opinion;No opinion;No opinion;Focus on innovation is important, regulatory framework should serve as an enabler.;4 - Important;3 - Neutral;4 - Important;4 - Important;4 - Important;4 - Important;/;Other;New regulation may be necessary, but shouldn't become a burden. Need for right balance between innovation and protection.;No opinion;;;;/;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;No opinion;/;No opinion;/;Other enforcement system;Combination of self-assessment ex-ante and, in rare cases, ex-ante external conformity assessment.;/;;/;Yes;/;;;No opinion;;/;
F514696;02-04-2020 22:18;Greek;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;4 - Important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;There is a need for a new legislation;;No;;;;AI Driving, AI Medicine;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Not at all;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Mental health risks;;Yes;;Yes;;No;;;
F514695;02-04-2020 18:39;German;EU Citizen;Peter;CORNELIUS;;;;;Germany;The feedback can be published with your personal information;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;;5 - Very important;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;;4 - Important;4 - Important;4 - Important;;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;;3 - Neutral;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;Yes;;Yes;;;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Much;;Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;;Personal security risks;;Yes;;Yes;;Yes, for all AI applications;;;
F514694;02-04-2020 16:18;English;;;;;;;;;The feedback can be published in an anonymous way;;;;;;;;;;;;;;;;;;;;;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;No;;;;Anything safety-related;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Very much;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Risks related to the loss of connectivity;;;;Yes;;Yes, for specific AI applications;;;
F514693;02-04-2020 15:14;German;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;;2 - Not important;4 - Important;2 - Not important;4 - Important;3 - Neutral;4 - Important;;There is a need for a new legislation;;No;;;;;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);Sollte nur in absoluten Ausnahmefällen (Terror, Pandemie etc.) und nur durch vorherige Genehmigung des jeweiligen Falles durch ein EU-Richtergremium erlaubt werden;Very much;evtl. Einstufung in verschiedene 'Gefahrenklassen', die dem Laien eine schnelle Einschätzung ermöglichen;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;Datenschutz;Yes;;Yes;;Yes, for specific AI applications;Keine konkreten Vorschläge. Sollte alle Anwendungen betreffen, die über eine 'einfache' Nutzung, bei der kein / kein großer Schaden angerichtet werden kann. hinausgehen. Mit zunehmender Erfahrung mit KI Produkten kann dieser Katalog angepasst werden.;;
F514692;01-04-2020 16:26;French;EU Citizen;Valerie;Pilcer;;;;;France;The feedback can be published with your personal information;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;;4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;Les actions portent beaucoup sur les structures ou les financements et pas assez sur le sens : au moment ou la raison d'être apparaît comme le point d'orgue de toutes les actions des entreprises, et alors même que l'action de la commission a tendance à se focaliser sur l'éthique et les valeurs européennes, il n'apparait pas clairement dans les diverses publications vers quelle Europe nous voulons aller et comment notre usage de l'IA va permettre d'y arriver et de nous différencier.;4 - Important;4 - Important;5 - Very important;Le facteur essentiel d'attractivité pour les meilleurs chercheurs sera bien sûr une question de moyen, mais surtout d'ambition par rapport au monde futur que nous voulons créer autour de nos valeurs. Comment notre recherche européenne se différentiera des autres recherches dans l'approche.;4 - Important;4 - Important;5 - Very important;4 - Important;4 - Important;Intégrer dans cette réflexion non seulement les PME qui sont structurées, mais les individus qui deviennent sous une forme ou une autre des travailleurs indépendants, des micro entrepreneurs qualifiés, mais non fédérés pour l'instant par ces recherches qui les concernent aussi.;4 - Important;5 - Very important;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;L'IA comporte tous ces risques. Mais les systèmes actuels également. Le risque est de voir l'IA modifier structurellement nos sociétés dans leur façon d'être, dans leur culture et dans leurs valeurs.;Current legislation may have some gaps;;Other;Il faut garder une approche différentiée et proportionnée mais être capable avec le recul suffisant de voir les risques induits par les transformations dans leur ensemble;;;;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;Other special requirements in addition to those mentioned in the question above should be imposed (please specify);L'identification biométrique est très utile pour la protection des citoyens, on l'a vu  notamment dans le cas de la crise du Corona Virus, les pays qui ont pu identifier les citoyens à confiner ont réduit drastiquement la contagion. L'utilisation devrait donc être généralisée pour le bien commun. Il est cependant nécessaire d'encadrer strictement son utilisation : il faut s'appuyer sur l'expérience de la RGPD pour le faire. Cependant, il faut engager une profonde réflexion sur la qualification du besoin pour l'utilisation des données, car le manque de clarté en la matière a amené à des destructions de données et donc de richesses inutiles par les entreprises qui ont pris la réglementation au pied de la lettre. C'est la même chose pour la reconnaissance faciale : on doit pouvoir s'appuyer sur ces données autant que de besoin pour des usages nécessaires au fonctionnement sécurisé de la collectivité. Il faudrait peut être un tiers de confiance pour garantir les usages. Par exemple un collectif formé de médecins dans le cadre d'une épidémie. Avec une supervision organisée et des contre pouvoirs. Cette gouvernance doit être réfléchie et nous permettre de bénéficier pleinement des apports de la technologie sans remettre en cause nos libertés individuelles;Very much;Une nouvelle forme d'agence de rating, qui noterait les systèmes à base d'IA et s'appuierait sur les alertes remontées par les associations professionnelles et les citoyens, en toute indépendance ;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;Les systèmes basés sur l'IA partent d'une analyse des comportements et des dernières recherches de la neuroscience pour augmenter leur pouvoir prescriptif. Le vrai risque est l'altération du libre arbitre des individus. Il faut donner aux citoyens européens une culture digitale et des outils leur permettant d'agir en conscience.;Yes;Les outils devraient pouvoir disposer de plusieurs modalités d'utilisation afin d'analyser les impacts de l'apprentissage ;Yes;;Yes, for specific AI applications;Les applications qui ne permettent pas une autoévaluation par les utilisateurs et engendrent des risques substantiels;;
F514607;31-03-2020 15:21;Finnish;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;"Kolmannen sektorin osallistaminen ja tukeminen; kolmannella sektorilla tehdään paljon työtä joka sijoittuu tutkimuksen, liiketoimitaa tekevien yritysten ja julkisen sektorin välimaastoon. Kolmannen sektorin toimijat pyrkivät usein käytännön toimin tukemaan ja kehittämään  tekoälyn hyödyntämistä yhteisen toiminnallisuuden parantamiseksi. ";5 - Very important;3 - Neutral;4 - Important;4 - Important;5 - Very important;3 - Neutral;ks. edellä;3 - Neutral;5 - Very important;3 - Neutral;;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;"PK-yritysten on usein vaikeaa tai mahdotonta päästä mukaan tai hyödyntämään tekoäly- tukimuksen tuloksia jos niillä ei ole tarvittavia kyvykkyyksiä tai resursseja perehtyä tutkimustuloksiin. Toisaalta (huippu)tutkimuksen ja tiedeyhteison organisaatiot eivät ole viritetty tutkimustulosten tiedottamiseen. Tässä asiassa puolueettomat toimialayhtyisöt voivat auttaa ""kansantajuistamaan"" ja levittämään tutkimuksen tuloksia sekä parantamaan niiden hyödyntämismahdollisuuksia pk-yritysten parissa.";4 - Important;5 - Very important;4 - Important;5 - Very important;4 - Important;4 - Important;;Current legislation may have some gaps;;;;;;Algoritmipohjaisen ja automaattisen, mahdollisesti ihmisten terveyteen, turvallisuuteen ja itsemääräämisoikeuteen  liittyvät päätökset ja suositukset, mikäli algoritmien päätöksentekologiikka ei ole tarkasti tiedossa.;4 - Important;3 - Neutral;4 - Important;;4 - Important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Personal security risks;;Yes;;No opinion;;Yes, for specific AI applications;Tekoälyn ja ihmisen toiminnan rajapinnassa toimivien sovellusten vastuusääntöjen selkeyttäminen;Toimialaorganisaatiot käyttäjärajapinnassa toimiessaan voivat kerätä käytännön esimerkkejä ja tapauksia tilanteista, joissa vastuusäännöt eivät ole selkeitä ja/tai käyttäjien yksiselitteisesti tulkittavissa ja saattaa niiden pohjalta asioita jatkokäsittelyyn ja siten vaikuttaa vastuukysymysten selkeyttämiseen ja rajanvetoihin vastuukysymyksissä.  ;
F514606;31-03-2020 14:44;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;;3 - Neutral;5 - Very important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;;4 - Important;;4 - Important;4 - Important;4 - Important;2 - Not important;2 - Not important;3 - Neutral;Job loses;There is a need for a new legislation;;Yes;;Yes;;;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);Currently they are used to register working times or to access to restricted areas;Rather not;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Personal security risks;;Yes;;Yes;;Yes, for all AI applications;;;
F514605;31-03-2020 12:00;Italian;EU Citizen;Stefano;BALDI;;;;;Italy;The feedback can be published with your personal information;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;4 - Important;;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;4 - Important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;There is a need for a new legislation;;No;;;;;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);Security reasons, presence of a big amount of people;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;Yes;;Yes, for all AI applications;;;
F514604;31-03-2020 08:51;German;Business Association;Regine;Maruska;;Zentralverband Deutsches Baugewerbe;692847531164-78;Small (< 50 employees);Germany;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;4 - Important;4 - Important;4 - Important;3 - Neutral;3 - Neutral;4 - Important;;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;No opinion;;Current legislation may have some gaps;;No;;;;autonomes Fahren (Verletzungsrisiko), biometrische Systeme (sensible Daten, Missbrauchsgefahr);4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Rather not;Solche freiwilligen Systeme haben keinen Wert. Die Unternehmen können schreiben, was sie wollen.;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Personal security risks;KI arbeitet mit Unmengen von Daten. Die dürfen weder gestohlen noch manipuliert werden. ;No opinion;;Yes;"Schon bei ganz gewöhnlicher Software (ohne KI) funktioniert die Produkthaftung nicht gut. Der normale Nutzer kann nicht nachweisen, dass es kein Bedienungsfehler war, wenn die Software nicht richtig funktioniert. Viele Unternehmen haben Software im Betrieb, die sie nicht nutzen. ""Umtausch"" ist auch nicht möglich. ";Yes, for specific AI applications;Hochrisiko-Anwendungen;;
F514603;30-03-2020 16:38;English;Business Association;Camille;Dornier;;Eurosmart;21856815315-64;Micro (< 10 employees);Belgium;The feedback can be published with your personal information;3 - Neutral;5 - Very important;4 - Important;3 - Neutral;5 - Very important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"-The use of AI for cybersecurity applications, including joint procurements of AI solutions for cybersecurity.
-Developing innovative and low power semiconductors, as these components will underpin AI systems. 
-Cyber-resilience of AI systems, including r";4 - Important;5 - Very important;5 - Very important;"AI cyber-resilience is key, especially for products that convey safety risks. The establishment of the European Cybersecurity Competence Centre can play an important role in strengthening the cybersecurity level of AI systems. This future structure could streamline EU funding to support research and innovation projects focusing on AI cyber-resilience. 
Research centres should also benefit from a legal framework giving them an easy and simplified access to publicly and privately owned data.";4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;SMEs often struggle more than big companies to deal with legal compliance, cybersecurity and privacy related issues. Digital Innovation Hubs can play a key role in helping them to uptake solutions dealing with these issues (access control, hardware-based solutions etc.) in the context of AI, or even assist them in completing the required tasks (legal counsel, support in assessing compliance of AI with the data protection framework).;5 - Very important;5 - Very important;No opinion;4 - Important;3 - Neutral;4 - Important;AI based products and solutions can be subject to cyber-attacks which can endanger safety and/or result in data loss and privacy violations. It is essential to develop methodologies, standards and certification schemes that strengthen the security of AI systems. Security-by-design is key in this respect. Duly attention must be paid to the robustness of algorithms and management of credentials. The Cybersecurity Act provides the relevant basis for future certification.;There is a need for a new legislation;;No;;;;"All AI applications should be covered by compulsory minimum requirements on (1) safety, (2) security, and (3) ethics. AI products and solutions should be considered as a product regulated by clear rules for placing on the market and ex post rules. All AI systems should be ethical by design and secure by design. These requirements should be based on European standards. 
For high-risk applications, stricter requirements and certification -supervised by a national authority, should apply.";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No further guidelines or regulations are needed;Biometric identification systems are already covered by the General Data Protection Regulation (GDPR). The processing of biometrics data for uniquely identifying purposes is forbidden pursuant to Article 9(1) of GDPR. Facial recognition can only take place if it falls under the scope of one of the exemptions listed in such article. Thus, an effective implementation of GDPR ensures that facial recognition is used in a duly justified manner and does not excessively interfere with the right to privacy. ;Rather not;A voluntary labelling system is not the preferred option as all AI systems should fall under the category of products for which CE marking is required and should therefore comply with minimum mandatory requirements before being placed on the EU market. CE marking should include specific requirements for AI systems, for instance on the quality of datasets used to train AI systems. Methodology, standards and conformity assessments are needed to check compliance with minimum requirements. ;A combination of ex-ante compliance and ex-post enforcement mechanisms;;"For high-risk applications: certification by national authorities conducted through strict rules by accredited European laboratories. Accreditation based on Regulation 765/2008, with additional requirements to ensure that (1) CABs and (2) laboratories are performing their tasks in the EU territory. Assessement repeated over lifetime of AI systems.
For low-risk: conformity with minimum requirements through European conformity standards and conformity assessments performed by third parties. ";Personal security risks;;Yes;"Risk assessment procedures need to take into account the possibility for an AI system to evolve over time. This means that new vulnerabilities might arise, which will need to be addressed through adequate corrective measures. Risk assessment will need to be repeated once a product is already placed on the market. 

";Yes;"AI should be considered a product pursuant to the Product Liability Directive. The definition of “damage” should include loss of data, privacy violations and non-ethical uses.

The White Paper proposes a strict liability for high risk applications, coupled with mandatory insurance. This option must be carefully assessed in the light of current technological evolutions. This new framework should not discourage manufacturers/users from developing/acquiring AI systems. ";No opinion;;;2020_03_17_Eurosmart_positionpaper_AI.pdf
F514602;30-03-2020 11:07;French;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;4 - Important;4 - Important;3 - Neutral;3 - Neutral;Soutien de la dynamique Open Source - Network of citizens - pour l'AI;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;4 - Important;4 - Important;Développer un pôle AI pour la diverstité culturelle. Celle-ci est un des plus grands bien commun de l'EU et peut être aidée dans sa survie par l'IA ... mais risque aussi de disparaître dans l'homogénéisation digitale.;5 - Very important;5 - Very important;3 - Neutral;Continuer à développer AI for language;4 - Important;4 - Important;5 - Very important;4 - Important;3 - Neutral;;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;;There is a need for a new legislation;;No;;;;;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;4 - Important;4 - Important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);Par les autorités publiques dans le cadre de leurs missions et sous couvert d'agrément spécifique pour les activités de gardiennage / de sécurité;Rather not;;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;;Mental health risks;Aujouter les risques pour la sécurité des biens culturels et du patrimoine naturel.;No opinion;;Yes;;No opinion;;;
F514601;29-03-2020 16:28;Spanish;Public authority;Julio;De Santiago Angulo;Regional;Servicio Canario de la Salud;;Large (250 or more);Spain;The feedback can be published with your personal information;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;;No opinion;No opinion;No opinion;;No opinion;No opinion;No opinion;No opinion;No opinion;;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
F514600;28-03-2020 21:08;German;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;4 - Important;5 - Very important;5 - Very important;1 - Not important at all;3 - Neutral;;5 - Very important;5 - Very important;3 - Neutral;4 - Important;4 - Important;3 - Neutral;;3 - Neutral;5 - Very important;1 - Not important at all;;3 - Neutral;4 - Important;5 - Very important;3 - Neutral;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;No;;;;Prognoseentscheidungen aller Art;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;Biometrische Identifikationssysteme in öffentlich zugänglichen Räumen bedeuten eine  absolute und anlasslose Überwachung dieses öffentlichen Raumes und ermöglichen es Bewegegungsprofile zu erstellen, wenn der öffentliche Raum groß genug ist bzw. viele zusammenhängende öffentliche Räume überwacht werden. Dies verstößt gegen Grundrechte und kann nicht dadurch gerechtfertig werden, dass möglicherweise Straftaten verhindert werden könnten.;Very much;KI-Systeme sollten generell kennzeichungspflicht sein.;Other enforcement system;Test und Bewertung des KI-Systems durch unabhängige Experten bevor das System in Verkehr gebracht wird. Möglicherwiese Einführung eines eurpaeinheintlichen Gütesiegels.;;Mental health risks;Risiken für die körperliche Unversehrheit;Yes;Updates und Änderungen der Software müssen denselben Prüf-, Genehmigungs- und Haftungskriterien unterliegen bevor sie in den Verkehr gebracht werden.;Yes;KI ist nicht mit den übiche,. althergebrachten Maßstäben und Richtlinie bewertbar. Deshalb müssen neue, speziell auf KI ausgreichtet Haftungsrichtlichnien parallel zu der Entwicklung von KI ausgearbeitet werden. Da die Entciklung im IT-Bereich eine äußerst hohe Dynamik aufweist, sollte jede neue Form eines KI-Systems nicht ohne darauf abgestimmte Haftungsregeln in den Verkehr gebracht werden dürfen. ;Yes, for all AI applications;;;
F514599;28-03-2020 19:30;German;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;2 - Not important;;3 - Neutral;4 - Important;3 - Neutral;4 - Important;5 - Very important;5 - Very important;;5 - Very important;3 - Neutral;4 - Important;;4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;4 - Important;No opinion;5 - Very important;"-  biometrische Systeme im öffentlichen Raum, da Gefahr für Grundrechte und wenig Bedeutung für öffentliche Sicherheit. Deswegen ein absolutes Verbot für biometische Systeme im öffentlichen Raum.
- Rassistische und frauenfeindliche algorithmische Systeme ";There is a need for a new legislation;;Yes;;No;;biometrische Systeme im öffentlichen Raum ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;absolutes Verbot von biometrischen System, da sehr gefährlich für Grundrechte und nur sehr geringer Nutzen für die öffentliche Sicherheit. ;Very much;;Other enforcement system;eine europäische Kontroll- und Aufsichtsbehörde mit eigener Zuständigkeit, Aufgabe und Befugnis, um die nationalen Behörden zu unterstützen und zu entlasten.;;Mental health risks;"- KI die bei gesellschaftlicher Partizipation eingesetzt wird, z.B. Wahlen.
- Systeme, die Grundrechte betreffen ";Yes;;Yes;Im Weißbuch ist eine zu geringe Risikoeinschätzung zu finden. Alle Systeme, die Grundrechte betreffen oder über den Zugang von Menschen zu Ressourcen oder zu gesellschaftlicher Partizipation entscheiden, sind risikobehaftete Systeme und müssen stärker einer Risikoeinschätzung unterzogen werden.;;;;
F514598;28-03-2020 18:17;German;;;;;;;;;The feedback can be published in an anonymous way;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;;4 - Important;4 - Important;3 - Neutral;4 - Important;4 - Important;3 - Neutral;;4 - Important;4 - Important;4 - Important;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;;4 - Important;5 - Very important;5 - Very important;4 - Important;No opinion;4 - Important;Um KI nutzen zu können, bedarf es in vielen Fällen Trainingsdaten. Auf deren Basis lernt die KI Unterscheidungen zu treffen. Hier habe ich Bedenken, dass durch die Auswahl und Interpretation der Trainingsdaten es zur Diskriminierung von EU-Bürgern kommen kann.;There is a need for a new legislation;;No;;;;Maschinelles Lernen. ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;Hierbei sehe ich eine starke Bedrohung der Grundrechte. Eine Massenüberwachung ist nicht verhältnismäßig. Die Nachteile überwiegen hierbei stark (Missbrauch, Datensicherheit kann nicht gewährleistet werden, hohe Fehleranfälligkeit, ...). Ich möchte nicht, dass Algorithmen die Bevölkerung überwachen. ;Very much;Ich würde freiwillig sogar streichen. KI-Systeme sollten grundsätzlich gekennzeichnet werden und von unabhängigen Institutionen überprüft werden. Wer legt die Kategorien für nicht hochriskante KI-Systeme und deren Anwendungfelder fest.;Other enforcement system;Es bedarf einer europäische Kontroll- und Aufsichtsbehörde mit eigener Zuständigkeit, Aufgabe und Befugnis, um die nationalen Behörden zu unterstützen und zu entlasten.;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F514597;28-03-2020 16:08;French;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;3 - Neutral;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;"Oui le livre blanc est à ce stade trop théorique et il faut absolument donner une vision sectorielle ambitieuse complémentaire. Si l'on veut aller vite alors se contenter de choisir quelques thèmes comme par exemple l'european green deal ou la nouvelle PAC; sinon ouvrir une nouvelle itération pour sélectionner quelques initiatives concrètes qui changeront le quotidien des Européens sous 5-7 ans";4 - Important;5 - Very important;3 - Neutral;;5 - Very important;5 - Very important;il faut poursuivre les travaux sur l'éthique en visant à associer plus largement la population pour faire prendre conscience des enjeux en éduquant à l'IA mais aussi donner un second souffle à la RGPD qui est déjà un premier succès segmentant pour placer l'Europe à sa place dans le monde;4 - Important;5 - Very important;3 - Neutral;"l'IA n'est pas un domaine spécifique pour les PPP; ils ont leur place sans plus... EN REVANCHE il faut absolument donner à voir beaucoup plus des travaux engagés dans la recherche et éventuellement des résultats pour faire toucher du doigt aux citoyens la rélaité des actions( et des dépenses) engagées.";5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;"promouvoir les actions des PME dans les domaines cibles (cf ma proposition plus haut); si demain une PME crée un dispositif d'élevage du poulpe grâce à l'IA ce serait bien, mais peut être dommage ";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"oui l'identité IA = progrès doit être autant que possible évitée, de m^me que toutes les fausses illusions du type tourisme spatiale, véhicule autonome etc... qui certes font rêver mais n'enrichissent que leurs promoteurs; l'IA doit être un plus, plus que du numérique, plus que de l'automatisation, plus que du digital washing; sortons donc un petit test type teste de turing qui nous permet de dire quand l'IA est ""bien"" et quand elle n'est qu'un luxe cosmétique ";No opinion;;;;;;"le véhicule autonome et à son exemple tout utilisation déshumanisée de l'IA doit être évitée ou au moins non subventionnée; NB: a fortirori les histoires de soldat augmenté ou de drone tueur...";4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;Biometric identification systems should never be allowed in publicly accessible spaces;ce n'est pas parce que l'on peut le faire qu'il le faut! Bien au contraire il faut absolument protéger le citoyen des tentations de tous les Adolf en puissance qui ne rêvent que d'automatiser le contrôle social.;Very much;"regardez ce qui ne se fait pas encore sur la certification de l'agriculture biologique et qui est encore absolument souhaitable puis faites le pour l'IA; cela permettra de prendre 20 ans d'avance dès l'origine ";A combination of ex-ante compliance and ex-post enforcement mechanisms;;"prévoyez une catégorie faisant appel à la population; c'est bien d'avoir des règles puis d'avoir des comités mais il faut pouvoir aussi ouvrir le débat et confronter les opinions ";Personal security risks;il faut affecter clairement les responsabilités en séparant bien ce qui relève du bon usage et ce qui relève de la malfaçon devant être corrigée au plus vite;Yes;ill faut dupliquer les dispositif de suivi organisés pour les médicaments par exemple avec une veille systématique et supervisée par des personnes qualifiées;Yes;compte tenu de la nature vraisemblablement numérique des produits incriminés il faut trouver un système d'évaluation très rapide plus proche de la chasse aux fake news des réseaux sociaux que du suivi qualité des dés à coudre ;No;;"l'IA est un sujet neuf il devrait être un sujet européen et les nationalités mises en sommeil sur ce sujet; si un feu rouge devient intelligent à Copenhague il devrait l'être à Nicosie; c'est peut être un point de vue hardie mais l'IA vaut cette bataille sinon demain les pandas arriveront à Rome ou à Bratislava avec des neurones augmentés ";
F514596;28-03-2020 14:06;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;The main actions focus on goods, the AI services sector is totally missing ;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Development of skills could be done via the Professional qualifications directive 2005/36/EC : common training frameworks;3 - Neutral;3 - Neutral;4 - Important;;2 - Not important;5 - Very important;4 - Important;4 - Important;3 - Neutral;;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;All your questions focus on goods, analysis of relevant legislation for services is missing ;Current legislation may have some gaps;;Other;Legislation for assessment of the risk for professional services already exists: See Proportionality test directive : risk assessment, human oversight, liability, ethics, skills and qualifications ;;;Military ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;You are proposing solutions for goods, not for services ;Personal security risks;;No opinion;;;How about services liability? See services directive, professional qualifications directive, proportionality test directive and e-commerce directive ;Yes, for all AI applications;;;
F514595;27-03-2020 14:44;German;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;4 - Important;;5 - Very important;4 - Important;5 - Very important;3 - Neutral;4 - Important;3 - Neutral;;3 - Neutral;4 - Important;3 - Neutral;;4 - Important;3 - Neutral;4 - Important;4 - Important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;No;;;;;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;No opinion;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Mental health risks;;Yes;;No opinion;;Yes, for all AI applications;;;
F514594;27-03-2020 10:52;Croatian;;;;;;;;;The feedback can be published in an anonymous way;;;1 - Not important at all;;;;;;1 - Not important at all;;;;;;1 - Not important at all;;;;1 - Not important at all;;;;;;1 - Not important at all;;;;;;;There is a need for a new legislation;;No;;;;;;1 - Not important at all;;;;;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);;Rather not;;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;;Personal security risks;;No;;No;;No;;fgh;Repository_of_A-Robotics_Initiatives.pdf
F514593;27-03-2020 03:42;German;;;;;;;;;The feedback can be published in an anonymous way;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;;3 - Neutral;3 - Neutral;3 - Neutral;Letzteres ausdrücklich nur öffentlich;No opinion;No opinion;No opinion;No opinion;No opinion;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;Current legislation may have some gaps;;Other; ;;;;1 - Not important at all;1 - Not important at all;5 - Very important;1 - Not important at all;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;Hohe Gefahr der Gefärdung persönlicher Grundrechte;No opinion;nein;Other enforcement system;Dies lässt sich nicht befriedigend sicherstellen, hohes Risoko;;Personal security risks;zusätzlich Risiken der psychischen Gedundheit.;Yes;;Yes;;Yes, for all AI applications;;;
F514592;26-03-2020 17:13;English;Other;Giacomo;TAVOLA;;Politecnico di Milano;479384532978-69;Large (250 or more);Italy;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;3 - Neutral;Sviluppo ella multidisciplinarietà scientifica nell'approccio ed interoperabilità delle piattaforme;5 - Very important;5 - Very important;4 - Important;3 - Neutral;5 - Very important;5 - Very important;;3 - Neutral;5 - Very important;4 - Important;;4 - Important;3 - Neutral;5 - Very important;3 - Neutral;3 - Neutral;;3 - Neutral;4 - Important;3 - Neutral;5 - Very important;No opinion;No opinion;;No opinion;;Yes;;Yes;;controllo sociale tramite processamento di immagini;5 - Very important;3 - Neutral;3 - Neutral;4 - Important;4 - Important;4 - Important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);sicurezza e sanità;Much;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;;Rischi per la libertà individuale di espressione ed opinione;No opinion;;No opinion;;No opinion;;;
F514591;26-03-2020 13:47;English;Business Association;Joonas;Mikkilä;;Federation of Finnish Enterprises (Suomen Yrittäjät);032592932156-20;Medium (< 250 employees);Finland;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;3 - Neutral;4 - Important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;In order to successfully reach out to SMEs, it is vital for the AI-specialised DIHs to establish a strong link to the business community by having SME associations and entrepreneurs organisations represented in their governing structure. ;5 - Very important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;4 - Important;As the great majority of SMEs are deployers, not developers, of AI-powered systems, it is important for them to understand their rights and responsibilities towards both the providers of their AI systems and customers using their AI-enhanced products. In other words, clarifying the liabilities between different parties in the AI value chain is an important means to promote the uptake of AI by SMEs.   ;Current legislation may have some gaps;;Yes;;Other;Although in principle the high-risk approach is advisable, given the fast-changing pace of the family of AI technologies, it may be difficult to outline which sectors and use-cases are high-risk, perhaps even to the point of making the approach unfeasible and/or undermining the competitiveness of European AI companies operating in these fields. The use of regulatory sandboxes, allowing incremental and well-targeted regulation, would help avoid the pitfalls of this approach.   ;;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);Biometric identification in public spaces should be allowed in well-defined public interest purposes only (e.g. public safety or R&D). In order to protect privacy and uphold the rule of law, these limited use-cases require clear rules of the road which ought to be as uniform as possible across the EU.  ;Much;Although a voluntary labelling system is welcome, it is important to ensure that such a scheme doesn't become too burdensome for European SMEs developing and deploying AI.;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;Any ex-ante conformity assessment mechanisms on AI applications should be approached with caution. As there are limited resources to conduct wide-reaching compliance checks, they could become a serious obstacle for European companies to bring their AI-powered products to the market, thus undermining their chances to compete globally.;;;No opinion;;Yes;As the great majority of SMEs are deployers, not developers, of AI-powered systems, it is important for them to understand their rights and responsibilities towards both the providers of their AI systems and customers using their AI-enhanced products. In other words, clarifying the liabilities between different parties in the AI value chain is an important means to promote the uptake of AI among SMEs.   ;No opinion;;;
F514590;26-03-2020 11:40;English;;;;National;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;"Actions to ensure that equality is safeguarded by AI systems, particularly actions which: empower more women and girls to study, work and advance their career in AI; provide training to AI professionals on equality; address bias, stereotypes and discrimination for instance through codes of ethics; raise awareness on discrimination and AI; provide information on how to analyse the outcomes of algorithms in terms of equality etc.

";5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;4 - Important;;4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;Current legislation may have some gaps;;No;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;No opinion;;Yes, for all AI applications;;;
F514589;26-03-2020 10:52;English;EU Citizen;Vasile;Tiple;;;;;Romania;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;Details in the recommendation paper attached.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Details in the recommendation paper attached.;5 - Very important;5 - Very important;5 - Very important;Details in the recommendation paper attached.;3 - Neutral;4 - Important;4 - Important;5 - Very important;4 - Important;Details in the recommendation paper attached.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Details in the recommendation paper attached.;There is a need for a new legislation;;Other;Details in the recommendation paper attached.;;;Details in the recommendation paper attached.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;Details in the recommendation paper attached.;Not at all;Details in the recommendation paper attached.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;As further described in the recommendation paper attached.;Mental health risks;Personal liability. Details in the recommendation paper attached.;Yes;Details in the recommendation paper attached.;Yes;Details in the recommendation paper attached.;Yes, for all AI applications;;Details in the recommendation paper attached.;Recommendations_on_the_Whitepaper_on_AI.docx
F514588;26-03-2020 10:43;Lithuanian;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;4 - Important;3 - Neutral;3 - Neutral;1 - Not important at all;4 - Important;1 - Not important at all;;Current legislation is fully sufficient;;Yes;;No;;;1 - Not important at all;3 - Neutral;4 - Important;5 - Very important;1 - Not important at all;3 - Neutral;No further guidelines or regulations are needed;;Not at all;;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;;Risks related to the loss of connectivity;;No;;No;;No;;;
F514587;25-03-2020 19:27;French;EU Citizen;Jérôme;MASSET;;;;;France;The feedback can be published with your personal information;4 - Important;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;"Création de plateformes européennes pour la gestion des données, cloud souverain européen et API IA.
Ouverture de la donnée, particulièrement en santé (anonymisées) aux acteurs européens
Création de nouveaux supercalculateurs européens.
Redonner de l'attrait à la recherche face au secteur privé, ou du cofinancement
Se réapproprier les données collectées par les acteurs non européens à travers de loi (extension du RGPD)
Créer un éco-système de A à Z (du CPU/GPU à l'API) 
";5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;"Attention à l'investissement en start-up, de trop nombreuses start-ups s'estampillent IA sans en faire, ou en faire sans finalité (cas d'usage). 
Le secteur public peut se réorganiser/restructurer grâce à l'IA, mais aussi du lean management.
La formation est essentielle, dès le plus jeune âge, il faut acculturer mais rappeler les basiques : coûts des algorithmes (énergie, complexité), de stockage";3 - Neutral;5 - Very important;5 - Very important;"Le partenariat public privé est très important, afin d'exploiter les potentiels de l'IA dans les domaines de l'agriculture (enjeu économique et écologique), gestion de l'eau et de l'énergie (traque aux pertes), maintenance prédictive.
Ce partenariat permettrait de relocaliser la production, d'accroître la productivité, de générer de la croissance, et de gagner en autonomie. A l'heure de la crise CoVID19, l'autonomie et la souveraineté de la production sont des enjeux majeurs. ";4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;"L'IA est avant tout l'intelligence humaine, l'intelligence collaborative. PME, startups, grands groupes, acteurs publics doivent collaborer, ouvrir leur données, pour accélérer l'IA (en recherche et en intégration).
Les PME doivent porter l'IA autant que les grands groupes, bien qu'ils ne puissent intégrer des équipes spécialisées. 
Les startups doivent montrer leur solidité financière pour obtenir des contrats à moyen/long terme. 
 ";4 - Important;4 - Important;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;"L'IA sera/fera ce que l'on en fera :
1) l'IA n'est pas exacte, tout comme l'homme, et c'est l'homme qui l'entraîne et choisit les données. Il faut laisser le choix : être traité par l'IA ou l'humain, ou les deux.
2) l'IA peut prendre des décisions en fonction du risque associé à la décision : fournir des recommandations claires (ou abaques) pour déterminer les zones rouges, grises ou vertes.
3) l'IA peut générer une phénomène d'auto convergence / appartenance : l'homme doit contrôler
";Other;"La législation de l'IA n'est pas la solution, c'est un frein à l'innovation. Adapter la législation aux droits de l'humain est nécessaire : droit de conserver le choix, le libre arbitre, d'être informé de l'usage de ses données (connaître sa catégorie, accéder aux traitements, ...)
définir rapidement un cadre éthique, morale, des recommandations claires pour améliorer l'humanité deviennent des actions primordiales.";Other;"l'IA n'est pas une personne morale ou physique, ce n'est pas une entité, mais un système.
le concepteur a des obligations, l'utilisateur des devoirs : c'est sur ce point qu'il faut légiférer.";;;"L'IA pour des système autonomes létaux, ministère des armées est la plus préoccupante. cybersécurité, usurpation (fake), détournement, terrorisme...?

Risque politique (manipulation de masse, fake news) avec la perte de liberté. Risque de discriminations (RH, bancaire, assurance, ...)

Les risques sont multiples : cybersécurité, écologique (énergivore), biais cognitifs ou des données, responsabilité...
Ces risques sont toujours liés à l'humain (en amont ou en aval) et au design de l'IA.";5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);"Les systèmes d'identification biométrique peuvent être autorisés :
1) dans les aéroports, ports, gares à dimension internationales, à des fins de contrôles
2) dans les zones publiques à risques (hôpitaux) si nécessaire.

Dans les autres cas, des systèmes non biométrique permettent déjà des traques, de la détection d'abandon de colis, ... Ces systèmes sont à encourager malgré quelques limites.";Much;"Ce système de label peut être complété par un audit des données et algorithmes, qui à défaut de ""transparence"" ou ""d'explicabilité"" indiquerait le respect d'un design éthique.
 Ce système de label doit être renouvelé annuellement, ou à chaque modification structurelle (algorithme, corpus de connaissance)
Ce label peut aller plus loin en déterminant si en fonction des cas d'usage, l'IA peut décider d'une façon autonome ou non. ";Other enforcement system;A l'instar de la fondation MAIF qui teste les voitures autonomes, création de laboratoire de tests publics et indépendants établissant des rapports sur des IA spécifiques (par exemple en santé, défense, jurisprudence, RH);il est nécessaire d'intégrer la notion de fiabilité/stabilité de la décision dans l'évaluation de la conformité.;Mental health risks;les risques liés à la maintenance ou l'évolution des IA : risques de régression ou de comportement instable dans le temps. Une IA doit garantir la pérennité de ses décisions. ;Yes;;No opinion;;Yes, for specific AI applications;"santé (création de médicaments, essais clinique, ...), défense (systèmes létaux), mobilité (voiture ""autonome"" à partir du niveau 2), politique/économique/juridique, RH ";;UE_Livre_blanc_sur_l_intelligence_artificielle___Une_approche_europ_enne_-_MASSET_J_r_me.pdf
F514586;25-03-2020 16:35;English;EU Citizen;Frédéric;HARDES;;;;;France;The feedback can be published with your personal information;5 - Very important;2 - Not important;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;Start seriously considering the legal implication with the parliament;3 - Neutral;5 - Very important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;;2 - Not important;5 - Very important;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;Yes;;Yes;;;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;5 - Very important;5 - Very important;No opinion;;Not at all;;No opinion;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F514585;25-03-2020 13:23;German;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;No opinion;No opinion;4 - Important;4 - Important;4 - Important;;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;;4 - Important;5 - Very important;4 - Important;;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;;There is a need for a new legislation;;No;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;Yes;;Yes, for all AI applications;;;
F514584;25-03-2020 10:26;German;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;1 - Not important at all;5 - Very important;3 - Neutral;3 - Neutral;1 - Not important at all;;5 - Very important;5 - Very important;3 - Neutral;2 - Not important;4 - Important;3 - Neutral;;3 - Neutral;4 - Important;3 - Neutral;;3 - Neutral;4 - Important;3 - Neutral;3 - Neutral;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;No;;;;;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;No opinion;;No opinion;;;
F514583;24-03-2020 22:04;German;EU Citizen;Helmuth;Fieltsch;;;;;Germany;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;;4 - Important;4 - Important;4 - Important;4 - Important;3 - Neutral;4 - Important;;There is a need for a new legislation;;Yes;;Yes;;;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Cyber risks;;Yes;;Yes;;Yes, for all AI applications;;;
F514582;23-03-2020 17:43;Italian;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;No;;;;L'uso per discriminare;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;La deroga al divieto generale porterebbe a una applicazione prater legem;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;E comunque dobbiamo passare dal Libro bianco sull'intelligenza artificiale a un Regolamento Europeo per l'intelligenza artificiale;Mental health risks;;Yes;;Yes;Assolutamente da tenere in considerazione le responsabilità di progettisti (anche dell'algoritmo), dei produttori e dei manutentori. Attenzione al c.d. autoapprendimento;Yes, for all AI applications;;Ovviamene il quadro normativo deve essere Europeo;
F514581;22-03-2020 12:25;German;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;;4 - Important;4 - Important;3 - Neutral;3 - Neutral;5 - Very important;5 - Very important;;4 - Important;5 - Very important;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;KI könnte in Kombination mit ausgeweiteten Sicherheitsmaßnahmen leicht dazu führen, die Grundlage für Verhaltenssteurung und somit einen wichtigen Pfeiler für autoritäte Tendenzen berietstellen - dies sollte in jedem Fall vermieden werden. Technologien sollen so reguliert werden, dass sie demokratische Prozesse, Rechte und Rahmenbedingungen fördern und verbessern, nicht, um sie ggf. einzuschränken.;There is a need for a new legislation;;No;;;;AutomatisierteGesichtserkennung und Strafverfolgung;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;evtl. Unterscheidung von Individual- und Kollektivrisiken anhand bestimmter Beispiele;Yes;;Yes;;Yes, for all AI applications;;;
F514580;21-03-2020 12:34;Spanish;EU Citizen;Aniceto;Pérez y Madrid;;;;;Spain;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;3 - Neutral;4 - Important;5 - Very important;"La Comunicación de los beneficios y riesgos de la IA. Una postura de los ciudadanos basada en políticas previamente decididas es excluyente. Muchos de los retrasos en el tratamiento del COVID-19 se han debido a la ocultación de información sobre esta enfermedad y sus características que la distinguen de una ""gripe"". Esa desinformación se ha debido en parte a intereses políticos y económicos, los afectados y fallecidos son los ciudadanos,";3 - Neutral;2 - Not important;5 - Very important;4 - Important;5 - Very important;2 - Not important;La mayor parte de los estudios universitarios se han centrado en programas master. Creo que es importante ampliar con el impulso necesario la creación de grados con enfoques diversos en IA y promover el uso aplicado de la IA en las demás áreas de conocimiento. La enseñanza de IA en la escuela a partir de 10-12 años es uno de los pilares de en China y EEUU.;2 - Not important;4 - Important;3 - Neutral;La Universidad es el mejor centro de investigación, generalmente, si se dispone de medios económicos. También las empresas, grandes, pequeñas, start-ups o consolidadas. Creo que es importante apoyar esa investigación básica y aplicada. Crear foros, congresos, simposios, que sirvan también de medida de cumplimiento de los requerimientos para justificar las ayudas. El acceso debe ser simplificado para no limitar el acceso, pero exigente en cuanto a resultados. Créditos a bajo interés ;2 - Not important;3 - Neutral;3 - Neutral;3 - Neutral;4 - Important;Los DIH corren el riesgo de ser el destino mayoritario de las ayudas y ser organismos burocráticos sin límites presupuestarios. Construir un modelo de IA es de una escala distinta a un LHC. El software es open-source en su mayoría, está en internet, en papers. El papel de un DIH es investigar, no en atender SMEs. Ninguno de los gigantes (Apple, Facebook, Amazon, IBM,...) se han creado desde DIH. SpaceX es líder aeroespacial privado, la NASA, financiada públicamente, está tecnológicamente vacía;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;En la ética de la IA se suele hablar de FAT(fairness, accuracy, transparency). El Libro Blanco no habla sobre fairness y es muy importante. En las aplicaciones de alto riesgo donde debe existir supervisión humana. La supervisión en las fases previas a la operación no garantiza un resultado éticamente satisfactorio. La equidad no se puede programar tal como explica este paper https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3547922 La supervisión final humana es la forma de lograr fairness ;Current legislation may have some gaps;;Yes;;Other;"Creo que cualquier sistema inteligente cuyo resultado afecte a un ser humano es de alto riesgo, se deben aplicar criterios éticos. Considero que las características indicadas de ""alto-riesgo"" son tan difusas como insuficientes. Por ejemplo se habla del reconocimiento facial automático en lugares públicos ¿Y en los privados? El GDPR aplica en uno y en otro";La IA en el sector público aplica directamente a personas físicas y jurídicas. Todas deberían ser de alto riesgo. La concesión de permisos, subvenciones, créditos, investigación de impuestos supone un riesgo en ambos casos.;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;Los sistemas de identificación biométrica deberían estar permitidos en espacios no públicos bajo regulación especial;Very much;Creo que un sistema de sellos de calidad por tecnologías o por sectores o por riesgos facilita mucho la comprensión y la confianza, como la identificación de lugares con cámaras de vigilancia, el símbolo de pago por tarjeta sin contacto o los usos posibles del menaje de cocina con las distintas tecnologías de calentamiento (gas, vitro, inducción...);No opinion;;Un sistema de identificación de productos y denuncia de fallos con capacidad de investigación de incidentes, recabar pruebas y resolver con agilidad en el idioma del requirente;Mental health risks;Riesgos de datos. Los datos son pasivos, no reaccionan con su uso, por tanto una vez liberados nada impide su copia y utilización para fines para los que no se ha dado el consentimiento, si es que se ha llegado a dar. Los formularios de consentimiento son farragosos e incomprensibles incluso para especialistas. Los formularios facilitan la aceptación pero hacen muy complicado denegarla y no garantizan cualquier uso no expresamente indicado. ;Yes;El ritmo de crecimiento de los proyectos de IA se duplica cada 3,5 meses, según los expertos. Es obvio que los procedimientos tienen una validez más que efímera;Yes;Con la adopción de lA en diversos dispositivos se abren nuevos escenarios, nuevos agentes, nuevas mediaciones y flujos de datos, desarrollo, cadenas de responsabilidad que deben ser tenidas en cuenta. Sin necesidad de entrar en detalles tecnológicos de  dejarían la legislación obsoleta incluso antes de ser aprobada, creo que es más efectivo centrarse en el objetivo que es la protección de los derechos de los ciudadanos, de forma unificada en la UE. pero considerando nuevas situaciones;Yes, for all AI applications;;Responsabilidades por el uso no consentido de datos, personales o no. Responsabilidades por el uso de datos para fines nuevos o cruzados con otros. Extracción de perfiles.;Comentarios_al_Libro_Blanco_sobre_el_enfoque_a_la_excelencia_y_la_confianza__1_.pdf
F514579;21-03-2020 00:19;German;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;3 - Neutral;;4 - Important;5 - Very important;No opinion;2 - Not important;4 - Important;5 - Very important;Richtlinien und Gesetze bezüglich der KI;4 - Important;5 - Very important;3 - Neutral;;3 - Neutral;5 - Very important;4 - Important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;No opinion;;No opinion;;;;KI im Bereich der Gesichtserkennung, des Predictive Policings (wenn personenbezogen), in Gerichtsprozessen;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Cyber risks;;Yes;;No opinion;;No opinion;;;
F514578;20-03-2020 18:48;Spanish;Public authority;Pancho;Glz Audicana Z;National;Administración de justicia;716838314731-31;Large (250 or more);Spain;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;Fomentar también la IA fiable, ética y antropocéntrica, con el resto del mundo;4 - Important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;5 - Very important;Becas y ayudas a empresas innovadoras. Premios y concursos;5 - Very important;4 - Important;4 - Important;;4 - Important;4 - Important;5 - Very important;4 - Important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;La IA es revolucionaria, necesaria, sigilosa e imparable;Other;"Dos ámbitos.
- Una regulación que aproveche la ya desarrollada en el seno de la Unión Europea y proteja los derechos fundamentales, especialmente la protección de los datos personales y de la privacidad y la no discriminación. 
- Un marco regulatorio que asegure de manera clara la responsabilidad de los diferentes intervinientes y operadores, fomentando el principio, relativo a que cada obligación debe dirigirse a la persona que esté en mejor posición de asumir el riesgo.
";Yes;;Yes;;Cualquiera que compro,meta un derecho fundamental, sobre todo, seguridad, justicia y finanzas;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);Deben utilizarse, pero debe asegurarse su correcto uso por las autoridades;Very much;Que sea claramente visible en la web del servicio ofrecido en la UE;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Cyber risks;;Yes;;Yes;Debemos regular no centrando la visión en el sector si no en el concreto servicio ofrecido, regulando actividades, servicios mediante Directivas;Yes, for all AI applications;;;PROPUESTAS_AL_LIBRO_BLANCO_SOBRE_LA_INTELIGENCIA_ARTIFICIAL.pdf
F514577;20-03-2020 14:23;French;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;;4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;;4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;;Current legislation may have some gaps;;Yes;;Yes;;Transport, défense, recrutment ;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;No further guidelines or regulations are needed;;Very much;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Risks related to the loss of connectivity;;Yes;;Yes;;Yes, for specific AI applications;;;
F514576;20-03-2020 08:09;Finnish;NGO (Non-governmental organisation);Erkko;Meri;;Finland Chamber of Commerce;;Small (< 50 employees);Finland;The feedback can be published with your personal information;4 - Important;4 - Important;5 - Very important;4 - Important;3 - Neutral;4 - Important;Mahdollisen regulaation osalta Keskuskauppakamari pitää tärkeänä, että sääntely keskittyy ainoastaan korkean riskin sektoreihin ja järjestelmiin. Sääntelyssä on tärkeää tunnistaa sektorikohtaiset erityispiirteet sekä joustavuus. Ylisääntelyyn tulee suhtautua torjuvasti. Fragementaation estäminen jäsenvaltioiden välillä on erittäin tärkeää. Myös ;4 - Important;3 - Neutral;4 - Important;4 - Important;5 - Very important;5 - Very important;Luonnollisten henkilöiden tietoisuuden ja osaamisen lisääminen tekoälyyn liittyvistä kysymyksistä on kriittisen tärkeää tulevaisuudessa. Tältä osin olennaista on mm. yliopistojen roolin tunnistaminen.;4 - Important;;3 - Neutral;;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;;3 - Neutral;4 - Important;3 - Neutral;2 - Not important;2 - Not important;3 - Neutral;;Current legislation may have some gaps;;Yes;;Yes;;;3 - Neutral;3 - Neutral;2 - Not important;3 - Neutral;4 - Important;4 - Important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;No opinion;;Yes;;Yes, for specific AI applications;;;
F514575;19-03-2020 19:15;Spanish;Trade Union;Jose;VARELA FERRIO;;UGT (Unión General de Trabajadores);963162918784-50;Large (250 or more);Spain;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;3 - Neutral;"El libro blanco sobre IA propuesto sufre de una gravísima omisión: las repercusiones de la IA y la algoritmia en las relaciones laborales.

En la Unión Europa hay más de 220 millones de personas trabajadoras, que pasan una media de 40,2 semanales en el trabajo. Sin embargo, este hecho se deprecia a la hora de regular la Inteligencia Artificial en Europa.

Es urgente y prioritario incluir las consecuencias de la IA en el trabajo, como una parte importante y esencial del Libro Blanco.";4 - Important;4 - Important;4 - Important;3 - Neutral;5 - Very important;4 - Important;Tal y como hemos indicado en la respuesta anterior, de nuevo, se vuelve a obviar el factor trabajo en toda la estrategia, cuando el empleo y la empleabilidad siguen siendo una pieza fundamental del Estado del Bienestar europeo, tanto desde un punto de vista económico y fiscal, como de dignidad humana. El trabajo dota de un medio de vida a las personas, les proporciona libertad, progreso e inclusión social. ;No opinion;No opinion;No opinion;;5 - Very important;4 - Important;4 - Important;4 - Important;3 - Neutral;Como en los casos anteriores, no se puede arrumbar el factor trabajo en cualquier política sobre IA aplicable a las PYME. ¿Cuales son las consecuencias de una aplicación masiva de la IA en las pequeñas y medianas empresas? ¿Se destruirá empleo neto? ¿Empeorarán las condiciones de trabajo? ¿Aumentará la polarización o la precariedad? Es inasumible que una estrategia de estas caracteristicas no se haga estas preguntas y ni siquiera intente responderlas.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Efectivamente, la IA presenta múltiples riesgos para las personas trabajadoras. Al igual que no se pueden despreciar sus beneficios, es un ejercicio de irresponsabilidad no evaluar y contemplar sus repercusiones en en las relaciones laborales.Y más inexplicable es aun cuando la Comisión Europea reconoce las consecuencias de este uso masivo de los algoritmos en los mercados de trabajo: crecimiento de la desigualdad, comportamientos discriminatorios por género o raza o dislocación del trabajo.;There is a need for a new legislation;;Yes;;No;;Todas aquellas reconocidas y acreditas por las Comisarias de Empleo y Asuntos Sociales y de Economía y Sociedad Digitales, elaboradas  por el grupo de expertos en empleo en abril de 2019: https://ec.europa.eu/digital-single-market/en/high-level-expert-group-impact-digital-transformation-eu-labour-markets ;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;Se trata de una evidente amenaza para la privacidad de las personas, un atentado contra la intimidad y una fuente de posibles ilegalidades como el robo de identidad.;Not at all;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;Todos aquellos relacionados con las relaciones laborales y la seguridad y salud en el trabajo.;Yes;;Yes;;Yes, for all AI applications;;;
F514574;18-03-2020 07:57;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;;5 - Very important;4 - Important;5 - Very important;;4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;;3 - Neutral;4 - Important;4 - Important;5 - Very important;2 - Not important;4 - Important;For clarification: all above mentioned concerns have to be taken seriously. This is the basis to create trust. Our rating reflects how relevant we estimate the respective risk. A further point to consider is, that risk of AI depends on the environment the AI is implemented in, for example “endangering safety” is low risk in washing machines but possibly high risk with weapons.;Current legislation may have some gaps;;Yes;;No;;The sectorial approach of the EU when defining high risk seems to lead to sectorial legislation in the future. We would plead for a horizontal approach as we fear that a vertical approach would lead to significant legal gaps and uncertainty. It would be very difficult to revise all relevant vertical legislation in a coherent manner. ;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Not at all;RB sees a volunatry labelling scheme critical. One could only trust it to a limited extent such as in case of a self-assessment. Before any type of voluntary labelling scheme is introduced clear transparent rules and metrics based on international standards have to be agreed on. National schemes should be avoided in any case.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Risks related to the loss of connectivity;The EU product safety regulation is largely fit for purpose and covers AI as a risk cause sufficiently even though there may still be gaps to be closed. For example, product safety legislation should make clear to what extent it also covers software. In particular, it is in particular an open question whether stand-alone software is covered by EU product safety legislation. Apart from that, EU product safety legislation should be extended to security and, if necessary, also to services.;No;;No;We would advocate for a careful approach with regard to a potential update on the legisla-tive liability framework. According to our assessment, the current framework addresses products based on AI in an adequate way, by taking our understanding of “high risk” into account. If “high risk” is applied to non-life-threatening areas like worker’s rights , we were to argue for applying a more specific definition. ;No;;"National ruling always bears the risk of fragmentation of the single market. National rules should be as much as possible harmonised within the EU 


";
F514572;17-03-2020 17:41;German;;;;;;;;;The feedback can be published in an anonymous way;3 - Neutral;3 - Neutral;3 - Neutral;4 - Important;4 - Important;4 - Important;;5 - Very important;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;;3 - Neutral;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;3 - Neutral;;There is a need for a new legislation;;No;;;;Autonomous Driving;3 - Neutral;5 - Very important;3 - Neutral;3 - Neutral;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Risks related to the loss of connectivity;;Yes;;Yes;;Yes, for all AI applications;;;
F514571;17-03-2020 15:13;German;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;3 - Neutral;5 - Very important;4 - Important;4 - Important;4 - Important;;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;;3 - Neutral;3 - Neutral;3 - Neutral;;4 - Important;4 - Important;5 - Very important;4 - Important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;4 - Important;No opinion;5 - Very important;;Current legislation may have some gaps;;No;;;;;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Risks related to the loss of connectivity;;Yes;;Yes;;Yes, for all AI applications;;;
F514570;15-03-2020 14:41;English;Academic/Research Institution;Elif Mendos;Kuskonmaz;;Queen Mary University of London, University of Portsmouth, Doughty Street Chambers, Centre d’études sur les conflits;;Large (250 or more);United Kingdom;The feedback can be published with your personal information;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;Comment_Guild_Kuskonmaz_Moffatt_Bigo_March_2020.pdf
F514569;14-03-2020 20:32;French;Academic/Research Institution;Philippe;Besse;;Université de Toulouse INSA / Institut de Mathématiques UMR CNRS 5219;;Large (250 or more);France;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;4 - Important;;4 - Important;5 - Very important;4 - Important;3 - Neutral;4 - Important;5 - Very important;;1 - Not important at all;5 - Very important;3 - Neutral;"Attirer les meilleurs doctorants en proposant  plus de postes avec des salaires plus attractifs! C'est l'étendu d'un réseau de recherche vaste et performant qui permet l'émergence de singularités exceptionnelles, pas la création ex nihilo d'un centre ""phare""!. C'est typiquement ce qui se passe ou qui s'est passé en Mathématiques en France avec la filière ENS suivi d'un poste en CDI de CR CNRS. Cf. le nombre de médailles Fields.";3 - Neutral;3 - Neutral;4 - Important;4 - Important;3 - Neutral;;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;"La prise en compte concrète de ces préoccupations nécessite des outils de détection de risque et donc des normes  ""quantitatives"". Par exemple, la notion  d'adverse ou disparate impact (effet disproportionné) aux USA  est insuffisante pour détecter / mesurer toutes les capacité discriminatoires d'un algorithme mais c'est un début efficient sur le développement des systèmes d'IA pour le pré-recrutement. Imposer une  norme pour informer sur les risques d'erreur d'un système d'IA.";There is a need for a new legislation;;No;;;;La distinction entre applications à haut  / bas risque ne me semble pas pertinente. Exemple: le profilage publicitaire a évidemment moins d'impact sur une personne qu'un diagnostic médical ou un profilage professionnel mais le profilage publicitaire peut s'adresser à tellement de monde qu'il serait risqué d'en sous-estimer les éventuelles dérives et impacts sociétaux. Cf. l'affaire Cambridge Analytica.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;"Un label ex ante nécessite la définition de normes quantitatives (comme un label bio !) venant préciser la liste d'évaluation du guide des experts. Cette liste peut évaluer  tout type d’application quel que soit le ""niveau"" de risque. C'est la fréquence des contrôles et le niveau des sanctions qui doit s'adapter proportionnellement au niveau de risque. Ceci complète la réponse ci-dessous.";Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;J'ai beaucoup de doute sur la capacité à mettre en place des contrôles ex post pertinents et efficaces. Je propose de renforcer l'analyse volontaire de la conformité ex ante pour toutes les applications comme pour le privacy impact assessment: liste d'évaluation assortie de normes quantitatives. D'ailleurs en matière de protection des données, les seules actions ex post sont justement déclenchées, trop tard, par des fuites de données. ;Personal security risks;Les risques liés à une utilisation abusives des données personnelles collectées;Yes;;Yes;Le point clef me semble être l'obligation d'identifier un responsable humain à chaque étape: constitution de la base d'apprentissage, entraînement, test, validation , certification, exploitation d'un système d'IA afin de pouvoir en remonter les maillons en cas d'incident. Il ne s'agit pas de diluer les responsabilités mais d'imposer la mise en place d'une boucle vertueuse du process qualité, jusqu'à pouvoir compléter la base d'apprentissage si nécessaire, durant la durée de vie du système d'IA.;Yes, for all AI applications;;Cf. question précédente.;IA-et-bioethiquev2.pdf
F514568;14-03-2020 18:40;French;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;2 - Not important;5 - Very important;5 - Very important;1 - Not important at all;1 - Not important at all;Shared infrastructure as a Service across the Union to compete with GAFA;1 - Not important at all;5 - Very important;1 - Not important at all;1 - Not important at all;5 - Very important;5 - Very important;If there is any financing or facilitation for private companies, it should be in exchange of ownership by the people. We are stakeholders and should be shareholders, not sugar daddies.;;5 - Very important;1 - Not important at all;Giving free benefits or knowledge to private companies doesn't trickle down. Applications and spin-offs should be owned by the people. We are stakeholders and should be shareholders, not sugar daddies.;5 - Very important;5 - Very important;5 - Very important;1 - Not important at all;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;2 - Not important;5 - Very important;3 - Neutral;Concentration of power. If a company can become the only one to be able to train certain models, we are all dependent on them.;There is a need for a new legislation;;No;;;;Use of AI in policing people and for military purposes mainly.;5 - Very important;5 - Very important;4 - Important;3 - Neutral;2 - Not important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Rather not;Why would we need a label? If a system is regulated, it shouldn't need a label. If it's not, the capitalist market has proven it doesn't care.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F514567;14-03-2020 17:52;French;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;4 - Important;;5 - Very important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;Yes;;Yes;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;Article_CastetsRenard.pdf
F514566;14-03-2020 15:48;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;4 - Important;4 - Important;No opinion;2 - Not important;2 - Not important;;4 - Important;3 - Neutral;1 - Not important at all;4 - Important;4 - Important;No opinion;;3 - Neutral;No opinion;4 - Important;;No opinion;No opinion;4 - Important;No opinion;No opinion;;3 - Neutral;5 - Very important;4 - Important;4 - Important;3 - Neutral;3 - Neutral;;There is a need for a new legislation;;No;;;;Live facial Recognition in public spaces. Same goes for other biometric recognition (like gait). Detection of behavior in public spaces via intelligent video analysis. ;5 - Very important;No opinion;4 - Important;4 - Important;2 - Not important;3 - Neutral;Biometric identification systems should never be allowed in publicly accessible spaces;;Very much;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;;;No opinion;;No opinion;;No opinion;;;
F514565;13-03-2020 18:09;English;EU Citizen;Bogdan;MICU;;;;;Luxembourg;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;4 - Important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;3 - Neutral;5 - Very important;4 - Important;"foster intra-EU mobility and attract international talent by (i.a.) promoting English as administrative language at all levels throughout EU

rationale :
one attraction point is the ability to carry out one's life using today's global lingua franca, English; imagine trying to settle in an unfamiliar country / culture without being able to communicate with state bureaucracies
so it is possible that EU might be able to develop local talent, but to subsequently lose it to English-speaking areas";4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;3 - Neutral;No opinion;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"* dark AI : AI use is relatively easy to conceal, so monitoring and enforcing compliance with EU regulations is difficult
* how to regulate a system that cannot be pinpointed geographically?
* how to distinguish b/w malfunction and error, or b/w AI suicide and hardware malfunction?";There is a need for a new legislation;;No;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;https://trizniak.blogspot.com/2018/09/trusted-ai.html;Personal security risks;;No opinion;;No opinion;;No opinion;;;
F514564;13-03-2020 16:54;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;"Making sure SMEs, and other companies conducting AI research, can benefit from the TDM exception in Art. 3 of the copyright in the digital single market directive (see pp. 27-28 of my paper ""A Legal Framework for AI Training Data"", attached)";5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;3 - Neutral;;5 - Very important;5 - Very important;4 - Important;;4 - Important;5 - Very important;5 - Very important;4 - Important;2 - Not important;;5 - Very important;4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;"data quality; IT security; see, in detail, my analysis on pp. 2-4 of my paper ""A Legal Framework for AI Training Data"", attached";There is a need for a new legislation;;No;;;;"""life and limb"":
- medical AI
- autonomous driving";5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;4 - Important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);"general proportionality for public actors; use by private actors only to avert imminent danger";Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;"See my analysis on pp. 14-17 of my paper ""AI Regulation in Europe"", attached";;;Yes;"Product monitoring and update obligations, see pp. 14-15 of my paper ""AI Regulation in Europe"", attached";Yes;"1) Extension of scope to software (including AI); 2) specification of term 'defect' -> pp. 13-14 of my paper ""AI Regulation in Europe"", attached";Yes, for all AI applications;;"rebuttable presumptions in case regulatory framework is violated, possibly installed in EU law, not national law, see pp. 16 and 18 of my paper ""AI Regulation in Europe"", attached";Hacker_2020_on_AI_regulation_in_EU_and_on_AI_training_data.pdf
F514563;13-03-2020 12:27;Spanish;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;;3 - Neutral;3 - Neutral;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;4 - Important;4 - Important;3 - Neutral;3 - Neutral;3 - Neutral;4 - Important;;There is a need for a new legislation;;Yes;;Yes;;;4 - Important;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;No opinion;;No opinion;;No opinion;;;
F514562;12-03-2020 16:06;Lithuanian;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;3 - Neutral;4 - Important;5 - Very important;;4 - Important;4 - Important;5 - Very important;;5 - Very important;4 - Important;4 - Important;;3 - Neutral;;4 - Important;5 - Very important;4 - Important;4 - Important;5 - Very important;3 - Neutral;;Current legislation may have some gaps;;Yes;;Yes;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Cyber risks;;Yes;;Yes;;Yes, for specific AI applications;Saugumo, Privatumo;;
F514561;11-03-2020 17:34;English;NGO (Non-governmental organisation);Maximilian J.;Haarich;;Embassy of the Republic of Užupis to Munich;740500937616-83;Micro (< 10 employees);Germany;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;4 - Important;Inclusion of civil society (beyond public online consultations). Discussion about the overall goal - what is the world we want to live in?;2 - Not important;2 - Not important;3 - Neutral;4 - Important;5 - Very important;5 - Very important;Finding a clear position between data totalitarism and data capitalism.;1 - Not important at all;1 - Not important at all;3 - Neutral;Integrate arts and humanities > exchange with the EU program STARTS (Science, technology, arts). We need to have our own vision of the future. So far, we always try to beat foreign competitors in a game that they invented.;1 - Not important at all;3 - Neutral;5 - Very important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"We need humans on the loop. The Coronacrisis demonstrates what happens when news/trade/surveillance algorithms goad each other decoupled from human reason: AI ""interpretations"" are taken as facts and people act accordingly.";Other;We need to prioritize individual ethics. Ethics works without laws but not the other way around.;No;;;;News algorithms optimized for engagement. People spend too much time in their filter bubble and tend to believe in a preprocessed reality with all the negative consequences of populism, hate speech, etc.;3 - Neutral;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;"We are unintentionally building a totalitarian system. The measures to prevent harm start to cause harm themselves. We have to accept a certain amount of ""wrong behaviours"" beause we are all humans. A life in which nothing bad can happen is not worth living.";Very much;We sould label the AI designers instead of the AI companies. Otherwise it could become a mere marketing asset.;Other enforcement system;Ask the people affected by AI before and during implementation. If they are not OK with it, don't do it. If they suffer harm, compensate them. Society is not a technology testing lab.;It sounds naive: simply ask the decision makers whether they feel good about what they are doing. If you see proudly sparkling eyes, it will be OK.;Mental health risks;Not being able to opt out of being affected by AI at all.;Yes;Democratically involve the people affected by AI and respect their opinion.;No;;Yes, for all AI applications;;Companies only react on monetary sanctions. Make sure that these can apply whenever people were harmed, even if now law was broken.;Uzupis_Principles_for_Trustworthy_AI.pdf
F514560;11-03-2020 13:19;Danish;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Guidelines omkring informationssikkerhed i forhold til AI, deling af data, GDPR. Konkrete retningslinjer og muligheder for sikker deling af data. Fælles standard udarbejdes.;4 - Important;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;;4 - Important;5 - Very important;4 - Important;3 - Neutral;5 - Very important;5 - Very important;;Current legislation may have some gaps;;Yes;;No opinion;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Other special requirements in addition to those mentioned in the question above should be imposed (please specify);;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;Yes;;Yes, for all AI applications;;;
F514559;11-03-2020 09:39;English;EU Citizen;gloria;macia;;;;;Spain;The feedback can be published with your personal information;5 - Very important;4 - Important;3 - Neutral;5 - Very important;4 - Important;2 - Not important;See attachement;4 - Important;2 - Not important;;5 - Very important;3 - Neutral;1 - Not important at all;;2 - Not important;4 - Important;5 - Very important;See attachement;4 - Important;2 - Not important;4 - Important;5 - Very important;5 - Very important;See attachement;3 - Neutral;5 - Very important;4 - Important;3 - Neutral;4 - Important;2 - Not important;See attachement;Current legislation is fully sufficient;;;;;;In medtech (MDR/IVDR) - Guidances or international standards are required, not new EU legislation - coordinate with MDCG e.g. to establish mapping between the standard and MDR provisions;4 - Important;4 - Important;4 - Important;4 - Important;3 - Neutral;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;All the abovementioned points are important (necessary but not sufficient conditions in math terms), human oversight is unclear. ;No opinion;As it is presented in the White Paper, I see it ony as an overhead for SMEs. If the idea is further develop, it could be a good idea. ;A combination of ex-ante compliance and ex-post enforcement mechanisms;;I would foresee the same conformity assessments we have currently in place for medical devices being applicable. The only difference is that the NBs would have to adress conformity considering a new standard;;;Yes;See attachment. Check FDA framework draft for CLS. Use the agency's approach or consider them design changes which may require re-certification ;Yes;Applicable only to non-highly regulated industries, standalone software is already covered in MDR. ;No;;As stated in the MDR, the manufacturer who places the product into the market in his same is liable for the product and all its supply change (tracebility);youtube_video.pdf
F514558;10-03-2020 17:08;German;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;4 - Important;3 - Neutral;5 - Very important;5 - Very important;4 - Important;"Keine weiteren Regelungen, die für KMU´s Zusatzbelastungen bedeuten.
Keine zusätzlichen Regulierungen für KMU.";5 - Very important;3 - Neutral;2 - Not important;5 - Very important;3 - Neutral;2 - Not important;KI für KMU nur bei Nachfrage durch die jeweilige KMU.;5 - Very important;4 - Important;4 - Important;;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;Keine zusätzlichen Regulierungen für KMU. Plan für Komplettausfall des Internet in ganz Europa.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Keine zusätzlichen Regulierungen für KMU.  Einsatz biometrischer Fernidentifikationssysteme (z. B. Gesichtserkennung) verbieten! KI für Behinderte unüberwindlich, für diese Behinderten DRINGEND Ausnahmen schaffen! Komplettausfall des Internet in ganz Europa.;There is a need for a new legislation;;No;;;;Gesundheitsdaten, Daten über Finanztransaktionen;3 - Neutral;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;Ernstfall vorsehen und vorplanen bei Komplettausfall des Internet in ganz Europa.;Yes;;Yes;;Yes, for all AI applications;;;
F514557;10-03-2020 13:03;German;Academic/Research Institution;Institut für angewandte Arbeitswissenschaft;IFAA;;ifaa - Institut für angewandte Arbeitswissenschaft;;Small (< 50 employees);Germany;The feedback can be published with your personal information;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;3 - Neutral;Sozialpartner (Arbeitgeber und Gewerkschaften) sollen hierbei mitwirken. Daher sollte an dieser Stelle konkretisiert werden, welche Maßnahmen geplant sind und wie die Sozialpartner einbezogen werden könnten: Inwieweit könnten die Sozialpartner am Aufbau der Uni- und HS-Netze (s. Maßnahme 3) mitwirken? Inwieweit könnten die Sozialpartner an den Entwicklungen von Curriculae oder Lehrplänen mitwirken?;4 - Important;5 - Very important;3 - Neutral;4 - Important;4 - Important;5 - Very important;;2 - Not important;5 - Very important;4 - Important;;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;"Darunter müssen auch Unternehmen gefasst werden, die aufgrund von Konzernstruktur keinen Anspruch auf FuE-Förderung haben. Die Erfahrung zeigt, dass viele derartige Betriebe wie ""echte"" KMU agieren (geringe Beschäftigtenzahl, keine eigene FuE, geringe Investitionen) und dieselben Herausforderungen haben, aber nicht auf die FuE-Ressourcen der verbundenen Unternehmen zurückgreifen können. Sie würden bei den FuE-Maßnahmen und Investitionen zu KI unberücksichtigt bleiben und einen Nachteil erleiden.";3 - Neutral;2 - Not important;2 - Not important;4 - Important;2 - Not important;3 - Neutral;;Current legislation may have some gaps;;Yes;;Yes;;;2 - Not important;4 - Important;4 - Important;2 - Not important;4 - Important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Not at all;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Cyber risks;;No;;No;;No;;;
F514556;10-03-2020 09:34;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;4 - Important;5 - Very important;;5 - Very important;3 - Neutral;2 - Not important;;4 - Important;4 - Important;4 - Important;2 - Not important;2 - Not important;;1 - Not important at all;5 - Very important;4 - Important;2 - Not important;4 - Important;5 - Very important;;There is a need for a new legislation;;Yes;;Yes;;;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Very much;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Personal security risks;;Yes;;Yes;;Yes, for all AI applications;;;
F514555;09-03-2020 21:50;Hungarian;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;;3 - Neutral;5 - Very important;4 - Important;;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;4 - Important;;Current legislation may have some gaps;;;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No further guidelines or regulations are needed;;Much;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Risks related to the loss of connectivity;A hálózati összekapcsoltság során téves adatközlésb?l ered? kockázatok;Yes;;Yes;;Yes, for specific AI applications;közlekedés, egészségügy, energiaszektor, közszféra egy része;;
F514554;09-03-2020 18:53;German;EU Citizen;Bianca;Weber-Lewerenz;;;;;Germany;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Adapt Education, esp. of Engineers, to Digitalization and AI Technologies. Multidisciplinary Learning for best Competencies and Talents. Interdisciplinary Research and Cooperations to face challenges. Maintain and enhance a Culture of Knowledge and of Awareness. Distinguish and set clear transparent Limits between Human and Artificial Intelligence. Bridge gap betw. Sciences/Research & Implementation. Keep human being&Ethic Values in the center of all AI Discussions and Technological Development.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;3 - Neutral;3 - Neutral;3 - Neutral;5 - Very important;3 - Neutral;5 - Very important;Humans feed and train AI. The more data is fed, the more accurate it may work. Attention to inaccurate algorithms: very difficult to return or re-direct. The person  feeding data, installing algorithms and training AI requires AI Knowledge via AI education.Here: interdisciplinary Approach is key! Include ethic aspects is key for prosperous, sustainable human ecosystem and social wellbeing in the value chain!;Current legislation may have some gaps;;No;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Rather not;;Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;;Personal security risks;Accurate algorithms to avoid intransparency. Human control overall. Maintain ethic values.;Yes;Risk Assessment is a lifelong Review process to enable adjustments, innovations and sustainability. It is part of trust building.;Yes;;Yes, for all AI applications;;National liability rules are Framework for AI application.;Stellungnahme_zum_Weissbuch_EU_Kommission_Bianca_Weber-Lewerenz_09.03.2020.pdf
F514553;09-03-2020 16:55;French;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;3 - Neutral;5 - Very important;4 - Important;5 - Very important;4 - Important;La création d'un comité stratégique intégrant industriels et académiques pilotant les initiatives au niveau européen;4 - Important;4 - Important;4 - Important;5 - Very important;No opinion;No opinion;;5 - Very important;5 - Very important;4 - Important;;3 - Neutral;4 - Important;4 - Important;5 - Very important;5 - Very important;;4 - Important;3 - Neutral;3 - Neutral;5 - Very important;2 - Not important;3 - Neutral;L'IA doit elle systématiquement supervisée ?;There is a need for a new legislation;;Yes;;No opinion;;Transports, médecine, justice, énergie ;5 - Very important;5 - Very important;3 - Neutral;4 - Important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);Uniquement à des fins de protection de la population ;Rather not;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Personal security risks;;Yes;;Yes;;Yes, for specific AI applications;;;
F514552;09-03-2020 14:10;German;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;Gesichtserkennung verbieten;4 - Important;5 - Very important;5 - Very important;3 - Neutral;4 - Important;3 - Neutral;;5 - Very important;;3 - Neutral;;4 - Important;3 - Neutral;4 - Important;4 - Important;2 - Not important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;No;;;;Gesichtserkennung;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;Biometric identification systems should never be allowed in publicly accessible spaces;;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;Yes;;Yes, for all AI applications;;;
F514551;09-03-2020 12:36;German;Public authority;Stephan;Dünki;National;Suva;;Large (250 or more);Switzerland;The feedback can be published with your personal information;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;;;3 - Neutral;4 - Important;5 - Very important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;;3 - Neutral;5 - Very important;5 - Very important;4 - Important;4 - Important;2 - Not important;;Current legislation may have some gaps;;Yes;;Yes;;Nutzung medizinischer Daten, Soziale Daten;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;No further guidelines or regulations are needed;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;No;;No;;;
F514550;08-03-2020 23:46;Italian;EU Citizen;Gabriele;Polastri;;;;;Italy;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Permettere più sperimentazione della guida autonoma (anche ai cittadini tramite AP delle vetture che lo permettono, cambiando alcune regole legislative)- Creare un unico assistente virtuale a livello Europeo che sia obbligatorio per la sanità e per il trasporto;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Permettere più libertà (controllata da un comitato) ai ricercatori IA indipendenti magari facendo sottoscrivere un contratto ai beta tester dei loro software- ;5 - Very important;5 - Very important;5 - Very important;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;Current legislation may have some gaps;;No;;;;"1) Guida completamente autonoma
2) La completa automazione di molti lavori potrebbe causare centinaia di migliaia di posti lavoro persi, andrebbe a colpire la fascia di persone con un livello formativo non sufficientemente alto per rientrare nel mercato, se non ci cauteliamo lavorando su un sistema di wellfare elevato, potremmo assistere a una catastrofe socioeconomica. 
3) DeepFake
4) Cercare di trovare accordi a livello ""mondiale"" per quanto riguarda l'uso di sistemi IA nel settore bellico.";4 - Important;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);Inizialmente dovremmo usare sistemi biometrici nei posti ritenuti indispensabili (Strutture sanitarie) oppure durante situazioni importanti (eventi politici, religiosi)- La parte più difficile è gestire i dati biometrici in maniera sicura. Bisognerebbe fare delle leggi sull'utilizzo e sulla cancellazione dei dati dopo n tempo. ;Much;;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;Creare varie fasi per l'immissione completa della IA sul totale territorio Europeo. ;Personal security risks;;Yes;"Esporre meno dati possibili al ""debutto"" di un nuovo sistema AI ad alto rischio. Creare varie fasi per la completa immissione.";Yes;;No opinion;;;
F514549;08-03-2020 04:42;English;Academic/Research Institution;Stefania;Druga;;University of Washington;;Large (250 or more);Romania;The feedback can be published with your personal information;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;AI Literacy as a citizen right ;5 - Very important;4 - Important;2 - Not important;3 - Neutral;4 - Important;5 - Very important;AI education for k12, universities and non-technical audience ;5 - Very important;3 - Neutral;4 - Important;Policy center focused on regulating this new technology ;;3 - Neutral;4 - Important;;3 - Neutral;Connect AI experts with domain specific demands ;3 - Neutral;5 - Very important;4 - Important;5 - Very important;4 - Important;2 - Not important;Low SES populations are more vulnerable to have their data collected ;There is a need for a new legislation;;Yes;;Yes;;Predictive policing ;4 - Important;4 - Important;5 - Very important;4 - Important;3 - Neutral;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F514548;06-03-2020 19:49;English;Other;Carlos;Pons;;European Digital Society;;Small (< 50 employees);Spain;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;4 - Important;;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;;There is a need for a new legislation;;Yes;;Yes;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Much;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F514547;06-03-2020 12:37;English;Academic/Research Institution;Simone;BORSCI;;University of Twente;769258827114-68;Large (250 or more);Netherlands;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;An important and uncharted aspect  is not how you create AI systems and make them reliable, but how you may assess these systems and model/predict their impact. A Digital Innovation Hub should also focus concurrently on methods for design and assess, or at least is what we are doing at the University of Twente in the Netherlands.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;To appear trustworthy and to be trustworthy are two different things. The key is to develop methods to disambiguate appearance and reality behind the black box. ;There is a need for a new legislation;;Yes;;Yes;;;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Rather not;;Other enforcement system;;;Mental health risks;;Yes;;Yes;;Yes, for specific AI applications;Health and well-being;;
F514546;05-03-2020 18:41;Spanish;Academic/Research Institution;Gerard;Rincón;;"Estudiante Doctorado en Derecho Universitat de Barcelona, España.
Profesor Universidad Santiago de Cali, Colombia.";;Large (250 or more);Spain;The feedback can be published with your personal information;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;Aprobar una Directiva en derecho civil sobre robótica y trabajar en cómo dar mayor alcance al soft law de los Principles of European Tort Law y el Draft Common Frame of Reference, para establecer un nuevo régimen de responsabilidad civil en el marco de la robótica e inteligencia artificial y solventar problemas de imputación de la responsabilidad.;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;Other;En parte, estoy de acuerdo en establecer requisitos para las aplicaciones de alto riesgo. Pero el aprendizaje autónomo y la inteligencia artificial provocarán problemas de imputación de responsabilidad por daños ante cualquier prototipo o aplicativo robótico con inteligencia artificial, por lo que se requiere replantear qué régimen de responsabilidad es aplicable, estudiar bien la implementación de un sistema de seguros obligatorio y de un fondo de compensación y garantía estatal.;;;;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Risks related to the loss of connectivity;;Yes;;Yes;Hay que trabajar en una hipotética legislación cuyo eje central sea un sistema de gestión de riesgos para todos los agentes implicados (productor, distribuidor, usuario, etc.). Estudiar si un sistema de responsabilidad objetiva o cuasi-objetiva iría en contra del plan estratégico de la UE acerca de incentivar la innovación tecnológica. Además, todavía no se puede descartar por completo la creación de una personalidad legal específica para los robots.;Yes, for all AI applications;;Para garantizar la seguridad jurídica y crear un marco de confianza hacia la IA y la robótica necesitamos normas comunitarias de transposición obligatoria para armonizar la legislación de derecho de daños en esta materia. ;
F514545;05-03-2020 16:29;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;;4 - Important;4 - Important;2 - Not important;;3 - Neutral;5 - Very important;3 - Neutral;3 - Neutral;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;No;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;"Social risks - exclusion, prejudice, discrimination, etc.
Health risks
";Yes;;Yes;;Yes, for all AI applications;;;
F514544;05-03-2020 13:45;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;2 - Not important;1 - Not important at all;4 - Important;;There is a need for a new legislation;;No;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;No further guidelines or regulations are needed;;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;No opinion;;Yes, for all AI applications;;;
F514543;04-03-2020 18:27;English;NGO (Non-governmental organisation);Bart;De Witte;;HIPPO AI Foundation;;Micro (< 10 employees);Germany;The feedback can be published with your personal information;4 - Important;3 - Neutral;5 - Very important;5 - Very important;5 - Very important;No opinion;Focus on Open Source Communities, Create Open Data Sets, New Open Licence Models that protect our fundamental rights. Data in healthcare is not a commodity, it is human life. We should forbid the monetisation of data and privatisation of knowledge in healthcare. ;5 - Very important;2 - Not important;4 - Important;4 - Important;5 - Very important;4 - Important;AI should be more people driven, and non-profit and NGO's should be part of the equation. Funding for social enterprises that help to make AI in healthcare a public good should get support. Startups that are financed by the EU should not be allowed to leave the EU when exiting. ;3 - Neutral;4 - Important;2 - Not important;There are already many research centers in Europe that work on AI. Europe is not about centralisation, it about federalism. Change the incentives of researchers and create more open source tools (eg. Tensorflow) for European Startups and SME's to build on. The focus should be on AI on the EDGE, federated learning and open AI technologies. ;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;There are already tons of Digital Health Hubs - this is not a task of the EU. ;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;4 - Important;1 - Not important at all;Suppose you have cancer and you have to choose between a black box AI surgeon that cannot explain how it works but has a 90% cure rate and a human surgeon with an 80% cure rate. Do you want the AI surgeon to be illegal? Do not include medicine that has dealt with inaccuracy since it's beginning in this equation. Implement Double Blind Prospective Trails instead of reference center.;Other;"Article 3 of our charta for fundamental rights, mentions the prohibition on making the human body and its parts as such a source of financial gain. As we can replicate human by digital twins, we need to extend ""its parts"" into data as well.";No;;;;;5 - Very important;4 - Important;5 - Very important;5 - Very important;3 - Neutral;4 - Important;Biometric identification systems should never be allowed in publicly accessible spaces;biometric identification can extract different knowledge (face -> genotype data);Very much;;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;;Mental health risks;risk in loss of the right to mental and physical integrity. risk of loss of ownership of digital self (digital twin).;Yes;;Yes;;Yes, for specific AI applications;autonomous machines;;
F514542;04-03-2020 13:30;English;EU Citizen;David;Shaw;;;;;United Kingdom;The feedback can be published with your personal information;4 - Important;3 - Neutral;5 - Very important;3 - Neutral;4 - Important;5 - Very important;"Liaise or co-ordination with World Health Organisation;  IBM/Microsoft/Pope guidelines and proposals;  progressive novel work currently underway in Californian organisations.";4 - Important;5 - Very important;3 - Neutral;4 - Important;5 - Very important;3 - Neutral;;5 - Very important;4 - Important;3 - Neutral;;5 - Very important;3 - Neutral;5 - Very important;4 - Important;4 - Important;;5 - Very important;5 - Very important;3 - Neutral;4 - Important;5 - Very important;2 - Not important;;There is a need for a new legislation;;Yes;;Yes;;Healthcare and allied data collection, live data repositories, - and high-grade encryption fully from end-to-end capture, data handling and analysis, access points from/to accredited organisations [taken offline and secured, where necessary].;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;3 - Neutral;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);;Rather not;"Voluntary unlikely to work or be secure; refer Cambridge Analytica et al. ";Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Risks related to the loss of connectivity;;Yes;;Yes;;Yes, for all AI applications;;;
F514541;03-03-2020 23:39;English;NGO (Non-governmental organisation);Jürgen;Geuter;;Otherwise Network;;Small (< 50 employees);Germany;The feedback can be published with your personal information;4 - Important;4 - Important;5 - Very important;4 - Important;3 - Neutral;2 - Not important;"Important would be to actually think about where ""AI"" systems should be used and where not. Right now the whitepaper never asks that question, there is only the fetishist belief that it has to happen. ";4 - Important;4 - Important;2 - Not important;3 - Neutral;5 - Very important;4 - Important;"Again: this is all about playing catch up. There is no money or support for actually finding out what the often referenced (but never defined) ""values"" mean and how that manifests in ""AI"". ";3 - Neutral;4 - Important;2 - Not important;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;"""AI"" is still a hype and most SMEs don't have the data to train machine learning systems. SMEs need - aside from what is proposed - people who can actually help them decide if ""AI"" systems are _not_right for them just as much as pro voices. There are many snakeoil salespeople in the ""AI"" space.";5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;;There is a need for a new legislation;;Other;"The idea that risk is as easily quantifiable as the whitepaper says is false. Looking at one system isn't sufficient, a lot of the time ""AI"" systems create emergent phenomena that aren't predictable. ";;;;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Not at all;Voluntary labelling is not sufficient. If labels are to be introduced they need to be required.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;;;No opinion;;Yes;;Yes, for all AI applications;;;mozilla.pdf
F514540;03-03-2020 14:09;Spanish;Academic/Research Institution;Marc;MASMIQUEL MENDIARA;;"m2ishere.com
Masmiquel & Mendiara (m2) es un estudio creativo de diseño, ingeniería y comunicación.";;Micro (< 10 employees);Spain;The feedback can be published with your personal information;1 - Not important at all;5 - Very important;5 - Very important;No opinion;No opinion;No opinion;Es crucial que el aspecto práctico sea fundamental, es decir el factor de aplicabilidad. En muchos caso infinidad de buenos proyectos no llegan a realizarse por falta de criterio o análisis de potenciales partners.;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;Ejemplos de premios, reconocimientos y pruebas de la innovación del proyecto.;4 - Important;3 - Neutral;3 - Neutral;Como siempre sin olvidar vías de acceso de talento individual, basado no tanto en las estructura públicas, de investigación o privadas de las que procede, si no en los proyectos per se.;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;Para que cualquier nivel de AI puede implementarse se precisa de una definición clara de lo que son estrategias aplicadas inteligentes. Cegarse en la estructura global puede ocultar conceptualizar bien la casuística del problema y de la solución que se persigue.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;Current legislation may have some gaps;;Yes;;Yes;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;;Much;;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;;Mental health risks;;No opinion;;No opinion;;Yes, for all AI applications;;;
F514539;03-03-2020 12:52;English;EU Citizen;Giorgio;SONNINO;;;;;Japan;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Apart from the problem related to data use legislation, how Europe will be supplied with super-computers which, as known, are currently owned by USA and China and they are accessible (almost free of charge) only by some English universities (e.g. the Imperial College, University College London etc.) ?;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;As we know, once the Quantum Computer will be created, almost all cryptographic technologies (such as the technologies based on Elliptic functions) will be violated within a few hours. Hence, the following spontaneous question arises: Apart from the development of Quantum Computers (topic that has been mentioned in the speeches of the workshop held on 29 January), concretely what are the actions that are, and will be, undertaken by the European Commission in the field of Quantum Cryptography?;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;4 - Important;No opinion;;Current legislation may have some gaps;;Yes;;No opinion;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Cyber risks;;No opinion;;No opinion;;Yes, for specific AI applications;;;
F514538;03-03-2020 10:04;Spanish;Public authority;Ramon;HERRERA DE LAS HERAS;Regional;Parlamento de Andalucía;Parlamento;Large (250 or more);Spain;The feedback can be published with your personal information;5 - Very important;4 - Important;3 - Neutral;4 - Important;4 - Important;5 - Very important;;4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;4 - Important;5 - Very important;4 - Important;;4 - Important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;4 - Important;;There is a need for a new legislation;;No;;;;;4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Personal security risks;;Yes;;Yes;;Yes, for all AI applications;;;
F514537;03-03-2020 08:52;English;Company/Business organisation;Maher;Ameri;;AL-ROOKAL CO. FOR ENGINEERING INSPECTION LTD.;N/A;Small (< 50 employees);Iraq;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;No opinion;5 - Very important;4 - Important;N/A;4 - Important;4 - Important;3 - Neutral;4 - Important;5 - Very important;5 - Very important;N/A;3 - Neutral;4 - Important;5 - Very important;N/A;3 - Neutral;4 - Important;5 - Very important;5 - Very important;4 - Important;N/A;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;N/A;No opinion;;Yes;;Yes;;N/A;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;N/A;Very much;N/A;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;N/A;Mental health risks;N/A;Yes;N/A;No opinion;N/A;Yes, for all AI applications;;N/A;
F514505;02-03-2020 18:49;English;Academic/Research Institution;Fernando;Galdon;;Royal College of Art;;Large (250 or more);Spain;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;We need to insert Accountability and Reparation as a posteriori strategies to build and maintain trust;There is a need for a new legislation;;Other;As the nature of the system is incremental in nature, We need to move from standards to calibration systems. For instance, a Virtual Assistant today provides the weather, but in the future it will be capable of diagnosing and providing treatment, therefore the low becomes high. we need calibration systems to deal with these elements. standards refer to fixed object that do not change over time (e.g. a chair). ;;;the insertion of biometric data. ;5 - Very important;4 - Important;3 - Neutral;4 - Important;4 - Important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Very much;"From an historical perspective trust has been structured in three main eras; the local was based on first hand knowledge, the institutional was based on institutions mediating via certifications, and the distributed was structured via peer-reviews. in a sense we need to go back to the institutional as the distributed has not been working. ";A combination of ex-ante compliance and ex-post enforcement mechanisms;;a combination of ex-ante via simulation tools, meanwhile via calibration tools and ex-post via reparation tools. see document attached for publications I have produced in these  areas. ;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;We should move to a continental framework rather than national. ;EU_Comision_report_-_Final.pdf
F514504;02-03-2020 11:31;English;Other;JOHN;HAYS;;Erasmus University Medical Centre Rotterdam (Erasmus MC);724240721429-43;Large (250 or more);Netherlands;The feedback can be published with your personal information;5 - Very important;4 - Important;4 - Important;4 - Important;3 - Neutral;5 - Very important;;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;;Current legislation may have some gaps;;Yes;;Yes;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;Legislation should establish a quorum of regional stakeholders (police, judges, members of the public, human rights advisors) who have the power to enact such legislation. Also, EU-wide legal guidelines are required to define what circumstances, and for how long, the Biometric Identification may be in place. Also punitive measures and safeguarding of the system from criminal, national and international disruptive elements (cyberwarfare) ar eessential.;Very much;Backed up with international legislation and punitive measures if found to be incorrect or deceiving.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;Risk to a persons identity (identity theft) - especially in an online international context where national rules may be irrelevant;Yes;;Yes;;Yes, for all AI applications;;;
F514503;01-03-2020 13:19;Italian;EU Citizen;Marco;Vicario;;;;;Italy;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;;4 - Important;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;3 - Neutral;;There is a need for a new legislation;;Yes;;Yes;;;3 - Neutral;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F514502;01-03-2020 12:48;English;Company/Business organisation;Norbert;JASTROCH;;MET Communications;;Micro (< 10 employees);Germany;The feedback can be published with your personal information;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;;4 - Important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;;Current legislation may have some gaps;;Other;Regulative means should attempt to cover also 'black swan' type of risk, i.e. unknowable dangers of AI applications.;;;All AI systems brought into operation for which responsibility (and legal liability) is not transparent.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);Biometric identification systems should be clearly restricted by legal ruling that is subject to democratic ruling procedures.;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;Steady monitoring by an independent institutionalized body of experts. ;Yes;Given that AI is assumed to evolve dynamically, there is need for ongoing adjustment.;Yes, for all AI applications;;Given that AI impact will also evolve dynamically, there is need for ongoing adjustment here, too.;
F514501;29-02-2020 15:04;English;Academic/Research Institution;Ozgur;GUNGOR;;Istanbul Okan University;;Large (250 or more);Turkey;The feedback can be published with your personal information;3 - Neutral;4 - Important;5 - Very important;4 - Important;4 - Important;2 - Not important;Education programmes shall include extensive trainings and workshops for students.;4 - Important;3 - Neutral;3 - Neutral;5 - Very important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;4 - Important;;5 - Very important;3 - Neutral;3 - Neutral;4 - Important;4 - Important;;3 - Neutral;2 - Not important;3 - Neutral;2 - Not important;3 - Neutral;3 - Neutral;;There is a need for a new legislation;;No;;;;;4 - Important;5 - Very important;3 - Neutral;4 - Important;5 - Very important;4 - Important;No further guidelines or regulations are needed;;Much;;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;;Personal security risks;;Yes;There will be other potential risk assesment requirements those we can not foresee for now. But, this issue is critical and shall be worked on contionously. ;Yes;;No opinion;;;
F514500;29-02-2020 00:22;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;4 - Important;3 - Neutral;5 - Very important;4 - Important;;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;;3 - Neutral;5 - Very important;4 - Important;;2 - Not important;5 - Very important;4 - Important;5 - Very important;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;;There is a need for a new legislation;;Yes;;Yes;;;4 - Important;4 - Important;5 - Very important;3 - Neutral;3 - Neutral;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;No;;Yes;;Yes, for specific AI applications;Personal security risks, health risks;;
F514499;28-02-2020 23:00;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;4 - Important;4 - Important;3 - Neutral;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;Yes;;Yes;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F514498;28-02-2020 16:45;English;Company/Business organisation;Cristobal;Ortega Artero;;Jump TV Solutions;;Small (< 50 employees);Spain;The feedback can be published with your personal information;4 - Important;5 - Very important;2 - Not important;No opinion;5 - Very important;3 - Neutral;;4 - Important;5 - Very important;4 - Important;No opinion;3 - Neutral;No opinion;"Talking as individual, I really feel a clear strategy on startup acceleration is critical at this stage. So any plan at European or National level that can help SME to accelerate, create and develop the market of AI is really important. Also to consider; the ethical part of IA and ML where the small companies don't have enough muscle to face some challenges. So in this acceleration a framework must be created to ensure the fulfillment of ethics considerations and GDPR";5 - Very important;5 - Very important;5 - Very important;;2 - Not important;4 - Important;2 - Not important;No opinion;No opinion;;2 - Not important;5 - Very important;5 - Very important;3 - Neutral;4 - Important;4 - Important;;Current legislation may have some gaps;;Yes;;Yes;;;5 - Very important;5 - Very important;5 - Very important;2 - Not important;4 - Important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;Personal Information and Data ;Yes;;Yes;;Yes, for specific AI applications;High Risk applications or those that could use personal non-anonymized data;;
F514497;28-02-2020 14:27;English;Academic/Research Institution;Horst;BISCHOF;;Graz University of Technology;;Large (250 or more);Austria;The feedback can be published with your personal information;5 - Very important;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;3 - Neutral;;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;;3 - Neutral;5 - Very important;4 - Important;;4 - Important;4 - Important;5 - Very important;5 - Very important;3 - Neutral;;2 - Not important;2 - Not important;3 - Neutral;3 - Neutral;2 - Not important;4 - Important;;Current legislation may have some gaps;;No;;;;;5 - Very important;5 - Very important;4 - Important;5 - Very important;3 - Neutral;3 - Neutral;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;No;;No;;No;;;
F514496;28-02-2020 13:52;German;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Reliability of AI Systems and Models;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;No;;;;Autonomous Systems (Automotive, Mobility, Industry 4.0, Avionics, ...);5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);;Much;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Personal security risks;;Yes;;Yes;;Yes, for specific AI applications;safety-/security critical applications;Fair Sharing / distribution of corresponding risks necessary incl. realistic statistic models;
F514495;28-02-2020 13:40;Swedish;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;4 - Important;5 - Very important;3 - Neutral;5 - Very important;4 - Important;;4 - Important;4 - Important;4 - Important;3 - Neutral;5 - Very important;4 - Important;;3 - Neutral;4 - Important;4 - Important;;4 - Important;3 - Neutral;3 - Neutral;5 - Very important;3 - Neutral;;4 - Important;5 - Very important;4 - Important;3 - Neutral;No opinion;5 - Very important;;There is a need for a new legislation;;No opinion;;;;;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;No opinion;;No opinion;;;Personal security risks;;No opinion;;Yes;;Yes, for specific AI applications;;;
F514494;28-02-2020 13:36;English;;;;;;;;;The feedback can be published in an anonymous way;3 - Neutral;5 - Very important;4 - Important;1 - Not important at all;4 - Important;5 - Very important;Education of pundits and media, so they stop babling silly stuff around AI;5 - Very important;1 - Not important at all;5 - Very important;4 - Important;4 - Important;1 - Not important at all;;1 - Not important at all;5 - Very important;3 - Neutral;The UE seems focused on Big Centers, they like opening new buildings. Not necessary, networking and FUNDING existing talent is necessary to allow they to gow organically;5 - Very important;1 - Not important at all;4 - Important;3 - Neutral;3 - Neutral;;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;This so-called concerns are in fact flase statments (as they are written here). We have funamental rights being crushed in Catalonia by the Spanish Empire and nobody case at the EC;Current legislation may have some gaps;;No;;;;Government actions in economy and repression are moste concerning, AI  being used by governments is higher risk than Google or Apple;5 - Very important;3 - Neutral;4 - Important;4 - Important;4 - Important;3 - Neutral;Biometric identification systems should never be allowed in publicly accessible spaces;;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Cyber risks;;Yes;;No;;Yes, for specific AI applications;HIgh risk applications, like government and police capture an use of data against human rights;;
F514493;27-02-2020 19:57;English;;;;International;;;;;The feedback can be published in an anonymous way;1 - Not important at all;4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;3 - Neutral;;1 - Not important at all;3 - Neutral;4 - Important;;4 - Important;4 - Important;5 - Very important;4 - Important;3 - Neutral;;5 - Very important;5 - Very important;4 - Important;4 - Important;3 - Neutral;1 - Not important at all;;There is a need for a new legislation;;No;;;;;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;No further guidelines or regulations are needed;;Much;;;;;Cyber risks;;No;;No opinion;;No;;;
F514492;27-02-2020 18:33;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;4 - Important;5 - Very important;;5 - Very important;5 - Very important;3 - Neutral;4 - Important;5 - Very important;3 - Neutral;;4 - Important;5 - Very important;3 - Neutral;;5 - Very important;4 - Important;5 - Very important;5 - Very important;4 - Important;;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;3 - Neutral;;There is a need for a new legislation;;No;;;;;5 - Very important;4 - Important;3 - Neutral;5 - Very important;5 - Very important;4 - Important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);Monitoring for security reasons;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Cyber risks;;Yes;;Yes;;Yes, for specific AI applications;;;
F514491;27-02-2020 16:15;English;EU Citizen;Benjamin;Le Guyader;;;;;France;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;3 - Neutral;;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;5 - Very important;3 - Neutral;;5 - Very important;4 - Important;2 - Not important;;4 - Important;4 - Important;3 - Neutral;5 - Very important;3 - Neutral;;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;"AI errors are mostly programming and deep learning mistakes.
Real AI is not soon to be released neither be left alone to decide, until several decades at least. ";Current legislation may have some gaps;;No;;;;"All Applications that are designed to pick one from a larger batch of samples are at risk.
Non-objectives criteria could be easily add to AI main programs.
Nobody will be able to control each system...";4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;Is the AI creating company trustworthy ? It should be enough.;Personal security risks;To give more access to personal datas is a major issue. ;Yes;"Financial algorithms are already a problem and not enough controlled.
AI might not help. ";No;"Each future problem should be studied in depths.
All algorithms should be open source accessible.";Yes, for all AI applications;;"The programming company should be held accountable for problems.
Healthcare misdiagnosis, road traffic accidents, industrial disasters...
AI must supervised (24/7) at all cost, the longer the better.";
F514490;27-02-2020 16:02;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;;Early involvement of EU social partners, especially of the sector representing knowledge-based occupation e.g. banking industry (EBF-Banking Committee for European Social Affairs and UNI Europa Finance);5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Joint capacity building of social partners (emplyoer and employee representatives) to ensure proper handling in the European Sectoral Dialogue Committees;4 - Important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;In order to ensure the role of the EU Social Model, involvement of topics relating to areas of social partner expertise is key ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;;Current legislation may have some gaps;;Yes;;No opinion;;;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Very much;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Risks related to the loss of connectivity;;Yes;;Yes;;Yes, for all AI applications;;Mandatory liability insurance of adequate coverage as prerequisite to use of AI, tailored to specific use and risks involved;
F514489;27-02-2020 15:41;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;3 - Neutral;efforts concerning personal rights;5 - Very important;5 - Very important;4 - Important;3 - Neutral;2 - Not important;3 - Neutral;;4 - Important;5 - Very important;3 - Neutral;;4 - Important;4 - Important;4 - Important;4 - Important;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;AI may distore People or may lead to 'false security';Current legislation may have some gaps;;No;;;;UAV Operations;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;Yes;;No opinion;;;
F514488;27-02-2020 13:05;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;;4 - Important;5 - Very important;3 - Neutral;3 - Neutral;5 - Very important;5 - Very important;;4 - Important;4 - Important;4 - Important;;3 - Neutral;2 - Not important;2 - Not important;4 - Important;3 - Neutral;;4 - Important;4 - Important;5 - Very important;4 - Important;4 - Important;5 - Very important;;There is a need for a new legislation;;No;;;;Transportation, health care, citizen services and screening of people;5 - Very important;;4 - Important;4 - Important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;Needs to be very clear and understandable to users;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F514487;27-02-2020 12:28;German;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;4 - Important;5 - Very important;3 - Neutral;3 - Neutral;5 - Very important;education of the entire population;5 - Very important;3 - Neutral;3 - Neutral;3 - Neutral;5 - Very important;4 - Important;;4 - Important;5 - Very important;4 - Important;;4 - Important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;;4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;;;There is a need for a new legislation;;No;;;;;3 - Neutral;3 - Neutral;4 - Important;5 - Very important;2 - Not important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;;Yes, for specific AI applications;;;
F514486;27-02-2020 08:29;Italian;EU Citizen;Alessandro;Cortese;;;;;Italy;The feedback can be published with your personal information;4 - Important;4 - Important;5 - Very important;4 - Important;5 - Very important;4 - Important;;4 - Important;5 - Very important;3 - Neutral;3 - Neutral;4 - Important;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;;4 - Important;4 - Important;4 - Important;5 - Very important;3 - Neutral;;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;;Current legislation may have some gaps;;Yes;;Yes;;;5 - Very important;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Rather not;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Mental health risks;;Yes;;No;;Yes, for all AI applications;;;
F514485;26-02-2020 23:00;French;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;3 - Neutral;4 - Important;5 - Very important;4 - Important;Créer plus d'uniformité (fiscalité, taxes, douanes) sur le Marché Européen, pour favoriser l’émergence et la croissance rapide de PME et ETI à l’échelle Européenne.;4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;_;4 - Important;5 - Very important;4 - Important;Réfléchir sur les applications concrètes et les usages émergents liés à l'usage de l'IA, réaliser une veille poussée sur les avancées technos des grands centres de R&D non européens.;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;Simplifier l'accès à l'information et aux financements, en particulier pour les startups, qui manquent de temps et de ressources;3 - Neutral;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;;Current legislation may have some gaps;;Yes;;Yes;;Utilisation en matière de sécurité / enquêtes criminelles / maintien de l'ordre / systèmes d'armes autonomes;5 - Very important;3 - Neutral;5 - Very important;4 - Important;4 - Important;4 - Important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;No;;No opinion;;No;;;
F514484;26-02-2020 19:21;English;EU Citizen;Juhani;Kauppi;;;;;Finland;The feedback can be published with your personal information;No opinion;4 - Important;3 - Neutral;No opinion;No opinion;3 - Neutral;;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;;No opinion;No opinion;No opinion;;No opinion;No opinion;No opinion;No opinion;No opinion;;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;;No opinion;;No opinion;;;;;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;;No opinion;;No opinion;;;;;No opinion;;No opinion;;No opinion;;;
F514483;26-02-2020 19:03;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;;4 - Important;4 - Important;5 - Very important;;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;Data Trust as the basis for Artificial Intelligence;4 - Important;4 - Important;5 - Very important;Advanced training in data science and AI;4 - Important;5 - Very important;4 - Important;4 - Important;5 - Very important;;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;;Current legislation may have some gaps;;Other;Introduce ethics, complaince and trust in processes involving AI;;;Smart cities and organizational decision-making processes with an impact on human lifestyle.;4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;Other special requirements in addition to those mentioned in the question above should be imposed (please specify);Ethics, responsibility and traceability;Very much;Control of data flows supported in tokenization;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Cyber risks;False or incorrect data;Yes;;Yes;;Yes, for all AI applications;;;Trusted_Data_s_Marketplace.pdf
F514482;26-02-2020 15:35;English;EU Citizen;Tamas;GYULAI;;;;;Hungary;The feedback can be published with your personal information;4 - Important;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;Focus on Digital Innovation Hubs;4 - Important;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;AI focused Digital Innovation Hubs as implementers of public actions;4 - Important;5 - Very important;4 - Important;AI research excellence centres shall cooperate with Digital Innovation Hubs;4 - Important;5 - Very important;4 - Important;5 - Very important;4 - Important;Digital Innovation Hubs shall coordinate cooperative AI projects between academia, large entreprises and SMEs.;4 - Important;3 - Neutral;4 - Important;3 - Neutral;3 - Neutral;5 - Very important;AI shall obtain trust by sticking to clear rules that can always be understood.;Current legislation may have some gaps;;Yes;;Yes;;self driving cars;5 - Very important;4 - Important;5 - Very important;4 - Important;4 - Important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Cyber risks;;No opinion;;Yes;;Yes, for all AI applications;;;
F514481;26-02-2020 14:52;English;Business Association;Narayanan;Vaidyanathan;;Association of Chartered Certified Accountants (ACCA);;Large (250 or more);United Kingdom;The feedback can be published with your personal information;4 - Important;4 - Important;5 - Very important;4 - Important;4 - Important;3 - Neutral;;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;;4 - Important;4 - Important;5 - Very important;;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;3 - Neutral;;Current legislation may have some gaps;;Yes;;Yes;;;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;No opinion;;Rather not;;Other enforcement system;;;;;Yes;;Yes;;Yes, for specific AI applications;;;
F514480;26-02-2020 12:46;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;4 - Important;4 - Important;3 - Neutral;5 - Very important;2 - Not important;;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;3 - Neutral;;5 - Very important;5 - Very important;4 - Important;;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;;5 - Very important;5 - Very important;4 - Important;5 - Very important;3 - Neutral;5 - Very important;;No opinion;;Yes;;No opinion;;;3 - Neutral;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;Biometric identification systems should never be allowed in publicly accessible spaces;Do not trust oversight systems;Much;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Personal security risks;;Yes;;No opinion;;No opinion;;;
F514479;26-02-2020 12:27;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;4 - Important;3 - Neutral;5 - Very important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Trust, privacy and human rights and freedoms are one of the prime areas the EU should focus on, using its size and influence to raise standards indirectly globally, or at very least across Europe (EU and non-EU). ;There is a need for a new legislation;;No;;;;Facial recognition and other aspects that collect individual data without individual knowledge / consent;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;Biometric identification systems, if deployed at all, should only be deployed in extreme / urgent circumstances, temporarily and under tight control and oversight.;Not at all;In such a high risk area, with so much to gain by abuse, I would not trust a voluntary system.;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;In this highly pervasive technology, I would not trust self-assessment mechanisms.  Too much at stake.  ;Risks related to the loss of connectivity;;Yes;;Yes;;Yes, for all AI applications;;;
F514478;26-02-2020 09:43;Spanish;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;3 - Neutral;4 - Important;No opinion;5 - Very important;;5 - Very important;4 - Important;5 - Very important;4 - Important;3 - Neutral;5 - Very important;;5 - Very important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;4 - Important;4 - Important;3 - Neutral;;4 - Important;5 - Very important;3 - Neutral;5 - Very important;3 - Neutral;3 - Neutral;;There is a need for a new legislation;;Yes;;Yes;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;;Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;;Risks related to the loss of connectivity;;Yes;;Yes;;Yes, for all AI applications;;;
F514477;26-02-2020 02:33;English;EU Citizen;Oscar;Clemente;;;;;United States;The feedback can be published with your personal information;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;4 - Important;5 - Very important;;2 - Not important;4 - Important;5 - Very important;;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;;2 - Not important;5 - Very important;3 - Neutral;3 - Neutral;;1 - Not important at all;;There is a need for a new legislation;;Yes;;Yes;;;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Very much;;Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;;Cyber risks;;Yes;;Yes;;Yes, for all AI applications;;;
F514476;25-02-2020 21:49;English;Non-EU Citizen;Artem;Chaykovskyy;;;;;Canada;The feedback can be published with your personal information;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;4 - Important;Benchmarks for success;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;4 - Important;Increase the levels of collaboration and transparency between the private sector and government organizations;5 - Very important;5 - Very important;5 - Very important;;4 - Important;4 - Important;3 - Neutral;5 - Very important;4 - Important;Ensure SMEs receive regulatory consulting services to ensure maximum end-to-end benefits;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Inherent bias and neutrality training for developers of AI;Current legislation may have some gaps;;Yes;;Yes;;;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);Only in instances of court-issued warrants, if explicit violations of the law have occurred, or if there is a genuine public safety issue.;Much;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Risks related to the loss of connectivity;;Yes;;Yes;;Yes, for specific AI applications;;;
F514475;25-02-2020 19:56;English;;;;;;;;;The feedback can be published in an anonymous way;2 - Not important;2 - Not important;2 - Not important;2 - Not important;2 - Not important;2 - Not important;"AI itself is quite undefined. We are currently seeing mostly ""hype"" and marketing to acquire personal data for individual or corporate enrichment. Because of that fact, regulation at the EU level that can act quickly is essential.";3 - Neutral;3 - Neutral;2 - Not important;4 - Important;4 - Important;3 - Neutral;;3 - Neutral;3 - Neutral;3 - Neutral;;2 - Not important;2 - Not important;2 - Not important;2 - Not important;2 - Not important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;No;;;;Facial/voice/genetic(such as fingerprints) recognition/processing and pooling of data from different sources (such as transportation, autonomous cars, and payment processing) that are effectively unavoidable in a modern society.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;National governments and/or corporations will extend the use of this data to allow for purposes not originally intended. Because this is a fact, biometric identification must be extremely restricted and limited to when it is absolutely essential because there is no other way no matter the cost.;Not at all;"Corporations will go to extreme lengths to sell products/services that are in fact high-risk, but with enough mental gymnastics and dishonesty, could be seen as ""low risk"".

Because established abuse of the spirit of the law, labeling must be mandatory no matter the ""risk level"".";Other enforcement system;"ONLY EU level enforcement authority that is not subject to resource starvation by national governments that do in fact not intend to enforce anything.

Enforcement must be aggressive, brutal and, for repeat offending ""convicted"" corporations, not possible to appeal for a significant period of time. ""Earn the trust"" must be a guiding principle in recidivist cases.";"""High risk"" products/services must be licensed (where applicable) under the General Public License version 3 (GPLv3) or later.";Risks related to the loss of connectivity;Privacy issues.;Yes;;Yes;;Yes, for all AI applications;;;
F514474;25-02-2020 17:35;English;Academic/Research Institution;Fernando;Galdon;;Royal College of Art, London.;;Large (250 or more);Spain;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Building an European version of the Alan Turing Institute ;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Building an European version of UK's Catapults initiative ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"In front of the impossibility of fully monitoring the systems in realtime due to its increasing complexity, You need to develop reparation strategies. (From Apology to Compensation: A Multi-level Taxonomy of Trust Reparation for Highly Automated Virtual Assistants
January 2020. DOI: 10.1007/978-3-030-25629-6_7) and accountability levels (Addressing Accountability in Highly Autonomous Virtual Assistants. DOI: 10.1007/978-3-030-25629-6_2)";There is a need for a new legislation;;Other;Yeas and no. If we look at the current state of the art of Virtual Assistants, they are totally harmless, however patents by big tech companies want to transform the Virtual Assistant into a doctor capable of diagnosing and providing treatments, therefore the nature of the system evolves over time. ;;;health and social. (the use of biometric data for other purposes than health and wellbeing);5 - Very important;4 - Important;2 - Not important;4 - Important;4 - Important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;Biometric data is an atomic bomb in the making. I said that in my Master thesis in cognitive computing at Goldsmiths back in 2012. People at the time thought it was nonsense, now they are calling me back to give lectures. ;Very much;but, I think it should be human certified. ;A combination of ex-ante compliance and ex-post enforcement mechanisms;;I am concluding a PhD on Trust design in the context of highly autonomous systems. You need a combination of ex-ante (simulation systems), meanwhile (calibration systems), and ex-post (reparation system) to fully address trust in these emerging highly autonomous systems. ;Mental health risks;harm by unintended consequences associated to the services enabled by these technologies.;Yes;"trust is structured in 4 levels; systems, organisations, persons and products. and each of these levels present a range of considerations. this structure if fundamental to envision any kind of evaluation. Then, you need to build a simulation tool to understand the impact of the potential interaction on trust (See a trust calculator in - Optimising user engagement in highly automated virtual assistants to improve energy management and consumption)";Yes;"In front of the impossibility of fully monitoring the systems in realtime due to its increasing complexity, You need to develop reparation strategies. (From Apology to Compensation: A Multi-level Taxonomy of Trust Reparation for Highly Automated Virtual Assistants
January 2020. DOI: 10.1007/978-3-030-25629-6_7) and accountability levels (Addressing Accountability in Highly Autonomous Virtual Assistants. DOI: 10.1007/978-3-030-25629-6_2)";No;;We need an European framework;EU_Comision_report.pdf
F514473;25-02-2020 14:49;Italian;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;4 - Important;5 - Very important;4 - Important;3 - Neutral;5 - Very important;Rethinking education (radically). Research since 1995 https://link.springer.com/article/10.1007/s40309-017-0126-4;5 - Very important;5 - Very important;4 - Important;3 - Neutral;5 - Very important;5 - Very important;;5 - Very important;4 - Important;4 - Important;;4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;;Current legislation is fully sufficient;;No;;;;;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Risks related to the loss of connectivity;;Yes;;Yes;;No;;;
F514472;25-02-2020 09:42;Slovenian;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;2 - Not important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;;3 - Neutral;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;;3 - Neutral;5 - Very important;4 - Important;AI should be connected to other DIHs and tech areas (not isolated), like IoT/IIoT, AR/VR, cyber securty and privacy, blockchain/DLT/holochain etc.;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;;4 - Important;5 - Very important;4 - Important;3 - Neutral;4 - Important;5 - Very important;Source of data used by the AI, as EU has not developed their own data system (like Google, Yandex, Baidu, ...);There is a need for a new legislation;;No;;;;;3 - Neutral;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;No opinion;;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F514471;25-02-2020 09:40;German;Company/Business organisation;Jan;DEMEER;;smartspace®lab.eu GmbH;smartspacelab.eu GmbH;Micro (< 10 employees);Germany;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;Partnerships with International Standardization Organizations;4 - Important;5 - Very important;4 - Important;3 - Neutral;4 - Important;5 - Very important;;3 - Neutral;4 - Important;4 - Important;;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;4 - Important;;There is a need for a new legislation;;Yes;;Yes;;decision-making based on insufficient data and computational resources;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;5 - Very important;4 - Important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;"due to inherit uncertainty of AI including biometric identification, it requires clear regulations and of course the legislation of institutions; ";No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;No opinion;;No opinion;;;
F514470;24-02-2020 23:15;Dutch;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;;5 - Very important;4 - Important;4 - Important;2 - Not important;3 - Neutral;1 - Not important at all;;3 - Neutral;4 - Important;4 - Important;;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;2 - Not important;;4 - Important;4 - Important;5 - Very important;4 - Important;4 - Important;3 - Neutral;;No opinion;;No opinion;;;;;4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;Biometric identification systems should never be allowed in publicly accessible spaces;;Much;;No opinion;;;Mental health risks;;No opinion;;No opinion;;No opinion;;;
F514469;24-02-2020 21:54;Spanish;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;4 - Important;Open Data in European public sector;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;Open Data and services;4 - Important;5 - Very important;4 - Important;To have a common European policy of research investment.;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;4 - Important;4 - Important;2 - Not important;3 - Neutral;;There is a need for a new legislation;;Yes;;Yes;;;5 - Very important;5 - Very important;4 - Important;4 - Important;;4 - Important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);Terorism risk, crowded events;Much;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Risks related to the loss of connectivity;;Yes;;Yes;;Yes, for all AI applications;;;
F514468;24-02-2020 20:23;Spanish;;;;;;;;;The feedback can be published in an anonymous way;3 - Neutral;3 - Neutral;3 - Neutral;4 - Important;;4 - Important;;4 - Important;3 - Neutral;4 - Important;5 - Very important;5 - Very important;;;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;4 - Important;3 - Neutral;3 - Neutral;4 - Important;4 - Important;4 - Important;;There is a need for a new legislation;;Yes;;Yes;;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Rather not;;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F514467;24-02-2020 17:58;Portuguese;Non-EU Citizen;Carlos Eduardo;Oliveira Amorim;;;;;Brazil;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;invest in data governance and data security;5 - Very important;5 - Very important;5 - Very important;Support the establishment of more than one lighthouse research centres that are world class and able to attract the best minds;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;No opinion;No opinion;No opinion;No opinion;;Current legislation may have some gaps;;Yes;;No opinion;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Cyber risks;;Yes;;Yes;;Yes, for all AI applications;;;
F514466;24-02-2020 17:46;English;EU Citizen;Alina Dora;Crisan;;;;;Romania;The feedback can be published with your personal information;5 - Very important;4 - Important;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;"Broad conducing lines should orient the European AI innovation efforts, without hampering private or public initiatives. European policy should encourage AI development and scale-up at global scale, all while respecting fundamental rights and values. AI benefits: 1/Improve current product and services across industries and public service and democratise public access to these solutions; 2/Identify new products and services at lesser costs; 3/foster collaboration and community building.";1 - Not important at all;1 - Not important at all;3 - Neutral;5 - Very important;4 - Important;;Ethic and strategic implementation reinforced by strategic financial investments, complemented with regulatory measures represent the pillars of an excellence-oriented research and AI development. Specific skills development is essential for developing a successful European AI ecosystem. ;3 - Neutral;4 - Important;4 - Important;A tested mechanism to sustain R&D is the Crédit d'impôt recherche (CIR), tax reduction mechanism put in place in France. The CIR is a flexible, reimbursement-based system proportional to the R&D investments. Processes and evaluation criteria insure the mechanism’s transparency. Th system does not require significant reorganizations (e.g. around excellence poles), and circumvolutes the obligation of building collaborative associations to secure funding that can be detrimental to innovation.    ;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;1 - Not important at all;A healthy concurrence should be the main driver in developing an European AI market to facilitate the creation of global reaching actors, capable to sustain a harsh business and product-based global concurrence.   ;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;Retro-engineering will always be available to reconstruct the rationale for a machine-based decision, but this may happen long after the decision's consequences will set in (e.g. the proliferation and virality of fake information and deepfake technologies). AI risks should be discussed and mitigated before the technological implementation, as societal consequences are difficult to undue. ;Current legislation may have some gaps;;No;;;;"Classical vertical rules definitions are irrelevant for AI applications due to the transverse character of the data funneled in the system. An innocuous ad-tech application can process sensible personal data (specific to healthcare for example) to offer personalised recommendations. A ""high-risk"" label can be derived only from the nature of the data used by the application, the usage given to it and its impact (including data leaking, miss-management and possible inference with other data). ";5 - Very important;2 - Not important;4 - Important;2 - Not important;3 - Neutral;4 - Important;Biometric identification systems should never be allowed in publicly accessible spaces;"Any AI regulation should take into consideration the quality and the proportionality of the data with respect to the system purpose (training, entry data, inference data), code documentation, and accepted system performance parameters. Robustness, accuracy, liability safety rules are derived from the above characteristics. All framework needs to take into account the data's transversal character, rendering application field classifications (e.g. health = high risk) useless.
Face recognition identification should not be implemented due to the important security risks posed by current systems and possible negative consequences at individual and community level.   ";Rather not;All framework needs to take into account the data's transversal character, rendering application field classifications (e.g. health -> high risk) us. A RGPD-like framework with proportional compliance obligations to the system's criticity level should be considered. Criteria to take into consideration: the quality and the proportionality of the data with respect to the system purpose (training, entry data, inference data), code documentation, and accepted performance parameters.;A combination of ex-ante compliance and ex-post enforcement mechanisms;;An RGPD-like framework with compliance obligations and ex-post enforcement proportional to the system's criticity level should be considered. All framework needs to take into account the data's transversal character, rendering application field classifications (e.g. health = high risk) useless. Criteria to consider: quality and proportionality of the data with respect to the system's purpose (training, entry data, inference data), code documentation and accepted performance parameters.;Mental health risks;All framework needs to take into account principles of safety and accessibility, as well as the need for product and service providers to specify the characteristics and conditions of use of their offer in an intelligible way for all consumers. AI related standards need to emerge in a reasonable time-frame. Consumers should be properly informed of possible dangers, but regulation should not become too constrictive to the point to hamper innovation. ;Yes;All safety legislative framework should continually be updated in order to respond to their core vocation and protect users in the context of new products and usages released on the market. An agile approach is best suited to both assess and regulate fast evolving industries like digital and AI that are still in its growing phase (and thus lacking standards).;Yes;All legislative framework should be continually updated to optimally protect users in the context of emergent industries' products reaching costumers. An agile approach is best suited to both assess and regulate digital and AI product and services due to their fast developing character. The most important dangers posed by nefarious AI usages need to be regulated, when possible, ex-ante technological deployment to minimise consumer exposition and mitigate producer liability. ;Yes, for all AI applications;;All legislative framework should be continually updated to respond to their core vocation and protect users in the context of new products and services released on the market. An agile, proportional approach is best suited to both assess and regulate industries like digital and AI due to their fast developing character, but should not result in excessive burden placed on the product/service developer and hamper innovation.;
F514465;24-02-2020 17:03;English;;;;;;;;;The feedback can be published in an anonymous way;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AI_-_Public_consultation.docx
F514464;24-02-2020 16:13;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Highlight the ground work EU agencies and EU bodies already created in the field through success stories. ;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;4 - Important;;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;4 - Important;5 - Very important;3 - Neutral;4 - Important;2 - Not important;The need for diversity in working with AI, and how to mitigate bias in the system.;There is a need for a new legislation;;No;;;;;5 - Very important;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;;Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;;Personal security risks;;Yes;;Yes;;Yes, for all AI applications;;;
F514463;24-02-2020 14:35;French;EU Citizen;Virgile;Dier;;;;;France;The feedback can be published with your personal information;5 - Very important;3 - Neutral;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;;5 - Very important;2 - Not important;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;;3 - Neutral;2 - Not important;5 - Very important;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;;2 - Not important;4 - Important;3 - Neutral;4 - Important;No opinion;2 - Not important;;Current legislation may have some gaps;;No;;;;;5 - Very important;4 - Important;3 - Neutral;5 - Very important;3 - Neutral;4 - Important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);;Very much;;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;;Personal security risks;;Yes;;Yes;;Yes, for all AI applications;;;
F514462;24-02-2020 14:32;Danish;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
F514461;24-02-2020 11:11;Spanish;;;;;;;;;The feedback can be published in an anonymous way;3 - Neutral;4 - Important;4 - Important;4 - Important;3 - Neutral;4 - Important;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;;4 - Important;4 - Important;4 - Important;;5 - Very important;4 - Important;5 - Very important;5 - Very important;4 - Important;;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;;No opinion;;Yes;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;No opinion;;No opinion;;;Mental health risks;;No opinion;;No opinion;;No opinion;;;
F514460;24-02-2020 11:10;French;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;;4 - Important;5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;;4 - Important;5 - Very important;4 - Important;;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;;There is a need for a new legislation;;Yes;;Yes;;"Use of personal data in the course of political campaigns
And the use of remote biometric identification systems in the private sector";5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);"It should only be used in the following cases:
National security
Defense
Investigation of criminal offenses
Public security
";Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;Yes;;Yes, for all AI applications;;;
F514459;24-02-2020 10:32;Dutch;;;;National;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;;5 - Very important;5 - Very important;4 - Important;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;Yes;;Yes;;;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F514458;24-02-2020 07:55;Italian;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;;4 - Important;4 - Important;4 - Important;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;;There is a need for a new legislation;;Yes;;Yes;;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;Other special requirements in addition to those mentioned in the question above should be imposed (please specify);;Much;;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;;Mental health risks;;Yes;;Yes;;Yes, for specific AI applications;;;
F514457;24-02-2020 07:49;Dutch;Business Association;Igor;WORTEL;;PHALANXES;;Micro (< 10 employees);Netherlands;The feedback can be published with your personal information;3 - Neutral;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;Adaption level - the ability of domains (individual, business, political, legal) to adapt to technological change. All technological changes in the past have shown that we are not very good as humans to adapt (example result here is climate change).;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;International structures to work on data - technology doesn't obey the geopolitical structure as we have it at the moment, combined with adaption levels we will face challenges;4 - Important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;Yes;;Yes;;;3 - Neutral;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;We can't prevent it from happening in the future but we need legislation before public application.;No opinion;Any labeling system is too complex and might limit the beneficial outcome of the application of AI;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;It should apply to the system dynamics and so process phasing;Yes;;Yes, for all AI applications;;;
F514456;23-02-2020 20:27;French;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;4 - Important;5 - Very important;4 - Important;2 - Not important;4 - Important;Promoting to the people as important skill;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;5 - Very important;4 - Important;people should be allowed to participate and be included in researches programs;5 - Very important;5 - Very important;2 - Not important;Private and business research should not be a priority;4 - Important;4 - Important;5 - Very important;4 - Important;5 - Very important;Social innovation process should be considered, not only financials targets;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;Yes;;Yes;;Citizen controls should be involved and reinforced;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;Democracy obligations;Very much;Actually, I think that each IA application should be considered with a high-risk level;Other enforcement system;Citizen controls and high legal and financials risks for private misuses;Regular revisions of the legislation, according with consultations;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F514455;23-02-2020 17:33;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;5 - Very important;3 - Neutral;4 - Important;4 - Important;;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;;5 - Very important;4 - Important;4 - Important;;5 - Very important;5 - Very important;4 - Important;3 - Neutral;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;;Current legislation may have some gaps;;Yes;;Yes;;;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Risks related to the loss of connectivity;;Yes;;Yes;;Yes, for specific AI applications;;;
F514454;23-02-2020 17:22;English;Non-EU Citizen;Sray;Agarwal;;;;;India;The feedback can be published with your personal information;4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;3 - Neutral;5 - Very important;4 - Important;;4 - Important;3 - Neutral;4 - Important;4 - Important;5 - Very important;;3 - Neutral;4 - Important;5 - Very important;5 - Very important;5 - Very important;;;Current legislation may have some gaps;;Yes;;Yes;;;5 - Very important;4 - Important;3 - Neutral;5 - Very important;5 - Very important;4 - Important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F514453;23-02-2020 17:19;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;4 - Important;4 - Important;5 - Very important;3 - Neutral;3 - Neutral;;5 - Very important;3 - Neutral;3 - Neutral;4 - Important;4 - Important;5 - Very important;;5 - Very important;4 - Important;3 - Neutral;;4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;;There is a need for a new legislation;;No;;;;;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Not at all;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;Yes;;No opinion;;;
F514452;23-02-2020 16:39;English;;;;;;;;;The feedback can be published in an anonymous way;3 - Neutral;5 - Very important;4 - Important;4 - Important;3 - Neutral;3 - Neutral;;4 - Important;4 - Important;3 - Neutral;2 - Not important;3 - Neutral;5 - Very important;;3 - Neutral;4 - Important;2 - Not important;;3 - Neutral;4 - Important;4 - Important;5 - Very important;3 - Neutral;;5 - Very important;4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;;Current legislation may have some gaps;;Yes;;Yes;;hiring;4 - Important;3 - Neutral;5 - Very important;3 - Neutral;4 - Important;4 - Important;Biometric identification systems should never be allowed in publicly accessible spaces;Mass biometric identification is an unacceptable violation of European citizens' right to privacy, no matter whether it's business or the state identifying;Rather not;I doubt that such a system would actually be meaningful;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Personal security risks;;No opinion;;Yes;responsibility for e.g. AI-driven car crashes should be clarified, while taking into account that neither AI nor humans can be infallible (so e.g. car AI manufacturers should be held responsible for abnormal crash rates or egregious situations, but not if their product still does a better job than a human);No opinion;;;
F514451;23-02-2020 16:13;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;4 - Important;4 - Important;3 - Neutral;No opinion;5 - Very important;;4 - Important;3 - Neutral;No opinion;3 - Neutral;4 - Important;3 - Neutral;;3 - Neutral;4 - Important;4 - Important;;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;;;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;3 - Neutral;;Current legislation may have some gaps;;Yes;;Other;Too simple. Done elsewhere and applicable analogously (critical infrastructures);;4 - Important;3 - Neutral;3 - Neutral;5 - Very important;5 - Very important;5 - Very important;No opinion;;Not at all;it is not a new proposal, self regulation has existed for a long time so this is frivolous;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;No opinion;;Yes, for all AI applications;;;
F514450;23-02-2020 16:08;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;4 - Important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;;5 - Very important;4 - Important;2 - Not important;4 - Important;3 - Neutral;5 - Very important;;1 - Not important at all;5 - Very important;4 - Important;;4 - Important;3 - Neutral;3 - Neutral;5 - Very important;4 - Important;;3 - Neutral;5 - Very important;4 - Important;5 - Very important;2 - Not important;2 - Not important;;There is a need for a new legislation;;Yes;;No opinion;;;5 - Very important;3 - Neutral;3 - Neutral;4 - Important;3 - Neutral;4 - Important;Biometric identification systems should never be allowed in publicly accessible spaces;The systematic use of remote bio ID is in complete opposition to the freedoms that the union enforces. The EU has done well until now to find solutions where security does not mean giving up freedoms. As an example take the solution for biometric passports where the fingerprint is stored on the RFID chip and nowhere else, brilliant!;Not at all;"The proposed labelling system seems to borrow from existing consumer related initiatives. However, the (justified) complexity of both the legal framework and the problem space itself seem to run counter to a simple labelling system. Instead of helping the decision making process it would allow stakeholders to ""slack"" on their due diligence when evaluating systems.";Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;;;;Yes;This is long overdue and has been relevant since OTA updates have become the norm.;No opinion;;No opinion;;;
F514449;23-02-2020 16:07;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;;4 - Important;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;3 - Neutral;;4 - Important;4 - Important;4 - Important;;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;;There is a need for a new legislation;;No;;;;;4 - Important;5 - Very important;5 - Very important;2 - Not important;4 - Important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Much;;Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F514448;23-02-2020 16:06;Italian;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;3 - Neutral;;4 - Important;4 - Important;3 - Neutral;4 - Important;3 - Neutral;5 - Very important;;4 - Important;4 - Important;3 - Neutral;;4 - Important;5 - Very important;3 - Neutral;4 - Important;3 - Neutral;;4 - Important;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;4 - Important;;There is a need for a new legislation;;Yes;;Yes;;;3 - Neutral;5 - Very important;4 - Important;5 - Very important;5 - Very important;4 - Important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;Yes;;No opinion;;;
F514447;23-02-2020 15:59;French;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;3 - Neutral;;5 - Very important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;No;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F514446;23-02-2020 15:55;French;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;4 - Important;4 - Important;4 - Important;2 - Not important;;5 - Very important;4 - Important;3 - Neutral;4 - Important;4 - Important;3 - Neutral;;5 - Very important;4 - Important;5 - Very important;;4 - Important;4 - Important;3 - Neutral;5 - Very important;3 - Neutral;;4 - Important;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;3 - Neutral;;There is a need for a new legislation;;No;;;;;4 - Important;3 - Neutral;4 - Important;4 - Important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Personal security risks;;Yes;;No opinion;;Yes, for all AI applications;;;
F514445;23-02-2020 15:55;German;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;4 - Important;5 - Very important;3 - Neutral;2 - Not important;3 - Neutral;;4 - Important;4 - Important;2 - Not important;3 - Neutral;5 - Very important;4 - Important;;4 - Important;4 - Important;2 - Not important;;3 - Neutral;4 - Important;4 - Important;3 - Neutral;3 - Neutral;;3 - Neutral;5 - Very important;4 - Important;3 - Neutral;4 - Important;5 - Very important;;Current legislation may have some gaps;;No;;;;Verwendung in Gesichtserkennung und in der Verarbeitung von Daten, im speziellen personenbezogenen Daten, auch wenn anonymisiert;5 - Very important;3 - Neutral;4 - Important;4 - Important;3 - Neutral;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;No opinion;;Yes, for all AI applications;;;
F514444;23-02-2020 15:42;English;EU Citizen;Julia;Pitterman;;;;;Italy;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;Establish and agree upon a general ethics / human-centered framework on AI to strengthen coordination;4 - Important;5 - Very important;3 - Neutral;;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;Other;Agreed that new compulsory requirements should be introduced and limited to high-risk applications yet new legislation should also be adopted providing general guidance on what could be deemed harmful for society and what shouldn't be undertaken (e.g. AI for detrimental purposes to society);;;AI application in healthcare diagnosis and judiciary recommendations as high-risk without human intervention / monitoring;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Other special requirements in addition to those mentioned in the question above should be imposed (please specify);Additional special requirements pertaining to data storage of biometric information and legal standpoint on lawful necessity ;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;Yes;;No opinion;;;
F514443;23-02-2020 15:28;Greek;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;No opinion;4 - Important;No opinion;1 - Not important at all;1 - Not important at all;;No opinion;3 - Neutral;2 - Not important;4 - Important;4 - Important;2 - Not important;;4 - Important;4 - Important;2 - Not important;;4 - Important;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;Current legislation may have some gaps;;Yes;;Yes;;;4 - Important;3 - Neutral;4 - Important;;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;No opinion;;Yes, for all AI applications;;;
F514442;23-02-2020 13:35;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;2 - Not important;Special consideration must be kept in regards towards persona privacy.;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;Data privacy and avoidance of manipulation/abuse of the citizens through it;5 - Very important;4 - Important;4 - Important;AI should not fall solely in the hands of corporations, nor the state, it should be shared.;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;4 - Important;AI capabilities in correlation are incredibly dangerous if not strictly monitored;There is a need for a new legislation;;Yes;;No;;Surveillance, Personality profiling, Manipulation through disinformation;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;"Biometric Surveillance state creates an environment where personal freedoms are violated in favor of little to no gain. Not to mention the risk for this information to be leaked, breached, stolen, or abused, is too high.

No one should have access to this kind of information.";No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;All processes should be confirmed and vetoed by more than one entity, and there needs to be at least one third-party entity in this process where there is an insurance of no conflict of interests.;Mental health risks;;Yes;The risks AI poses are barely understood today, new ways it could prove dangerous are yet to be discovered, and this should be accounted for.;Yes;;Yes, for all AI applications;;;
F514441;23-02-2020 12:45;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;4 - Important;5 - Very important;4 - Important;3 - Neutral;;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;4 - Important;5 - Very important;;4 - Important;5 - Very important;4 - Important;;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;;There is a need for a new legislation;;No;;;;;4 - Important;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Risks related to the loss of connectivity;;Yes;;Yes;;Yes, for all AI applications;;;
F514440;23-02-2020 12:37;French;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;;5 - Very important;5 - Very important;3 - Neutral;4 - Important;LE CONTROLE DES DONNEES GEREES PAR L I.A.;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;IL FAUT UN PROTOCOLE D ACCORDS BUDGETAIRES  POUR GARANTIR LA SECURITE;There is a need for a new legislation;;No;;;;LA SECURITE DES ETRES VIVANTS ET DES INSTALLATIONS ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;C EST LIBERTICIDE;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F514439;23-02-2020 08:14;French;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;3 - Neutral;;There is a need for a new legislation;;Yes;;Yes;;Defense, private liberty ;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F514438;23-02-2020 05:01;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;Exponential and Converging Technologies;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;;3 - Neutral;5 - Very important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;;3 - Neutral;3 - Neutral;3 - Neutral;3 - Neutral;2 - Not important;2 - Not important;;No opinion;;No opinion;;;;;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No further guidelines or regulations are needed;;Rather not;;Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;;Personal security risks;;No;;No;;No;;;
F514437;21-02-2020 16:24;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;4 - Important;No opinion;No opinion;4 - Important;4 - Important;;4 - Important;3 - Neutral;4 - Important;5 - Very important;No opinion;5 - Very important;;3 - Neutral;5 - Very important;4 - Important;;2 - Not important;3 - Neutral;5 - Very important;5 - Very important;5 - Very important;;1 - Not important at all;3 - Neutral;5 - Very important;5 - Very important;1 - Not important at all;5 - Very important;AI might jeopardise EU & Member States sovereignty (China/US technology dependency);There is a need for a new legislation;;Yes;;Yes;;Government and Police use of AI systems to predict probability to offend (crime & terrorism) or re-offend (parole);5 - Very important;4 - Important;4 - Important;5 - Very important;3 - Neutral;4 - Important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);Sensible places like airports and train stations, and with a documented and EU-validated safety purpose;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;self-assessment ex-ante allows minimum trustworthy requirements are met while allowing quick go-to-market, and ex-post external assessment ensures actual compliance, maybe random to avoid over-burden (much like random state tax audits);Risks related to the loss of connectivity;;Yes;;No opinion;;No opinion;;;
F514436;21-02-2020 16:15;English;;;;;;;;;The feedback can be published in an anonymous way;3 - Neutral;4 - Important;5 - Very important;2 - Not important;4 - Important;4 - Important;;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;3 - Neutral;;3 - Neutral;4 - Important;4 - Important;;4 - Important;4 - Important;3 - Neutral;3 - Neutral;5 - Very important;;2 - Not important;5 - Very important;5 - Very important;4 - Important;2 - Not important;2 - Not important;;There is a need for a new legislation;;Yes;;No opinion;;;4 - Important;5 - Very important;4 - Important;4 - Important;3 - Neutral;3 - Neutral;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Cyber risks;;No opinion;;No opinion;;No opinion;;;
F514435;21-02-2020 13:44;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;Tax incentives for those who use EU-labelled AI;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;">>Establish DATA GOVERNANCE Council of Statistical Institutes of EU and Member States.
Statistical Institutes have a long history of managing data in the interest of society, with established methodology for ""Statistical Disclosure Control"": en.wikipedia.org
Founder of French institute INSEE, René Carmille was first European killed for protecting citizen data, sent to Dachau concentration camp, after torture  by K Barbie: fr.wikipedia.org/wiki/René_Carmille  (FR version more ceomplete)
";2 - Not important;5 - Very important;5 - Very important;"LEVERAGE ON COMPLETE HORIZON EUROPE programme:
-any research project in any area either 
i) produces new AI methodology
or
ii) uses some AI data processing

Therefore the complete HORIZON EUROPE programme is Europe's surface of AI. How to  organise? FROM THE PROPOSAL submission stage: every proposal should DECLARE its ""AI use method"" or ""its new AI production strategy"". The CALLS FOR PROPOSALS should orchestrate this for the completeness of IMPACT of Horizon Europe in AI";3 - Neutral;4 - Important;4 - Important;5 - Very important;No opinion;"CREATE or reinforce Industrial PhD schemes (e.g. CIFRE in France) with special attention at programme level to completeness of coverage of all sectors: no sector left behind. In particular agriculture should receive its share.
Such PhD projects put the objective on a concrete economic/societal problem, and address it by developing new AI, beyond the state of the art. This is AI in action, but scientifically explainable and structured AI. Brute force DNN or similar should not be eligible. ";4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;"What is at stake is not AI as such, but the modalities and context of its use.
The only reasonable approach is end to end, systems and services using AI: real world is observed, data analytics/AI decision taken, resulting in action on real world. The digital/AI element should not be taken in isolation.Idem there is no ""AI ethics"", there is sectorial ethics to which AI must conform (medical, media, etc)";Current legislation is fully sufficient;;Other;"The governance and ethics of every specific economic and societal sector should be reviewed thoroughly, to understand the outcome of AI being used, and existing guidelines available in those sector should be expanded for systems and services where an AI module is inserted.
A scenario review would help ";;;"Risk hierarchy for humans should be discussed. As in railways accidents (longest history of 200 years) how many lives at risk (1, 10, 10 , 1000, etc)? Granulatity of risk...
The risk hierarchy could be LTE
i)  L=life and physical/personal harm, [irreversible]
ii) T=theft and economic damage, [reversible]
iii) E= ecosystem reputation e.g. defamation, offensive content etc, [somewhat reversible]
DATA: ""location data"" drives i), whereas ""name and identification"" drive iii)
   ";4 - Important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;"The starting point is a democratic Europe. However history has shown that the transition from democracy to dictatorship can play out fast. Therefore data access should never be assumed to be in safe-hands in absolute terms with no controls and safeguards, even if the trusted third party (say government) is trusted today, it may not be tomorrow.
Hence structural controls for citizens to be able to reverse the access they grant to trusted authorities or trusted third parties should be available.
A yellow star, even an invisible one, should never again be pinned on people in Europe.";Much;">> BENEFICIARY vs COLLATERAL VICTIM of system using AI
Let us bear in mind the automatic lawn mower which uses cameras. The user and beneficiary of the automatic lawn mower may enjoy a great comfort, but what about the neighbours in the garden next, being watched by their neighbour's lawn-mower?
Complete use scenarios and context should be reviewed prior to any labelling... 
Cybersecurity measures protecting and restricting the access to the lawn-mower's camera may solve the issue.";Other enforcement system;Real time: no before, not after, but during... See road traffic, air traffic, etc...;">> Autonomous driving
UNECE WP29 specifies the rules and principles and the EU and Member States implement them.
That's ex-ante and ex-post, and real-time enforcement as traffic happens!

The time of transition between no autonomy and mostly autonomy-enabled vehicles on the road is critical. Mixed traffic is more difficult to manage than homogeneous traffic: today light motor vehicles, lorries, and cycles, can serve as an example.
Coexistence and non-interference is essential...";Risks related to the loss of connectivity;"Hierarchy of risk should be human centric. See above LTE hierarchy model: L=life at physical harm risk; T=theft or any other economic loss risk; E=ecosystem reputation risk, including social networks exposure, harmful content, etc...
A child risks more from its position being disclosed than from its name being disclosed, food for thought on GDPR... Data on people are not just personal data...";No;Consider the use context and all surrounding factors (scenarios);No;;No;;The Law (current law) can assess intentions, actions, consequences as for non-AI...;
F514434;21-02-2020 13:19;English;EU Citizen;Antonio;SOARES;;;;;Portugal;The feedback can be published with your personal information;4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;Supporting small entrepreneurs that are/have developed AI solutions but cannot launch them due to lack or resources (time and funds). This simple action would enable a large scale creation of AI value propositions at European level.;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Increase financing to small entrepreneurs (e.g. freelancers or tinkerers) that are / have developed AI applications.;1 - Not important at all;4 - Important;3 - Neutral;Allow small entrepreneurs to be part of the research and innovation community so that they can use the latest research results in their work. Let them be the linch pins between sophisticated research and SMEs. The lighthouse idea sounds like AI is only for the few enlightened minds.;5 - Very important;5 - Very important;5 - Very important;4 - Important;1 - Not important at all;Equity financing is not the proper way to finance the adption and wide spread of AI among SMEs. Financing should be available to as many AI entrepreneurs and developers as possible. Think US entrepreneurs, but without volture venture capitalists or hungry business angels.;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;2 - Not important;AI agents can control key decision-making without human intervention. The hill-made decision-making could be due to biased/incomplete data, erroneous learning algorithms or simple software bugs.;Current legislation may have some gaps;;No;;;;"Medical diagnostics; Court decisions; Employment Decisions; Any AI agent that acts directly on the real world with severe consequences to humans around it.";5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;Not at all;A certification program that clearly certifies an AI agent as non high-risk.;Other enforcement system;By testing deployed AI agents periodically. ;AI creators/deployers must create secured gateways accessible in the internet that can be used by the relevant authorities to test compliance with initial purpose, accuracy, data provacy, non-biased behaviour, etc.;;Biased decisions. Accuracy limits. ;Yes;Testing not also for the knowns but also unknowns.;Yes;It should be clear what decisions and level of control the user is giving up when using an AI agent.;Yes, for all AI applications;;"If an AI agent misbehaves and physical damages are incurred, accountability needs to be clearly assigned through a clear role system defining which entities are responsible for what aspect of the AI agent's design and behaviour. This should allow a quasi-direct allocation of responsibilities after the root cause is identified, followed up by a proportional compensation depending on the case at hand. 
In any case accountabilities and responsibilities should not be left unassigned.";
F514433;21-02-2020 11:54;English;EU Citizen;Ignacio;MARTINEZ SORIANO;;;;;Spain;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Securing Access to Data with FAIR principles (Findable, Accessible, Interoperable and Reusable);5 - Very important;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;;3 - Neutral;4 - Important;5 - Very important;4 - Important;5 - Very important;;3 - Neutral;3 - Neutral;2 - Not important;3 - Neutral;3 - Neutral;4 - Important;;There is a need for a new legislation;;Yes;;Yes;;;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;No;;Yes, for specific AI applications;health applications;;
F514432;21-02-2020 10:58;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;4 - Important;2 - Not important;4 - Important;4 - Important;;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;4 - Important;4 - Important;;3 - Neutral;5 - Very important;2 - Not important;;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;Current legislation may have some gaps;;Yes;;Yes;;;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);;Rather not;;Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;;Personal security risks;;Yes;;Yes;;Yes, for specific AI applications;;;
F514431;21-02-2020 10:15;English;;;;;;;;;The feedback can be published in an anonymous way;2 - Not important;3 - Neutral;5 - Very important;4 - Important;5 - Very important;5 - Very important;;5 - Very important;2 - Not important;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;;1 - Not important at all;5 - Very important;3 - Neutral;;3 - Neutral;4 - Important;5 - Very important;5 - Very important;2 - Not important;;2 - Not important;5 - Very important;4 - Important;1 - Not important at all;3 - Neutral;1 - Not important at all;;Current legislation may have some gaps;;Yes;;Yes;;;5 - Very important;5 - Very important;4 - Important;2 - Not important;2 - Not important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;Yes;;Yes, for all AI applications;;;
F514430;21-02-2020 10:03;English;EU Citizen;Attila;Makk;;;;;Hungary;The feedback can be published with your personal information;3 - Neutral;4 - Important;2 - Not important;5 - Very important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;3 - Neutral;5 - Very important;3 - Neutral;4 - Important;;4 - Important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;4 - Important;2 - Not important;2 - Not important;3 - Neutral;5 - Very important;5 - Very important;;Current legislation may have some gaps;;No;;;;At first should be clarified the legal status of AI. Is AI a lifeless object or - possibly - a subject of law? Or even we need to create a new legal entity?;3 - Neutral;4 - Important;4 - Important;4 - Important;3 - Neutral;5 - Very important;No further guidelines or regulations are needed;;Much;;No opinion;;My answer: when one tries to influence the decision of an AI system - that is not an AI any more. An AI - a real AI - works alone and makes decisions alone. We use AI in the cases when we do not know the answer, the solution is unknown. ;Mental health risks;The influenced AI. (When the AI works with falsified inputs, boundary conditions);No opinion;;No;;Yes, for all AI applications;;;
F514429;21-02-2020 09:05;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;4 - Important;4 - Important;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;3 - Neutral;3 - Neutral;4 - Important;4 - Important;3 - Neutral;4 - Important;;Current legislation may have some gaps;;No;;;;The concern is that your *high-risk* does not apply to advertising technology or consumer privacy, can have big effects and which aren’t being addressed even under GDPR. And these should be part of the *high-risk* concerns too. For example take simply the national elections that already have been misguided by AI in USA. ;3 - Neutral;4 - Important;5 - Very important;4 - Important;3 - Neutral;4 - Important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);The AI should be monitoring public spaces to prevent for any threat as terrorism and etc. Here the value for the society outranks individual privacy/protection regulation. ;No opinion;;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;;Cyber risks;;No opinion;;Yes;;Yes, for all AI applications;;;
F514428;21-02-2020 07:44;English;;;;;;;;;The feedback can be published in an anonymous way;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion.;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion.;No opinion;No opinion;No opinion;No opinion.;No opinion;No opinion;No opinion;No opinion;No opinion;No opinion.;4 - Important;No opinion;No opinion;4 - Important;4 - Important;4 - Important;No opinion.;Current legislation is fully sufficient;;Yes;;Yes;;No opinion.;4 - Important;4 - Important;No opinion;4 - Important;4 - Important;5 - Very important;No further guidelines or regulations are needed;Existing guidelines are sufficient.;Rather not;No;A combination of ex-ante compliance and ex-post enforcement mechanisms;;No;Risks related to the loss of connectivity;No;No;Safety directives like the 2006/42/EC already contain a technology-neutral obligation to risk assess throughout the entire life-cycle of a product.;No;No;No;;No;
F514427;21-02-2020 07:18;English;Company/Business organisation;Lawrence;Ampofo;;Digital Mindfulness;digital mindfulness;Micro (< 10 employees);United Kingdom;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;The EU's commitment and indication to collaborate with China and the US from a regulatory perspective to ensure innovation in AI is not dampened by regulation. ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Increase support for international collaboration with other international institutions for European SMEs advancing in AI innovation ;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;Yes;;Yes;;The use of AI in medical applications, financial services, judiciary process and transportation.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);The use of biometric identification should be used in places where there is credible risk to life, for example when a security threat has been identified. In addition, it should be used, when consent is given, to protect the public (such as children) from danger that may not be apparent to caregivers (children in public places being at risk from predators).;Very much;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F514426;20-02-2020 18:55;Romanian;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;5 - Very important;2 - Not important;5 - Very important;Making people understand that AI helps citizens and does not exclude the human factor in all actions (it does not imply fireing people from their jobs and it makes their job easier). Help to raise SME’s awareness about potential benefits of AI.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;3 - Neutral;4 - Important;2 - Not important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;No;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;No opinion;;No opinion;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;Yes;;Yes, for all AI applications;;;
F514425;20-02-2020 17:11;English;Company/Business organisation;Maud;Chidiac;;Amexci;Amexci;Large (250 or more);Sweden;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;Make bridges between organisations (intra nation industry to government, inter nation industry to industry and intra nation industry to goverments). Work as consortium, binding together industrial players with politics (ex Amexci in Sweden is a R&D joint venture for Additive Manufacturing using AI, deeply routed in the Swedish economic and political strategy plan and led by the wallenberg foundation). Europe needs more of those consortium set up ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;To hire unexperienced people (no need for a phd in data science) and give them the tools, right ressources/training and budget for experiment and train them by doing. If we wait to have only excellence in terms of looking for adequate university background, we won't speed up the adoption of such a technology. ;3 - Neutral;5 - Very important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Help SMEs to break down AI to tangible use cases according to their reality.;4 - Important;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;;There is a need for a new legislation;;Yes;;;;For example a wrong AI based diagostic for cancer, or a wrong decision making process (ex: autonomous cars that make the wrong interpretation, armed UAV that attack the wrong target etc). I think AI should only smoothen the decision making process, not replace it.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;No further guidelines or regulations are needed;;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F514424;20-02-2020 14:57;English;EU Citizen;Alexia;Femia;;;;;Italy;The feedback can be published with your personal information;;4 - Important;4 - Important;4 - Important;4 - Important;3 - Neutral;Protecting the privacy of EU citizens and limiting the use of AI to specific fields to prevent its misuse. ;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;4 - Important;No opinion;5 - Very important;;There is a need for a new legislation;;No;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Very much;;Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F514423;20-02-2020 14:08;Spanish;;;;;;;;;The feedback can be published in an anonymous way;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
F514422;20-02-2020 12:59;German;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;;4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;;4 - Important;5 - Very important;4 - Important;;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;;2 - Not important;4 - Important;4 - Important;2 - Not important;1 - Not important at all;2 - Not important;;Current legislation may have some gaps;;Yes;;Yes;;;4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);Security relevant areas like airport, railway station etc.;Much;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Personal security risks;;Yes;;Yes;;Yes, for specific AI applications;Life and death relevant AI applications ;;
F518585;20-02-2020 12:40;English;Company/Business organisation;Aitor;Calero;;Esri Spain Geospatial Solutions;;Medium (< 250 employees);Spain;The feedback can be published with your personal information;4 - Important;4 - Important;3 - Neutral;5 - Very important;5 - Very important;4 - Important;;4 - Important;5 - Very important;5 - Very important;4 - Important;2 - Not important;5 - Very important;;2 - Not important;5 - Very important;4 - Important;;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;;4 - Important;No opinion;No opinion;5 - Very important;No opinion;3 - Neutral;;Current legislation may have some gaps;;Yes;;No opinion;;;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Very much;;No opinion;;;Cyber risks;;Yes;;No opinion;;No opinion;;;
F518584;20-02-2020 12:03;English;Business Association;Ivan;Vasilev;;BESCO - The Bulgarian Startup Association;;Large (250 or more);Bulgaria;The feedback can be published with your personal information;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;4 - Important;More centralised approach for creating and implementing AI strategy. As a startup organisation in Bulgaria, we are not familiar with any actions taken by the government in the field of AI. Just the opposite. Strong centralised leadership on the EU level is the key to speed up the AI development and to try to be a world leader in this technology. ;4 - Important;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;;4 - Important;3 - Neutral;4 - Important;;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;4 - Important;1 - Not important at all;2 - Not important;4 - Important;3 - Neutral;4 - Important;;Current legislation may have some gaps;;Yes;;No opinion;;;5 - Very important;4 - Important;4 - Important;4 - Important;3 - Neutral;5 - Very important;No opinion;;No opinion;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Risks related to the loss of connectivity;;No opinion;;No opinion;;Yes, for all AI applications;;;
F518583;20-02-2020 11:17;Italian;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;4 - Important;5 - Very important;3 - Neutral;4 - Important;5 - Very important;;5 - Very important;4 - Important;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;;4 - Important;5 - Very important;5 - Very important;;4 - Important;4 - Important;5 - Very important;4 - Important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;Yes;;Yes;;;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;Yes;;Yes, for all AI applications;;;
F518582;20-02-2020 10:46;English;Academic/Research Institution;Mircea;GIURGIU;;Technical University of Cluj-Napoca;;Large (250 or more);Romania;The feedback can be published with your personal information;4 - Important;5 - Very important;4 - Important;4 - Important;No opinion;No opinion;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;;4 - Important;5 - Very important;4 - Important;;4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;;3 - Neutral;5 - Very important;4 - Important;5 - Very important;3 - Neutral;3 - Neutral;;Current legislation may have some gaps;;Yes;;Yes;;user identity (face recognition, speaker recognition) and the emotional states;5 - Very important;4 - Important;4 - Important;5 - Very important;4 - Important;4 - Important;Biometric identification systems should never be allowed in publicly accessible spaces;;Very much;This labelling system should be accompanied with specific labelling instructions and usage recommendations.;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;;Personal security risks;;Yes;;Yes;;No;;;
F518581;20-02-2020 08:21;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Industrial Focus and Prioritization;4 - Important;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;;5 - Very important;5 - Very important;4 - Important;;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;;There is a need for a new legislation;;No;;;;Unfortunately there should be a learn and adapt approach. The limitation may cause some blind spots to avoid or predict related outcomes.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Very much;The crowdsourcing approach will be faster and cheaper than any other model. We should clarify a necessary community and qualifications to leverage this power. On the other hand, we should be selective and careful to avoid some negative impacts.;Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;;Risks related to the loss of connectivity;;Yes;;Yes;;Yes, for all AI applications;;;
F518580;20-02-2020 08:08;Spanish;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;;4 - Important;3 - Neutral;5 - Very important;4 - Important;4 - Important;4 - Important;;3 - Neutral;4 - Important;;;4 - Important;3 - Neutral;3 - Neutral;3 - Neutral;4 - Important;;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;;There is a need for a new legislation;;Yes;;Yes;;;4 - Important;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Very much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;;;Yes, for specific AI applications;;;
F518579;20-02-2020 02:56;English;NGO (Non-governmental organisation);Peder;Iblher;;Giordano-Bruno-Stiftung;;Large (250 or more);Germany;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;AI offers opportunities, but is a powerful tool, comparable to owning chemical weapons. Society has a right to regulate it's use. Make privacy Europe’s strong point in AI. Including true anonymization, self sovereign identities, privacy by design, rights of the individual. Only then we are trustworthy to deal with big data for the common good. Profiling individuals consented by them, violations prosecuted severely. Ban mass surveillance and data retention, exceptions only by rare judicial order.;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;The brain drain towards the US and possibly China is massive and one of the hardest challenges. It's not all about money but also about interesting working fields and opportunities, lighthouse institutions, an open minded atmosphere, interdisciplinarity, places with living, and – last not least – the idea of working for a good cause. All this should have a place in future Europe.;5 - Very important;5 - Very important;4 - Important;Some of the main institutions must be devoted to the common good only. A market player will always follow it's own interest and in the case of AI this may well lead to megalomania. Think of any CEO owning an atom bomb in his/her private portfolio.;3 - Neutral;4 - Important;4 - Important;4 - Important;4 - Important;Targets must be mixed, near to the market but also visionary and fundamental research. SME often don't have the phantasy of what is possible unless they see it. Also they often don't see the point in basic concepts, such as explainability, anti-bias, focus on privacy etc. Still they can contribute important practical learnings, of course.;5 - Very important;5 - Very important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;"- People may not be educated and prepared well enough to estimate the (not)accuracy or usefulness of AI outcomes.
- AI may concentrate too much knowledge and thus power in the hands of few.
- Ethics in AI may only be the ethics of the prevailing groups. I";;;No;;;;Health, financial, mobility, telecom and metadata are crucial to be protected. Profiling individuals, even by using public data, face recognition etc., must be strictly limited, violations must be prosecuted severely.;4 - Important;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;;It is crucial for our personal freedom and integrity to not be tracked in normal life. Future systems will be so powerful that you may call them totalitarian by design. Europe has had many experiences with totalitarianism, so we should resist this temptation, however more convenient, secure or money saving it may appear to us.;Much;The process of labelling should contain a system of participation of civil society, parties concerned, possibly unions etc. Why not make it mandatory at a certain level of risk?;;;A constant re-check (monitoring, assessment) of the outcomes is crucial. Biases may become visible only later on und ethics change over time.;Mental health risks;Risk of leaks and abuse of power connected to the accumulated knowledge.;Yes;It must be supervised by a neutral body.;Yes;Natural persons must be responsible and held liable for automated decisions by their machines.;Yes, for all AI applications;;National rules should be harmonised towards a European standard over time.;howshoulditwork_s.pdf
F518578;19-02-2020 20:53;German;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;3 - Neutral;4 - Important;No opinion;4 - Important;5 - Very important;regulate use of AI in public sector;4 - Important;5 - Very important;2 - Not important;2 - Not important;4 - Important;4 - Important;increase safety measures relating data usage by AI;4 - Important;4 - Important;5 - Very important;;4 - Important;5 - Very important;2 - Not important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;"AI may become ""smarter"" faster than our regulation can hold up with it";There is a need for a new legislation;;No;;;;the danger that we will soon relay on AI to much and stop reflecting things ourselves;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;the risk of loosing control over a system is to high and we should not be allowed to take this risk moo matter the benefits;Rather not;;Other enforcement system;we should do everything we can to insure AI security and never stop remembering its dangers;;Mental health risks;we should put great value on making everyone aware of the potential risks;Yes;;Yes;;Yes, for all AI applications;;;
F518577;19-02-2020 19:54;Spanish;Company/Business organisation;Carlos;RODRIGUEZ;;ATEM NUEVAS TECNOLOGÍAS S.L.;;Micro (< 10 employees);Spain;The feedback can be published with your personal information;4 - Important;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;4 - Important;5 - Very important;5 - Very important;3 - Neutral;4 - Important;;4 - Important;3 - Neutral;5 - Very important;;4 - Important;4 - Important;4 - Important;5 - Very important;5 - Very important;;1 - Not important at all;No opinion;1 - Not important at all;4 - Important;1 - Not important at all;2 - Not important;;Current legislation is fully sufficient;;No;;;;;5 - Very important;4 - Important;3 - Neutral;5 - Very important;1 - Not important at all;4 - Important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);;No opinion;;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;;Risks related to the loss of connectivity;;No;;No;;No;;;
F518576;19-02-2020 18:18;French;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;5 - Very important;4 - Important;3 - Neutral;3 - Neutral;AI often needs data, and lack of recognized secure data anonymization (like https is widely recognized for secure communication) is hampering willingness of people to allow processing of their data hence making AI application more difficult;5 - Very important;4 - Important;3 - Neutral;4 - Important;5 - Very important;5 - Very important;Any value beyond 5 for SAFE (and recognized as such by EU population) European Data Space ? like 10 for ex ?;5 - Very important;4 - Important;3 - Neutral;;4 - Important;;4 - Important;;4 - Important;;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;3 - Neutral;;Other;Legislation should be translated in technology through an easy to understand and stable conceptual layer. Again taking the ex. of https, legislation can make it mandatory to be at least 64bits one year, then 256 bits or 1024 bits later, people just remember https is safe. Then other regulation can be built on that safe abstraction layer, being shielded from everchanging techno adjustment (e.g. bank connexion must be https, whatever https evolution will mean, that rule will remain stable);Other;Regulatory classifications of techno in the above abstract layer that people can understand is first needed (provable AI or not, safe anonymization of my data or not, advisory or decisional AI process...), to allow people to select/rely on this or that without having to be a geek. Then some key area (personal data or autonomous vehicles) might need more regulation but classification to allow people to excert decision should be enough in many cases;;;Targeting individuals based on machine learning;3 - Neutral;3 - Neutral;5 - Very important;5 - Very important;4 - Important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;That accredited policemen have such a mobile system that they can deploy under specific conditions (terrorist attack, huge mass event with risks...) could be OK if advertised, but in no way we should have such systems deployed in a permanent manner making continuous biometric identification  tracking of people;Very much;But convincing people to voluntarily contribute with data or being labeled heavily depend on their ability to understand clearly what is at stake hence again need for a non technocratic nor lawer jargon & guaranteed abstract level of concepts people can use to undersatnd what they volonteer to and what would be the consequences (secure anonymization, temporary data used for study then deleted...) ;Other enforcement system;Dissuasive penalty in case of deliberate fraud with compliance criteria (not like for diesel gate where the benefits made through fraud exceed by far the fine... making it a clear calculated risk to fraud again !);;Risks related to the loss of connectivity;;Yes;Large part of it is directly linked to detailed technical aspects that are obsolete within a few months, leading to very clumsy application or in-applicability in many cases. Again, law should apply on concepts that encapsulate technologies, so that we can follow the techno revolution bu just updating definition of these concepts based on techno evolution without having to modify the high level laws themselves;Yes;"No dissuasive vs values at stake and complexity of business international legal setup, still very focused on companies so easy to have ""fuse"" companies to protect real beneficiaries in case of fraud discovery
Hence the need to have law supported by strong and validated technology... and let people have the opportunity to select transparent open source implementation of it ";No;;National laws even less dissuasive and easier to escape than EU ones for international operators;
F518575;19-02-2020 17:51;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;data to do good - data and tracing can do very well for both people and planet. show the advantage to the citizens;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;we need a European model in crucial domains like for instance health and security;5 - Very important;5 - Very important;5 - Very important;research (in all areas) should be obliged to communicate much more about what they do, why and what is achieved. We need more public support for research and science.;4 - Important;4 - Important;4 - Important;4 - Important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;AI is only as good a the underlining systems behind it. As long as the underlining system can be deconstructed it can be reviewed and we will be able to build a just system also on the premise of AI. ;No opinion;;No opinion;;;;security ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;No opinion;;No opinion;;No opinion;;;Mental health risks;;Yes;;Yes;;Yes, for all AI applications;;;
F518574;19-02-2020 17:41;English;EU Citizen;Martin;Dieussaert;;;;;Belgium;The feedback can be published with your personal information;3 - Neutral;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;4 - Important;5 - Very important;3 - Neutral;;4 - Important;4 - Important;5 - Very important;;4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;;4 - Important;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;2 - Not important;;There is a need for a new legislation;;No;;;;;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;3 - Neutral;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);;Much;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Personal security risks;;Yes;;Yes;;Yes, for all AI applications;;;
F518573;19-02-2020 16:57;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;3 - Neutral;2 - Not important;;5 - Very important;4 - Important;4 - Important;4 - Important;4 - Important;5 - Very important;;5 - Very important;5 - Very important;2 - Not important;;4 - Important;5 - Very important;3 - Neutral;4 - Important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;4 - Important;5 - Very important;5 - Very important;;There is a need for a new legislation;;No;;;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;No opinion;;Compliance of high-risk applications should be assessed ex-ante by means of an external conformity assessment procedure;;;Personal security risks;;No opinion;;No opinion;;Yes, for all AI applications;;;
F518572;19-02-2020 16:46;German;Company/Business organisation;Andreas;Sommer;;SEC-SOmmer Engineering And Consulting;;Micro (< 10 employees);Germany;The feedback can be published with your personal information;5 - Very important;5 - Very important;5 - Very important;3 - Neutral;5 - Very important;5 - Very important;;4 - Important;4 - Important;3 - Neutral;1 - Not important at all;4 - Important;5 - Very important;Define Minimum Security, Minimum Privacy, Minimum Safety;4 - Important;4 - Important;4 - Important;;3 - Neutral;4 - Important;4 - Important;4 - Important;1 - Not important at all;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;There is a need for a new legislation;;No;;;;All where personal data or meta data is involved;4 - Important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;There will and can never a data security / privacy / safety guarantee.;Very much;;Other enforcement system;Nobody can ensure that AI is trustworthy and secure, personal AI involvement should only be on strict volontary basis;Personal AI involvement should only be on strict volontary basis;Mental health risks;"Update GDPR (e.g. ""right to be forgotten"" in reallity cannot be implemented for old (tape) backups and e.g. blockchain whre data is stored forever - how will this work for AI?). Strict storage of all data incl. backups only in EU countries, and only in certified data and regular audited data centers, only high end-to-end encryption without lawful interception requirements for all data transfer and storage.";Yes;;Yes;;Yes, for all AI applications;;;
F518571;19-02-2020 16:14;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;;3 - Neutral;2 - Not important;4 - Important;5 - Very important;A broad, pre-emptive promotional and educational long term campaign in EU societies, combating superstitions, fears or even impediments undertaken by various entities.;4 - Important;No opinion;4 - Important;5 - Very important;4 - Important;5 - Very important;;3 - Neutral;5 - Very important;3 - Neutral;;3 - Neutral;3 - Neutral;3 - Neutral;5 - Very important;4 - Important;;5 - Very important;5 - Very important;3 - Neutral;2 - Not important;3 - Neutral;1 - Not important at all;;Current legislation may have some gaps;;No;;;;;3 - Neutral;3 - Neutral;2 - Not important;3 - Neutral;5 - Very important;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;;Not at all;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Risks related to the loss of connectivity;;Yes;;Yes;;Yes, for all AI applications;;;
F518570;19-02-2020 16:09;English;;;;;;;;;The feedback can be published in an anonymous way;3 - Neutral;5 - Very important;4 - Important;2 - Not important;2 - Not important;3 - Neutral;;5 - Very important;5 - Very important;2 - Not important;2 - Not important;4 - Important;5 - Very important;;4 - Important;4 - Important;2 - Not important;;3 - Neutral;4 - Important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;4 - Important;4 - Important;;3 - Neutral;2 - Not important;;There is a need for a new legislation;;Yes;;Yes;;;4 - Important;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;Biometric identification systems should be allowed in publicly accessible spaces only in certain cases or if certain conditions are fulfilled (please specify);if required by the law;Not at all;;Ex-post market surveillance after the AI-enabled high-risk product or service has been put on the market and, where needed, enforcement by relevant competent authorities;;;Personal security risks;;Yes;;Yes;;No;;;
F518569;19-02-2020 15:58;German;Business Association;kubilay;halis;;CUBE ROBOT X by haleez.com;;Micro (< 10 employees);Germany;The feedback can be published with your personal information;No opinion;No opinion;5 - Very important;5 - Very important;5 - Very important;5 - Very important;We are looking for funding ;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
F518568;19-02-2020 14:33;English;;;;;;;;;The feedback can be published in an anonymous way;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;4 - Important;;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;;4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;;4 - Important;5 - Very important;5 - Very important;5 - Very important;4 - Important;4 - Important;;Current legislation may have some gaps;;No;;;;;4 - Important;4 - Important;5 - Very important;4 - Important;5 - Very important;5 - Very important;Biometric identification systems should never be allowed in publicly accessible spaces;;No opinion;;Compliance of high-risk applications with the identified requirements should be self-assessed ex-ante (prior to putting the system on the market);;;Personal security risks;;Yes;;No opinion;;No opinion;;;
F518567;19-02-2020 13:17;English;;;;;;;;;The feedback can be published in an anonymous way;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
F518566;19-02-2020 13:14;English;;;;;;;;;The feedback can be published in an anonymous way;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;Increasing EU citizens' understanding of AI (both benefits and risks);5 - Very important;4 - Important;4 - Important;4 - Important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;4 - Important;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;4 - Important;5 - Very important;5 - Very important;4 - Important;4 - Important;5 - Very important;It would be concerning if ownership of IP or infrastructure related to AI was tied to one or a few private companies, making access/use dependent upon agreeing to the T&Cs of a tech monopoly, especially if ownership of these tech companies lies outside of the EU.;There is a need for a new legislation;;Yes;;Yes;;;5 - Very important;5 - Very important;5 - Very important;5 - Very important;;5 - Very important;Use of Biometric identification systems in publicly accessible spaces, by way of exception to the current general prohibition, should not take place until a specific guideline or legislation at EU level is in place.;I can see some use-cases for this (anti-crime, anti-terrorism) but it must be used with extreme caution and not violate Eu citizen's fundamental right to privacy. ;Rather not;;A combination of ex-ante compliance and ex-post enforcement mechanisms;;;Risks related to the loss of connectivity;;Yes;;Yes;With respect to autonomous systems like self-driving cars, I don't think current legislation is sufficient.;Yes, for specific AI applications;Those within the public sector (e.g. hospitals), industrial.;;
